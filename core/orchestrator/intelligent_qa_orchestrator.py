# core/orchestrator/intelligent_qa_orchestrator.py - é‡æ„ç®€åŒ–ç‰ˆ
"""
ğŸ§  Claudeé©±åŠ¨çš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨ - ç®€åŒ–ç‰ˆ
èŒè´£æ˜ç¡®ï¼šClaudeç†è§£ + APIè·å– + GPTè®¡ç®— + Claudeå›ç­”

é‡æ„æ”¹è¿›ï¼š
- åˆ é™¤å†—ä½™çš„ processor
- ç®€åŒ–æµç¨‹ï¼šClaude â†’ API â†’ è®¡ç®— â†’ å›ç­”
- ä½¿ç”¨ç»Ÿä¸€è®¡ç®—å™¨
- ä¿æŒå¯¹è¯ç®¡ç†åŠŸèƒ½
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime
import asyncio
import json
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import traceback

# ğŸ¯ ç®€åŒ–åçš„æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from core.analyzers.query_parser import (
    SmartQueryParser, create_smart_query_parser, QueryAnalysisResult,
    QueryComplexity, QueryType, BusinessScenario
)
from core.data_orchestration.smart_data_fetcher import (
    SmartDataFetcher, create_smart_data_fetcher,
    ExecutionResult as FetcherExecutionResult,
    DataQualityLevel as FetcherDataQualityLevel,
    ExecutionStatus as FetcherExecutionStatus
)
from utils.calculators.statistical_calculator import (
    UnifiedCalculator, create_unified_calculator, UnifiedCalculationResult,
    CalculationType
)

# å·¥å…·ç±»å’Œå…¶ä»–å¿…è¦ç»„ä»¶
from utils.helpers.date_utils import DateUtils, create_date_utils
from utils.formatters.financial_formatter import FinancialFormatter, create_financial_formatter
from utils.formatters.chart_generator import ChartGenerator, create_chart_generator, ChartType
from utils.formatters.report_generator import ReportGenerator, create_report_generator
from data.models.conversation import ConversationManager, create_conversation_manager
from data.connectors.database_connector import DatabaseConnector, create_database_connector

# AI å®¢æˆ·ç«¯å¯¼å…¥
from core.models.claude_client import ClaudeClient, CustomJSONEncoder
from core.models.openai_client import OpenAIClient
from config import Config as AppConfig

logger = logging.getLogger(__name__)


# ============= ç®€åŒ–çš„æ•°æ®ç±»å®šä¹‰ =============

@dataclass
class BusinessInsight:
    """ç®€åŒ–çš„ä¸šåŠ¡æ´å¯Ÿç±» - æ›¿ä»£å·²åˆ é™¤çš„å¤æ‚ç‰ˆæœ¬"""
    title: str
    summary: str
    confidence_score: float = 0.8
    insight_type: str = "general"
    recommendations: List[str] = field(default_factory=list)

    def __post_init__(self):
        if self.recommendations is None:
            self.recommendations = []


class ProcessingStrategy(Enum):
    """ç®€åŒ–çš„å¤„ç†ç­–ç•¥"""
    SIMPLE_DATA = "simple_data"  # ç®€å•æ•°æ®è·å–
    DATA_WITH_CALC = "data_with_calc"  # æ•°æ®è·å– + è®¡ç®—
    COMPREHENSIVE = "comprehensive"  # å…¨é¢åˆ†æ
    ERROR_HANDLING = "error_handling"  # é”™è¯¯å¤„ç†


@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ - ç®€åŒ–ç‰ˆ"""
    session_id: str
    query_id: str
    success: bool
    response_text: str
    insights: List[BusinessInsight] = field(default_factory=list)
    key_metrics: Dict[str, Any] = field(default_factory=dict)
    visualizations: List[Dict[str, Any]] = field(default_factory=list)
    processing_strategy: ProcessingStrategy = ProcessingStrategy.SIMPLE_DATA
    processors_used: List[str] = field(default_factory=list)
    ai_collaboration_summary: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 0.0
    data_quality_score: float = 0.0
    response_completeness: float = 0.0
    total_processing_time: float = 0.0
    ai_processing_time: float = 0.0
    data_fetching_time: float = 0.0
    processing_metadata: Dict[str, Any] = field(default_factory=dict)
    error_info: Optional[Dict[str, Any]] = None
    conversation_id: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    query_analysis_snapshot: Optional[Dict[str, Any]] = None


# ============= ä¸»ç¼–æ’å™¨ç±» =============

class IntelligentQAOrchestrator:
    """ğŸ§  ç®€åŒ–ç‰ˆæ™ºèƒ½é—®ç­”ç¼–æ’å™¨"""

    _instance: Optional['IntelligentQAOrchestrator'] = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(IntelligentQAOrchestrator, cls).__new__(cls)
        return cls._instance

    def __init__(self, claude_client_instance: Optional[ClaudeClient] = None,
                 gpt_client_instance: Optional[OpenAIClient] = None,
                 db_connector_instance: Optional[DatabaseConnector] = None,
                 app_config_instance: Optional[AppConfig] = None):

        if hasattr(self, 'initialized') and self.initialized:
            return  # é¿å…é‡å¤åˆå§‹åŒ–

        # åŸºç¡€é…ç½®
        self.claude_client = claude_client_instance
        self.gpt_client = gpt_client_instance
        self.db_connector = db_connector_instance
        self.app_config = app_config_instance if app_config_instance is not None else AppConfig()
        self.config = self._load_orchestrator_config()

        self.initialized = False
        self._initialize_component_placeholders()

        # ç»Ÿè®¡å’Œç¼“å­˜
        self.orchestrator_stats = self._default_stats()
        self.result_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = self.config.get('cache_ttl_seconds', 1800)

        logger.info("ç®€åŒ–ç‰ˆ IntelligentQAOrchestrator åˆ›å»ºå®Œæˆ")

    def _initialize_component_placeholders(self):
        """ğŸ¯ å¤§å¹…ç®€åŒ–çš„ç»„ä»¶åˆå§‹åŒ–"""
        # ğŸ¯ æ ¸å¿ƒä¸‰å‰‘å®¢ - åªä¿ç•™å¿…è¦çš„
        self.query_parser: Optional[SmartQueryParser] = None
        self.data_fetcher: Optional[SmartDataFetcher] = None
        self.statistical_calculator: Optional[UnifiedCalculator] = None

        # å·¥å…·ç»„ä»¶
        self.date_utils: Optional[DateUtils] = None
        self.financial_formatter: Optional[FinancialFormatter] = None
        self.chart_generator: Optional[ChartGenerator] = None
        self.report_generator: Optional[ReportGenerator] = None
        self.conversation_manager: Optional[ConversationManager] = None

        # âŒ åˆ é™¤çš„å†—ä½™ç»„ä»¶ - å·²ä¸å­˜åœ¨çš„æ–‡ä»¶
        # self.insight_generator = None
        # self.financial_data_analyzer = None
        # self.data_requirements_analyzer = None
        # self.current_data_processor = None
        # self.historical_analysis_processor = None
        # self.prediction_processor = None

    def _load_orchestrator_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        cfg = {
            'max_processing_time': getattr(self.app_config, 'MAX_PROCESSING_TIME', 120),
            'enable_intelligent_caching': getattr(self.app_config, 'ENABLE_INTELLIGENT_CACHING', True),
            'min_confidence_threshold': getattr(self.app_config, 'MIN_CONFIDENCE_THRESHOLD', 0.6),
            'cache_ttl_seconds': getattr(self.app_config, 'CACHE_TTL', 1800),
            'claude_timeout': getattr(self.app_config, 'CLAUDE_TIMEOUT', 60),
            'gpt_timeout': getattr(self.app_config, 'GPT_TIMEOUT', 40),
            'DEBUG': getattr(self.app_config, 'DEBUG', False),
            'version': getattr(self.app_config, 'VERSION', '2.2.0-simplified')
        }

        # APIé…ç½®
        if hasattr(self.app_config, 'CLAUDE_API_KEY'):
            cfg['CLAUDE_API_KEY'] = self.app_config.CLAUDE_API_KEY
        if hasattr(self.app_config, 'OPENAI_API_KEY'):
            cfg['OPENAI_API_KEY'] = self.app_config.OPENAI_API_KEY
        if hasattr(self.app_config, 'DATABASE_CONFIG'):
            cfg['DATABASE_CONFIG'] = self.app_config.DATABASE_CONFIG

        # APIè¿æ¥å™¨é…ç½®
        api_connector_cfg = {}
        if hasattr(self.app_config, 'FINANCE_API_BASE_URL'):
            api_connector_cfg['base_url'] = self.app_config.FINANCE_API_BASE_URL
        if hasattr(self.app_config, 'FINANCE_API_KEY'):
            api_connector_cfg['api_key'] = self.app_config.FINANCE_API_KEY
        if api_connector_cfg:
            cfg['api_connector_config'] = api_connector_cfg

        return cfg

    async def initialize(self):
        """ğŸ¯ ç®€åŒ–ç‰ˆåˆå§‹åŒ–"""
        if self.initialized:
            logger.debug("Orchestrator already initialized.")
            return

        logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–ç®€åŒ–ç‰ˆæ™ºèƒ½ç¼–æ’å™¨...")
        start_init_time = time.time()

        try:
            # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
            if not self.claude_client and self.config.get('CLAUDE_API_KEY'):
                self.claude_client = ClaudeClient(api_key=self.config['CLAUDE_API_KEY'])
            if not self.gpt_client and self.config.get('OPENAI_API_KEY'):
                self.gpt_client = OpenAIClient(api_key=self.config['OPENAI_API_KEY'])

            # æ•°æ®åº“è¿æ¥å™¨
            if not self.db_connector and self.config.get('DATABASE_CONFIG'):
                db_cfg = self.config['DATABASE_CONFIG']
                if all(key in db_cfg for key in ['user', 'password', 'host', 'database']):
                    self.db_connector = create_database_connector(db_cfg)
                    logger.info("DatabaseConnector åˆå§‹åŒ–å®Œæˆ")

            # ğŸ¯ æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– - å¤§å¹…ç®€åŒ–
            self.query_parser = create_smart_query_parser(self.claude_client, self.gpt_client)

            fetcher_config = self.config.get('api_connector_config', {})
            self.data_fetcher = create_smart_data_fetcher(self.claude_client, self.gpt_client, fetcher_config)

            # ğŸ†• ä½¿ç”¨ç»Ÿä¸€è®¡ç®—å™¨
            self.statistical_calculator = create_unified_calculator(self.gpt_client, precision=6)

            # å·¥å…·ç»„ä»¶
            self.date_utils = create_date_utils(self.claude_client)
            self.financial_formatter = create_financial_formatter()
            self.chart_generator = create_chart_generator()
            self.report_generator = create_report_generator()

            # å¯¹è¯ç®¡ç†å™¨
            if self.db_connector:
                self.conversation_manager = create_conversation_manager(self.db_connector)
                logger.info("ConversationManager åˆå§‹åŒ–å®Œæˆ")
            else:
                logger.warning("æ•°æ®åº“è¿æ¥å™¨ä¸å¯ç”¨ï¼ŒConversationManager ä½¿ç”¨å†…å­˜æ¨¡å¼")
                self.conversation_manager = ConversationManager(database_connector=None)

            self.initialized = True
            init_duration = time.time() - start_init_time
            logger.info(f"âœ… ç®€åŒ–ç‰ˆæ™ºèƒ½ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_duration:.2f}s)")

        except Exception as e:
            self.initialized = False
            logger.error(f"âŒ æ™ºèƒ½ç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}\n{traceback.format_exc()}")

    # ================================================================
    # ğŸ¯ æ ¸å¿ƒæ–¹æ³•ï¼šç®€åŒ–ç‰ˆæ™ºèƒ½æŸ¥è¯¢å¤„ç†
    # ================================================================

    async def process_intelligent_query(self, user_query: str, user_id: int = 0,
                                        conversation_id: Optional[str] = None,
                                        preferences: Optional[Dict[str, Any]] = None) -> ProcessingResult:
        """ğŸ¯ ç®€åŒ–ç‰ˆæ™ºèƒ½æŸ¥è¯¢å¤„ç† - æ ¸å¿ƒæµç¨‹"""

        if not self.initialized:
            await self.initialize()
            if not self.initialized:
                return self._create_error_result("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥", user_query)

        session_id = str(uuid.uuid4())
        query_id = f"q_{datetime.now().strftime('%Y%m%d%H%M%S%f')}_{hashlib.md5(user_query.encode('utf-8')).hexdigest()[:6]}"
        start_time = time.time()

        logger.info(f"ğŸ¯ QueryID: {query_id} - å¼€å§‹å¤„ç†: '{user_query[:50]}...'")
        self.orchestrator_stats['total_queries'] += 1

        # å¤„ç†å¯¹è¯ID
        conversation_id_for_db = self._parse_conversation_id(conversation_id)

        # ä¿å­˜ç”¨æˆ·è¾“å…¥
        user_message_saved = await self._save_user_message_if_needed(
            conversation_id_for_db, user_query, query_id)

        try:
            # ğŸš€ æ–°å¢ï¼šå¿«é€ŸæŸ¥è¯¢æ£€æµ‹å’Œå¤„ç†
            quick_response = await self._try_quick_response(user_query, query_id)
            if quick_response:
                logger.info(f"âš¡ QueryID: {query_id} - ä½¿ç”¨å¿«é€Ÿå“åº”è·¯å¾„")

                # æ„å»ºå¿«é€Ÿç»“æœ
                total_processing_time = time.time() - start_time
                result = ProcessingResult(
                    session_id=session_id,
                    query_id=query_id,
                    success=True,
                    response_text=quick_response['response'],
                    key_metrics=quick_response.get('metrics', {}),
                    processing_strategy=ProcessingStrategy.SIMPLE_DATA,
                    processors_used=["QuickResponse", "APIConnector"],
                    confidence_score=0.95,  # ç®€å•æŸ¥è¯¢é«˜ç½®ä¿¡åº¦
                    data_quality_score=quick_response.get('data_quality', 0.9),
                    response_completeness=1.0,  # ç›´æ¥æ•°æ®æŸ¥è¯¢å®Œæ•´åº¦é«˜
                    total_processing_time=total_processing_time,
                    ai_processing_time=0.0,  # å¿«é€Ÿè·¯å¾„æ— AIå¤„ç†
                    data_fetching_time=quick_response.get('fetch_time', 0.0),
                    processing_metadata={
                        'query_type': 'quick_data_query',
                        'api_used': quick_response.get('api_method', ''),
                        'quick_response': True
                    },
                    conversation_id=conversation_id
                )

                # æ›´æ–°ç»Ÿè®¡
                self._update_stats(result)

                # ä¿å­˜AIå“åº”
                if conversation_id_for_db and user_message_saved:
                    await self._save_ai_response_if_needed(conversation_id_for_db, result, query_id)

                logger.info(f"âš¡ QueryID: {query_id} - å¿«é€Ÿå¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_processing_time:.2f}s")
                return result

            # ğŸ¯ åŸæœ‰çš„å®Œæ•´å¤„ç†æµç¨‹
            timing = {}

            # 1ï¸âƒ£ Claude ç†è§£æŸ¥è¯¢
            start_t = time.time()
            query_analysis = await self._claude_understand_query(user_query, conversation_id_for_db)
            timing['parsing'] = time.time() - start_t

            # 2ï¸âƒ£ è·å–æ•°æ®
            start_t = time.time()
            data_result = await self._execute_simplified_data_fetch(query_analysis)
            timing['data_fetching'] = time.time() - start_t

            # 3ï¸âƒ£ è®¡ç®—å¤„ç† (å¦‚æœéœ€è¦)
            start_t = time.time()
            calculation_result = None
            if query_analysis.needs_calculation:
                calculation_result = await self._execute_unified_calculation(query_analysis, data_result)
            timing['calculation'] = time.time() - start_t

            # 4ï¸âƒ£ Claude ç”Ÿæˆæœ€ç»ˆå›ç­”
            start_t = time.time()
            response_text = await self._claude_generate_final_response(
                user_query, query_analysis, data_result, calculation_result)
            timing['response_generation'] = time.time() - start_t

            # 5ï¸âƒ£ ç®€åŒ–çš„æ´å¯Ÿç”Ÿæˆ
            start_t = time.time()
            insights = await self._generate_simple_insights(data_result, calculation_result, query_analysis)
            timing['insights'] = time.time() - start_t

            # 6ï¸âƒ£ ç®€åŒ–çš„å¯è§†åŒ–ç”Ÿæˆ
            start_t = time.time()
            visualizations = await self._generate_simple_visualizations(data_result, calculation_result)
            timing['visualization'] = time.time() - start_t

            # æ„å»ºç»“æœ
            total_processing_time = time.time() - start_time
            confidence = self._calculate_confidence(query_analysis, data_result, calculation_result, insights)

            result = ProcessingResult(
                session_id=session_id,
                query_id=query_id,
                success=True,
                response_text=response_text,
                insights=insights,
                key_metrics=self._extract_key_metrics(data_result, calculation_result),
                visualizations=visualizations,
                processing_strategy=self._determine_strategy(query_analysis),
                processors_used=self._get_processors_used(query_analysis, calculation_result),
                ai_collaboration_summary=self._get_ai_summary(timing),
                confidence_score=confidence,
                data_quality_score=getattr(data_result, 'confidence_level', 0.8) if data_result else 0.5,
                response_completeness=self._calculate_completeness(data_result, calculation_result, insights),
                total_processing_time=total_processing_time,
                ai_processing_time=timing.get('parsing', 0) + timing.get('response_generation', 0),
                data_fetching_time=timing.get('data_fetching', 0),
                processing_metadata={
                    'query_complexity': query_analysis.complexity.value,
                    'query_type': query_analysis.query_type.value,
                    'step_times': timing,
                    'simplified_architecture': True
                },
                conversation_id=conversation_id,
                query_analysis_snapshot=query_analysis.to_dict()
            )

            # æ›´æ–°ç»Ÿè®¡å’Œç¼“å­˜
            self._update_stats(result)
            await self._cache_result_if_appropriate(result)

            # ä¿å­˜AIå“åº”
            if conversation_id_for_db and user_message_saved:
                await self._save_ai_response_if_needed(conversation_id_for_db, result, query_id)

            logger.info(f"âœ… QueryID: {query_id} - å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {total_processing_time:.2f}s")
            return result

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"âŒ QueryID: {query_id} - å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            return await self._handle_error(session_id, query_id, user_query, str(e), error_time, conversation_id)

    # ================================================================
    # ğŸ¯ ç®€åŒ–åçš„æ ¸å¿ƒå¤„ç†æ–¹æ³•
    # ================================================================

    async def _try_quick_response(self, user_query: str, query_id: str) -> Optional[Dict[str, Any]]:
        """ğŸš€ å°è¯•å¿«é€Ÿå“åº” - æ£€æµ‹ç®€å•æ•°æ®æŸ¥è¯¢"""
        try:
            query_lower = user_query.lower()

            # ğŸ” å¿«é€ŸæŸ¥è¯¢æ¨¡å¼æ£€æµ‹
            quick_patterns = self._detect_quick_query_patterns(query_lower)

            if not quick_patterns:
                return None

            logger.info(f"âš¡ æ£€æµ‹åˆ°å¿«é€ŸæŸ¥è¯¢æ¨¡å¼: {quick_patterns}")

            # ğŸš€ æ‰§è¡Œå¿«é€Ÿæ•°æ®è·å–
            start_fetch_time = time.time()
            api_result = await self._execute_quick_api_call(quick_patterns)
            fetch_time = time.time() - start_fetch_time

            if not api_result or not api_result.get('success'):
                logger.warning(f"å¿«é€ŸAPIè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´æµç¨‹")
                return None

            # ğŸ¯ æ ¼å¼åŒ–å¿«é€Ÿå“åº”
            formatted_response = self._format_quick_response(api_result, quick_patterns, user_query)

            return {
                'response': formatted_response['text'],
                'metrics': formatted_response.get('metrics', {}),
                'data_quality': api_result.get('validation', {}).get('confidence', 0.9),
                'fetch_time': fetch_time,
                'api_method': quick_patterns['api_method']
            }

        except Exception as e:
            logger.error(f"å¿«é€Ÿå“åº”å¤„ç†å¤±è´¥: {e}")
            return None

    def _detect_quick_query_patterns(self, query_lower: str) -> Optional[Dict[str, Any]]:
        """ğŸ” æ£€æµ‹å¿«é€ŸæŸ¥è¯¢æ¨¡å¼ - å¢å¼ºç‰ˆ"""

        # ğŸ¯ æ›´ç²¾ç¡®çš„æ¨¡å¼åŒ¹é…
        patterns = [
            # ç‰¹å®šæ—¥æœŸçš„æ¯æ—¥æ•°æ®æŸ¥è¯¢
            {
                'name': 'specific_date_daily',
                'regex': r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]',
                'keywords': ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œ', 'æ•°æ®', 'è´­ä¹°'],
                'api_method': 'get_daily_data',
                'description': 'ç‰¹å®šæ—¥æœŸæ¯æ—¥æ•°æ®'
            },

            # YYYYMMDDæ ¼å¼æ—¥æœŸæŸ¥è¯¢
            {
                'name': 'date_format_daily',
                'regex': r'(\d{4})(\d{2})(\d{2})',
                'keywords': ['æ•°æ®', 'å…¥é‡‘', 'å‡ºé‡‘'],
                'api_method': 'get_daily_data',
                'description': 'YYYYMMDDæ ¼å¼æ—¥æœŸæŸ¥è¯¢'
            },

            # ç‰¹å®šæ—¥æœŸåˆ°æœŸæŸ¥è¯¢
            {
                'name': 'specific_date_expiry',
                'regex': r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]',
                'keywords': ['åˆ°æœŸ', 'è¿‡æœŸ', 'äº§å“åˆ°æœŸ'],
                'api_method': 'get_product_end_data',
                'description': 'ç‰¹å®šæ—¥æœŸåˆ°æœŸæŸ¥è¯¢'
            },

            # ä»Šæ—¥æŸ¥è¯¢
            {
                'name': 'today_query',
                'keywords': ['ä»Šå¤©', 'ä»Šæ—¥', 'å½“å¤©'],
                'data_types': ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œ', 'æ•°æ®', 'åˆ°æœŸ'],
                'api_method': 'auto',  # æ ¹æ®æ•°æ®ç±»å‹è‡ªåŠ¨é€‰æ‹©
                'description': 'ä»Šæ—¥æ•°æ®æŸ¥è¯¢'
            },

            # ç³»ç»Ÿæ¦‚è§ˆæŸ¥è¯¢
            {
                'name': 'system_simple',
                'keywords': ['æ´»è·ƒä¼šå‘˜', 'æ€»ç”¨æˆ·', 'ç”¨æˆ·æ•°', 'æ€»ä½™é¢', 'å½“å‰èµ„é‡‘'],
                'data_types': [],
                'api_method': 'get_system_data',
                'description': 'ç³»ç»Ÿæ¦‚è§ˆæŸ¥è¯¢'
            },

            # ğŸ†• åŒºé—´åˆ°æœŸæŸ¥è¯¢ï¼ˆç®€å•ç‰ˆï¼‰
            {
                'name': 'simple_interval_expiry',
                'regex': r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·].*?(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]',
                'keywords': ['åˆ°æœŸ', 'äº§å“åˆ°æœŸ'],
                'exclude_keywords': ['å¤æŠ•', 'è®¡ç®—', 'é¢„è®¡', 'åˆ†æ'],  # æ’é™¤å¤æ‚è®¡ç®—
                'api_method': 'get_product_end_interval',
                'description': 'ç®€å•åŒºé—´åˆ°æœŸæŸ¥è¯¢'
            }
        ]

        # ğŸ” é€ä¸ªåŒ¹é…æ¨¡å¼
        for pattern in patterns:
            if self._match_pattern(query_lower, pattern):
                return self._extract_pattern_info(query_lower, pattern)

        return None

    def _match_pattern(self, query_lower: str, pattern: Dict[str, Any]) -> bool:
        """åŒ¹é…å•ä¸ªæ¨¡å¼"""
        import re

        # æ£€æŸ¥æ’é™¤å…³é”®è¯
        if 'exclude_keywords' in pattern:
            if any(exc in query_lower for exc in pattern['exclude_keywords']):
                return False

        # æ­£åˆ™åŒ¹é…
        if 'regex' in pattern:
            if not re.search(pattern['regex'], query_lower):
                return False

        # å…³é”®è¯åŒ¹é…
        if 'keywords' in pattern:
            if not any(kw in query_lower for kw in pattern['keywords']):
                return False

        # æ•°æ®ç±»å‹åŒ¹é…
        if 'data_types' in pattern and pattern['data_types']:
            if not any(dt in query_lower for dt in pattern['data_types']):
                return False

        return True

    def _extract_pattern_info(self, query_lower: str, pattern: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ¨¡å¼ä¿¡æ¯å’Œå‚æ•°"""
        import re

        result = {
            'pattern': pattern['name'],
            'description': pattern['description'],
            'params': {}
        }

        # ğŸ¯ æ ¹æ®æ¨¡å¼ç±»å‹ç¡®å®šAPIæ–¹æ³•å’Œå‚æ•°
        if pattern['name'] == 'specific_date_daily':
            date_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]', query_lower)
            if date_match:
                month, day = int(date_match.group(1)), int(date_match.group(2))
                date_str = f"2024{month:02d}{day:02d}"  # å‡è®¾2024å¹´
                result['api_method'] = 'get_daily_data'
                result['params'] = {'date': date_str}

        elif pattern['name'] == 'date_format_daily':
            date_match = re.search(r'(\d{8})', query_lower)
            if date_match:
                result['api_method'] = 'get_daily_data'
                result['params'] = {'date': date_match.group(1)}

        elif pattern['name'] == 'specific_date_expiry':
            date_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]', query_lower)
            if date_match:
                month, day = int(date_match.group(1)), int(date_match.group(2))
                date_str = f"2024{month:02d}{day:02d}"
                result['api_method'] = 'get_product_end_data'
                result['params'] = {'date': date_str}


        elif pattern['name'] == 'today_query':

            # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©API

            if any(kw in query_lower for kw in ['åˆ°æœŸ', 'è¿‡æœŸ']):

                result['api_method'] = 'get_expiring_products_today'

                result['params'] = {}

            else:

                result['api_method'] = 'get_daily_data'

                result['params'] = {'date': datetime.now().strftime('%Y%m%d')}

        elif pattern['name'] == 'system_simple':
            result['api_method'] = 'get_system_data'

        elif pattern['name'] == 'simple_interval_expiry':
            dates = re.findall(r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]', query_lower)
            if len(dates) >= 2:
                start_month, start_day = int(dates[0][0]), int(dates[0][1])
                end_month, end_day = int(dates[1][0]), int(dates[1][1])
                start_date = f"2024{start_month:02d}{start_day:02d}"
                end_date = f"2024{end_month:02d}{end_day:02d}"
                result['api_method'] = 'get_product_end_interval'
                result['params'] = {'start_date': start_date, 'end_date': end_date}

        return result

    def _parse_date_from_query(self, query_lower: str) -> Optional[str]:
        """ğŸ” ä»æŸ¥è¯¢ä¸­è§£ææ—¥æœŸ"""
        import re

        # åŒ¹é… "XæœˆXæ—¥" æˆ– "XæœˆXå·" æ ¼å¼
        date_pattern = r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]'
        match = re.search(date_pattern, query_lower)

        if match:
            month = int(match.group(1))
            day = int(match.group(2))
            current_year = datetime.now().year

            try:
                # æ„é€ æ—¥æœŸ
                target_date = datetime(current_year, month, day)
                return target_date.strftime('%Y%m%d')
            except ValueError:
                logger.warning(f"æ— æ•ˆæ—¥æœŸ: {month}æœˆ{day}æ—¥")
                return None

        # åŒ¹é… "YYYYMMDD" æ ¼å¼
        date_pattern2 = r'(\d{8})'
        match2 = re.search(date_pattern2, query_lower)
        if match2:
            date_str = match2.group(1)
            try:
                # éªŒè¯æ—¥æœŸæ ¼å¼
                datetime.strptime(date_str, '%Y%m%d')
                return date_str
            except ValueError:
                return None

        return None

    async def _execute_quick_api_call(self, pattern_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ğŸš€ æ‰§è¡Œå¿«é€ŸAPIè°ƒç”¨ - ä¿®å¤ç‰ˆ"""
        try:
            api_method = pattern_info['api_method']
            params = pattern_info['params']

            if not self.data_fetcher or not self.data_fetcher.api_connector:
                logger.error("APIè¿æ¥å™¨ä¸å¯ç”¨")
                return None

            # ğŸ¯ æ ¹æ®æ–¹æ³•åè°ƒç”¨å¯¹åº”çš„API
            api_connector = self.data_fetcher.api_connector

            logger.info(f"ğŸš€ æ‰§è¡Œå¿«é€ŸAPIè°ƒç”¨: {api_method}, å‚æ•°: {params}")

            if api_method == 'get_daily_data':
                if 'date' in params:
                    result = await api_connector.get_daily_data(params['date'])
                else:
                    result = await api_connector.get_daily_data()

            elif api_method == 'get_system_data':
                result = await api_connector.get_system_data()

            elif api_method == 'get_expiring_products_today':
                result = await api_connector.get_expiring_products_today()

            elif api_method == 'get_product_end_data':
                if 'date' in params:
                    result = await api_connector.get_product_end_data(params['date'])
                else:
                    logger.error("get_product_end_data éœ€è¦dateå‚æ•°")
                    return None

            elif api_method == 'get_product_end_interval':
                if 'start_date' in params and 'end_date' in params:
                    result = await api_connector.get_product_end_interval(
                        params['start_date'], params['end_date'])
                else:
                    logger.error("get_product_end_interval éœ€è¦start_dateå’Œend_dateå‚æ•°")
                    return None
            else:
                logger.error(f"æœªçŸ¥çš„APIæ–¹æ³•: {api_method}")
                return None

            # ğŸ” è°ƒè¯•ï¼šæ‰“å°APIè¿”å›ç»“æœ
            logger.info(
                f"ğŸ” APIè°ƒç”¨ç»“æœ: success={result.get('success')}, data_keys={list(result.get('data', {}).keys()) if result.get('data') else 'None'}")

            # ğŸ¯ å³ä½¿éªŒè¯å¤±è´¥ï¼Œåªè¦APIè°ƒç”¨æˆåŠŸå°±ç»§ç»­å¤„ç†
            if result.get('success'):
                return result
            else:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥: {result.get('message', 'Unknown error')}")
                return None

        except Exception as e:
            logger.error(f"å¿«é€ŸAPIè°ƒç”¨å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def _format_quick_response(self, api_result: Dict[str, Any],
                               pattern_info: Dict[str, Any],
                               original_query: str) -> Dict[str, Any]:
        """ğŸ¯ æ ¼å¼åŒ–å¿«é€Ÿå“åº” - ä¿®å¤ç‰ˆ"""
        try:
            # ğŸ” è°ƒè¯•ï¼šæ‰“å°è¾“å…¥æ•°æ®
            logger.info(f"ğŸ¯ æ ¼å¼åŒ–å¿«é€Ÿå“åº”: pattern={pattern_info['pattern']}")
            logger.info(f"ğŸ” APIç»“æœkeys: {list(api_result.keys())}")

            data = api_result.get('data', {})

            if not data:
                logger.warning("APIè¿”å›çš„dataä¸ºç©º")
                return {
                    'text': f"è·å–åˆ°{pattern_info['description']}ï¼Œä½†æ•°æ®ä¸ºç©ºã€‚",
                    'metrics': {}
                }

            # ğŸ” è°ƒè¯•ï¼šæ‰“å°dataå†…å®¹
            logger.info(f"ğŸ” æ•°æ®å†…å®¹: {data}")

            pattern = pattern_info['pattern']

            if pattern == 'specific_date_daily' or pattern == 'date_format_daily':
                return self._format_daily_data_response(data, original_query)

            elif pattern == 'system_simple':
                return self._format_system_overview_response(data, original_query)

            elif pattern == 'specific_date_expiry':
                return self._format_expiry_response(data, original_query)

            elif pattern == 'simple_interval_expiry':
                return self._format_interval_expiry_response(data, original_query)

            else:
                return {
                    'text': f"å·²è·å–{pattern_info['description']}æ•°æ®ï¼Œä½†æ ¼å¼åŒ–å™¨æœªå®ç°ã€‚æ•°æ®ï¼š{str(data)[:200]}",
                    'metrics': {}
                }

        except Exception as e:
            logger.error(f"å“åº”æ ¼å¼åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'text': f"æ•°æ®è·å–æˆåŠŸï¼Œä½†æ ¼å¼åŒ–æ—¶é‡åˆ°é—®é¢˜ï¼š{str(e)}",
                'metrics': {}
            }

    def _format_daily_data_response(self, data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """ğŸ“… æ ¼å¼åŒ–æ¯æ—¥æ•°æ®å“åº” - ä¿®å¤ç‰ˆ"""
        try:
            logger.info(f"ğŸ“… æ ¼å¼åŒ–æ¯æ—¥æ•°æ®: {data}")

            if not data:
                return {'text': "æœªæ‰¾åˆ°å½“æ—¥æ•°æ®ã€‚", 'metrics': {}}

            # ğŸ” ä»APIè¿”å›çš„æ•°æ®ä¸­æå–å­—æ®µ
            date = data.get('æ—¥æœŸ', data.get('date', 'æœªçŸ¥æ—¥æœŸ'))
            inflow = float(data.get('å…¥é‡‘', data.get('inflow', 0)))
            outflow = float(data.get('å‡ºé‡‘', data.get('outflow', 0)))
            registrations = int(data.get('æ³¨å†Œäººæ•°', data.get('registrations', 0)))
            purchases = int(data.get('è´­ä¹°äº§å“æ•°é‡', data.get('purchases', 0)))
            holdings = int(data.get('æŒä»“äººæ•°', data.get('holdings', 0)))
            expired_products = int(data.get('åˆ°æœŸäº§å“æ•°é‡', data.get('expired_products', 0)))

            # ğŸ¯ æ ¹æ®æŸ¥è¯¢å†…å®¹é‡ç‚¹çªå‡ºç›¸å…³æ•°æ®
            query_lower = query.lower()

            response_parts = [f"ğŸ“… {date} æ•°æ®æ¦‚è§ˆï¼š"]

            # ğŸ¯ æ™ºèƒ½å“åº”ï¼šæ ¹æ®æŸ¥è¯¢å…³é”®è¯å†³å®šæ˜¾ç¤ºå†…å®¹
            if 'å…¥é‡‘' in query_lower:
                response_parts.append(f"ğŸ’° å…¥é‡‘ï¼šÂ¥{inflow:,.2f}")
                if inflow > 0:
                    response_parts.append("ğŸ“ˆ å…¥é‡‘æƒ…å†µè‰¯å¥½")

            elif 'å‡ºé‡‘' in query_lower:
                response_parts.append(f"ğŸ’¸ å‡ºé‡‘ï¼šÂ¥{outflow:,.2f}")

            elif 'æ³¨å†Œ' in query_lower:
                response_parts.append(f"ğŸ‘¥ æ–°å¢æ³¨å†Œï¼š{registrations}äºº")

            elif 'è´­ä¹°' in query_lower or 'äº§å“' in query_lower:
                response_parts.append(f"ğŸ›ï¸ äº§å“è´­ä¹°ï¼š{purchases}ç¬”")

            else:
                # å®Œæ•´æ•°æ®å±•ç¤º
                response_parts.extend([
                    f"ğŸ’° å…¥é‡‘ï¼šÂ¥{inflow:,.2f}",
                    f"ğŸ’¸ å‡ºé‡‘ï¼šÂ¥{outflow:,.2f}",
                    f"ğŸ‘¥ æ–°å¢æ³¨å†Œï¼š{registrations}äºº",
                    f"ğŸ›ï¸ äº§å“è´­ä¹°ï¼š{purchases}ç¬”"
                ])

                if holdings > 0:
                    response_parts.append(f"ğŸ“Š æŒä»“äººæ•°ï¼š{holdings}äºº")
                if expired_products > 0:
                    response_parts.append(f"â° åˆ°æœŸäº§å“ï¼š{expired_products}ç¬”")

            # æ·»åŠ ç®€å•åˆ†æ
            net_flow = inflow - outflow
            if net_flow > 0:
                response_parts.append(f"ğŸ“ˆ å‡€æµå…¥ï¼šÂ¥{net_flow:,.2f}")
            elif net_flow < 0:
                response_parts.append(f"ğŸ“‰ å‡€æµå‡ºï¼šÂ¥{abs(net_flow):,.2f}")
            else:
                response_parts.append("âš–ï¸ èµ„é‡‘æµå¹³è¡¡")

            return {
                'text': '\n'.join(response_parts),
                'metrics': {
                    'å…¥é‡‘': inflow,
                    'å‡ºé‡‘': outflow,
                    'å‡€æµå…¥': net_flow,
                    'æ³¨å†Œäººæ•°': registrations,
                    'è´­ä¹°äº§å“æ•°é‡': purchases,
                    'æŒä»“äººæ•°': holdings
                }
            }

        except Exception as e:
            logger.error(f"æ¯æ—¥æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"æ•°æ®è·å–æˆåŠŸï¼ŒåŸå§‹æ•°æ®ï¼š{str(data)}",
                'metrics': {}
            }

    def _format_interval_expiry_response(self, data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """ğŸ“Š æ ¼å¼åŒ–åŒºé—´åˆ°æœŸæ•°æ®å“åº”"""
        try:
            if not data:
                return {'text': "æœªæ‰¾åˆ°åŒºé—´åˆ°æœŸæ•°æ®ã€‚", 'metrics': {}}

            date_range = data.get('æ—¥æœŸ', 'æœªçŸ¥æ—¶é—´èŒƒå›´')
            total_count = int(data.get('åˆ°æœŸæ•°é‡', 0))
            total_amount = float(data.get('åˆ°æœŸé‡‘é¢', 0))

            response_parts = [f"ğŸ“Š {date_range} äº§å“åˆ°æœŸæ±‡æ€»ï¼š"]
            response_parts.append(f"ğŸ“¦ æ€»åˆ°æœŸæ•°é‡ï¼š{total_count}ç¬”")
            response_parts.append(f"ğŸ’° æ€»åˆ°æœŸé‡‘é¢ï¼šÂ¥{total_amount:,.2f}")

            if total_count > 0:
                avg_amount = total_amount / total_count
                response_parts.append(f"ğŸ“Š å¹³å‡é‡‘é¢ï¼šÂ¥{avg_amount:,.2f}")

            # å¦‚æœæœ‰äº§å“åˆ—è¡¨ï¼Œæ˜¾ç¤ºå‰å‡ ä¸ª
            product_list = data.get('äº§å“åˆ—è¡¨', [])
            if product_list:
                response_parts.append(f"\nğŸ” æ¶‰åŠäº§å“ï¼š")
                for i, product in enumerate(product_list[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    name = product.get('äº§å“åç§°', f'äº§å“{i + 1}')
                    amount = float(product.get('åˆ°æœŸé‡‘é¢', 0))
                    if amount > 0:
                        response_parts.append(f"  â€¢ {name}ï¼šÂ¥{amount:,.2f}")

            return {
                'text': '\n'.join(response_parts),
                'metrics': {
                    'åˆ°æœŸæ•°é‡': total_count,
                    'åˆ°æœŸé‡‘é¢': total_amount
                }
            }

        except Exception as e:
            logger.error(f"åŒºé—´åˆ°æœŸæ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"æ•°æ®è·å–æˆåŠŸï¼ŒåŸå§‹æ•°æ®ï¼š{str(data)}",
                'metrics': {}
            }

    def _format_system_overview_response(self, data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """ğŸ¦ æ ¼å¼åŒ–ç³»ç»Ÿæ¦‚è§ˆå“åº”"""
        if not data:
            return {'text': "æœªæ‰¾åˆ°ç³»ç»Ÿæ•°æ®ã€‚", 'metrics': {}}

        total_balance = float(data.get('æ€»ä½™é¢', 0))
        total_inflow = float(data.get('æ€»å…¥é‡‘', 0))
        total_outflow = float(data.get('æ€»å‡ºé‡‘', 0))

        user_stats = data.get('ç”¨æˆ·ç»Ÿè®¡', {})
        total_users = int(user_stats.get('æ€»ç”¨æˆ·æ•°', 0))
        active_users = int(user_stats.get('æ´»è·ƒç”¨æˆ·æ•°', 0))

        response_parts = ["ğŸ¦ ç³»ç»Ÿæ¦‚è§ˆï¼š"]

        query_lower = query.lower()

        if 'ä½™é¢' in query_lower or 'èµ„é‡‘' in query_lower:
            response_parts.append(
                f"ğŸ’° æ€»ä½™é¢ï¼š{self.financial_formatter.format_currency(total_balance) if self.financial_formatter else f'{total_balance:.2f}'}")

        if 'å…¥é‡‘' in query_lower:
            response_parts.append(
                f"ğŸ“ˆ æ€»å…¥é‡‘ï¼š{self.financial_formatter.format_currency(total_inflow) if self.financial_formatter else f'{total_inflow:.2f}'}")

        if 'å‡ºé‡‘' in query_lower:
            response_parts.append(
                f"ğŸ“‰ æ€»å‡ºé‡‘ï¼š{self.financial_formatter.format_currency(total_outflow) if self.financial_formatter else f'{total_outflow:.2f}'}")

        if 'ç”¨æˆ·' in query_lower or 'æ´»è·ƒ' in query_lower:
            response_parts.append(f"ğŸ‘¥ æ€»ç”¨æˆ·ï¼š{total_users}äºº")
            response_parts.append(f"ğŸ”¥ æ´»è·ƒç”¨æˆ·ï¼š{active_users}äºº")
            if total_users > 0:
                activity_rate = (active_users / total_users) * 100
                response_parts.append(f"ğŸ“Š æ´»è·ƒç‡ï¼š{activity_rate:.1f}%")

        # å¦‚æœæ²¡æœ‰ç‰¹å®šå…³é”®è¯ï¼Œæ˜¾ç¤ºæ ¸å¿ƒæ•°æ®
        if len(response_parts) == 1:
            response_parts.extend([
                f"ğŸ’° æ€»ä½™é¢ï¼š{self.financial_formatter.format_currency(total_balance) if self.financial_formatter else f'{total_balance:.2f}'}",
                f"ğŸ‘¥ æ€»ç”¨æˆ·ï¼š{total_users}äºº",
                f"ğŸ”¥ æ´»è·ƒç”¨æˆ·ï¼š{active_users}äºº"
            ])

        return {
            'text': '\n'.join(response_parts),
            'metrics': {
                'æ€»ä½™é¢': total_balance,
                'æ€»å…¥é‡‘': total_inflow,
                'æ€»å‡ºé‡‘': total_outflow,
                'æ€»ç”¨æˆ·æ•°': total_users,
                'æ´»è·ƒç”¨æˆ·æ•°': active_users
            }
        }

    def _format_expiry_response(self, data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """â° æ ¼å¼åŒ–åˆ°æœŸæ•°æ®å“åº”"""
        if not data:
            return {'text': "æœªæ‰¾åˆ°åˆ°æœŸæ•°æ®ã€‚", 'metrics': {}}

        date = data.get('æ—¥æœŸ', 'ä»Šæ—¥')
        expiry_count = int(data.get('åˆ°æœŸæ•°é‡', 0))
        expiry_amount = float(data.get('åˆ°æœŸé‡‘é¢', 0))

        response_parts = [f"â° {date} äº§å“åˆ°æœŸæƒ…å†µï¼š"]
        response_parts.append(f"ğŸ“¦ åˆ°æœŸæ•°é‡ï¼š{expiry_count}ç¬”")
        response_parts.append(
            f"ğŸ’° åˆ°æœŸé‡‘é¢ï¼š{self.financial_formatter.format_currency(expiry_amount) if self.financial_formatter else f'{expiry_amount:.2f}'}")

        if expiry_count > 0:
            avg_amount = expiry_amount / expiry_count
            response_parts.append(
                f"ğŸ“Š å¹³å‡é‡‘é¢ï¼š{self.financial_formatter.format_currency(avg_amount) if self.financial_formatter else f'{avg_amount:.2f}'}")

        return {
            'text': '\n'.join(response_parts),
            'metrics': {
                'åˆ°æœŸæ•°é‡': expiry_count,
                'åˆ°æœŸé‡‘é¢': expiry_amount
            }
        }
    async def _claude_generate_final_response(self, user_query: str,
                                              query_analysis: QueryAnalysisResult,
                                              data_result: Optional[FetcherExecutionResult],
                                              calculation_result: Optional[Dict[str, Any]]) -> str:
        """ğŸ¯ Claudeç”Ÿæˆæœ€ç»ˆå›ç­”"""
        try:
            if not self.claude_client:
                return self._generate_fallback_response(user_query, data_result, calculation_result, [])

            # å‡†å¤‡æ•°æ®æ‘˜è¦
            data_summary = self._summarize_data_for_claude(data_result, user_query, query_analysis)
            calc_summary = self._summarize_calculation_for_claude(calculation_result)

            # Claudeç”Ÿæˆå›ç­”çš„æç¤º
            response_prompt = f"""
            åŸºäºä»¥ä¸‹æ•°æ®åˆ†æï¼Œè¯·ä¸ºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ï¼š

            ç”¨æˆ·æŸ¥è¯¢ï¼š"{user_query}"

            æ•°æ®æ‘˜è¦ï¼š{json.dumps(data_summary, ensure_ascii=False, indent=2)}

            è®¡ç®—ç»“æœï¼š{json.dumps(calc_summary, ensure_ascii=False, indent=2)}

            è¯·ç”Ÿæˆï¼š
            1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
            2. æä¾›å…·ä½“æ•°æ®æ”¯æŒ
            3. ç»™å‡ºç®€æ˜çš„åˆ†æç»“è®º
            4. å¦‚æœ‰å¿…è¦ï¼Œæä¾›å»ºè®®

            å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€æ˜“æ‡‚ã€‚
            """

            # è°ƒç”¨Claude
            result = await self.claude_client.analyze_complex_query(response_prompt, {
                "user_query": user_query,
                "data_summary": data_summary,
                "calculation_summary": calc_summary
            })

            if result.get("success"):
                return result.get("analysis", "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†å›ç­”")
            else:
                return self._generate_fallback_response(user_query, data_result, calculation_result, [])

        except Exception as e:
            logger.error(f"Claudeç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return self._generate_fallback_response(user_query, data_result, calculation_result, [])

    async def _legacy_data_fetch(self, query_analysis: QueryAnalysisResult) -> Optional[FetcherExecutionResult]:
        """é™çº§æ•°æ®è·å–é€»è¾‘"""
        try:
            # ğŸ¯ æ„å»ºç®€åŒ–çš„æ•°æ®è·å–è®¡åˆ’
            api_calls = query_analysis.api_calls_needed

            if not api_calls:
                # é»˜è®¤è·å–ç³»ç»Ÿæ•°æ®
                api_calls = [{"method": "get_system_data", "params": {}, "reason": "é»˜è®¤æ•°æ®è·å–"}]

            # ğŸ¯ åˆ›å»ºæ¨¡æ‹Ÿçš„acquisition_planå¯¹è±¡
            @dataclass
            class SimpleAcquisitionPlan:
                plan_id: str = f"plan_{int(time.time())}"
                api_call_plans: List = field(default_factory=list)
                parallel_groups: List = field(default_factory=list)
                total_estimated_time: float = 30.0
                data_requirements: List = field(default_factory=list)

            # è½¬æ¢api_callsæ ¼å¼ä»¥åŒ¹é…SmartDataFetcheræœŸæœ›çš„æ ¼å¼
            acquisition_plan = SimpleAcquisitionPlan()

            for api_call in api_calls:
                # åˆ›å»ºæ¨¡æ‹Ÿçš„call_planå¯¹è±¡
                call_plan = type('CallPlan', (), {
                    'call_method': api_call.get('method', 'get_system_data'),
                    'parameters': api_call.get('params', {}),
                    'reason': api_call.get('reason', 'æ•°æ®è·å–'),
                    'priority': type('Priority', (), {'value': 'normal'})(),
                    'retry_strategy': 'default'
                })()

                acquisition_plan.api_call_plans.append(call_plan)

            # è°ƒç”¨SmartDataFetcherçš„ä¸»è¦æ–¹æ³•
            execution_result = await self.data_fetcher.execute_data_acquisition_plan(acquisition_plan)
            return execution_result

        except Exception as e:
            logger.error(f"é™çº§æ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def _extract_user_insights(self, user_data: Dict[str, Any], time_scope: str) -> Dict[str, Any]:
        """æå–ç”¨æˆ·æ´å¯Ÿ"""
        return {"ç”¨æˆ·æ•°æ®": user_data.get("summary", "ç”¨æˆ·æ•°æ®å¤„ç†ä¸­")}

    def _extract_product_insights(self, product_data: Dict[str, Any], time_scope: str) -> Dict[str, Any]:
        """æå–äº§å“æ´å¯Ÿ"""
        return {"äº§å“æ•°æ®": product_data.get("summary", "äº§å“æ•°æ®å¤„ç†ä¸­")}

    def _extract_expiry_insights(self, expiry_data: Dict[str, Any], time_scope: str) -> Dict[str, Any]:
        """æå–åˆ°æœŸæ´å¯Ÿ"""
        return {"åˆ°æœŸæ•°æ®": expiry_data.get("summary", "åˆ°æœŸæ•°æ®å¤„ç†ä¸­")}

    async def _claude_understand_query(self, user_query: str,
                                       conversation_id_for_db: Optional[int]) -> QueryAnalysisResult:
        """1ï¸âƒ£ Claude ç†è§£æŸ¥è¯¢"""
        logger.debug(f"ğŸ§  Claude ç†è§£æŸ¥è¯¢: {user_query[:50]}...")

        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        context = {}
        if conversation_id_for_db and self.conversation_manager:
            try:
                context = self.conversation_manager.get_context(conversation_id_for_db)
            except Exception as e:
                logger.warning(f"è·å–å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}")

        # è°ƒç”¨é‡æ„åçš„ query_parser
        query_analysis = await self.query_parser.parse_complex_query(user_query, context)

        if not query_analysis or query_analysis.confidence_score < 0.3:
            logger.warning("Claude ç†è§£å¤±è´¥ï¼Œä½¿ç”¨é™çº§è§£æ")
            # query_parser å†…éƒ¨å·²æœ‰é™çº§é€»è¾‘

        return query_analysis

    async def _execute_simplified_data_fetch(self, query_analysis: QueryAnalysisResult) -> Optional[
        FetcherExecutionResult]:
        """2ï¸âƒ£ å¢å¼ºç‰ˆæ•°æ®è·å–"""
        logger.debug(f"ğŸ“Š æ‰§è¡Œå¢å¼ºç‰ˆæ•°æ®è·å–")

        if not self.data_fetcher:
            raise RuntimeError("SmartDataFetcher æœªåˆå§‹åŒ–")

        try:
            # ğŸ¯ ä½¿ç”¨å¢å¼ºç‰ˆæ™ºèƒ½æ•°æ®è·å–
            if hasattr(self.data_fetcher.api_connector, 'intelligent_data_fetch_enhanced'):
                execution_result = await self.data_fetcher.api_connector.intelligent_data_fetch_enhanced(
                    query_analysis.to_dict()
                )
            else:
                # é™çº§åˆ°åŸæœ‰é€»è¾‘
                execution_result = await self._legacy_data_fetch(query_analysis)

            return execution_result

        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    async def _execute_unified_calculation(self, query_analysis: QueryAnalysisResult,
                                           data_result: Optional[FetcherExecutionResult]) -> Optional[Dict[str, Any]]:
        """3ï¸âƒ£ æ‰§è¡Œè®¡ç®—å¤„ç†"""
        if not query_analysis.needs_calculation:
            return None

        logger.debug(f"ğŸ§® æ‰§è¡Œè®¡ç®—: {query_analysis.calculation_type}")

        if not self.statistical_calculator:
            logger.error("ç»Ÿä¸€è®¡ç®—å™¨æœªåˆå§‹åŒ–")
            return None

        try:
            # å‡†å¤‡è®¡ç®—æ•°æ®
            calc_data = self._prepare_calculation_data(data_result)
            calc_params = self._extract_calculation_params(query_analysis)

            # ğŸ†• ä½¿ç”¨ç»Ÿä¸€è®¡ç®—å™¨
            calc_result = await self.statistical_calculator.calculate(
                calculation_type=query_analysis.calculation_type,
                data=calc_data,
                params=calc_params
            )

            return {
                'calculation_result': calc_result,
                'calculation_type': query_analysis.calculation_type,
                'success': calc_result.success if calc_result else False,
                'confidence': calc_result.confidence if calc_result else 0.5
            }

        except Exception as e:
            logger.error(f"è®¡ç®—æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'calculation_result': None,
                'calculation_type': query_analysis.calculation_type,
                'success': False,
                'error': str(e)
            }

    async def _generate_simple_insights(self, data_result: Optional[FetcherExecutionResult],
                                        calculation_result: Optional[Dict[str, Any]],
                                        query_analysis: QueryAnalysisResult) -> List[BusinessInsight]:
        """4ï¸âƒ£ ç”Ÿæˆç®€åŒ–çš„ä¸šåŠ¡æ´å¯Ÿ"""
        logger.debug("ğŸ’¡ ç”Ÿæˆç®€åŒ–ä¸šåŠ¡æ´å¯Ÿ")

        insights = []

        try:
            # åŸºäºæ•°æ®ç»“æœç”ŸæˆåŸºç¡€æ´å¯Ÿ
            if data_result and hasattr(data_result, 'processed_data'):
                system_data = data_result.processed_data.get('system_data', {})
                if system_data:
                    total_balance = system_data.get('total_balance', 0)
                    total_inflow = system_data.get('total_inflow', 0)
                    total_outflow = system_data.get('total_outflow', 0)

                    if total_balance > 0:
                        net_flow = total_inflow - total_outflow
                        outflow_ratio = total_outflow / total_inflow if total_inflow > 0 else 0

                        # ç°é‡‘æµæ´å¯Ÿ
                        if net_flow > 0:
                            insights.append(BusinessInsight(
                                title="æ­£å‘ç°é‡‘æµ",
                                summary=f"å½“å‰å‡€æµå…¥ä¸º {self.financial_formatter.format_currency(net_flow) if self.financial_formatter else f'{net_flow:.2f}'}ï¼Œèµ„é‡‘çŠ¶å†µè‰¯å¥½",
                                confidence_score=0.9,
                                insight_type="cash_flow",
                                recommendations=["ç»§ç»­ä¿æŒè‰¯å¥½çš„èµ„é‡‘ç®¡ç†"]
                            ))
                        elif net_flow < 0:
                            insights.append(BusinessInsight(
                                title="è´Ÿå‘ç°é‡‘æµ",
                                summary=f"å½“å‰å‡€æµå‡ºä¸º {self.financial_formatter.format_currency(-net_flow) if self.financial_formatter else f'{-net_flow:.2f}'}ï¼Œéœ€è¦å…³æ³¨èµ„é‡‘çŠ¶å†µ",
                                confidence_score=0.9,
                                insight_type="cash_flow",
                                recommendations=["è€ƒè™‘ä¼˜åŒ–æ”¯å‡ºç»“æ„", "å¢åŠ èµ„é‡‘æ¥æº"]
                            ))

                        # æ”¯å‡ºæ¯”ä¾‹æ´å¯Ÿ
                        if outflow_ratio > 0.8:
                            insights.append(BusinessInsight(
                                title="é«˜æ”¯å‡ºæ¯”ä¾‹è­¦å‘Š",
                                summary=f"æ”¯å‡ºå å…¥é‡‘æ¯”ä¾‹ä¸º {outflow_ratio:.1%}ï¼Œå»ºè®®æ§åˆ¶æ”¯å‡º",
                                confidence_score=0.85,
                                insight_type="risk_warning",
                                recommendations=["å®¡æ ¸æ”¯å‡ºé¡¹ç›®", "åˆ¶å®šæ”¯å‡ºæ§åˆ¶ç­–ç•¥"]
                            ))

            # åŸºäºè®¡ç®—ç»“æœç”Ÿæˆæ´å¯Ÿ
            if calculation_result and calculation_result.get('success'):
                calc_res = calculation_result.get('calculation_result')
                if calc_res and hasattr(calc_res, 'detailed_results'):
                    calc_type = calculation_result.get('calculation_type', '')

                    if 'trend' in calc_type.lower():
                        insights.append(BusinessInsight(
                            title="è¶‹åŠ¿åˆ†æç»“æœ",
                            summary="åŸºäºå†å²æ•°æ®çš„è¶‹åŠ¿åˆ†æå·²å®Œæˆ",
                            confidence_score=calc_res.confidence,
                            insight_type="trend_analysis",
                            recommendations=["æ ¹æ®è¶‹åŠ¿è°ƒæ•´ä¸šåŠ¡ç­–ç•¥"]
                        ))
                    elif 'roi' in calc_type.lower() or 'financial' in calc_type.lower():
                        insights.append(BusinessInsight(
                            title="è´¢åŠ¡æŒ‡æ ‡åˆ†æ",
                            summary="å…³é”®è´¢åŠ¡æ¯”ç‡å·²è®¡ç®—å®Œæˆ",
                            confidence_score=calc_res.confidence,
                            insight_type="financial_analysis",
                            recommendations=["å…³æ³¨ROIæŒ‡æ ‡å˜åŒ–", "ä¼˜åŒ–æŠ•èµ„ç»“æ„"]
                        ))

        except Exception as e:
            logger.error(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
            insights.append(BusinessInsight(
                title="ç³»ç»Ÿæç¤º",
                summary="æ•°æ®åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•",
                confidence_score=0.3,
                insight_type="system_message"
            ))

        return insights

    def _summarize_data_for_claude(self, data_result: Optional[FetcherExecutionResult],
                                   user_query: str = "", query_analysis: Optional[QueryAnalysisResult] = None) -> Dict[
        str, Any]:
        """ğŸ¯ ç®€åŒ–ç‰ˆæ•°æ®æ€»ç»“ - ä¸“æ³¨äºæ ¸å¿ƒæ•°æ®"""
        if not data_result:
            return {"status": "æ— æ•°æ®"}

        summary = {
            "status": "æ•°æ®è·å–æˆåŠŸ",
            "data_quality": getattr(data_result, 'confidence_level', 0.8),
            "core_data": {}
        }

        # ç®€åŒ–æ•°æ®æå– - åªå…³æ³¨æ ¸å¿ƒæ•°æ®
        if hasattr(data_result, 'processed_data') and data_result.processed_data:
            processed = data_result.processed_data

            # æå–ç³»ç»Ÿæ•°æ®
            if 'system_data' in processed or any('system' in str(k).lower() for k in processed.keys()):
                summary["core_data"]["ç³»ç»Ÿæ¦‚è§ˆ"] = "å·²è·å–ç³»ç»Ÿæ¦‚è§ˆæ•°æ®"

            # æå–æ—¥å¸¸æ•°æ®
            if 'daily_data' in processed or any('daily' in str(k).lower() for k in processed.keys()):
                summary["core_data"]["æ¯æ—¥æ•°æ®"] = "å·²è·å–æ¯æ—¥æ•°æ®"

            # æå–äº§å“æ•°æ®
            if 'product_data' in processed or any('product' in str(k).lower() for k in processed.keys()):
                summary["core_data"]["äº§å“æ•°æ®"] = "å·²è·å–äº§å“æ•°æ®"

            # æå–åˆ°æœŸæ•°æ®
            if 'expiry_data' in processed or any(
                    'expiry' in str(k).lower() or 'end' in str(k).lower() for k in processed.keys()):
                summary["core_data"]["åˆ°æœŸæ•°æ®"] = "å·²è·å–åˆ°æœŸæ•°æ®"

        return summary



    async def _generate_simple_visualizations(self, data_result: Optional[FetcherExecutionResult],
                                              calculation_result: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """6ï¸âƒ£ ç”Ÿæˆç®€åŒ–çš„å¯è§†åŒ–"""
        visualizations = []

        if not self.chart_generator:
            return visualizations

        try:
            # åŸºäºæ•°æ®ç»“æœç”Ÿæˆç®€å•å›¾è¡¨
            if data_result and hasattr(data_result, 'processed_data'):
                processed_data = data_result.processed_data

                # èµ„é‡‘æµå‘å›¾è¡¨
                if 'system_data' in processed_data:
                    system_data = processed_data['system_data']
                    inflow = system_data.get('total_inflow', 0)
                    outflow = system_data.get('total_outflow', 0)
                    balance = system_data.get('total_balance', 0)

                    if inflow > 0 or outflow > 0:
                        chart_data = {
                            'labels': ['å…¥é‡‘', 'å‡ºé‡‘', 'ä½™é¢'],
                            'values': [inflow, outflow, balance]
                        }

                        visualizations.append({
                            'type': 'bar',
                            'title': 'èµ„é‡‘æ¦‚è§ˆ',
                            'data_payload': chart_data,
                            'description': 'å½“å‰èµ„é‡‘æµå‘å’Œä½™é¢çŠ¶å†µ'
                        })

                # æ—¥åº¦è¶‹åŠ¿å›¾è¡¨
                if 'daily_data' in processed_data:
                    daily_data = processed_data['daily_data']
                    if isinstance(daily_data, list) and len(daily_data) > 1:
                        dates = [d.get('date', '') for d in daily_data]
                        inflows = [d.get('daily_inflow', 0) for d in daily_data]

                        chart_data = {
                            'labels': dates[-7:],  # æœ€è¿‘7å¤©
                            'values': inflows[-7:]
                        }

                        visualizations.append({
                            'type': 'line',
                            'title': 'å…¥é‡‘è¶‹åŠ¿',
                            'data_payload': chart_data,
                            'description': 'æœ€è¿‘7å¤©çš„å…¥é‡‘å˜åŒ–è¶‹åŠ¿'
                        })

            # åŸºäºè®¡ç®—ç»“æœç”Ÿæˆå›¾è¡¨
            if calculation_result and calculation_result.get('success'):
                calc_res = calculation_result.get('calculation_result')
                if calc_res and hasattr(calc_res, 'detailed_results'):
                    # è¿™é‡Œå¯ä»¥æ ¹æ®ä¸åŒçš„è®¡ç®—ç±»å‹ç”Ÿæˆå¯¹åº”çš„å›¾è¡¨
                    visualizations.append({
                        'type': 'info',
                        'title': 'è®¡ç®—ç»“æœå›¾è¡¨',
                        'data_payload': {'message': 'è®¡ç®—ç»“æœå¯è§†åŒ–åŠŸèƒ½å¼€å‘ä¸­'},
                        'description': 'è®¡ç®—ç»“æœçš„å›¾å½¢åŒ–å±•ç¤º'
                    })

        except Exception as e:
            logger.error(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

        return visualizations

    # ================================================================
    # ğŸ› ï¸ è¾…åŠ©æ–¹æ³•
    # ================================================================

    def _prepare_calculation_data(self, data_result: Optional[FetcherExecutionResult]) -> Dict[str, Any]:
        """å‡†å¤‡è®¡ç®—æ•°æ®"""
        if not data_result:
            return {}

        calc_data = {}

        # ä» processed_data æå–
        if hasattr(data_result, 'processed_data') and data_result.processed_data:
            calc_data.update(data_result.processed_data)

        # ä» fetched_data æå–
        if hasattr(data_result, 'fetched_data') and data_result.fetched_data:
            calc_data.update(data_result.fetched_data)

        return calc_data

    def _extract_calculation_params(self, query_analysis: QueryAnalysisResult) -> Dict[str, Any]:
        """æå–è®¡ç®—å‚æ•°"""
        params = {}

        # æ—¶é—´èŒƒå›´
        if query_analysis.time_range:
            params.update(query_analysis.time_range)

        # å¤„ç†å…ƒæ•°æ®
        if query_analysis.processing_metadata:
            params.update(query_analysis.processing_metadata)

        return params



    def _summarize_calculation_for_claude(self, calculation_result: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ä¸ºClaudeæ€»ç»“è®¡ç®—ç»“æœ"""
        if not calculation_result:
            return {"status": "æ— è®¡ç®—"}

        if not calculation_result.get('success'):
            return {"status": "è®¡ç®—å¤±è´¥", "error": calculation_result.get('error')}

        calc_res = calculation_result.get('calculation_result')
        if not calc_res:
            return {"status": "è®¡ç®—ç»“æœä¸ºç©º"}

        return {
            "status": "è®¡ç®—æˆåŠŸ",
            "calculation_type": calculation_result.get('calculation_type'),
            "confidence": calculation_result.get('confidence', 0.5),
            "has_detailed_results": bool(getattr(calc_res, 'detailed_results', None))
        }

    def _generate_fallback_response(self, user_query: str, data_result: Optional[FetcherExecutionResult],
                                    calculation_result: Optional[Dict[str, Any]],
                                    insights: List[BusinessInsight]) -> str:
        """ç”Ÿæˆé™çº§å›ç­”"""
        response_parts = ["æ ¹æ®ç³»ç»Ÿåˆ†æï¼š\n"]

        # æ•°æ®çŠ¶æ€
        if data_result and hasattr(data_result, 'execution_status'):
            response_parts.append(f"æ•°æ®è·å–çŠ¶æ€ï¼š{'æˆåŠŸ' if data_result.execution_status else 'éƒ¨åˆ†æˆåŠŸ'}")

        # è®¡ç®—ç»“æœ
        if calculation_result and calculation_result.get('success'):
            calc_res = calculation_result.get('calculation_result')
            if calc_res and hasattr(calc_res, 'primary_result'):
                response_parts.append(f"è®¡ç®—ç»“æœï¼š{calc_res.primary_result}")

        # æ´å¯Ÿ
        if insights:
            response_parts.append("\nå…³é”®æ´å¯Ÿï¼š")
            for i, insight in enumerate(insights[:2], 1):
                response_parts.append(f"{i}. {insight.title}: {insight.summary}")

        if len(response_parts) == 1:  # åªæœ‰å¼€å¤´
            response_parts.append("æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•æä¾›è¯¦ç»†åˆ†æã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚")

        return "\n".join(response_parts)

    def _determine_strategy(self, query_analysis: QueryAnalysisResult) -> ProcessingStrategy:
        """ç¡®å®šå¤„ç†ç­–ç•¥"""
        if query_analysis.needs_calculation:
            return ProcessingStrategy.DATA_WITH_CALC
        elif query_analysis.complexity in [QueryComplexity.COMPLEX, QueryComplexity.EXPERT]:
            return ProcessingStrategy.COMPREHENSIVE
        else:
            return ProcessingStrategy.SIMPLE_DATA

    def _get_processors_used(self, query_analysis: QueryAnalysisResult,
                             calculation_result: Optional[Dict[str, Any]]) -> List[str]:
        """è·å–ä½¿ç”¨çš„å¤„ç†å™¨"""
        processors = ["Claude", "SmartDataFetcher"]

        if query_analysis.needs_calculation and calculation_result:
            processors.append("UnifiedCalculator")

        return processors

    def _calculate_confidence(self, query_analysis: QueryAnalysisResult,
                              data_result: Optional[FetcherExecutionResult],
                              calculation_result: Optional[Dict[str, Any]],
                              insights: List[BusinessInsight]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        confidence_factors = []

        # æŸ¥è¯¢ç†è§£ç½®ä¿¡åº¦
        confidence_factors.append(query_analysis.confidence_score)

        # æ•°æ®è´¨é‡
        if data_result:
            confidence_factors.append(getattr(data_result, 'confidence_level', 0.5))

        # è®¡ç®—ç½®ä¿¡åº¦
        if calculation_result:
            confidence_factors.append(calculation_result.get('confidence', 0.5))

        # æ´å¯Ÿè´¨é‡
        if insights:
            insight_confidence = sum(insight.confidence_score for insight in insights) / len(insights)
            confidence_factors.append(insight_confidence)

        return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5

    def _calculate_completeness(self, data_result: Optional[FetcherExecutionResult],
                                calculation_result: Optional[Dict[str, Any]],
                                insights: List[BusinessInsight]) -> float:
        """è®¡ç®—å›ç­”å®Œæ•´æ€§"""
        completeness = 0.0

        if data_result and hasattr(data_result, 'execution_status'):
            completeness += 0.4

        if calculation_result and calculation_result.get('success'):
            completeness += 0.3

        if insights:
            completeness += 0.3

        return min(completeness, 1.0)

    def _extract_key_metrics(self, data_result: Optional[FetcherExecutionResult],
                             calculation_result: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–å…³é”®æŒ‡æ ‡"""
        metrics = {}

        # ä»è®¡ç®—ç»“æœæå–
        if calculation_result and calculation_result.get('success'):
            calc_res = calculation_result.get('calculation_result')
            if calc_res and hasattr(calc_res, 'detailed_results'):
                metrics.update(calc_res.detailed_results)

        # ä»æ•°æ®ç»“æœæå–åŸºç¡€æŒ‡æ ‡
        if data_result and hasattr(data_result, 'processed_data'):
            processed_data = data_result.processed_data
            if isinstance(processed_data, dict):
                # æå–ç³»ç»Ÿæ¦‚è§ˆæ•°æ®ä¸­çš„å…³é”®æŒ‡æ ‡
                for key, value in processed_data.items():
                    if isinstance(value, dict) and 'æ€»ä½™é¢' in value:
                        metrics.update({
                            'æ€»ä½™é¢': value.get('æ€»ä½™é¢'),
                            'æ€»å…¥é‡‘': value.get('æ€»å…¥é‡‘'),
                            'æ€»å‡ºé‡‘': value.get('æ€»å‡ºé‡‘')
                        })
                        break

        return metrics

    def _get_ai_summary(self, timing: Dict[str, float]) -> Dict[str, Any]:
        """è·å–AIåä½œæ‘˜è¦"""
        return {
            'claude_used': self.claude_client is not None,
            'gpt_used': self.gpt_client is not None,
            'total_ai_time': timing.get('parsing', 0) + timing.get('response_generation', 0),
            'architecture': 'simplified_claude_dominant'
        }

    # ================================================================
    # ğŸ¯ ä¿ç•™çš„å¿…è¦æ–¹æ³• (ç®€åŒ–ç‰ˆ)
    # ================================================================

    def _parse_conversation_id(self, conversation_id: Optional[str]) -> Optional[int]:
        """è§£æå¯¹è¯ID"""
        if not conversation_id:
            return None
        try:
            return int(conversation_id)
        except ValueError:
            logger.warning(f"æ— æ•ˆçš„å¯¹è¯ID: {conversation_id}")
            return None

    async def _save_user_message_if_needed(self, conversation_id_for_db: Optional[int],
                                           user_query: str, query_id: str) -> bool:
        """ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if not conversation_id_for_db or not self.conversation_manager:
            return False

        try:
            # ç®€å•çš„é‡å¤æ£€æŸ¥
            recent_messages = getattr(self.conversation_manager, 'get_recent_messages', lambda *args: [])(
                conversation_id_for_db, 2)

            # æ£€æŸ¥æœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦ç›¸åŒ
            for msg in recent_messages:
                if isinstance(msg, dict):
                    is_user = msg.get('is_user', False)
                    content = msg.get('content', '')
                    if is_user and content.strip() == user_query.strip():
                        return False  # è·³è¿‡é‡å¤ä¿å­˜

            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            self.conversation_manager.add_message(conversation_id_for_db, True, user_query)
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
            return False

    async def _save_ai_response_if_needed(self, conversation_id_for_db: int,
                                          result: ProcessingResult, query_id: str):
        """ä¿å­˜AIå“åº”ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if not self.conversation_manager:
            return

        try:
            # ä¿å­˜AIå“åº”
            ai_message_id = self.conversation_manager.add_message(
                conversation_id_for_db, False, result.response_text)

            # ä¿å­˜å¯è§†åŒ–ï¼ˆå¦‚æœæ”¯æŒï¼‰
            if result.visualizations and hasattr(self.conversation_manager, 'add_visual'):
                for i, vis in enumerate(result.visualizations):
                    if isinstance(vis, dict):
                        self.conversation_manager.add_visual(
                            message_id=ai_message_id,
                            visual_type=vis.get('type', 'chart'),
                            visual_order=i,
                            title=vis.get('title', 'å›¾è¡¨'),
                            data=vis.get('data_payload', {})
                        )
        except Exception as e:
            logger.error(f"ä¿å­˜AIå“åº”å¤±è´¥: {e}")

    def _default_stats(self) -> Dict[str, Any]:
        """é»˜è®¤ç»Ÿè®¡"""
        return {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'avg_processing_time': 0.0,
            'avg_confidence_score': 0.0,
            'cache_hits': 0,
            'cache_misses': 0
        }

    def _update_stats(self, result: ProcessingResult):
        """æ›´æ–°ç»Ÿè®¡"""
        if result.success:
            self.orchestrator_stats['successful_queries'] += 1
        else:
            self.orchestrator_stats['failed_queries'] += 1

        # æ›´æ–°å¹³å‡å€¼
        total = self.orchestrator_stats['total_queries']
        if total > 0:
            current_avg_time = self.orchestrator_stats['avg_processing_time']
            self.orchestrator_stats['avg_processing_time'] = (
                    (current_avg_time * (total - 1) + result.total_processing_time) / total)

            current_avg_conf = self.orchestrator_stats['avg_confidence_score']
            self.orchestrator_stats['avg_confidence_score'] = (
                    (current_avg_conf * (total - 1) + result.confidence_score) / total)

    async def _cache_result_if_appropriate(self, result: ProcessingResult):
        """ç¼“å­˜ç»“æœï¼ˆå¦‚æœåˆé€‚ï¼‰"""
        if (self.config.get('enable_intelligent_caching', True) and
                result.success and result.confidence_score > 0.7):
            cache_key = f"result_{result.query_id}"
            self.result_cache[cache_key] = {
                'data': result,
                'timestamp': time.time()
            }

    def _create_error_result(self, error_msg: str, user_query: str,
                             query_id: str = None) -> ProcessingResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return ProcessingResult(
            session_id=str(uuid.uuid4()),
            query_id=query_id or f"error_{int(time.time())}",
            success=False,
            response_text=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶é‡åˆ°é—®é¢˜ï¼š{error_msg}",
            processing_strategy=ProcessingStrategy.ERROR_HANDLING,
            processors_used=["ErrorHandler"],
            error_info={"message": error_msg, "query": user_query}
        )

    async def _handle_error(self, session_id: str, query_id: str, user_query: str,
                            error_msg: str, processing_time: float,
                            conversation_id: Optional[str]) -> ProcessingResult:
        """å¤„ç†é”™è¯¯"""
        self.orchestrator_stats['failed_queries'] += 1

        return ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            response_text=f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯ã€‚æˆ‘ä»¬å·²è®°å½•æ­¤é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            processing_strategy=ProcessingStrategy.ERROR_HANDLING,
            processors_used=["ErrorHandler"],
            total_processing_time=processing_time,
            error_info={"message": error_msg, "query": user_query},
            conversation_id=conversation_id
        )

    # ================================================================
    # ğŸ”§ æ¥å£æ–¹æ³• (ä¿æŒå…¼å®¹æ€§)
    # ================================================================

    def get_orchestrator_stats(self) -> Dict[str, Any]:
        """è·å–ç¼–æ’å™¨ç»Ÿè®¡"""
        stats = self.orchestrator_stats.copy()
        total = stats.get('total_queries', 0)
        if total > 0:
            stats['success_rate'] = stats.get('successful_queries', 0) / total
            stats['failure_rate'] = stats.get('failed_queries', 0) / total
        return stats

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        if not self.initialized:
            try:
                await self.initialize()
            except Exception as e:
                return {
                    'status': 'unhealthy',
                    'reason': 'Initialization failed',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }

        return {
            'status': 'healthy',
            'architecture': 'simplified',
            'version': self.config.get('version', '2.2.0-simplified'),
            'components': {
                'claude_available': self.claude_client is not None,
                'gpt_available': self.gpt_client is not None,
                'query_parser_ready': self.query_parser is not None,
                'data_fetcher_ready': self.data_fetcher is not None,
                'calculator_ready': self.statistical_calculator is not None
            },
            'statistics': self.get_orchestrator_stats(),
            'timestamp': datetime.now().isoformat()
        }

    async def close(self):
        """å…³é—­ç¼–æ’å™¨"""
        logger.info("å…³é—­ç®€åŒ–ç‰ˆæ™ºèƒ½ç¼–æ’å™¨...")

        if self.data_fetcher and hasattr(self.data_fetcher, 'close'):
            await self.data_fetcher.close()

        if self.db_connector and hasattr(self.db_connector, 'close'):
            await self.db_connector.close()

        self.result_cache.clear()
        self.initialized = False

        logger.info("ç®€åŒ–ç‰ˆæ™ºèƒ½ç¼–æ’å™¨å·²å…³é—­")


# ================================================================
# ğŸ­ å·¥å‚å‡½æ•°å’Œå…¨å±€å®ä¾‹ç®¡ç†
# ================================================================

_orchestrator_instance: Optional[IntelligentQAOrchestrator] = None


def get_orchestrator(claude_client_instance: Optional[ClaudeClient] = None,
                     gpt_client_instance: Optional[OpenAIClient] = None,
                     db_connector_instance: Optional[DatabaseConnector] = None,
                     app_config_instance: Optional[AppConfig] = None) -> IntelligentQAOrchestrator:
    """è·å–ç¼–æ’å™¨å®ä¾‹"""
    global _orchestrator_instance

    if _orchestrator_instance is None:
        logger.info("åˆ›å»ºæ–°çš„ç®€åŒ–ç‰ˆæ™ºèƒ½ç¼–æ’å™¨å®ä¾‹")
        _orchestrator_instance = IntelligentQAOrchestrator(
            claude_client_instance, gpt_client_instance,
            db_connector_instance, app_config_instance
        )

    return _orchestrator_instance