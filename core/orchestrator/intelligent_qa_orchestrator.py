# core/orchestrator/intelligent_qa_orchestrator.py
"""
ğŸš€ AIé©±åŠ¨çš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨
æ•´ä¸ªé‡‘èAIåˆ†æç³»ç»Ÿçš„æ ¸å¿ƒå¤§è„‘ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰ç»„ä»¶ååŒå·¥ä½œ
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import asyncio
import json
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import traceback

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥ (ä¸æ‚¨ä¹‹å‰æä¾›çš„ä¸€è‡´)
from core.analyzers.query_parser import (
    SmartQueryParser, create_smart_query_parser, QueryAnalysisResult,
    QueryComplexity as QueryParserComplexity,
    QueryType as QueryParserQueryType,
    BusinessScenario as QueryParserBusinessScenario,
    ExecutionStep as QueryParserExecutionStep
)
from core.analyzers.data_requirements_analyzer import (
    DataRequirementsAnalyzer, create_data_requirements_analyzer, DataAcquisitionPlan
)
from core.analyzers.financial_data_analyzer import (
    FinancialDataAnalyzer, create_financial_data_analyzer, AnalysisResult as FinancialAnalysisResultType,
    TrendAnalysis as FinancialTrendAnalysis, AnomalyDetection as FinancialAnomalyDetection
)
from core.analyzers.insight_generator import (
    InsightGenerator, create_insight_generator, BusinessInsight
)
from core.data_orchestration.smart_data_fetcher import (
    SmartDataFetcher, create_smart_data_fetcher, ExecutionResult as FetcherExecutionResult,
    DataQualityLevel as FetcherDataQualityLevel
)
from data.processors.current_data_processor import CurrentDataProcessor, CurrentDataResponse
from data.processors.historical_analysis_processor import HistoricalAnalysisProcessor, HistoricalAnalysisResponse
from data.processors.prediction_processor import PredictionProcessor, PredictionResponse

# å·¥å…·ç±»å¯¼å…¥ (ä¸æ‚¨ä¹‹å‰æä¾›çš„ä¸€è‡´)
from utils.helpers.date_utils import DateUtils, create_date_utils
from utils.formatters.financial_formatter import FinancialFormatter, create_financial_formatter
from utils.formatters.chart_generator import ChartGenerator as UtilsChartGenerator, \
    create_chart_generator as create_utils_chart_generator, ChartType as VisChartType
from utils.formatters.report_generator import ReportGenerator, create_report_generator, Report as ReportObject, \
    ReportSection
from data.models.conversation import ConversationManager, create_conversation_manager, Message as ConversationMessage, \
    Visual as ConversationVisual
from data.connectors.database_connector import DatabaseConnector, create_database_connector
from data.connectors.api_connector import APIConnector

# AI å®¢æˆ·ç«¯å¯¼å…¥ (ä¸æ‚¨ä¹‹å‰æä¾›çš„ä¸€è‡´)
from core.models.claude_client import ClaudeClient, CustomJSONEncoder
from core.models.openai_client import OpenAIClient

# åº”ç”¨é…ç½®å¯¼å…¥ (ä¸æ‚¨ä¹‹å‰æä¾›çš„ä¸€è‡´)
from config import Config as AppConfig

logger = logging.getLogger(__name__)


class OrchestratorProcessingStrategy(Enum):
    DIRECT_RESPONSE = "direct_response"
    SINGLE_PROCESSOR = "single_processor"
    MULTI_PROCESSOR = "multi_processor"
    FULL_PIPELINE = "full_pipeline"
    ERROR_HANDLING = "error_handling"


@dataclass
class ProcessingResult:
    # (ä¸æ‚¨ä¹‹å‰æä¾›çš„å®šä¹‰ä¸€è‡´)
    session_id: str
    query_id: str
    success: bool
    response_text: str
    insights: List[BusinessInsight] = field(default_factory=list)
    key_metrics: Dict[str, Any] = field(default_factory=dict)
    visualizations: List[Dict[str, Any]] = field(default_factory=list)
    processing_strategy: OrchestratorProcessingStrategy = OrchestratorProcessingStrategy.SINGLE_PROCESSOR
    processors_used: List[str] = field(default_factory=list)
    ai_collaboration_summary: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 0.0
    data_quality_score: float = 0.0
    response_completeness: float = 0.0
    total_processing_time: float = 0.0
    ai_processing_time: float = 0.0
    data_fetching_time: float = 0.0
    processing_metadata: Dict[str, Any] = field(default_factory=dict)
    error_info: Optional[Dict[str, Any]] = None
    conversation_id: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    query_analysis_snapshot: Optional[Dict[str, Any]] = None


class IntelligentQAOrchestrator:
    _instance: Optional['IntelligentQAOrchestrator'] = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(IntelligentQAOrchestrator, cls).__new__(cls)
        return cls._instance

    def __init__(self, claude_client_instance: Optional[ClaudeClient] = None,
                 gpt_client_instance: Optional[OpenAIClient] = None,
                 db_connector_instance: Optional[DatabaseConnector] = None,
                 app_config_instance: Optional[AppConfig] = None):

        if hasattr(self, 'initialized') and self.initialized:
            # (æ›´æ–°ä¾èµ–å¹¶æ ‡è®°é‡åˆå§‹åŒ–çš„é€»è¾‘)
            needs_reinit = False
            if claude_client_instance and self.claude_client != claude_client_instance: self.claude_client = claude_client_instance; needs_reinit = True
            if gpt_client_instance and self.gpt_client != gpt_client_instance: self.gpt_client = gpt_client_instance; needs_reinit = True
            if db_connector_instance and self.db_connector != db_connector_instance: self.db_connector = db_connector_instance; needs_reinit = True
            if app_config_instance and self.app_config != app_config_instance:
                self.app_config = app_config_instance
                self.config = self._load_orchestrator_config()
                needs_reinit = True
            if needs_reinit:
                self.initialized = False
                logger.info(
                    "Orchestrator dependencies updated, will re-initialize components on next use or explicit call to initialize().")
            return

        self.claude_client = claude_client_instance
        self.gpt_client = gpt_client_instance
        self.db_connector = db_connector_instance
        self.app_config = app_config_instance if app_config_instance is not None else AppConfig()
        self.config = self._load_orchestrator_config()

        self.initialized = False
        self._initialize_component_placeholders()
        self.active_sessions: Dict[str, Any] = {}
        self.orchestrator_stats = self._default_stats()
        self.result_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = self.config.get('cache_ttl_seconds', 1800)
        logger.info("IntelligentQAOrchestrator instance created. Call async initialize() to set up components.")

    def _initialize_component_placeholders(self):
        # (ä¸ä¹‹å‰ä¸€è‡´)
        self.query_parser: Optional[SmartQueryParser] = None
        self.data_requirements_analyzer: Optional[DataRequirementsAnalyzer] = None
        self.financial_data_analyzer: Optional[FinancialDataAnalyzer] = None
        self.insight_generator: Optional[InsightGenerator] = None
        self.data_fetcher: Optional[SmartDataFetcher] = None
        self.current_data_processor: Optional[CurrentDataProcessor] = None
        self.historical_analysis_processor: Optional[HistoricalAnalysisProcessor] = None
        self.prediction_processor: Optional[PredictionProcessor] = None
        self.date_utils: Optional[DateUtils] = None
        self.financial_formatter: Optional[FinancialFormatter] = None
        self.chart_generator: Optional[UtilsChartGenerator] = None
        self.report_generator: Optional[ReportGenerator] = None
        self.conversation_manager: Optional[ConversationManager] = None

    def _default_stats(self) -> Dict[str, Any]:
        # (ä¸ä¹‹å‰ä¸€è‡´)
        return {
            'total_queries': 0, 'successful_queries': 0, 'failed_queries': 0,
            'avg_processing_time': 0.0, 'avg_confidence_score': 0.0,
            'ai_collaboration_count': 0, 'cache_hits': 0, 'cache_misses': 0,
            'processor_usage': {}, 'error_types': {}
        }

    def _load_orchestrator_config(self) -> Dict[str, Any]:
        # (ä¸ä¹‹å‰ä¸€è‡´)
        cfg = {
            'max_processing_time': getattr(self.app_config, 'MAX_PROCESSING_TIME', 120),
            'enable_parallel_processing': getattr(self.app_config, 'ENABLE_PARALLEL_PROCESSING', True),
            'enable_intelligent_caching': getattr(self.app_config, 'ENABLE_INTELLIGENT_CACHING', True),
            'enable_ai_collaboration': getattr(self.app_config, 'ENABLE_AI_COLLABORATION', True),
            'min_confidence_threshold': getattr(self.app_config, 'MIN_CONFIDENCE_THRESHOLD', 0.6),
            'min_confidence_threshold_parsing': 0.7,
            'enable_result_validation': getattr(self.app_config, 'ENABLE_RESULT_VALIDATION', True),
            'enable_quality_monitoring': getattr(self.app_config, 'ENABLE_QUALITY_MONITORING', True),
            'cache_ttl_seconds': getattr(self.app_config, 'CACHE_TTL', 1800),
            'max_cache_size': getattr(self.app_config, 'MAX_CACHE_SIZE', 100),
            'max_concurrent_queries': getattr(self.app_config, 'MAX_CONCURRENT_QUERIES', 50),
            'enable_smart_routing': getattr(self.app_config, 'ENABLE_SMART_ROUTING', True),
            'claude_timeout': getattr(self.app_config, 'CLAUDE_TIMEOUT', 60),
            'gpt_timeout': getattr(self.app_config, 'GPT_TIMEOUT', 40),
            'max_ai_retries': getattr(self.app_config, 'MAX_AI_RETRIES', 2),
            'enable_graceful_degradation': getattr(self.app_config, 'ENABLE_GRACEFUL_DEGRADATION', True),
            'fallback_response_enabled': getattr(self.app_config, 'FALLBACK_RESPONSE_ENABLED', True),
            'DEBUG': getattr(self.app_config, 'DEBUG', False),
            'version': getattr(self.app_config, 'VERSION', '2.1.9-fuller-methods-no-main-fixed-prompts')  # æ›´æ–°ç‰ˆæœ¬
        }
        if hasattr(self.app_config, 'CLAUDE_API_KEY'): cfg['CLAUDE_API_KEY'] = self.app_config.CLAUDE_API_KEY
        if hasattr(self.app_config, 'OPENAI_API_KEY'): cfg['OPENAI_API_KEY'] = self.app_config.OPENAI_API_KEY
        if hasattr(self.app_config, 'DATABASE_CONFIG'): cfg['DATABASE_CONFIG'] = self.app_config.DATABASE_CONFIG
        api_connector_cfg = {}
        if hasattr(self.app_config, 'FINANCE_API_BASE_URL'): api_connector_cfg[
            'base_url'] = self.app_config.FINANCE_API_BASE_URL
        if hasattr(self.app_config, 'FINANCE_API_KEY'): api_connector_cfg['api_key'] = self.app_config.FINANCE_API_KEY
        if api_connector_cfg: cfg['api_connector_config'] = api_connector_cfg
        return cfg

    async def initialize(self):
        # (ä¸ä¹‹å‰ä¸€è‡´ï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶è¢«æ­£ç¡®åˆ›å»º)
        if self.initialized:
            logger.debug("Orchestrator already initialized.")
            return

        logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ™ºèƒ½ç¼–æ’å™¨ç»„ä»¶...")
        start_init_time = time.time()
        try:
            if not self.claude_client and self.config.get('CLAUDE_API_KEY'):
                self.claude_client = ClaudeClient(api_key=self.config['CLAUDE_API_KEY'])
            if not self.gpt_client and self.config.get('OPENAI_API_KEY'):
                self.gpt_client = OpenAIClient(api_key=self.config['OPENAI_API_KEY'])

            if not self.claude_client: logger.warning("ClaudeClientæœªé…ç½®ã€‚")
            if not self.gpt_client: logger.warning("OpenAIClientæœªé…ç½®ã€‚")

            if not self.db_connector and self.config.get('DATABASE_CONFIG'):
                db_cfg_details = self.config['DATABASE_CONFIG']
                if db_cfg_details.get('user') and db_cfg_details.get('password') and db_cfg_details.get(
                        'host') and db_cfg_details.get('database'):
                    self.db_connector = create_database_connector(db_cfg_details)
                    logger.info(f"DatabaseConnector for host '{db_cfg_details.get('host')}' implicitly initialized.")
                else:
                    logger.error("DATABASE_CONFIGä¸å®Œæ•´ã€‚")
            elif not self.db_connector:
                logger.warning("DatabaseConnectoræœªé…ç½®ã€‚å¯¹è¯å†å²ç­‰åŠŸèƒ½å°†ä½¿ç”¨å†…å­˜æ¨¡å¼æˆ–å—é™ã€‚")

            self.query_parser = create_smart_query_parser(self.claude_client, self.gpt_client)
            self.data_requirements_analyzer = create_data_requirements_analyzer(self.claude_client, self.gpt_client)
            fetcher_config = self.config.get('api_connector_config', {})
            self.data_fetcher = create_smart_data_fetcher(self.claude_client, self.gpt_client, fetcher_config)
            self.financial_data_analyzer = create_financial_data_analyzer(self.claude_client, self.gpt_client)
            self.insight_generator = create_insight_generator(self.claude_client, self.gpt_client)
            self.current_data_processor = CurrentDataProcessor(self.claude_client, self.gpt_client)
            self.historical_analysis_processor = HistoricalAnalysisProcessor(self.claude_client, self.gpt_client)
            self.prediction_processor = PredictionProcessor(self.claude_client, self.gpt_client)
            self.date_utils = create_date_utils(self.claude_client)
            self.financial_formatter = create_financial_formatter()
            self.chart_generator = create_utils_chart_generator()
            self.report_generator = create_report_generator()

            if self.db_connector:
                self.conversation_manager = create_conversation_manager(self.db_connector)
                logger.info("ConversationManager initialized with DB backend.")
            else:
                logger.warning("DatabaseConnector not available. ConversationManager using non-persistent in-memory.")
                self.conversation_manager = ConversationManager(database_connector=None)

            self._inject_dependencies()
            self.initialized = True
            init_duration = time.time() - start_init_time
            logger.info(f"âœ… æ™ºèƒ½ç¼–æ’å™¨ç»„ä»¶åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_duration:.2f}s)ã€‚")
        except Exception as e:
            self.initialized = False
            logger.error(f"âŒ æ™ºèƒ½ç¼–æ’å™¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}\n{traceback.format_exc()}")

    def _inject_dependencies(self):
        # (ä¸ä¹‹å‰ä¸€è‡´)
        logger.debug("Injecting dependencies into components...")
        if not self.data_fetcher:
            logger.error("SmartDataFetcher is not initialized, cannot inject APIConnector.")
            return

        api_connector_instance = getattr(self.data_fetcher, 'api_connector', None)
        if not api_connector_instance and self.data_fetcher:
            logger.error("APIConnector is missing from SmartDataFetcher! This is critical.")
            api_conn_config_details = self.config.get('api_connector_config', {})
            if api_conn_config_details and api_conn_config_details.get('base_url'):
                from data.connectors.api_connector import create_enhanced_api_connector
                self.data_fetcher.api_connector = create_enhanced_api_connector(api_conn_config_details,
                                                                                self.claude_client, self.gpt_client)
                api_connector_instance = self.data_fetcher.api_connector
                logger.info("Dynamically re-initialized APIConnector for SmartDataFetcher during DI.")
            else:
                logger.error("Cannot dynamically re-init APIConnector for SmartDataFetcher due to missing config.")

        components_to_inject_into = [
            self.current_data_processor, self.historical_analysis_processor, self.prediction_processor,
            self.financial_data_analyzer, self.insight_generator,
            self.query_parser, self.data_requirements_analyzer, self.data_fetcher
        ]

        for component in components_to_inject_into:
            if not component: continue
            if api_connector_instance and isinstance(component, (
            CurrentDataProcessor, HistoricalAnalysisProcessor, PredictionProcessor, FinancialDataAnalyzer)):
                if not hasattr(component, 'api_connector') or component.api_connector is None:
                    component.api_connector = api_connector_instance

            if self.financial_data_analyzer and isinstance(component, (
            HistoricalAnalysisProcessor, PredictionProcessor, InsightGenerator)):
                if not hasattr(component, 'financial_data_analyzer') or component.financial_data_analyzer is None:
                    component.financial_data_analyzer = self.financial_data_analyzer

            financial_calculator_instance = getattr(self.financial_data_analyzer, 'financial_calculator',
                                                    None) if self.financial_data_analyzer else None
            if financial_calculator_instance and isinstance(component,
                                                            (HistoricalAnalysisProcessor, PredictionProcessor)):
                if not hasattr(component, 'financial_calculator') or component.financial_calculator is None:
                    component.financial_calculator = financial_calculator_instance

            if self.date_utils and (not hasattr(component, 'date_utils') or component.date_utils is None):
                component.date_utils = self.date_utils
        logger.debug("Dependency injection check complete.")

    # ========================================================================
    # ============= æ ¸å¿ƒæµç¨‹æ–¹æ³• process_intelligent_query (ä¿æŒä¸å˜) ============
    # ========================================================================
    async def process_intelligent_query(self, user_query: str, user_id: int = 0,
                                        conversation_id: Optional[str] = None,
                                        preferences: Optional[Dict[str, Any]] = None) -> ProcessingResult:
        if not self.initialized:
            logger.critical("CRITICAL: Orchestrator not initialized. Attempting to initialize now.")
            await self.initialize()
            if not self.initialized:
                return self._handle_processing_error_sync(  # ä½¿ç”¨å·²å®šä¹‰çš„åŒæ­¥é”™è¯¯å¤„ç†å™¨
                    str(uuid.uuid4()), f"init_fail_{int(time.time())}", user_query,
                    "ç³»ç»Ÿæ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚", 0.0, conversation_id
                )

        session_id = str(uuid.uuid4())
        query_id = f"q_{datetime.now().strftime('%Y%m%d%H%M%S%f')}_{hashlib.md5(user_query.encode('utf-8')).hexdigest()[:6]}"
        start_time = time.time()

        conversation_id_for_db: Optional[int] = None
        if conversation_id:
            try:
                conversation_id_for_db = int(conversation_id)
            except ValueError:
                logger.warning(f"QueryID: {query_id} - Invalid Conversation ID '{conversation_id}'.")
                conversation_id = None

        logger.info(
            f"ğŸ§  QueryID: {query_id} - UserID: {user_id} - ConvStrID: {conversation_id} (DBIntID: {conversation_id_for_db}) - Processing: '{user_query[:100]}...'")
        self.orchestrator_stats['total_queries'] += 1

        if conversation_id_for_db and self.conversation_manager:
            try:
                user_msg_id = self.conversation_manager.add_message(conversation_id_for_db, True, user_query)
                logger.debug(
                    f"QueryID: {query_id} - User query (MsgID: {user_msg_id}) logged to ConvID {conversation_id_for_db}.")
            except Exception as conv_err:
                logger.error(
                    f"QueryID: {query_id} - Failed to log user query for ConvID {conversation_id_for_db}: {conv_err}")

        parsing_time, data_fetching_time, business_processing_time = 0.0, 0.0, 0.0
        insight_processing_time, response_formatting_time, visualization_processing_time = 0.0, 0.0, 0.0
        query_analysis: Optional[QueryAnalysisResult] = None
        data_acquisition_result: Optional[FetcherExecutionResult] = None
        business_result_data: Dict[str, Any] = {}
        insights_list: List[BusinessInsight] = []
        visualizations_data: List[Dict[str, Any]] = []
        response_text: str = ""
        routing_strategy: OrchestratorProcessingStrategy = OrchestratorProcessingStrategy.ERROR_HANDLING
        data_quality: float = 0.1

        try:
            if self.config.get('enable_intelligent_caching', True):
                current_context_for_cache = {}
                if conversation_id_for_db and self.conversation_manager:
                    current_context_for_cache = self.conversation_manager.get_context(conversation_id_for_db)
                cache_key_full_query = f"full_res_{self._generate_query_cache_key(user_query, current_context_for_cache)}"
                cached_full_result = self._get_cached_result(cache_key_full_query)
                if cached_full_result and isinstance(cached_full_result, ProcessingResult):
                    logger.info(f"QueryID: {query_id} - Full result cache HIT.")
                    self.orchestrator_stats['cache_hits'] += 1
                    cached_full_result.total_processing_time = time.time() - start_time
                    self._update_orchestrator_stats(cached_full_result)
                    if conversation_id_for_db and self.conversation_manager:
                        self._log_ai_response_to_conversation(conversation_id_for_db, cached_full_result, query_id)
                    return cached_full_result
            self.orchestrator_stats['cache_misses'] += 1

            parsing_start_time = time.time()
            query_analysis = await self._intelligent_query_parsing(user_query, conversation_id_for_db)
            parsing_time = time.time() - parsing_start_time
            logger.info(
                f"QueryID: {query_id} - 1. Query Parsing ({parsing_time:.3f}s): Complexity='{query_analysis.complexity.value}', Type='{query_analysis.query_type.value}'")

            routing_strategy = await self._determine_processing_strategy(query_analysis, preferences)
            logger.info(f"QueryID: {query_id} - 2. Processing Strategy: {routing_strategy.value}")

            data_acquisition_start_time = time.time()
            data_acquisition_result = await self._orchestrate_data_acquisition(query_analysis, routing_strategy)
            data_fetching_time = time.time() - data_acquisition_start_time
            data_quality = getattr(data_acquisition_result, 'confidence_level', 0.3)
            logger.info(
                f"QueryID: {query_id} - 3. Data Acquisition ({data_fetching_time:.3f}s): Quality={data_quality:.2f}, Status='{getattr(data_acquisition_result, 'execution_status', 'unknown').value if hasattr(getattr(data_acquisition_result, 'execution_status', None), 'value') else 'unknown'}'")

            if not data_acquisition_result or \
                    (not getattr(data_acquisition_result, 'fetched_data', None) and not getattr(data_acquisition_result,
                                                                                                'processed_data',
                                                                                                None)) or \
                    (hasattr(data_acquisition_result,
                             'execution_status') and data_acquisition_result.execution_status.value not in ['completed',
                                                                                                            'partial_success']):
                err_msg = f"Critical data acquisition failed. Status: {getattr(data_acquisition_result, 'execution_status', {}).get('value', 'unknown')}. Error: {getattr(data_acquisition_result, 'error_info', 'N/A')}"
                logger.error(f"QueryID: {query_id} - {err_msg}")
                raise ValueError(err_msg)

            business_processing_start_time = time.time()
            business_result_data = await self._orchestrate_business_processing(query_analysis, data_acquisition_result,
                                                                               routing_strategy)
            business_processing_time = time.time() - business_processing_start_time
            logger.info(
                f"QueryID: {query_id} - 4. Business Processing ({business_processing_time:.3f}s): Processor='{business_result_data.get('processor_used', 'N/A')}'")

            insight_generation_start_time = time.time()
            insights_list = await self._orchestrate_insight_generation(business_result_data, query_analysis,
                                                                       data_acquisition_result)
            insight_processing_time = time.time() - insight_generation_start_time
            logger.info(
                f"QueryID: {query_id} - 5. Insight Generation ({insight_processing_time:.3f}s): Found {len(insights_list)} insights.")

            response_formatting_start_time = time.time()
            response_text = await self._generate_intelligent_response(user_query, query_analysis, business_result_data,
                                                                      insights_list)
            response_formatting_time = time.time() - response_formatting_start_time
            logger.info(f"QueryID: {query_id} - 6. Response Text Generation ({response_formatting_time:.3f}s).")

            visualization_start_time = time.time()
            visualizations_data = await self._generate_visualizations(business_result_data, query_analysis,
                                                                      insights_list)
            visualization_processing_time = time.time() - visualization_start_time
            logger.info(
                f"QueryID: {query_id} - 7. Visualization Data Generation ({visualization_processing_time:.3f}s): Found {len(visualizations_data)} visuals.")

            total_ai_time = parsing_time
            biz_proc_metadata = business_result_data.get('metadata', {})
            total_ai_time += biz_proc_metadata.get('ai_processing_time', 0.0) if isinstance(biz_proc_metadata,
                                                                                            dict) else 0.0
            insight_gen_metadata = business_result_data.get('insight_generation_metadata',
                                                            {}) if business_result_data else {}
            total_ai_time += insight_gen_metadata.get('ai_time', insight_processing_time) if isinstance(
                insight_gen_metadata, dict) else insight_processing_time
            total_ai_time += response_formatting_time

            total_processing_time = time.time() - start_time
            overall_confidence = self._calculate_overall_confidence(query_analysis, business_result_data, insights_list,
                                                                    data_quality)

            final_result = ProcessingResult(
                session_id=session_id, query_id=query_id, success=True,
                response_text=response_text,
                insights=insights_list,
                key_metrics=self._extract_key_metrics(business_result_data),
                visualizations=visualizations_data,
                processing_strategy=routing_strategy,
                processors_used=self._get_processors_used(business_result_data, routing_strategy),
                ai_collaboration_summary=self._get_ai_collaboration_summary(query_analysis, business_result_data,
                                                                            insights_list),
                confidence_score=overall_confidence,
                data_quality_score=data_quality,
                response_completeness=self._calculate_response_completeness(business_result_data, insights_list),
                total_processing_time=total_processing_time,
                ai_processing_time=total_ai_time,
                data_fetching_time=data_fetching_time,
                processing_metadata={
                    'query_complexity': query_analysis.complexity.value,
                    'query_type': query_analysis.query_type.value,
                    'business_scenario': query_analysis.business_scenario.value if query_analysis.business_scenario else "N/A",
                    'data_sources_used': getattr(data_acquisition_result, 'data_sources_used', []),
                    'step_times': {
                        'parsing': round(parsing_time, 3), 'data_fetching': round(data_fetching_time, 3),
                        'business_processing': round(business_processing_time, 3),
                        'insight_generation': round(insight_processing_time, 3),
                        'response_formatting': round(response_formatting_time, 3),
                        'visualization': round(visualization_processing_time, 3)
                    },
                    'ai_models_invoked': self._get_ai_models_invoked_summary(query_analysis, business_result_data,
                                                                             insights_list)
                },
                conversation_id=conversation_id,
                query_analysis_snapshot=query_analysis.to_dict() if hasattr(query_analysis,
                                                                            'to_dict') else query_analysis.__dict__ if query_analysis else None
            )

            self._update_orchestrator_stats(final_result)
            if self.config.get('enable_intelligent_caching', True) and final_result.success:
                self._cache_result(cache_key_full_query, final_result)

            if conversation_id_for_db and self.conversation_manager:
                self._log_ai_response_to_conversation(conversation_id_for_db, final_result, query_id)

            logger.info(
                f"QueryID: {query_id} - âœ… æ™ºèƒ½æŸ¥è¯¢å¤„ç†æˆåŠŸã€‚æ€»è€—æ—¶: {total_processing_time:.3f}s, ç½®ä¿¡åº¦: {final_result.confidence_score:.2f}")
            return final_result

        except Exception as e:
            total_processing_time_on_error = time.time() - start_time
            logger.error(f"âŒ QueryID: {query_id} - æ™ºèƒ½æŸ¥è¯¢å¤„ç†ä¸»æµç¨‹å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            return await self._handle_processing_error(
                session_id, query_id, user_query, str(e), total_processing_time_on_error, conversation_id
            )

    # ========================================================================
    # ============= ä»¥ä¸‹æ˜¯è¡¥å…¨çš„è¾…åŠ©æ–¹æ³• (ä¸²è”æ ¸å¿ƒé€»è¾‘) ========================
    # ========================================================================

        # åœ¨ IntelligentQAOrchestrator ç±»ä¸­æ·»åŠ è¿™äº›æ–¹æ³•ï¼š

    def _generate_basic_response(self, business_result_data: Dict[str, Any],
                                     insights_list: List[BusinessInsight]) -> str:
            """
            åœ¨AIå“åº”ç”Ÿæˆå¤±è´¥æ—¶ï¼Œæä¾›ä¸€ä¸ªåŸºäºæ¨¡æ¿çš„ã€æ›´ç®€å•çš„ä¸­æ–‡æ–‡æœ¬å“åº”ã€‚
            """
            logger.warning("Generating basic (non-AI) response due to previous errors or lack of AI client.")
            response_parts = ["æ ¹æ®ç³»ç»Ÿåˆ†æï¼š\n"]

            key_metrics = self._extract_key_metrics(business_result_data)  # å‡è®¾æ­¤æ–¹æ³•å·²å®ç°
            if key_metrics:
                response_parts.append("å…³é”®æŒ‡æ ‡ï¼š")
                # ç¡®ä¿ FinancialFormatter å·²åˆå§‹åŒ–å¹¶å¯ç”¨
                # for name, value in list(key_metrics.items())[:5]: # æœ€å¤šæ˜¾ç¤º5ä¸ª
                #     response_parts.append(f"  - {name}: {value}") # value åº”å·²ç”± _extract_key_metrics æ ¼å¼åŒ–
                # ç®€åŒ–ç‰ˆï¼Œä¸ä¾èµ– _extract_key_metrics å®Œå…¨æ­£ç¡®ï¼š
                metric_items = []
                temp_metrics_to_show = {}
                primary_res_content = business_result_data.get('primary_result', {})
                if isinstance(primary_res_content, dict):
                    if 'key_metrics' in primary_res_content: temp_metrics_to_show.update(
                        primary_res_content['key_metrics'])
                    if 'metrics' in primary_res_content: temp_metrics_to_show.update(primary_res_content['metrics'])
                    if 'main_prediction' in primary_res_content: temp_metrics_to_show.update(
                        primary_res_content['main_prediction'])

                for name, value in list(temp_metrics_to_show.items())[:5]:
                    val_str = str(value)
                    if self.financial_formatter and isinstance(value, (int, float)):  # å°è¯•æ ¼å¼åŒ–
                        try:
                            if "rate" in name.lower() or "ratio" in name.lower() or "%" in val_str:
                                val_str = self.financial_formatter.format_percentage(
                                    float(val_str.replace('%', '')) / 100 if "%" in val_str else float(val_str))
                            else:
                                val_str = self.financial_formatter.format_currency(float(value))
                        except:
                            pass  # ä¿æŒåŸæ ·
                    metric_items.append(f"  - {name}: {val_str}")
                if metric_items:
                    response_parts.extend(metric_items)
                else:
                    response_parts.append("  æš‚æ— å…³é”®æŒ‡æ ‡å¯ç›´æ¥å±•ç¤ºã€‚")
            else:
                primary_res = business_result_data.get('primary_result')
                main_ans_candidate = "åˆ†æå·²æ‰§è¡Œï¼Œä½†æ— ç‰¹å®šæŒ‡æ ‡è¾“å‡ºã€‚"
                if isinstance(primary_res, dict):
                    main_ans_candidate = primary_res.get('main_answer', primary_res.get('summary', main_ans_candidate))
                elif primary_res:
                    main_ans_candidate = getattr(primary_res, 'main_answer',
                                                 getattr(primary_res, 'summary', main_ans_candidate))
                response_parts.append(f"ä¸»è¦ä¿¡æ¯: {str(main_ans_candidate)[:300]}")

            if insights_list:
                response_parts.append("\né‡è¦æ´å¯Ÿï¼š")
                for i, insight in enumerate(insights_list[:3], 1):  # æœ€å¤š3æ¡
                    title = getattr(insight, 'title', f"æ´å¯Ÿ{i}")
                    summary = getattr(insight, 'summary', "è¯·æŸ¥çœ‹è¯¦ç»†åˆ†æã€‚")
                    response_parts.append(f"  {i}. **{title}**: {summary}")
            else:
                response_parts.append("\næš‚æ— ç‰¹åˆ«çš„ä¸šåŠ¡æ´å¯Ÿã€‚")

            confidence = business_result_data.get('confidence_score', 0.5)
            data_quality_src = business_result_data.get('data_quality_score',
                                                        getattr(business_result_data.get('data_acquisition_result', {}),
                                                                'confidence_level', 0.5)
                                                        )
            response_parts.append(f"\næœ¬æ¬¡åˆ†æçš„æ•´ä½“ç½®ä¿¡åº¦çº¦ä¸º {confidence:.0%}ï¼Œæ•°æ®è´¨é‡è¯„åˆ†ä¸º {data_quality_src:.0%}")
            if confidence < 0.7 or data_quality_src < 0.7:
                response_parts.append("è¯·æ³¨æ„ï¼Œç”±äºæ•°æ®è´¨é‡æˆ–åˆ†æå¤æ‚æ€§ï¼Œç»“æœå¯èƒ½å­˜åœ¨ä¸€å®šä¸ç¡®å®šæ€§ã€‚")

            return "\n".join(response_parts)

    def _summarize_business_result_for_ai(self, business_result_data: Dict[str, Any]) -> Dict[str, Any]:
            """ä¸ºAIå‡†å¤‡ä¸šåŠ¡ç»“æœçš„æ‘˜è¦ï¼Œä»¥ä¾¿AIèƒ½æ›´å¥½åœ°æ•´åˆä¿¡æ¯ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤ã€‚"""
            summary = {}
            if not business_result_data or not isinstance(business_result_data, dict):
                logger.warning("_summarize_business_result_for_ai: business_result_data is empty or invalid.")
                return {"æ‘˜è¦": "ä¸šåŠ¡å¤„ç†é˜¶æ®µæœªè¿”å›æœ‰æ•ˆç»“æœã€‚"}

            summary['processing_type'] = business_result_data.get('processing_type')
            summary['confidence_score'] = round(business_result_data.get('confidence_score', 0.0), 2)

            primary_res_payload = business_result_data.get('primary_result')  # This is object or dict of objects

            # è¾…åŠ©å‡½æ•°ï¼Œå°†å¤„ç†å™¨è¿”å›çš„å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿æ‘˜è¦
            def _convert_to_dict_for_summary(obj: Any) -> Optional[Dict[str, Any]]:
                if hasattr(obj, 'to_dict') and callable(obj.to_dict): return obj.to_dict()
                if hasattr(obj, '__dict__'): return obj.__dict__  # For dataclasses
                if isinstance(obj, dict): return obj
                logger.warning(f"Cannot convert object of type {type(obj)} to dict for summary.")
                return None

            if isinstance(primary_res_payload, dict) and 'multi_results' in primary_res_payload:
                summary['multi_analysis_highlights'] = {}
                for key, res_obj_or_dict in primary_res_payload['multi_results'].items():
                    res_dict = _convert_to_dict_for_summary(res_obj_or_dict)
                    if res_dict and 'error' not in res_dict:
                        single_summary = {}
                        # ä» CurrentDataResponse, HistoricalAnalysisResponse, PredictionResponse ä¸­æå–å…³é”®ä¿¡æ¯
                        if 'main_answer' in res_dict: single_summary['answer_snippet'] = str(res_dict['main_answer'])[
                                                                                         :100] + "..."

                        metrics_src = res_dict.get('key_metrics') or res_dict.get('metrics') or res_dict.get(
                            'main_prediction')
                        if isinstance(metrics_src, dict): single_summary['top_metrics_sample'] = dict(
                            list(metrics_src.items())[:2])  # æœ€å¤š2ä¸ªç¤ºä¾‹æŒ‡æ ‡

                        if 'trend_summary' in res_dict and res_dict['trend_summary']: single_summary['trend_snapshot'] = \
                        res_dict['trend_summary']
                        if 'key_findings' in res_dict and isinstance(res_dict['key_findings'], list) and res_dict[
                            'key_findings']:
                            single_summary['top_finding'] = str(res_dict['key_findings'][0])[:150] + "..."

                        if single_summary:  # åªæ·»åŠ æœ‰å†…å®¹çš„æ‘˜è¦
                            summary['multi_analysis_highlights'][key] = single_summary
            else:  # å•ä¸ªå¤„ç†å™¨çš„ç»“æœ (å¯¹è±¡æˆ–å…¶å­—å…¸)
                res_dict = _convert_to_dict_for_summary(primary_res_payload)
                if res_dict:
                    if 'main_answer' in res_dict: summary['main_answer_snippet'] = str(res_dict['main_answer'])[
                                                                                   :200] + "..."
                    metrics_src = res_dict.get('key_metrics') or res_dict.get('metrics') or res_dict.get(
                        'main_prediction')
                    if isinstance(metrics_src, dict): summary['main_metrics_sample'] = dict(
                        list(metrics_src.items())[:3])  # æœ€å¤š3ä¸ªç¤ºä¾‹æŒ‡æ ‡
                    if 'trend_summary' in res_dict and res_dict['trend_summary']: summary['trend_snapshot'] = res_dict[
                        'trend_summary']
                    if 'key_findings' in res_dict and isinstance(res_dict['key_findings'], list) and res_dict[
                        'key_findings']:
                        summary['top_finding'] = str(res_dict['key_findings'][0])[:150] + "..."
                elif primary_res_payload:  # å¦‚æœè½¬æ¢å­—å…¸å¤±è´¥ï¼Œä½†å¯¹è±¡å­˜åœ¨
                    summary['raw_primary_result_type'] = str(type(primary_res_payload))

            if not summary.get('main_answer_snippet') and not summary.get('multi_analysis_highlights'):
                summary['general_summary'] = "ç³»ç»Ÿå·²å®Œæˆå¤šé¡¹åˆ†æï¼Œå…·ä½“ç»†èŠ‚è¯·å‚è€ƒæŒ‡æ ‡å’Œæ´å¯Ÿéƒ¨åˆ†ã€‚"

            return summary

    async def _generate_fallback_response(self, user_query: str, error: str) -> str:
            """åœ¨å¤„ç†å¤±è´¥æ—¶ç”Ÿæˆä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ä¸­æ–‡é™çº§å“åº”ã€‚"""
            logger.info(f"ä¸ºæŸ¥è¯¢ç”Ÿæˆé™çº§å“åº”: '{user_query[:50]}...'ï¼Œé”™è¯¯: {error[:100]}")

            # å°è¯•ä½¿ç”¨AIï¼ˆå¦‚æœä¸€ä¸ªæ¨¡å‹å¯ç”¨ä¸”é”™è¯¯ä¸æ˜¯AIæœåŠ¡æœ¬èº«çš„é—®é¢˜ï¼‰ç”Ÿæˆæ›´æ™ºèƒ½çš„é”™è¯¯æç¤º
            ai_client_for_fallback = None
            error_lower = error.lower()

            # ä¼˜å…ˆé€‰æ‹©ä¸å¯¼è‡´é”™è¯¯çš„æ¨¡å‹ä¸åŒçš„æ¨¡å‹ï¼Œæˆ–è€…å¦‚æœé”™è¯¯ä¸AIæ— å…³ï¼Œåˆ™ä¼˜å…ˆClaude
            if "claude" not in error_lower and self.claude_client:
                ai_client_for_fallback = self.claude_client
            elif ("gpt" not in error_lower and "openai" not in error_lower) and self.gpt_client:
                ai_client_for_fallback = self.gpt_client
            elif self.claude_client:  # å¦‚æœä¸¤ä¸ªéƒ½æ²¡åœ¨é”™è¯¯ä¿¡æ¯ä¸­ï¼Œé»˜è®¤å°è¯•Claude
                ai_client_for_fallback = self.claude_client
            elif self.gpt_client:  # å…¶æ¬¡å°è¯•GPT
                ai_client_for_fallback = self.gpt_client

            if ai_client_for_fallback and self.config.get('fallback_response_enabled', True):
                try:
                    # ä¸­æ–‡æç¤ºè¯
                    fallback_prompt = f"""
                    ç”¨æˆ·ä¹‹å‰çš„æŸ¥è¯¢æ˜¯ï¼šâ€œ{user_query}â€
                    ç³»ç»Ÿåœ¨å°è¯•å¤„ç†è¿™ä¸ªæŸ¥è¯¢æ—¶é‡åˆ°äº†ä¸€ä¸ªå†…éƒ¨é”™è¯¯ï¼Œé”™è¯¯ä¿¡æ¯æ‘˜è¦å¦‚ä¸‹ï¼ˆæ­¤æ‘˜è¦ä»…ä¾›æ‚¨å‚è€ƒï¼Œä¸è¦ç›´æ¥å±•ç¤ºç»™ç”¨æˆ·ï¼‰ï¼š
                    â€œ{error[:150]}...â€

                    è¯·ä½ æ‰®æ¼”ä¸€ä¸ªä¹äºåŠ©äººä¸”ä¸“ä¸šçš„AIå®¢æœï¼Œç”¨ç®€ä½“ä¸­æ–‡ç»™ç”¨æˆ·ç”Ÿæˆä¸€ä¸ªç®€æ´ã€å‹å¥½çš„å›å¤ã€‚
                    æ‚¨çš„å›å¤åº”è¯¥åŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š
                    1. å¯¹ç”¨æˆ·è¡¨ç¤ºæ­‰æ„ï¼Œè¯´æ˜ä»–ä»¬çš„è¯·æ±‚æœªèƒ½æˆåŠŸå¤„ç†ã€‚
                    2. æ— éœ€å‘ç”¨æˆ·å¤è¿°å…·ä½“çš„é”™è¯¯ä¿¡æ¯æ‘˜è¦ï¼Œé™¤éæ˜¯éå¸¸æ˜ç¡®çš„ç”¨æˆ·è¾“å…¥å‚æ•°é—®é¢˜ï¼ˆä½†è¿™ç§æƒ…å†µå¾ˆå°‘ï¼‰ã€‚
                    3. å»ºè®®ç”¨æˆ·å¯ä»¥å°è¯•çš„æ“ä½œï¼Œä¾‹å¦‚ï¼š
                        - ç¨åé‡è¯•ã€‚
                        - å°è¯•ç”¨ä¸åŒçš„æ–¹å¼è¡¨è¿°ä»–ä»¬çš„é—®é¢˜ï¼Œæˆ–è€…ç®€åŒ–æŸ¥è¯¢ã€‚
                        - ï¼ˆå¦‚æœé€‚ç”¨ï¼‰æ£€æŸ¥ä»–ä»¬çš„è¾“å…¥æ˜¯å¦ç¬¦åˆé¢„æœŸæ ¼å¼ã€‚
                    4. å¦‚æœé”™è¯¯çœ‹èµ·æ¥æ˜¯ä¸´æ—¶çš„ç³»ç»Ÿæ€§é—®é¢˜ï¼Œå¯ä»¥æš—ç¤ºæŠ€æœ¯å›¢é˜Ÿå¯èƒ½å·²çŸ¥æ™“å¹¶åœ¨å¤„ç†ã€‚
                    è¯·ç¡®ä¿å›å¤è¯­æ°”ä¸“ä¸šã€å®‰æŠšç”¨æˆ·ï¼Œå¹¶ä¸”ä¸è¦æä¾›è™šå‡æ‰¿è¯ºã€‚å›å¤é•¿åº¦æ§åˆ¶åœ¨1-2å¥è¯ã€‚
                    """
                    ai_response_data = None
                    if isinstance(ai_client_for_fallback, ClaudeClient) and hasattr(ai_client_for_fallback,
                                                                                    'generate_text'):
                        ai_response_data = await ai_client_for_fallback.generate_text(fallback_prompt, max_tokens=200)
                    elif isinstance(ai_client_for_fallback, OpenAIClient) and hasattr(ai_client_for_fallback,
                                                                                      'generate_completion'):
                        ai_response_data = await ai_client_for_fallback.generate_completion(fallback_prompt,
                                                                                            max_tokens=200)

                    if ai_response_data and ai_response_data.get('success', False):
                        content = ai_response_data.get('text', ai_response_data.get('response',
                                                                                    ai_response_data.get('completion')))
                        if content and isinstance(content, str) and content.strip():
                            logger.info("AIæˆåŠŸç”Ÿæˆäº†é™çº§å“åº”ã€‚")
                            return content.strip()
                except Exception as ai_fallback_err:
                    logger.error(f"ä½¿ç”¨AIç”Ÿæˆé™çº§å“åº”æ—¶ä¹Ÿå‘ç”Ÿé”™è¯¯: {ai_fallback_err}")

            # å¦‚æœAIç”Ÿæˆé™çº§å“åº”å¤±è´¥ï¼Œæˆ–æ²¡æœ‰AIå®¢æˆ·ç«¯ï¼Œåˆ™ä½¿ç”¨åŸºäºè§„åˆ™çš„ç®€å•ä¸­æ–‡å“åº”
            query_lower = user_query.lower()
            if any(kw in query_lower for kw in ["ä½™é¢", "balance"]):
                return "æŠ±æ­‰ï¼Œç³»ç»Ÿå½“å‰æ— æ³•æŸ¥è¯¢ä½™é¢ä¿¡æ¯ã€‚æˆ‘ä»¬çš„å·¥ç¨‹å¸ˆæ­£åœ¨ç´§æ€¥å¤„ç†ï¼Œè¯·æ‚¨ç¨åé‡è¯•æˆ–è”ç³»æˆ‘ä»¬çš„æ”¯æŒå›¢é˜Ÿã€‚"
            elif any(kw in query_lower for kw in ["è¶‹åŠ¿", "trend", "å†å²", "history"]):
                return "æŠ±æ­‰ï¼Œå†å²æ•°æ®åˆ†æåŠŸèƒ½æš‚æ—¶é‡åˆ°ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚æˆ‘ä»¬æ­£åœ¨åŠªåŠ›ä¿®å¤ä¸­ï¼Œè¯·æ‚¨ç¨åå†è¯•ã€‚"
            elif any(kw in query_lower for kw in ["é¢„æµ‹", "predict", "é¢„è®¡", "forecast"]):
                return "æŠ±æ­‰ï¼Œé¢„æµ‹æœåŠ¡å½“å‰æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·æ‚¨ç¨ç­‰ç‰‡åˆ»å†å°è¯•æ‚¨çš„é¢„æµ‹è¯·æ±‚ã€‚"
            return f"å¤„ç†æ‚¨çš„æŸ¥è¯¢ '{user_query[:30]}...' æ—¶é‡åˆ°ä¸€ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æŠ€æœ¯å›¢é˜Ÿå·²ç»æ³¨æ„åˆ°æ­¤æƒ…å†µï¼Œå¹¶å°†å°½å¿«è§£å†³ã€‚ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ï¼Œæˆ‘ä»¬æ·±è¡¨æ­‰æ„ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    def _calculate_overall_confidence(self, query_analysis: Optional[QueryAnalysisResult],
                                      business_result_data: Dict[str, Any],
                                      insights_list: List[BusinessInsight],
                                      data_quality_score: float) -> float:
        """
        è®¡ç®—æ•´ä¸ªæŸ¥è¯¢å¤„ç†æµç¨‹çš„æ•´ä½“ç½®ä¿¡åº¦ã€‚
        ç»¼åˆè€ƒè™‘æŸ¥è¯¢è§£æã€æ•°æ®è´¨é‡ã€ä¸šåŠ¡å¤„ç†å’Œæ´å¯Ÿçš„ç½®ä¿¡åº¦ã€‚
        """
        factors: List[float] = []

        # 1. æŸ¥è¯¢è§£æç½®ä¿¡åº¦
        if query_analysis and hasattr(query_analysis, 'confidence_score'):
            factors.append(float(query_analysis.confidence_score))
        else:
            factors.append(0.5)  # å¦‚æœæ²¡æœ‰æŸ¥è¯¢åˆ†æç»“æœï¼Œç»™ä¸€ä¸ªè¾ƒä½çš„é»˜è®¤å€¼

        # 2. æ•°æ®è´¨é‡è¯„åˆ† (ç”± SmartDataFetcher æä¾›)
        factors.append(float(data_quality_score))

        # 3. ä¸šåŠ¡å¤„ç†ç½®ä¿¡åº¦
        # business_result_data åŒ…å«äº† 'confidence_score' é”®
        if business_result_data and isinstance(business_result_data, dict):
            factors.append(float(business_result_data.get('confidence_score', 0.7)))  # é»˜è®¤0.7å¦‚æœä¸šåŠ¡å¤„ç†å±‚æœªæä¾›
        else:
            factors.append(0.5)

        # 4. æ´å¯Ÿè´¨é‡ (åŸºäºæ´å¯Ÿåˆ—è¡¨å’Œå•ä¸ªæ´å¯Ÿçš„ç½®ä¿¡åº¦)
        if insights_list:
            insights_confidences = [
                float(getattr(i, 'confidence_score', 0.7)) for i in insights_list if hasattr(i, 'confidence_score')
            ]
            if insights_confidences:
                factors.append(sum(insights_confidences) / len(insights_confidences))
            else:
                factors.append(0.6)  # æœ‰æ´å¯Ÿä½†æ²¡æœ‰å…·ä½“ç½®ä¿¡åº¦
        else:
            factors.append(0.5)  # æ²¡æœ‰æ´å¯Ÿ

        # å®šä¹‰å„å› ç´ çš„æƒé‡ï¼Œæ€»å’Œä¸º1
        # æƒé‡å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼Œä¾‹å¦‚ï¼Œæ•°æ®è´¨é‡å¯èƒ½æ¯”æ´å¯Ÿç½®ä¿¡åº¦æ›´é‡è¦
        # é¡ºåºï¼šè§£æã€æ•°æ®è´¨é‡ã€ä¸šåŠ¡å¤„ç†ã€æ´å¯Ÿ
        weights = [0.20, 0.30, 0.30, 0.20]

        if len(factors) == len(weights):
            weighted_sum = sum(factor * weight for factor, weight in zip(factors, weights))
            final_confidence = round(weighted_sum, 3)
        elif factors:  # å¦‚æœæƒé‡å’Œå› ç´ æ•°é‡ä¸åŒ¹é…ï¼ˆä¸åº”å‘ç”Ÿï¼‰ï¼Œåˆ™å–ç®€å•å¹³å‡
            logger.warning(
                f"Confidence factors count ({len(factors)}) does not match weights count ({len(weights)}). Using simple average.")
            final_confidence = round(sum(factors) / len(factors), 3)
        else:  # ä¸åº”å‘ç”Ÿï¼Œä½†ä½œä¸ºä¿æŠ¤
            final_confidence = 0.3

        logger.debug(f"Calculated overall confidence: {final_confidence}, based on factors: {factors}")
        return final_confidence

    def _extract_key_metrics(self, business_result_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»ä¸šåŠ¡å¤„ç†ç»“æœä¸­æå–å…³é”®æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨ FinancialFormatter è¿›è¡Œæ ¼å¼åŒ–ã€‚
        è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯æŒ‡æ ‡åç§°ï¼Œå€¼æ˜¯æ ¼å¼åŒ–åçš„æŒ‡æ ‡å€¼ï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚
        """
        raw_metrics: Dict[str, Any] = {}  # å­˜å‚¨åŸå§‹æå–çš„æŒ‡æ ‡
        if not business_result_data or not isinstance(business_result_data, dict):
            logger.debug("_extract_key_metrics: business_result_data is empty or invalid.")
            return {}

        primary_res_content = business_result_data.get('primary_result')
        logger.debug(f"_extract_key_metrics: primary_res_content type is {type(primary_res_content)}")

        def _extract_from_single_source(source_dict: Any, prefix: str = ""):
            """è¾…åŠ©å‡½æ•°ï¼Œä»å•ä¸ªç»“æœå¯¹è±¡æˆ–å­—å…¸ä¸­æå–æŒ‡æ ‡ã€‚"""
            if not source_dict: return

            data_to_search: Optional[Dict[str, Any]] = None
            if hasattr(source_dict, 'to_dict') and callable(source_dict.to_dict):  # å¤„ç†dataclasså®ä¾‹
                data_to_search = source_dict.to_dict()
            elif hasattr(source_dict, '__dict__'):  # å¤„ç†å…¶ä»–æ™®é€šå¯¹è±¡å®ä¾‹
                data_to_search = source_dict.__dict__
            elif isinstance(source_dict, dict):  # å¦‚æœå·²ç»æ˜¯å­—å…¸
                data_to_search = source_dict

            if not data_to_search:
                logger.debug(
                    f"_extract_from_single_source: source_dict (prefix: {prefix}) is not dict-convertible or is None.")
                return

            # æŸ¥æ‰¾å¸¸è§çš„æŒ‡æ ‡å®¹å™¨é”®
            # æ‚¨çš„å¤„ç†å™¨å“åº”å¯¹è±¡ï¼ˆCurrentDataResponse, HistoricalAnalysisResponse, PredictionResponseï¼‰
            # ä¸­åŒ…å« 'key_metrics', 'metrics', 'main_prediction' ç­‰å­—æ®µ
            metric_container_keys = ['key_metrics', 'metrics', 'main_prediction', 'predictions_summary',
                                     'data']  # 'data' for CurrentDataResponse

            found_metrics_in_container = False
            for container_key in metric_container_keys:
                if container_key in data_to_search and isinstance(data_to_search[container_key], dict):
                    for k, v in data_to_search[container_key].items():
                        metric_name_to_store = f"{prefix}{k}".strip('_')
                        if metric_name_to_store in raw_metrics and raw_metrics[metric_name_to_store] != v:
                            logger.debug(
                                f"Metric conflict for '{metric_name_to_store}'. Current: {raw_metrics[metric_name_to_store]}, New: {v}. Making key unique.")
                            metric_name_to_store = f"{prefix}{container_key}_{k}".strip('_')  # å¢åŠ å”¯ä¸€æ€§
                        raw_metrics[metric_name_to_store] = v
                    found_metrics_in_container = True
                    # å¦‚æœä¸€ä¸ªå®¹å™¨æ‰¾åˆ°äº†æŒ‡æ ‡ï¼Œå¯èƒ½ä¸éœ€è¦å†æ£€æŸ¥æ­¤å¯¹è±¡çš„å…¶ä»–å®¹å™¨é”®äº†ï¼Œ
                    # ä½†è¿™å–å†³äºæ‚¨çš„æ•°æ®ç»“æ„è®¾è®¡ã€‚å¦‚æœå¤šä¸ªå®¹å™¨éƒ½å¯èƒ½åŒ…å«ç‹¬ç«‹æŒ‡æ ‡ï¼Œåˆ™ç§»é™¤breakã€‚
                    # break

            # å¦‚æœæ²¡æœ‰åœ¨æ ‡å‡†å®¹å™¨ä¸­æ‰¾åˆ°ï¼Œå¹¶ä¸”data_to_searchæœ¬èº«å°±æ˜¯é¡¶å±‚æŒ‡æ ‡ (ä¾‹å¦‚CurrentDataResponseçš„dataå­—æ®µ)
            if not found_metrics_in_container and data_to_search and container_key == 'data' and 'key_metrics' not in data_to_search and 'metrics' not in data_to_search:
                for k, v in data_to_search.items():  # å‡è®¾é¡¶å±‚é”®å€¼å¯¹æ˜¯æŒ‡æ ‡
                    # é¿å…æå–éæŒ‡æ ‡çš„å…ƒæ•°æ®ç­‰
                    if isinstance(v, (str, int, float, bool)) or (isinstance(v, list) and len(v) < 5 and all(
                            isinstance(i, (str, int, float)) for i in v)):  # ç®€å•ç±»å‹æˆ–çŸ­åˆ—è¡¨
                        metric_name_to_store = f"{prefix}{k}".strip('_')
                        if metric_name_to_store in raw_metrics and raw_metrics[metric_name_to_store] != v:
                            metric_name_to_store = f"{prefix}top_level_{k}".strip('_')
                        raw_metrics[metric_name_to_store] = v

        if isinstance(primary_res_content, dict):
            if 'multi_results' in primary_res_content and isinstance(primary_res_content['multi_results'], dict):
                logger.debug("_extract_key_metrics: Processing multi_results.")
                for proc_name, proc_res_obj_or_dict in primary_res_content['multi_results'].items():
                    if proc_res_obj_or_dict and not (
                            isinstance(proc_res_obj_or_dict, dict) and 'error' in proc_res_obj_or_dict):
                        _extract_from_single_source(proc_res_obj_or_dict, prefix=f"{proc_name}_")
            else:  # Single processor result (å¯èƒ½å·²ç»æ˜¯å­—å…¸ï¼Œä¹Ÿå¯èƒ½æ˜¯å¯¹è±¡)
                logger.debug("_extract_key_metrics: Processing single primary_result (dict or unknown obj).")
                _extract_from_single_source(primary_res_content)
        elif primary_res_content:  # It's a single response object (not a dict from multi_results)
            logger.debug(
                f"_extract_key_metrics: Processing single primary_result (object type: {type(primary_res_content)}).")
            _extract_from_single_source(primary_res_content, prefix="main_")

        # åº”ç”¨ FinancialFormatter è¿›è¡Œæ ¼å¼åŒ–
        if self.financial_formatter:
            formatted_metrics: Dict[str, Any] = {}
            logger.debug(f"_extract_key_metrics: Applying financial formatting to {len(raw_metrics)} raw metrics.")
            for k, v_original in raw_metrics.items():
                v_formatted = v_original  # é»˜è®¤ä½¿ç”¨åŸå§‹å€¼
                try:
                    # å°è¯•å°†å€¼è½¬æ¢ä¸ºå¯æ ¼å¼åŒ–çš„æ•°å€¼
                    numeric_value_for_formatting: Optional[Union[int, float]] = None
                    if isinstance(v_original, (int, float)):
                        numeric_value_for_formatting = v_original
                    elif isinstance(v_original, str):
                        val_str_cleaned = v_original.strip().replace('%', '').replace('Â¥', '').replace(',', '').replace(
                            'ï¿¥', '')
                        if val_str_cleaned.replace('.', '', 1).replace('-', '', 1).isdigit():
                            numeric_value_for_formatting = float(val_str_cleaned)

                    if numeric_value_for_formatting is not None:
                        key_lower = k.lower()
                        original_str_lower = str(v_original).lower()  # ç”¨äºæ£€æŸ¥åŸå§‹å­—ç¬¦ä¸²ä¸­çš„ '%' ç­‰

                        if "rate" in key_lower or "ratio" in key_lower or "growth" in key_lower or "change" in key_lower or "%" in original_str_lower:
                            # å¦‚æœåŸå§‹å­—ç¬¦ä¸²å« '%', å‡è®¾å®ƒæ˜¯ç™¾åˆ†ç‚¹ (ä¾‹å¦‚ '5%')ï¼Œå¦åˆ™æ˜¯å°æ•° (ä¾‹å¦‚ 0.05)
                            value_to_format_as_percentage = numeric_value_for_formatting / 100.0 if "%" in original_str_lower else numeric_value_for_formatting
                            v_formatted = self.financial_formatter.format_percentage(value_to_format_as_percentage)
                        elif any(curr_kw in key_lower for curr_kw in
                                 ["balance", "amount", "value", "inflow", "outflow", "èµ„é‡‘", "é‡‘é¢", "capital", "fund",
                                  "asset", "revenue", "profit", "cost", "ä½™é¢", "å…¥é‡‘", "å‡ºé‡‘"]):
                            v_formatted = self.financial_formatter.format_currency(numeric_value_for_formatting)
                        else:  # å…¶ä»–æ•°å€¼çš„é»˜è®¤æ ¼å¼åŒ–
                            v_formatted = f"{numeric_value_for_formatting:,.2f}" if isinstance(
                                numeric_value_for_formatting, float) else f"{numeric_value_for_formatting:,}"
                    # else v_formatted ä¿æŒ v_original (éæ•°å€¼ç±»å‹)
                    formatted_metrics[k] = v_formatted
                except Exception as fmt_e:
                    logger.warning(
                        f"Financial formatting failed for metric '{k}' (value: '{v_original}'): {fmt_e}. Using original value.")
                    formatted_metrics[k] = v_original  # å‡ºé”™æ—¶å›é€€åˆ°åŸå§‹å€¼
            logger.debug(f"_extract_key_metrics: Returning {len(formatted_metrics)} formatted metrics.")
            return formatted_metrics

        logger.debug(
            f"_extract_key_metrics: Returning {len(raw_metrics)} unformatted metrics (FinancialFormatter not available or no numeric values).")
        return raw_metrics  # å¦‚æœæ²¡æœ‰ formatterï¼Œè¿”å›åŸå§‹æå–çš„æŒ‡æ ‡

    def _get_processors_used(self, business_result_data: Dict[str, Any], strategy: OrchestratorProcessingStrategy) -> \
    List[str]:
        """
        ä»ä¸šåŠ¡å¤„ç†ç»“æœä¸­æå–å®é™…ä½¿ç”¨çš„å¤„ç†å™¨åç§°åˆ—è¡¨ã€‚
        """
        if not business_result_data or not isinstance(business_result_data, dict):
            logger.warning("_get_processors_used: business_result_data is empty or invalid.")
            return []

        # 'processor_used' å­—æ®µåº”ç”± _orchestrate_business_processing æ–¹æ³•åœ¨è¿”å›æ—¶è®¾ç½®
        # å®ƒé€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«å¤„ç†å™¨ç±»åçš„å­—ç¬¦ä¸²ï¼Œç”¨é€—å·åˆ†éš”ï¼Œæˆ–è€…æ˜¯ç±»ååˆ—è¡¨
        processor_info = business_result_data.get('processor_used')  # è¿™ä¸ªé”®æ˜¯åœ¨ _orchestrate_business_processing ä¸­è®¾ç½®çš„

        if isinstance(processor_info, str) and processor_info:
            # åˆ†å‰²é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œå¹¶å»é™¤ä¸¤ç«¯ç©ºæ ¼
            return [name.strip() for name in processor_info.split(',') if name.strip()]
        elif isinstance(processor_info, list):
            # ç¡®ä¿åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²
            return [str(name).strip() for name in processor_info if str(name).strip()]

        # å¦‚æœ 'processor_used' å­—æ®µç¼ºå¤±æˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œå°è¯•æ ¹æ®ç­–ç•¥æ¨æ–­ (è¿™åªæ˜¯ä¸€ä¸ªåå¤‡æ–¹æ¡ˆ)
        logger.warning(
            f"'_get_processors_used' could not determine processors from 'processor_used' field (value: {processor_info}). Falling back to strategy-based inference.")
        if strategy == OrchestratorProcessingStrategy.DIRECT_RESPONSE:
            return [
                self.current_data_processor.__class__.__name__ if self.current_data_processor else "CurrentDataProcessor"]
        if strategy == OrchestratorProcessingStrategy.SINGLE_PROCESSOR:
            # å¯¹äºå•å¤„ç†å™¨ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®è®°å½•ï¼Œå¾ˆéš¾ç²¾ç¡®çŸ¥é“æ˜¯å“ªä¸ªã€‚
            # _orchestrate_business_processing åº”è¯¥è®¾ç½® processor_usedã€‚
            # è¿™é‡Œè¿”å›ä¸€ä¸ªé€šç”¨å ä½ç¬¦æˆ–å°è¯•ä» primary_result çš„ç±»å‹æ¨æ–­ã€‚
            primary_res = business_result_data.get('primary_result')
            if isinstance(primary_res, CurrentDataResponse): return [CurrentDataProcessor.__class__.__name__]
            if isinstance(primary_res, HistoricalAnalysisResponse): return [
                HistoricalAnalysisProcessor.__class__.__name__]
            if isinstance(primary_res, PredictionResponse): return [PredictionProcessor.__class__.__name__]
            return ["UnknownSingleProcessor"]
        if strategy == OrchestratorProcessingStrategy.MULTI_PROCESSOR:
            # æå– multi_results ä¸­çš„é”®åä½œä¸ºå¤„ç†å™¨æ ‡è¯†ç¬¦
            multi_res = business_result_data.get('primary_result', {}).get('multi_results', {})
            if isinstance(multi_res, dict):
                return list(multi_res.keys())  # ä¾‹å¦‚ ['current_data_analysis', 'historical_analysis']
            return ["MultipleProcessorsInferred"]
        if strategy == OrchestratorProcessingStrategy.FULL_PIPELINE:
            multi_res = business_result_data.get('primary_result', {}).get('multi_results', {})
            processors = []
            if isinstance(multi_res, dict): processors.extend(list(multi_res.keys()))
            if 'financial_deep_dive' in processors:  # é€šå¸¸ financial_deep_dive æ˜¯ FinancialDataAnalyzer
                # Remove and add proper class name if available
                if self.financial_data_analyzer:
                    processors = [p for p in processors if p != 'financial_deep_dive']
                    processors.append(self.financial_data_analyzer.__class__.__name__)
            return list(set(processors))  # å»é‡

        return ["UnknownStrategyOrNoProcessors"]

    def _get_ai_collaboration_summary(self, query_analysis: Optional[QueryAnalysisResult],
                                      business_result_data: Dict[str, Any],
                                      insights_list: List[BusinessInsight]) -> Dict[str, Any]:
        """
        æ€»ç»“AIæ¨¡å‹ï¼ˆå¦‚Claude, GPTï¼‰åœ¨æ•´ä¸ªå¤„ç†æµç¨‹ä¸­çš„ä½¿ç”¨æƒ…å†µã€‚
        æ›´å‡†ç¡®çš„ç»Ÿè®¡éœ€è¦å„ç»„ä»¶åœ¨å…¶è¿”å›çš„å…ƒæ•°æ®ä¸­æŠ¥å‘ŠAIä½¿ç”¨æƒ…å†µã€‚
        """
        claude_invoked_count = 0
        gpt_invoked_count = 0
        other_ai_invoked_count = 0  # ä¸ºæœªæ¥æ‰©å±•

        # 1. æŸ¥è¯¢è§£æé˜¶æ®µçš„AIä½¿ç”¨
        if query_analysis and hasattr(query_analysis, 'ai_collaboration_plan') and \
                isinstance(query_analysis.ai_collaboration_plan, dict):
            plan = query_analysis.ai_collaboration_plan
            if plan.get('primary_ai', '').lower() == 'claude' or plan.get('secondary_ai', '').lower() == 'claude' or \
                    (isinstance(plan.get('claude_tasks'), list) and len(plan['claude_tasks']) > 0):
                claude_invoked_count += 1
            if plan.get('primary_ai', '').lower() == 'gpt' or plan.get('secondary_ai', '').lower() == 'gpt' or \
                    (isinstance(plan.get('gpt_tasks'), list) and len(plan['gpt_tasks']) > 0):
                gpt_invoked_count += 1
        elif query_analysis:  # å¦‚æœæ²¡æœ‰è¯¦ç»†è®¡åˆ’ï¼Œä½†è§£æç½®ä¿¡åº¦é«˜ï¼Œä¹Ÿå¯èƒ½ç”¨äº†AI
            if query_analysis.confidence_score > 0.7 and (self.claude_client or self.gpt_client):
                # æ— æ³•åŒºåˆ†æ˜¯å“ªä¸ªï¼Œå‡è®¾æ˜¯ä¸»AI (e.g., Claude)
                if self.claude_client:
                    claude_invoked_count += 1
                elif self.gpt_client:
                    gpt_invoked_count += 1

        # 2. ä¸šåŠ¡å¤„ç†é˜¶æ®µçš„AIä½¿ç”¨ (éœ€è¦å¤„ç†å™¨åœ¨å…¶è¿”å›çš„å…ƒæ•°æ®ä¸­æŠ¥å‘Š)
        # ä¾‹å¦‚ï¼Œ business_result_data['metadata']['ai_models_used_in_processing'] = ['claude', 'gpt']
        biz_proc_meta = business_result_data.get('metadata', {})
        if isinstance(biz_proc_meta, dict):
            models_in_biz = biz_proc_meta.get('ai_models_used_in_processing', [])  # æœŸæœ›æ˜¯åˆ—è¡¨
            if 'claude' in models_in_biz: claude_invoked_count += 1
            if 'gpt' in models_in_biz: gpt_invoked_count += 1
            # ä¹Ÿå¯ä»¥ä» primary_result.multi_results çš„æ¯ä¸ªç»“æœçš„metadataä¸­ç´¯åŠ 

        # 3. æ´å¯Ÿç”Ÿæˆé˜¶æ®µçš„AIä½¿ç”¨
        # InsightGenerator é€šå¸¸å¼ºä¾èµ–AI
        if insights_list:  # å¦‚æœç”Ÿæˆäº†æ´å¯Ÿ
            insight_gen_meta = business_result_data.get('insight_generation_metadata', {})
            if isinstance(insight_gen_meta, dict) and insight_gen_meta.get('ai_model_used'):
                model_str = str(insight_gen_meta['ai_model_used']).lower()
                if 'claude' in model_str:
                    claude_invoked_count += 1
                elif 'gpt' in model_str:
                    gpt_invoked_count += 1
            elif self.claude_client:  # é»˜è®¤InsightGeneratorä½¿ç”¨Claude
                claude_invoked_count += 1
            elif self.gpt_client:
                gpt_invoked_count += 1

        # 4. æœ€ç»ˆå“åº”æ–‡æœ¬ç”Ÿæˆé˜¶æ®µçš„AIä½¿ç”¨
        # _generate_intelligent_response ä¼šé€‰æ‹©ä¸€ä¸ªå®¢æˆ·ç«¯
        if self.claude_client or self.gpt_client:  # å¦‚æœæœ‰ä»»ä½•AIå®¢æˆ·ç«¯ç”¨äºå“åº”ç”Ÿæˆ
            if self.claude_client and not self.gpt_client:
                claude_invoked_count += 1  # å‡è®¾ä¼˜å…ˆClaude
            elif self.gpt_client and not self.claude_client:
                gpt_invoked_count += 1
            elif self.claude_client and self.gpt_client:
                claude_invoked_count += 1  # å‡è®¾ä¼˜å…ˆClaude

        claude_actually_used = claude_invoked_count > 0
        gpt_actually_used = gpt_invoked_count > 0

        collaboration_level = "none"
        if claude_actually_used and gpt_actually_used:
            collaboration_level = "dual_ai_collaboration"
        elif claude_actually_used or gpt_actually_used:
            collaboration_level = "single_ai_assist"

        return {
            'claude_used_in_process': claude_actually_used,
            'gpt_used_in_process': gpt_actually_used,
            'claude_invocation_count_estimate': claude_invoked_count,  # ä¼°ç®—è°ƒç”¨æ¬¡æ•°
            'gpt_invocation_count_estimate': gpt_invoked_count,
            'collaboration_level': collaboration_level,
            'ai_enhanced_parsing': getattr(query_analysis, 'confidence_score', 0.0) > 0.75 and \
                                   getattr(query_analysis, 'processing_metadata', {}).get(
                                       'parser_status') != 'fallback_rule_based' if query_analysis else False,
            'ai_generated_insights': bool(insights_list),
        }
    async def _intelligent_query_parsing(self, user_query: str,
                                         conversation_id_for_db: Optional[int] = None) -> QueryAnalysisResult:
        """
        AIé©±åŠ¨çš„æ™ºèƒ½æŸ¥è¯¢è§£æï¼Œä½¿ç”¨ SmartQueryParserã€‚
        è·å–å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œè°ƒç”¨æŸ¥è¯¢è§£æå™¨ï¼Œå¹¶å¤„ç†ç¼“å­˜ã€‚
        """
        method_start_time = time.time()
        logger.debug(
            f"Orchestrator: Calling QueryParser for '{user_query[:50]}...' with ConvID_DB: {conversation_id_for_db}")
        if not self.query_parser:
            logger.error("QueryParser not initialized in Orchestrator. Cannot parse query.")
            return await self._fallback_query_parsing(user_query)

        context_for_parser: Dict[str, Any] = {}
        if conversation_id_for_db and self.conversation_manager:
            try:
                context_for_parser = self.conversation_manager.get_context(
                    conversation_id_for_db)  # Implemented in ConversationManager
                logger.debug(
                    f"Context for QueryParser (ConvID {conversation_id_for_db}): Found {len(context_for_parser.get('recent_history', []))} history messages.")
            except Exception as e_ctx:
                logger.error(
                    f"Failed to get context for ConvID {conversation_id_for_db} from ConversationManager: {e_ctx}")

        cache_key = self._generate_query_cache_key(user_query, context_for_parser)  # Implemented
        cached_result = self._get_cached_result(cache_key)  # Implemented
        if cached_result and isinstance(cached_result, QueryAnalysisResult):
            self.orchestrator_stats['cache_hits'] += 1
            logger.info(f"QueryParser cache hit for: '{user_query[:50]}...'")
            return cached_result

        self.orchestrator_stats['cache_misses'] += 1
        logger.info(f"QueryParser cache miss. Executing AI parsing for: '{user_query[:50]}...'")

        try:
            query_analysis_result: QueryAnalysisResult = await self.query_parser.parse_complex_query(user_query,
                                                                                                     context_for_parser)
            if query_analysis_result and query_analysis_result.confidence_score > self.config.get(
                    'min_confidence_threshold_parsing_for_cache', 0.5):
                self._cache_result(cache_key, query_analysis_result)  # Implemented
            elif not query_analysis_result:
                logger.warning(f"QueryParser returned None for query: '{user_query[:50]}...'. Using fallback.")
                query_analysis_result = await self._fallback_query_parsing(user_query)  # Implemented
        except Exception as e_parse:
            logger.error(f"Error during query parsing by SmartQueryParser: {e_parse}\n{traceback.format_exc()}")
            query_analysis_result = await self._fallback_query_parsing(user_query)

        logger.debug(
            f"Query parsing took {time.time() - method_start_time:.3f}s. Complexity: {query_analysis_result.complexity.value if query_analysis_result else 'N/A'}")
        return query_analysis_result

    def _generate_query_cache_key(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        ä¸ºæŸ¥è¯¢å’Œå¯é€‰çš„ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€ä¸ªä¸€è‡´çš„ã€å¯ç”¨äºç¼“å­˜çš„å“ˆå¸Œé”®ã€‚
        ä½¿ç”¨ CustomJSONEncoder å¤„ç† context ä¸­å¯èƒ½å­˜åœ¨çš„å¤æ‚å¯¹è±¡ï¼ˆå¦‚ datetimeï¼‰ã€‚
        """
        try:
            # å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œè§„èŒƒåŒ–æ’åºï¼Œä»¥ç¡®ä¿ç›¸åŒå†…å®¹çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç›¸åŒçš„é”®
            context_str = json.dumps(context or {}, sort_keys=True, cls=CustomJSONEncoder, ensure_ascii=False)
        except TypeError as te:
            logger.warning(
                f"Failed to serialize context for cache key due to TypeError: {te}. Using empty context for key.")
            context_str = "{}"  # Fallback to empty context string if serialization fails

        cache_data = f"{query}_{context_str}"
        return hashlib.md5(cache_data.encode('utf-8')).hexdigest()

    def _get_cached_result(self, cache_key: str) -> Optional[Union[QueryAnalysisResult, ProcessingResult]]:
        """
        ä»ç¼“å­˜ä¸­è·å–ç»“æœã€‚å¦‚æœæ‰¾åˆ°ä¸”æœªè¿‡æœŸï¼Œåˆ™è¿”å›ç¼“å­˜æ•°æ®ã€‚
        å¦åˆ™ï¼Œç§»é™¤è¿‡æœŸæ¡ç›®å¹¶è¿”å› Noneã€‚
        """
        if not self.config.get('enable_intelligent_caching', False):
            return None

        if cache_key in self.result_cache:
            cache_entry = self.result_cache[cache_key]
            if (time.time() - cache_entry['timestamp']) < self.cache_ttl:
                logger.debug(f"Cache HIT for key: {cache_key[:10]}...")
                self.orchestrator_stats['cache_hits'] = self.orchestrator_stats.get('cache_hits', 0) + 1
                return cache_entry['data']
            else:
                logger.debug(f"Cache EXPIRED for key: {cache_key[:10]}... Removing.")
                del self.result_cache[cache_key]

        logger.debug(f"Cache MISS for key: {cache_key[:10]}...")
        self.orchestrator_stats['cache_misses'] = self.orchestrator_stats.get('cache_misses', 0) + 1
        return None

    def _cache_result(self, cache_key: str, result: Union[QueryAnalysisResult, ProcessingResult]):
        """
        å°†ç»“æœï¼ˆQueryAnalysisResult æˆ– ProcessingResultï¼‰å­˜å…¥ç¼“å­˜ã€‚
        ç®¡ç†ç¼“å­˜å¤§å°ï¼Œå¦‚æœè¶…å‡ºé™åˆ¶åˆ™ç§»é™¤æœ€æ—§çš„æ¡ç›®ã€‚
        """
        if not self.config.get('enable_intelligent_caching', False):
            return

        max_cache_size = self.config.get('max_cache_size', 100)
        if len(self.result_cache) >= max_cache_size:
            try:
                # ç®€å•çš„LRUè¿‘ä¼¼ï¼šç§»é™¤æ—¶é—´æˆ³æœ€æ—©çš„æ¡ç›®
                oldest_key = min(self.result_cache, key=lambda k: self.result_cache[k]['timestamp'])
                del self.result_cache[oldest_key]
                logger.debug(f"Cache max size ({max_cache_size}) reached. Removed oldest entry: {oldest_key[:10]}...")
            except ValueError:  # ç¼“å­˜å¯èƒ½åœ¨å¹¶å‘æƒ…å†µä¸‹å˜ç©º
                pass
            except Exception as e:
                logger.error(f"Error during cache eviction: {e}")

        self.result_cache[cache_key] = {
            'data': result,  # å¯ä»¥æ˜¯ QueryAnalysisResult æˆ– ProcessingResult
            'timestamp': time.time()
        }
        logger.debug(f"Cached result for key: {cache_key[:10]}... Cache size: {len(self.result_cache)}")

    def _update_orchestrator_stats(self, result: ProcessingResult):
        """æ›´æ–°ç¼–æ’å™¨çš„æ€§èƒ½å’Œä½¿ç”¨ç»Ÿè®¡ã€‚"""
        if not isinstance(result, ProcessingResult):
            logger.warning(f"Attempted to update stats with invalid result type: {type(result)}")
            return

        # total_queries åœ¨ process_intelligent_query å¼€å§‹æ—¶å·²å¢åŠ 
        total_queries = self.orchestrator_stats.get('total_queries', 0)
        if total_queries == 0:
            logger.warning(
                "Total queries is 0 in stats, cannot calculate averages. This might happen if called before total_queries is incremented.")
            # Potentially increment here if it's guaranteed this method is called once per query
            # self.orchestrator_stats['total_queries'] = 1
            # total_queries = 1
            return  # Avoid division by zero if total_queries is still 0

        if result.success:
            self.orchestrator_stats['successful_queries'] = self.orchestrator_stats.get('successful_queries', 0) + 1
        else:  # failed_queries å·²ç»åœ¨ _handle_processing_error æˆ– _handle_processing_error_sync ä¸­å¢åŠ 
            pass
            # self.orchestrator_stats['failed_queries'] = self.orchestrator_stats.get('failed_queries', 0) + 1
            # if result.error_info and isinstance(result.error_info, dict):
            #     error_type = result.error_info.get('error_type', 'unknown_in_stats_update')
            #     self.orchestrator_stats['error_types'][error_type] = \
            #         self.orchestrator_stats['error_types'].get(error_type, 0) + 1

        # å®‰å…¨åœ°è®¡ç®—å¹³å‡å€¼
        current_avg_time = float(self.orchestrator_stats.get('avg_processing_time', 0.0))
        self.orchestrator_stats['avg_processing_time'] = \
            (current_avg_time * (total_queries - 1) + float(result.total_processing_time)) / total_queries

        current_avg_confidence = float(self.orchestrator_stats.get('avg_confidence_score', 0.0))
        self.orchestrator_stats['avg_confidence_score'] = \
            (current_avg_confidence * (total_queries - 1) + float(result.confidence_score)) / total_queries

        for processor_name in result.processors_used:  # processors_used is List[str]
            if isinstance(processor_name, str):
                self.orchestrator_stats['processor_usage'][processor_name] = \
                    self.orchestrator_stats['processor_usage'].get(processor_name, 0) + 1

        # AIåä½œç»Ÿè®¡
        ai_collab_summary = result.ai_collaboration_summary
        if isinstance(ai_collab_summary, dict) and \
                ai_collab_summary.get('claude_used_in_process') and \
                ai_collab_summary.get('gpt_used_in_process'):
            self.orchestrator_stats['ai_collaboration_count'] = self.orchestrator_stats.get('ai_collaboration_count',
                                                                                            0) + 1

        logger.debug(f"Orchestrator stats updated for QueryID: {result.query_id}. Total queries: {total_queries}")

    def _log_ai_response_to_conversation(self, conversation_id_for_db: int,
                                         final_result: ProcessingResult, query_id: str):
        """è¾…åŠ©æ–¹æ³•ï¼šå°†AIå“åº”å’Œå¯è§†åŒ–å†…å®¹è®°å½•åˆ°å¯¹è¯å†å²ä¸­ã€‚"""
        if not self.conversation_manager:
            logger.warning(
                f"QueryID: {query_id} - ConversationManager not available. Skipping logging AI response to ConvID {conversation_id_for_db}.")
            return
        try:
            # ç¡®ä¿ conversation_id_for_db æ˜¯æ•´æ•° (ConversationManager çš„æ–¹æ³•æœŸæœ› int)
            if not isinstance(conversation_id_for_db, int):
                logger.error(
                    f"QueryID: {query_id} - Invalid conversation_id_for_db type: {type(conversation_id_for_db)}. Must be int for DB operations with ConversationManager. Skipping log.")
                return

            # æ·»åŠ AIå“åº”æ¶ˆæ¯
            # add_message è¿”å›æ–°æ¶ˆæ¯çš„ ID
            ai_message_id = self.conversation_manager.add_message(
                conversation_id=conversation_id_for_db,
                is_user=False,  # AIçš„å“åº”
                content=final_result.response_text,
                ai_model_used=str(final_result.processing_metadata.get('ai_models_invoked', [])),  # ä»å…ƒæ•°æ®è·å–
                ai_strategy=final_result.processing_strategy.value,  # ä½¿ç”¨æšä¸¾å€¼
                processing_time=final_result.total_processing_time,
                confidence_score=final_result.confidence_score
            )
            logger.debug(
                f"QueryID: {query_id} - AI response (MessageID in DB: {ai_message_id}) logged to ConvID {conversation_id_for_db}.")

            # æ·»åŠ å¯è§†åŒ–å†…å®¹åˆ° message_visuals è¡¨
            if final_result.visualizations and ai_message_id > 0:  # ç¡®ä¿æœ‰æ¶ˆæ¯ID
                for vis_idx, vis_data_dict in enumerate(final_result.visualizations):
                    if not isinstance(vis_data_dict, dict):
                        logger.warning(
                            f"QueryID: {query_id} - Invalid visual data for MsgID {ai_message_id}: {vis_data_dict}. Skipping.")
                        continue
                    try:
                        self.conversation_manager.add_visual(
                            message_id=ai_message_id,
                            visual_type=vis_data_dict.get('type', 'chart'),  # 'type' åœ¨ visualization dict ä¸­
                            visual_order=vis_idx,  # ç®€å•çš„é¡ºåº
                            title=vis_data_dict.get('title', 'ç”Ÿæˆçš„å›¾è¡¨'),
                            # 'data_payload' æˆ– 'data' é”®å¯èƒ½åŒ…å«å›¾è¡¨çš„å®é™…æ•°æ®/é…ç½®
                            data=vis_data_dict.get('data_payload', vis_data_dict.get('data', {}))
                        )
                    except Exception as e_vis_add:
                        logger.error(
                            f"QueryID: {query_id} - Failed to add visual to MsgID {ai_message_id} for ConvID {conversation_id_for_db}: {e_vis_add}")
                logger.debug(
                    f"QueryID: {query_id} - {len(final_result.visualizations)} visuals logged to MessageID {ai_message_id} for ConvID {conversation_id_for_db}.")
            elif final_result.visualizations and ai_message_id <= 0:
                logger.warning(
                    f"QueryID: {query_id} - Got visuals but AI message ID ({ai_message_id}) is invalid. Visuals not logged for ConvID {conversation_id_for_db}.")


        except Exception as conv_err:
            # æ•è· ConversationManager å¯èƒ½æŠ›å‡ºçš„ä»»ä½•å¼‚å¸¸
            logger.error(
                f"QueryID: {query_id} - Error logging AI response/visuals to ConvID {conversation_id_for_db}: {conv_err}\n{traceback.format_exc()}")
    def _handle_processing_error_sync(self, session_id: str, query_id: str,
                                      user_query: str, error_msg: str,
                                      total_time_on_error: float,
                                      conversation_id: Optional[str] = None) -> ProcessingResult:
        """
        åŒæ­¥å¤„ç†æµç¨‹ä¸­çš„å…³é”®é”™è¯¯ï¼Œç”Ÿæˆé”™è¯¯ç»“æœå¯¹è±¡ã€‚
        ç”¨äºåˆå§‹åŒ–å¤±è´¥ç­‰æ— æ³•è¿›å…¥å®Œæ•´å¼‚æ­¥æµç¨‹çš„åœºæ™¯ã€‚
        """
        # æ³¨æ„ï¼šæ­¤æ–¹æ³•æ˜¯åŒæ­¥çš„ï¼Œä¸åº”è°ƒç”¨ä»»ä½•å¼‚æ­¥æ“ä½œæˆ–ä¾èµ–AIç”Ÿæˆé™çº§å“åº”ã€‚

        current_stats = getattr(self, 'orchestrator_stats', self._default_stats())
        current_config = getattr(self, 'config', self._load_orchestrator_config())

        current_stats['failed_queries'] = current_stats.get('failed_queries', 0) + 1
        error_type = self._classify_error(error_msg)  # _classify_error å¿…é¡»æ˜¯åŒæ­¥çš„
        current_stats['error_types'][error_type] = \
            current_stats['error_types'].get(error_type, 0) + 1

        logger.error(f"QueryID: {query_id} - SYNC Handling CRITICAL processing error: {error_msg}, Type: {error_type}.")

        simple_fallback_text = f"æŠ±æ­‰ï¼Œç³»ç»Ÿåœ¨å¤„ç†æ‚¨çš„æŸ¥è¯¢ '{user_query[:30]}...' æ—¶é‡åˆ°ä¸€ä¸ªå…³é”®é”™è¯¯ã€‚"

        debug_mode = current_config.get("DEBUG", False)

        if debug_mode:
            simple_fallback_text += f" é”™è¯¯è¯¦æƒ…: {error_msg}"
        else:
            simple_fallback_text += " æŠ€æœ¯å›¢é˜Ÿå·²è¢«é€šçŸ¥ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»æ”¯æŒã€‚"

        error_processing_strategy = OrchestratorProcessingStrategy.ERROR_HANDLING

        error_result = ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            response_text=simple_fallback_text,
            insights=[],
            key_metrics={},
            visualizations=[],
            error_info={
                'error_type': error_type,
                'message': error_msg,
                'original_query': user_query,
                'trace': traceback.format_exc() if debug_mode else "Error trace hidden in sync error handler."
            },
            processing_strategy=error_processing_strategy,
            processors_used=['SyncErrorHandling'],
            total_processing_time=total_time_on_error,
            conversation_id=conversation_id,
            timestamp=datetime.now().isoformat()
        )

        # å¯¹äºåˆå§‹åŒ–é˜¶æ®µçš„é”™è¯¯ï¼Œå®Œæ•´çš„ç»Ÿè®¡æ›´æ–°å¯èƒ½ä¸é€‚ç”¨æˆ–ä¸å‡†ç¡®
        # æ­¤å¤„ä»…æ›´æ–°äº†å¤±è´¥è®¡æ•°å’Œé”™è¯¯ç±»å‹ã€‚
        # å¦‚æœéœ€è¦ï¼Œå¯ä»¥æœ‰æ¡ä»¶åœ°è°ƒç”¨ _update_orchestrator_statsï¼Œä½†è¦ç¡®ä¿ total_queries å·²æ­£ç¡®è®¡æ•°ã€‚
        # self._update_orchestrator_stats(error_result) # æš‚æ—¶æ³¨é‡Šä»¥é¿å…æ½œåœ¨é—®é¢˜

        return error_result
    async def _determine_processing_strategy(self, query_analysis: QueryAnalysisResult, preferences: Optional[
        Dict[str, Any]] = None) -> OrchestratorProcessingStrategy:
        """æ™ºèƒ½ç¡®å®šå¤„ç†ç­–ç•¥ï¼ŒåŸºäºè§„åˆ™å’ŒAIè¾…åŠ©ã€‚"""
        method_start_time = time.time()
        logger.debug(
            f"Determining processing strategy for query type: {query_analysis.query_type.value}, complexity: {query_analysis.complexity.value}")

        if preferences and 'processing_strategy' in preferences:
            try:
                preferred_strategy = OrchestratorProcessingStrategy(preferences['processing_strategy'])
                logger.info(f"Using user preferred strategy: {preferred_strategy.value}")
                return preferred_strategy
            except ValueError:
                logger.warning(
                    f"Invalid processing_strategy in preferences: {preferences['processing_strategy']}. Defaulting.")

        complexity_from_parser: QueryParserComplexity = query_analysis.complexity
        strategy_map = {
            QueryParserComplexity.SIMPLE: OrchestratorProcessingStrategy.DIRECT_RESPONSE,
            QueryParserComplexity.MEDIUM: OrchestratorProcessingStrategy.SINGLE_PROCESSOR,
            QueryParserComplexity.COMPLEX: OrchestratorProcessingStrategy.MULTI_PROCESSOR,
            QueryParserComplexity.EXPERT: OrchestratorProcessingStrategy.FULL_PIPELINE,
        }
        determined_strategy = strategy_map.get(complexity_from_parser, OrchestratorProcessingStrategy.SINGLE_PROCESSOR)

        # AIè¾…åŠ©å†³ç­– (ä¸­æ–‡æç¤ºè¯)
        if self.config.get('enable_smart_routing', True) and self.claude_client:
            try:
                ai_prompt_for_strategy = f"""
                ä½œä¸ºæ™ºèƒ½é‡‘èåˆ†æç³»ç»Ÿçš„å†³ç­–æ ¸å¿ƒï¼Œè¯·ä¸ºä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢çš„åˆ†æç»“æœé€‰æ‹©æœ€åˆé€‚çš„å¤„ç†ç­–ç•¥ã€‚
                ç”¨æˆ·æŸ¥è¯¢çš„åˆ†ææ¦‚è¦:
                - å¤æ‚åº¦è¯„ä¼°: {query_analysis.complexity.value}
                - æŸ¥è¯¢æ„å›¾ç±»å‹: {query_analysis.query_type.value}
                - æ¶‰åŠä¸šåŠ¡åœºæ™¯: {query_analysis.business_scenario.value if query_analysis.business_scenario else 'æœªæ˜ç¡®'}
                - ç³»ç»Ÿè§£æç½®ä¿¡åº¦: {query_analysis.confidence_score:.2f}
                - é¢„è®¡æ‰§è¡Œæ­¥éª¤æ•°: {len(query_analysis.execution_plan)}

                å¯é€‰çš„å¤„ç†ç­–ç•¥åŒ…æ‹¬:
                - "direct_response": ç”¨äºç®€å•ä¿¡æ¯è·å–æˆ–æ— éœ€å¤æ‚è®¡ç®—çš„æŸ¥è¯¢ã€‚
                - "single_processor": ç”¨äºéœ€è¦å•ä¸€ç±»å‹æ ¸å¿ƒå¤„ç†å™¨ï¼ˆå¦‚ä»…å½“å‰æ•°æ®ã€ä»…å†å²åˆ†æã€æˆ–ä»…é¢„æµ‹ï¼‰çš„æ ‡å‡†æŸ¥è¯¢ã€‚
                - "multi_processor": ç”¨äºæ¶‰åŠå¤šç§åˆ†æç»´åº¦ï¼Œå¯èƒ½éœ€è¦å¤šä¸ªå¤„ç†å™¨åä½œçš„å¤æ‚æŸ¥è¯¢ã€‚
                - "full_pipeline": ç”¨äºéœ€è¦ç³»ç»Ÿè¿›è¡Œæœ€å…¨é¢ã€æœ€æ·±åº¦åˆ†æçš„ä¸“å®¶çº§æŸ¥è¯¢ï¼Œå¯èƒ½è°ƒåŠ¨æ‰€æœ‰ç›¸å…³å¤„ç†å’Œåˆ†ææ¨¡å—ã€‚

                å½“å‰åŸºäºè§„åˆ™çš„åˆæ­¥å»ºè®®ç­–ç•¥æ˜¯: {determined_strategy.value}
                è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯æŸ¥è¯¢çš„å¤æ‚åº¦å’Œè§£æç½®ä¿¡åº¦ï¼Œç»™å‡ºæ‚¨çš„æœ€ç»ˆç­–ç•¥é€‰æ‹©ã€‚
                å¦‚æœè§£æç½®ä¿¡åº¦è¾ƒä½ï¼ˆä¾‹å¦‚ä½äº0.7ï¼‰ï¼Œè¯·å€¾å‘äºé€‰æ‹©æ›´ç®€å•ã€æ›´ç¨³å¥çš„ç­–ç•¥ã€‚
                è¯·ä»…è¿”å›ç­–ç•¥çš„è‹±æ–‡åå°å†™å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ï¼š"single_processor"ã€‚
                """
                # å‡è®¾ClaudeClientçš„analyze_complex_queryæ–¹æ³•ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯promptï¼Œç¬¬äºŒä¸ªæ˜¯ä¸Šä¸‹æ–‡ï¼ˆå¯ä¸ºç©ºï¼‰
                ai_suggestion_result = await self.claude_client.analyze_complex_query(ai_prompt_for_strategy, {})
                if ai_suggestion_result.get('success'):
                    suggested_strategy_str = ai_suggestion_result.get('response', ai_suggestion_result.get('analysis',
                                                                                                           '')).strip().lower()
                    try:
                        ai_determined_strategy = OrchestratorProcessingStrategy(suggested_strategy_str)
                        logger.info(
                            f"AI suggested strategy: {ai_determined_strategy.value} (Rule-based was: {determined_strategy.value})")
                        determined_strategy = ai_determined_strategy  # AI å»ºè®®è¦†ç›–è§„åˆ™
                    except ValueError:
                        logger.warning(f"AIè¿”å›äº†æ— æ•ˆçš„ç­–ç•¥å­—ç¬¦ä¸²: '{suggested_strategy_str}'")
                else:
                    logger.warning(f"AIç­–ç•¥å†³ç­–å¤±è´¥: {ai_suggestion_result.get('error', 'Unknown AI error')}")
            except Exception as e_ai_strat:
                logger.error(f"AIç­–ç•¥å†³ç­–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e_ai_strat}")

        if query_analysis.confidence_score < self.config.get('min_confidence_threshold_parsing', 0.7):
            logger.warning(
                f"è§£æç½®ä¿¡åº¦ä½ ({query_analysis.confidence_score:.2f}), æœ€ç»ˆç­–ç•¥ä» {determined_strategy.value} é™çº§ã€‚")
            if determined_strategy == OrchestratorProcessingStrategy.FULL_PIPELINE:
                determined_strategy = OrchestratorProcessingStrategy.MULTI_PROCESSOR
            elif determined_strategy == OrchestratorProcessingStrategy.MULTI_PROCESSOR:
                determined_strategy = OrchestratorProcessingStrategy.SINGLE_PROCESSOR
            elif determined_strategy == OrchestratorProcessingStrategy.SINGLE_PROCESSOR:
                determined_strategy = OrchestratorProcessingStrategy.DIRECT_RESPONSE

        logger.debug(
            f"Strategy determination took {time.time() - method_start_time:.3f}s. Final strategy: {determined_strategy.value}")
        return determined_strategy

    async def _orchestrate_data_acquisition(self, query_analysis: QueryAnalysisResult,
                                            strategy: OrchestratorProcessingStrategy) -> FetcherExecutionResult:
        """ç¼–æ’æ•°æ®è·å–æµç¨‹ï¼Œè°ƒç”¨ DataRequirementsAnalyzer å’Œ SmartDataFetcherã€‚"""
        method_start_time = time.time()
        logger.info(f"DataAcquisition: Analyzing requirements for query type '{query_analysis.query_type.value}'")
        if not self.data_requirements_analyzer: raise SystemError("DataRequirementsAnalyzer not initialized.")
        if not self.data_fetcher: raise SystemError("SmartDataFetcher not initialized.")

        data_plan: DataAcquisitionPlan = await self.data_requirements_analyzer.analyze_data_requirements(query_analysis)

        if not data_plan or not data_plan.api_call_plans:
            logger.warning(
                "DataRequirementsAnalyzer did not produce API call plans. This might be ok for very simple queries or indicate an issue.")
            # å³ä½¿æ²¡æœ‰è®¡åˆ’ï¼Œä¹Ÿè°ƒç”¨fetcherï¼Œå®ƒåº”è¯¥èƒ½å¤„ç†ç©ºè®¡åˆ’
            # æˆ–è€…å¦‚æœç»å¯¹éœ€è¦æ•°æ®ï¼Œå°è¯•è·å–åŸºç¡€æ•°æ®
            if strategy != OrchestratorProcessingStrategy.DIRECT_RESPONSE or query_analysis.query_type != QueryParserQueryType.GENERAL_KNOWLEDGE:
                logger.info(
                    "No API calls in plan, attempting fallback to fetch basic system data for non-direct/general queries.")
                # (Fallback logic from previous full version can be inserted here if needed)
                # For now, let SmartDataFetcher handle an empty plan if that's its design.
                # If not, we must ensure data_plan always has something or handle this case explicitly.
                # Assume SmartDataFetcher returns an ExecutionResult indicating no data if plan is empty.
                pass  # Allow execution of potentially empty plan

        logger.info(
            f"DataAcquisition: Executing data plan '{data_plan.plan_id if data_plan else 'N/A'}' with {len(data_plan.api_call_plans) if data_plan else 0} API calls.")
        fetch_result: FetcherExecutionResult = await self.data_fetcher.execute_data_acquisition_plan(
            data_plan)  # SmartDataFetcher handles empty plan

        logger.debug(
            f"Data acquisition took {time.time() - method_start_time:.3f}s. Fetch status: {fetch_result.execution_status.value if fetch_result.execution_status else 'N/A'}")
        return fetch_result

    async def _orchestrate_business_processing(self, query_analysis: QueryAnalysisResult,
                                               data_acquisition_result: FetcherExecutionResult,
                                               strategy: OrchestratorProcessingStrategy) -> Dict[str, Any]:
        # ++++++++++++++ æ·»åŠ æ­¤è¡Œ ++++++++++++++
        biz_proc_start_wall_time = time.time()
        # +++++++++++++++++++++++++++++++++++++

        logger.info(
            f"BusinessProcessing: Strategy '{strategy.value}', QueryType from Parser '{query_analysis.query_type.value}'")
        if not all([self.current_data_processor, self.historical_analysis_processor, self.prediction_processor,
                    self.financial_data_analyzer]):
            raise SystemError("One or more core processors/analyzers are not initialized for business processing.")

        processor_context = {'data_acquisition_result': data_acquisition_result, 'query_analysis': query_analysis}
        original_user_query = query_analysis.original_query

        processor_result_payload: Any = None
        processor_used_names: List[str] = []

        sum_of_reported_proc_times: float = 0.0
        confidences_from_procs: List[float] = []
        # biz_proc_metadata å·²ç§»åˆ° return è¯­å¥ä¸­åŠ¨æ€æ„å»ºï¼Œä»¥åŒ…å« ai_processing_time

        # --- SINGLE_PROCESSOR and DIRECT_RESPONSE ---
        if strategy == OrchestratorProcessingStrategy.DIRECT_RESPONSE:
            # ... (å¤„ç†é€»è¾‘) ...
            res_obj = await self.current_data_processor.process_current_data_query(original_user_query)
            processor_result_payload = res_obj
            processor_used_names.append(self.current_data_processor.__class__.__name__)
            sum_of_reported_proc_times = getattr(res_obj, 'processing_time', 0.1)
            confidences_from_procs.append(getattr(res_obj, 'response_confidence', 0.7))

        elif strategy == OrchestratorProcessingStrategy.SINGLE_PROCESSOR:
            # ... (å¤„ç†é€»è¾‘) ...
            qt_parser_enum: QueryParserQueryType = query_analysis.query_type
            selected_processor: Any = None

            if qt_parser_enum in [QueryParserQueryType.DATA_RETRIEVAL, QueryParserQueryType.CALCULATION,
                                  QueryParserQueryType.GENERAL_KNOWLEDGE]:  # å‡è®¾GENERAL_KNOWLEDGEå·²æ·»åŠ 
                selected_processor = self.current_data_processor
                res_obj = await selected_processor.process_current_data_query(original_user_query)
            elif qt_parser_enum in [QueryParserQueryType.TREND_ANALYSIS, QueryParserQueryType.COMPARISON]:
                selected_processor = self.historical_analysis_processor
                res_obj = await selected_processor.process_historical_analysis_query(original_user_query,
                                                                                     processor_context)
            elif qt_parser_enum in [QueryParserQueryType.PREDICTION, QueryParserQueryType.SCENARIO_SIMULATION,
                                    QueryParserQueryType.RISK_ASSESSMENT]:
                selected_processor = self.prediction_processor
                res_obj = await selected_processor.process_prediction_query(original_user_query, processor_context)
            else:
                selected_processor = self.current_data_processor  # Default
                res_obj = await selected_processor.process_current_data_query(original_user_query)

            processor_result_payload = res_obj
            processor_used_names.append(selected_processor.__class__.__name__)
            sum_of_reported_proc_times += getattr(res_obj, 'processing_time', 0.2)
            confidences_from_procs.append(
                getattr(res_obj, 'confidence_score',
                        getattr(res_obj, 'response_confidence',
                                getattr(res_obj, 'analysis_confidence',
                                        getattr(res_obj, 'prediction_confidence', 0.6)))))

        elif strategy in [OrchestratorProcessingStrategy.MULTI_PROCESSOR, OrchestratorProcessingStrategy.FULL_PIPELINE]:
            # ... (å¤šå¤„ç†å™¨/å…¨æµæ°´çº¿å¤„ç†é€»è¾‘ï¼Œä¸æ‚¨ä¹‹å‰æä¾›çš„å®Œæ•´ç‰ˆæœ¬ä¸€è‡´) ...
            multi_results_payload: Dict[str, Any] = {}
            tasks_to_run_coroutines_map: Dict[str, asyncio.Task] = {}  # type: ignore

            tasks_to_run_coroutines_map['current_data_analysis'] = asyncio.create_task(
                self.current_data_processor.process_current_data_query(original_user_query)
            )

            if query_analysis.query_type in [QueryParserQueryType.TREND_ANALYSIS, QueryParserQueryType.COMPARISON,
                                             QueryParserQueryType.PREDICTION, QueryParserQueryType.SCENARIO_SIMULATION,
                                             QueryParserQueryType.RISK_ASSESSMENT] or \
                    any(step.step_type == "trend_analysis" for step in query_analysis.execution_plan):
                tasks_to_run_coroutines_map['historical_analysis'] = asyncio.create_task(
                    self.historical_analysis_processor.process_historical_analysis_query(original_user_query,
                                                                                         processor_context)
                )

            if query_analysis.query_type in [QueryParserQueryType.PREDICTION, QueryParserQueryType.SCENARIO_SIMULATION,
                                             QueryParserQueryType.RISK_ASSESSMENT] or \
                    any(step.step_type == "prediction" for step in query_analysis.execution_plan):
                tasks_to_run_coroutines_map['prediction_analysis'] = asyncio.create_task(
                    self.prediction_processor.process_prediction_query(original_user_query, processor_context)
                )

            if strategy == OrchestratorProcessingStrategy.FULL_PIPELINE and self.financial_data_analyzer:
                scope_for_fda = query_analysis.business_scenario.value if query_analysis.business_scenario else 'financial_overview'
                time_range_days_fda = 30
                if query_analysis.time_requirements and query_analysis.time_requirements.get('time_range_days'):
                    time_range_days_fda = int(query_analysis.time_requirements['time_range_days'])

                tasks_to_run_coroutines_map['financial_deep_dive'] = asyncio.create_task(
                    self.financial_data_analyzer.analyze_business_performance(scope=scope_for_fda,
                                                                              time_range=time_range_days_fda
                                                                             )
                )

            logger.info(
                f"Executing {len(tasks_to_run_coroutines_map)} tasks for strategy {strategy.value}: {list(tasks_to_run_coroutines_map.keys())}")

            for name, task_coro in tasks_to_run_coroutines_map.items():
                try:
                    res_obj = await task_coro
                    multi_results_payload[name] = res_obj
                    processor_used_names.append(getattr(res_obj, '__class__', {}).__name__ or name)
                    sum_of_reported_proc_times += getattr(res_obj, 'processing_time', 0.1)
                    confidences_from_procs.append(
                        getattr(res_obj, 'confidence_score', getattr(res_obj, 'response_confidence', 0.6))
                    )
                except Exception as task_e:
                    multi_results_payload[name] = {"error": str(task_e), "details": traceback.format_exc()}
                    logger.error(f"Error processing task '{name}' in {strategy.value}: {task_e}")
                    confidences_from_procs.append(0.1)
            processor_result_payload = {"multi_results": multi_results_payload}
        else:
            logger.error(f"Unsupported processing strategy: {strategy.value}")
            raise ValueError(f"Unsupported processing strategy: {strategy.value}")

        # --- åœ¨æ‰€æœ‰ä¸šåŠ¡å¤„ç†é€»è¾‘å®Œæˆåè®¡ç®—å®é™…è€—æ—¶ ---
        combined_processing_time = time.time() - biz_proc_start_wall_time  # ç°åœ¨ biz_proc_start_wall_time å·²å®šä¹‰
        # --- ---

        overall_biz_confidence = sum(confidences_from_procs) / len(
            confidences_from_procs) if confidences_from_procs else 0.5

        # ç´¯åŠ å„å¤„ç†å™¨æŠ¥å‘Šçš„AIæ—¶é—´
        biz_proc_ai_time = 0.0
        if isinstance(processor_result_payload, dict) and 'multi_results' in processor_result_payload:
            for res_obj in processor_result_payload['multi_results'].values():
                if hasattr(res_obj, 'metadata') and isinstance(res_obj.metadata, dict):
                    biz_proc_ai_time += res_obj.metadata.get('ai_processing_time', 0.0)
        elif hasattr(processor_result_payload, 'metadata') and isinstance(processor_result_payload.metadata,
                                                                          dict):  # å•ä¸ªå¯¹è±¡
            biz_proc_ai_time = processor_result_payload.metadata.get('ai_processing_time', 0.0)

        return {
            'processing_type': strategy.value,
            'processor_used': ", ".join(list(set(processor_used_names))),
            'primary_result': processor_result_payload,
            'confidence_score': overall_biz_confidence,
            'processing_time': combined_processing_time,
            'metadata': {
                'data_input_source': 'from_smart_data_fetcher',
                'sum_of_individual_processor_reported_times': sum_of_reported_proc_times,
                'ai_processing_time': biz_proc_ai_time  # è®°å½•ä¸šåŠ¡å¤„ç†é˜¶æ®µçš„AIè€—æ—¶
            }
        }

    async def _orchestrate_insight_generation(self, business_result_data: Dict[str, Any],
                                              query_analysis: QueryAnalysisResult,
                                              data_acquisition_result: FetcherExecutionResult) -> List[BusinessInsight]:
        logger.debug("Orchestrator: Calling InsightGenerator...")
        if not self.insight_generator: raise SystemError("InsightGenerator not initialized.")

        analysis_objects_for_insights = []
        primary_res_payload = business_result_data.get('primary_result')  # This is object or dict of objects

        if isinstance(primary_res_payload, dict) and 'multi_results' in primary_res_payload:
            for res_key, res_obj in primary_res_payload['multi_results'].items():
                if res_obj and not (isinstance(res_obj, dict) and 'error' in res_obj):
                    analysis_objects_for_insights.append(res_obj)
        elif primary_res_payload and not (isinstance(primary_res_payload, dict) and 'error' in primary_res_payload):
            analysis_objects_for_insights.append(primary_res_payload)

        if not analysis_objects_for_insights:
            logger.warning("No valid business processing results to generate insights from.")
            return []

        # Prepare context for insight generator
        conv_id_for_db = None
        # conversation_id is from process_intelligent_query's parameters
        # If process_intelligent_query received a valid string conversation_id that was converted to int
        # that int conversation_id_for_db can be used here.
        # For simplicity, let's assume it's available if needed by get_context
        # This context logic needs to be robust based on how conv_id is managed.
        # For now, passing empty context if conv_id not available or invalid.
        parent_conversation_id_str = query_analysis.processing_metadata.get(
            'conversation_id_from_query_parsing') if query_analysis.processing_metadata else None
        if parent_conversation_id_str:
            try:
                conv_id_for_db = int(parent_conversation_id_str)
            except:
                pass

        user_context_for_insights = self.conversation_manager.get_context(
            conv_id_for_db) if conv_id_for_db and self.conversation_manager else {}

        insights, metadata = await self.insight_generator.generate_comprehensive_insights(
            analysis_results=analysis_objects_for_insights,
            user_context=user_context_for_insights,
            focus_areas=self._determine_focus_areas(query_analysis)  # Implemented
        )

        # Store AI time from insight generation
        ai_time_insights = metadata.get('ai_time', 0.0) if isinstance(metadata, dict) else 0.0
        business_result_data.setdefault('metadata', {}).update({'insight_gen_ai_time': ai_time_insights})
        business_result_data['insight_generation_metadata'] = metadata
        return insights

    async def _generate_intelligent_response(self, user_query: str, query_analysis: QueryAnalysisResult,
                                             business_result_data: Dict[str, Any],
                                             insights_list: List[BusinessInsight]) -> str:
        logger.debug("Orchestrator: Generating intelligent response text using CHINESE prompts.")
        if not self.claude_client and not self.gpt_client:
            logger.warning("AIå®¢æˆ·ç«¯ (Claude/GPT) æœªé…ç½®ï¼Œä½¿ç”¨åŸºç¡€æ¨¡æ¿ç”Ÿæˆå“åº”ã€‚")
            return self._generate_basic_response(business_result_data, insights_list)  # Implemented

        summarized_analysis = self._summarize_business_result_for_ai(business_result_data)  # Implemented
        summarized_insights = []
        for insight in insights_list[:3]:
            title = getattr(insight, 'title', "é‡è¦æ´å¯Ÿ")
            summary = getattr(insight, 'summary', "...")
            summarized_insights.append(f"{title}: {summary}")

        # --- CHINESE PROMPT ---
        prompt = f"""
        ä½œä¸ºä¸€ä½èµ„æ·±çš„AIé‡‘èä¸šåŠ¡é¡¾é—®ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½å…¨é¢ã€ä¸“ä¸šä¸”æ˜“äºç†è§£çš„ä¸­æ–‡å›ç­”ã€‚

        ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢:
        "{user_query}"

        ç³»ç»Ÿå¯¹æŸ¥è¯¢çš„åˆ†ææ¦‚è¦:
        - ç”¨æˆ·æ„å›¾æ ¸å¿ƒ: {query_analysis.query_type.value if query_analysis.query_type else "æœªçŸ¥"}
        - æ¶‰åŠä¸šåŠ¡åœºæ™¯: {query_analysis.business_scenario.value if query_analysis.business_scenario else "é€šç”¨"}
        - åˆ†æå¤æ‚åº¦è¯„ä¼°: {query_analysis.complexity.value if query_analysis.complexity else "ä¸­ç­‰"}

        æ ¸å¿ƒåˆ†æç»“æœç²¾ç‚¼ (JSONæ ¼å¼):
        {json.dumps(summarized_analysis, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}

        å…³é”®ä¸šåŠ¡æ´å¯Ÿæè¦:
        {chr(10).join([f"  - {si}" for si in summarized_insights]) if summarized_insights else "  - æš‚æœªç”Ÿæˆç‰¹åˆ«çš„ä¸šåŠ¡æ´å¯Ÿã€‚"}

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„å’Œè¦æ±‚ç”Ÿæˆæœ€ç»ˆç­”å¤ï¼ˆä½¿ç”¨Markdownæ ¼å¼ï¼‰ï¼š

        ### **å›ç­”æ ¸å¿ƒé—®é¢˜**
        [æ¸…æ™°ã€ç›´æ¥åœ°å›ç­”ç”¨æˆ·æŸ¥è¯¢çš„æ ¸å¿ƒé—®é¢˜ã€‚]

        ### **å…³é”®æ•°æ®ä¸å‘ç°**
        [åˆ—å‡ºå¹¶è§£é‡Š1-3ä¸ªæœ€é‡è¦çš„å›¾è¡¨æˆ–åˆ†æå‘ç°ã€‚ä¾‹å¦‚ï¼š**æ€»ä½™é¢è¶‹åŠ¿ï¼šè¿‡å»30å¤©å†…å¢é•¿äº†15%ï¼Œè¾¾åˆ°Â¥XXXã€‚**]

        ### **ä¸šåŠ¡è§£è¯»ä¸æ´å¯Ÿ**
        [ï¼ˆå¦‚æœæ´å¯Ÿåˆ—è¡¨ä¸ä¸ºç©ºï¼‰ç®€è¦è§£é‡Šè¿™äº›å‘ç°å¯¹ä¸šåŠ¡çš„æ½œåœ¨å½±å“æˆ–æ„ä¹‰ï¼Œè‡ªç„¶åœ°èå…¥æ´å¯Ÿçš„ç»“è®ºã€‚ä¾‹å¦‚ï¼š*æ´å¯Ÿæ˜¾ç¤ºï¼Œè¿‘æœŸç”¨æˆ·æ´»è·ƒåº¦æå‡å¯èƒ½ä¸XXæ´»åŠ¨æœ‰å…³ï¼Œå»ºè®®æŒç»­å…³æ³¨å¹¶ä¼˜åŒ–ç›¸å…³ç­–ç•¥ã€‚*]

        ### **å»ºè®®è¡ŒåŠ¨**
        [ï¼ˆå¦‚æœæ´å¯ŸåŒ…å«æ˜ç¡®å»ºè®®ï¼‰æä¾›1-2æ¡æœ€å…³é”®ã€æœ€å…·å¯æ“ä½œæ€§çš„å»ºè®®ã€‚]

        ### **æ€»ç»“ä¸å±•æœ›** (å¯é€‰)
        [å¯¹æ•´ä½“æƒ…å†µè¿›è¡Œç®€çŸ­æ€»ç»“æˆ–å¯¹æœªæ¥è¶‹åŠ¿åšåˆæ­¥å±•æœ›ã€‚]

        **è¯·æ³¨æ„ï¼š**
        - è¯­è¨€é£æ ¼éœ€ä¸“ä¸šã€å®¢è§‚ã€æ•°æ®é©±åŠ¨ï¼ŒåŒæ—¶ç¡®ä¿ä¸šåŠ¡äººå‘˜èƒ½å¤Ÿè½»æ¾ç†è§£ã€‚
        - é¿å…ä½¿ç”¨è¿‡äºæŠ€æœ¯æ€§çš„æœ¯è¯­ï¼Œå¿…è¦æ—¶è¿›è¡Œè§£é‡Šã€‚
        - å¦‚æœåˆ†æç»“æœä¸­å­˜åœ¨ä¸ç¡®å®šæ€§ã€é£é™©æˆ–æ•°æ®å±€é™æ€§ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®ã€å®¡æ…åœ°æŒ‡å‡ºã€‚
        - å¦‚æœæ•°æ®ä¸è¶³æˆ–åˆ†æç½®ä¿¡åº¦è¾ƒä½ï¼Œè¯·å¦è¯šè¯´æ˜ï¼Œå¹¶å¯ä»¥ç¤¼è²Œåœ°å»ºè®®ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯æˆ–è°ƒæ•´æŸ¥è¯¢èŒƒå›´ã€‚
        - æ€»å›ç­”é•¿åº¦å»ºè®®æ§åˆ¶åœ¨300-800å­—å·¦å³ï¼Œæ ¹æ®ä¿¡æ¯é‡çµæ´»è°ƒæ•´ã€‚
        """

        ai_client_to_use = self.claude_client if self.claude_client else self.gpt_client  # ä¼˜å…ˆClaude
        if not ai_client_to_use: return self._generate_basic_response(business_result_data, insights_list)

        try:
            ai_response_data = None
            logger.info(f"ä½¿ç”¨ {ai_client_to_use.__class__.__name__} ç”Ÿæˆæœ€ç»ˆå“åº”æ–‡æœ¬ã€‚")
            # å‡è®¾AIå®¢æˆ·ç«¯æœ‰é€šç”¨çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œæˆ–è€…éœ€è¦æ ¹æ®ç±»å‹é€‰æ‹©
            if isinstance(ai_client_to_use, ClaudeClient) and hasattr(ai_client_to_use,
                                                                      'generate_text'):  # å‡è®¾Claudeæœ‰æ­¤æ–¹æ³•
                ai_response_data = await ai_client_to_use.generate_text(prompt, max_tokens=1500)  # Claudeçš„å…¸å‹æ–¹æ³•
            elif isinstance(ai_client_to_use, OpenAIClient) and hasattr(ai_client_to_use, 'generate_completion'):
                ai_response_data = await ai_client_to_use.generate_completion(prompt, max_tokens=1500)  # OpenAIçš„å…¸å‹æ–¹æ³•
            else:  # Fallback for unknown client method signature
                logger.warning(f"AIå®¢æˆ·ç«¯ {ai_client_to_use.__class__.__name__} æœªæ‰¾åˆ°æ ‡å‡†çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ã€‚")
                return self._generate_basic_response(business_result_data, insights_list)

            # è§£æAIå“åº” (ä¸åŒå®¢æˆ·ç«¯çš„å“åº”ç»“æ„å¯èƒ½ä¸åŒ)
            if ai_response_data and ai_response_data.get('success', False):  # å‡è®¾å“åº”ä¸­æœ‰ 'success' æ ‡å¿—
                response_content = ai_response_data.get('text', ai_response_data.get('response', ai_response_data.get(
                    'completion')))  # å°è¯•ä¸åŒå¯èƒ½çš„é”®
                if response_content and isinstance(response_content, str):
                    return response_content.strip()
                else:
                    logger.warning(f"AIå“åº”å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®: {response_content}")
                    return self._generate_basic_response(business_result_data, insights_list)
            else:
                logger.warning(
                    f"AIç”Ÿæˆå“åº”æ–‡æœ¬å¤±è´¥: {ai_response_data.get('error', 'æœªçŸ¥AIé”™è¯¯') if ai_response_data else 'AIå®¢æˆ·ç«¯è¿”å›ç©º'}")
                return self._generate_basic_response(business_result_data, insights_list)
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ™ºèƒ½å“åº”æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}")
            return self._generate_basic_response(business_result_data, insights_list)

    async def _generate_visualizations(self, business_result_data: Dict[str, Any],
                                       query_analysis: QueryAnalysisResult,
                                       insights_list: List[BusinessInsight]) -> List[Dict[str, Any]]:
        # (ä¸ä¸Šä¸€ç‰ˆæœ¬å®Œæ•´å®ç°ä¸€è‡´ï¼Œç¡®ä¿ä»çœŸå®ä¸šåŠ¡ç»“æœä¸­æå–æ•°æ®å¹¶è°ƒç”¨ self.chart_generator)
        logger.debug("Orchestrator: Generating visualizations...")
        visualizations: List[Dict[str, Any]] = []
        if not self.chart_generator:
            logger.warning("ChartGeneratoræœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¯è§†åŒ–ç”Ÿæˆã€‚")
            return visualizations

        try:
            raw_numeric_metrics = self._extract_raw_numeric_metrics(business_result_data)

            if raw_numeric_metrics and len(raw_numeric_metrics) >= 2 and len(raw_numeric_metrics) <= 10:
                chart_labels = list(raw_numeric_metrics.keys())
                chart_values = list(raw_numeric_metrics.values())
                try:
                    bar_chart_result = self.chart_generator.generate_bar_chart(
                        data={'category': chart_labels, 'value': chart_values},
                        title="å…³é”®æŒ‡æ ‡å¿«ç…§", x_column='category', y_columns=['value']
                    )
                    if bar_chart_result and not bar_chart_result.get("error"):
                        visualizations.append({
                            "type": VisChartType.BAR.value,
                            "title": bar_chart_result.get('title', "å…³é”®æŒ‡æ ‡å¿«ç…§"),
                            "data_payload": bar_chart_result.get('data'),
                            "image_data_b64": bar_chart_result.get('image_data', {}).get('base64')
                        })
                except Exception as e_bar:
                    logger.error(f"Error generating bar chart for key metrics: {e_bar}")

            primary_res_payload = business_result_data.get('primary_result', {})
            # historical_analysis_res_obj should be the direct object from the processor or its dict representation
            historical_analysis_res_obj = None
            if isinstance(primary_res_payload, dict) and 'multi_results' in primary_res_payload:
                historical_analysis_res_obj = primary_res_payload['multi_results'].get('historical_analysis')
            elif isinstance(primary_res_payload, HistoricalAnalysisResponse):
                historical_analysis_res_obj = primary_res_payload
            elif isinstance(primary_res_payload, dict) and primary_res_payload.get(
                    'analysis_type') == QueryParserQueryType.TREND_ANALYSIS.value:
                historical_analysis_res_obj = primary_res_payload

            # Convert to dict if it's an object and has to_dict or __dict__
            historical_analysis_dict = None
            if historical_analysis_res_obj:
                if hasattr(historical_analysis_res_obj, 'to_dict') and callable(historical_analysis_res_obj.to_dict):
                    historical_analysis_dict = historical_analysis_res_obj.to_dict()
                elif hasattr(historical_analysis_res_obj, '__dict__'):
                    historical_analysis_dict = historical_analysis_res_obj.__dict__
                elif isinstance(historical_analysis_res_obj, dict):
                    historical_analysis_dict = historical_analysis_res_obj

            if historical_analysis_dict and isinstance(historical_analysis_dict.get('trends'), list):
                for trend_detail_item in historical_analysis_dict[
                    'trends']:  # trend_detail_item is FinancialTrendAnalysis or its dict
                    metric_name = "è¶‹åŠ¿"
                    data_points = []
                    if isinstance(trend_detail_item, FinancialTrendAnalysis):  # If it's the object
                        metric_name = getattr(trend_detail_item, 'metric_name', 'è¶‹åŠ¿')
                        data_points = getattr(trend_detail_item, 'chart_data', [])
                    elif isinstance(trend_detail_item, dict):  # If it's a dict
                        metric_name = trend_detail_item.get('metric_name', trend_detail_item.get('metric', 'è¶‹åŠ¿'))
                        data_points = trend_detail_item.get('chart_data',
                                                            trend_detail_item.get('data_points_for_chart', []))

                    if data_points and isinstance(data_points, list) and len(data_points) > 1:
                        if not (all(isinstance(dp, dict) and 'date' in dp and 'value' in dp for dp in data_points)):
                            logger.warning(
                                f"Trend data for {metric_name} is not in expected format for charting. Skipping.")
                            continue
                        try:
                            line_chart_result = self.chart_generator.generate_line_chart(
                                data={'dates': [dp['date'] for dp in data_points],
                                      metric_name: [dp['value'] for dp in data_points]},
                                title=f"{metric_name} è¶‹åŠ¿åˆ†æ", x_column='dates', y_columns=[metric_name]
                            )
                            if line_chart_result and not line_chart_result.get("error"):
                                visualizations.append({
                                    "type": VisChartType.LINE.value,
                                    "title": line_chart_result.get('title', f"{metric_name} è¶‹åŠ¿åˆ†æ"),
                                    "data_payload": line_chart_result.get('data'),
                                    "image_data_b64": line_chart_result.get('image_data', {}).get('base64')
                                })
                        except Exception as e_line:
                            logger.error(f"Error generating line chart for {metric_name}: {e_line}")

            logger.info(f"ç”Ÿæˆäº† {len(visualizations)} ä¸ªå¯è§†åŒ–æ•°æ®å¯¹è±¡ã€‚")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯è§†åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\n{traceback.format_exc()}")

        return visualizations

    # --- ä»¥ä¸‹æ˜¯æ‰€æœ‰ä¹‹å‰è®¨è®ºçš„ã€éœ€è¦ç¡®ä¿å­˜åœ¨çš„è¾…åŠ©æ–¹æ³• ---
    # _extract_raw_numeric_metrics, _summarize_business_result_for_ai,
    # _determine_focus_areas, _get_processors_used, _get_ai_collaboration_summary,
    # _calculate_overall_confidence, _calculate_response_completeness,
    # _get_ai_models_invoked_summary, _log_ai_response_to_conversation,
    # _fallback_query_parsing, _generate_basic_response

    def _calculate_response_completeness(self, business_result_data: Dict[str, Any],
                                         insights_list: List[BusinessInsight]) -> float:
        """
        è¯„ä¼°å“åº”çš„å®Œæ•´æ€§ã€‚
        è€ƒè™‘æ˜¯å¦è¦†ç›–äº†å…³é”®åˆ†æç»“æœå’Œæ´å¯Ÿã€‚
        è¿”å›ä¸€ä¸ª 0.0 åˆ° 1.0 ä¹‹é—´çš„åˆ†æ•°ã€‚
        """
        score = 0.0
        max_score = 1.0  # æ€»åˆ†å€¼

        # 1. æ˜¯å¦æœ‰ä¸»è¦çš„ä¸šåŠ¡å¤„ç†ç»“æœ (å 0.6åˆ†)
        if business_result_data and business_result_data.get('primary_result'):
            # è¿›ä¸€æ­¥æ£€æŸ¥ primary_result æ˜¯å¦çœŸçš„æœ‰å†…å®¹ï¼Œè€Œä¸æ˜¯ç©ºå£³æˆ–é”™è¯¯
            primary_res_content = business_result_data.get('primary_result')
            is_error_dict = isinstance(primary_res_content, dict) and 'error' in primary_res_content
            if primary_res_content and not is_error_dict:
                score += 0.6
            elif is_error_dict:  # å¦‚æœæ˜¯é”™è¯¯ï¼Œå®Œæ•´æ€§ä¼šä½ä¸€äº›
                score += 0.1
        else:  # å¦‚æœè¿ primary_result éƒ½æ²¡æœ‰ï¼Œåˆ™å¾—åˆ†å¾ˆä½
            score += 0.05

        # 2. æ˜¯å¦ç”Ÿæˆäº†æ´å¯Ÿ (å 0.3åˆ†)
        if insights_list and isinstance(insights_list, list) and len(insights_list) > 0:
            score += 0.3

        # 3. æ˜¯å¦æå–å‡ºå…³é”®æŒ‡æ ‡ (å 0.1åˆ†)
        # _extract_key_metrics å·²ç»å¤„ç†äº†å„ç§æƒ…å†µå¹¶è¿”å›æ ¼å¼åŒ–æŒ‡æ ‡
        key_metrics = self._extract_key_metrics(business_result_data)
        if key_metrics and isinstance(key_metrics, dict) and len(key_metrics) > 0:
            score += 0.1

        # ç¡®ä¿åˆ†æ•°åœ¨0åˆ°1ä¹‹é—´
        final_completeness = round(min(score, max_score), 3)
        logger.debug(f"Calculated response completeness: {final_completeness}")
        return final_completeness

    def _get_ai_models_invoked_summary(self, query_analysis: Optional[QueryAnalysisResult],
                                       business_result_data: Dict[str, Any],
                                       insights_list: List[BusinessInsight]) -> List[str]:
        """
        æ ¹æ®åˆ†æè¿‡ç¨‹æ€»ç»“å®é™…è°ƒç”¨çš„AIæ¨¡å‹ (ä¾‹å¦‚ ["Claude", "GPT"])ã€‚
        æ›´å‡†ç¡®çš„ç»Ÿè®¡éœ€è¦å„ç»„ä»¶åœ¨å…¶è¿”å›çš„å…ƒæ•°æ®ä¸­æ˜ç¡®æŠ¥å‘ŠAIä½¿ç”¨æƒ…å†µã€‚
        """
        models_invoked = set()

        # 1. ä»AIåä½œæ‘˜è¦ä¸­è·å–ï¼ˆå¦‚æœè¯¥æ‘˜è¦å·²å‡†ç¡®è®°å½•ï¼‰
        ai_collab_summary = self._get_ai_collaboration_summary(query_analysis, business_result_data, insights_list)
        if ai_collab_summary.get('claude_used_in_process'):
            models_invoked.add("Claude")
        if ai_collab_summary.get('gpt_used_in_process'):
            models_invoked.add("GPT")  # æˆ–æ›´å…·ä½“çš„æ¨¡å‹åå¦‚ "GPT-4o"

        # 2. (å¯é€‰) è¿›ä¸€æ­¥æ£€æŸ¥å„é˜¶æ®µçš„å…ƒæ•°æ®ï¼Œå¦‚æœå®ƒä»¬åŒ…å«æ›´ç»†è‡´çš„æ¨¡å‹ä½¿ç”¨ä¿¡æ¯
        # ä¾‹å¦‚ï¼Œquery_analysis.processing_metadata.get('parser_ai_model')
        # business_result_data.get('metadata', {}).get('processor_ai_model')
        # insights_list[0].metadata.get('generator_ai_model') ç­‰

        if not models_invoked:
            # å¦‚æœä¸Šè¿°éƒ½æœªæ˜ç¡®ï¼Œä½†AIå®¢æˆ·ç«¯å­˜åœ¨ï¼Œåšä¸€ä¸ªåŸºæœ¬å‡è®¾
            if self.claude_client: models_invoked.add("Claude (assumed)")
            if self.gpt_client: models_invoked.add("GPT (assumed)")

        return list(models_invoked) if models_invoked else ["None_Specifically_Reported"]

    async def _handle_processing_error(self, session_id: str, query_id: str,
                                       user_query: str, error_msg: str,
                                       total_time_on_error: float,
                                       conversation_id: Optional[str] = None) -> ProcessingResult:
        """
        ç»Ÿä¸€å¤„ç†æµç¨‹ä¸­çš„é”™è¯¯ï¼Œç”Ÿæˆé”™è¯¯ç»“æœå¯¹è±¡ã€‚æ­¤ä¸ºå¼‚æ­¥ç‰ˆæœ¬ã€‚
        """
        # total_queries å·²ç»åœ¨ process_intelligent_query å¼€å§‹æ—¶å¢åŠ 
        # è¿™é‡Œä¸»è¦æ›´æ–°å¤±è´¥è®¡æ•°å’Œé”™è¯¯ç±»å‹
        error_type = self._classify_error(error_msg)  # _classify_error æ˜¯åŒæ­¥çš„

        logger.error(
            f"QueryID: {query_id} - ASYNC Handling processing error: {error_msg}, Type: {error_type}. Full Traceback (if DEBUG):\n{traceback.format_exc() if self.config.get('DEBUG') else 'Traceback hidden in production.'}")

        fallback_text = f"æŠ±æ­‰ï¼Œåœ¨å¤„ç†æ‚¨çš„æŸ¥è¯¢ '{user_query[:30]}...' æ—¶ç³»ç»Ÿé‡åˆ°äº†ä¸€ä¸ªé¢„æœŸä¹‹å¤–çš„é—®é¢˜ã€‚"
        if self.config.get('fallback_response_enabled', True):
            try:
                # _generate_fallback_response æ˜¯å¼‚æ­¥çš„ï¼Œå› ä¸ºå®ƒå¯èƒ½è°ƒç”¨AI
                fallback_text = await self._generate_fallback_response(user_query, error_msg)
            except Exception as fallback_e:
                logger.error(f"QueryID: {query_id} - Generating fallback response itself also failed: {fallback_e}")
                # å¦‚æœAIç”Ÿæˆé™çº§å“åº”ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨æ›´ç®€å•ã€ç¡¬ç¼–ç çš„æ–‡æœ¬
                fallback_text = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯ã€‚é”™è¯¯è¯¦æƒ…: {error_msg if self.config.get('DEBUG') else 'å†…éƒ¨ç³»ç»Ÿé”™è¯¯ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚'}"

        error_processing_result = ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            response_text=fallback_text,
            insights=[],  # é”™è¯¯æƒ…å†µä¸‹ï¼Œæ´å¯Ÿç­‰ä¸ºç©º
            key_metrics={},
            visualizations=[],
            error_info={
                'error_type': error_type,
                'message': error_msg,
                'original_query': user_query,
                'trace': traceback.format_exc() if self.config.get("DEBUG") else "Error details hidden for security."
            },
            processing_strategy=OrchestratorProcessingStrategy.ERROR_HANDLING,
            processors_used=['AsyncErrorHandling'],  # æ ‡è®°ä¸ºå¼‚æ­¥é”™è¯¯å¤„ç†å™¨
            total_processing_time=total_time_on_error,
            conversation_id=conversation_id,  # ä¿æŒä¼ å…¥çš„ str ID
            timestamp=datetime.now().isoformat()
        )

        # æ›´æ–°ç¼–æ’å™¨ç»Ÿè®¡ï¼ˆåŒ…æ‹¬å¤±è´¥è®¡æ•°å’Œé”™è¯¯ç±»å‹ï¼‰
        self._update_orchestrator_stats(error_processing_result)

        # å°è¯•è®°å½•é”™è¯¯å“åº”åˆ°å¯¹è¯å†å²
        if conversation_id and self.conversation_manager:
            conv_id_for_db_err: Optional[int] = None
            try:
                conv_id_for_db_err = int(conversation_id)
            except ValueError:
                logger.warning(
                    f"QueryID: {query_id} - Cannot log error to conversation due to invalid conversation_id format: {conversation_id}")

            if conv_id_for_db_err is not None:
                self._log_ai_response_to_conversation(conv_id_for_db_err, error_processing_result, query_id)

        return error_processing_result

        # åœ¨ IntelligentQAOrchestrator ç±»ä¸­ä¿®æ”¹æ­¤æ–¹æ³•ï¼š
    async def _fallback_query_parsing(self, user_query: str) -> QueryAnalysisResult:
        """
        åœ¨AIæŸ¥è¯¢è§£æå¤±è´¥æ—¶ï¼Œæä¾›ä¸€ä¸ªåŸºäºè§„åˆ™çš„ã€åŸºç¡€çš„ QueryAnalysisResultã€‚
        """
        logger.warning(f"Executing fallback query parsing for: '{user_query[:50]}...'")

        query_lower = user_query.lower()
        # ä½¿ç”¨ query_parser.py ä¸­å®šä¹‰çš„ QueryParserQueryType å’Œ QueryParserBusinessScenario
        qt = QueryParserQueryType.GENERAL_KNOWLEDGE if hasattr(QueryParserQueryType,
                                                               'GENERAL_KNOWLEDGE') else QueryParserQueryType.DEFINITION_EXPLANATION  # å‡è®¾æœ‰ä¸€ä¸ªé€šç”¨ç±»å‹æˆ–è§£é‡Šç±»å‹
        bs = QueryParserBusinessScenario.UNKNOWN  # é»˜è®¤ä½¿ç”¨ UNKNOWN
        qc = QueryParserComplexity.SIMPLE

        if any(kw in query_lower for kw in ["ä½™é¢", "balance", "å¤šå°‘é’±", "èµ„é‡‘ç°çŠ¶"]):
            qt = QueryParserQueryType.DATA_RETRIEVAL
            bs = QueryParserBusinessScenario.FINANCIAL_OVERVIEW  # ä½¿ç”¨å·²å®šä¹‰çš„ FINANCIAL_OVERVIEW
            qc = QueryParserComplexity.SIMPLE
        elif any(kw in query_lower for kw in ["è¶‹åŠ¿", "å†å²", "trend", "history", "è¿‡å»", "å¯¹æ¯”"]):
            qt = QueryParserQueryType.TREND_ANALYSIS
            bs = QueryParserBusinessScenario.HISTORICAL_PERFORMANCE  # ä½¿ç”¨å·²å®šä¹‰çš„ HISTORICAL_PERFORMANCE
            qc = QueryParserComplexity.MEDIUM
        elif any(kw in query_lower for kw in
                 ["é¢„æµ‹", "é¢„è®¡", "æœªæ¥", "scenario", "å¦‚æœ", "what if"]):  # scenario è‹±æ–‡å°å†™
            qt = QueryParserQueryType.PREDICTION
            bs = QueryParserBusinessScenario.FUTURE_PROJECTION  # ä½¿ç”¨å·²å®šä¹‰çš„ FUTURE_PROJECTION
            qc = QueryParserComplexity.COMPLEX
        elif any(kw in query_lower for kw in ["ç”¨æˆ·", "ä¼šå‘˜", "user", "member"]):
            bs = QueryParserBusinessScenario.USER_ANALYSIS
        elif any(kw in query_lower for kw in ["äº§å“", "product"]):
            bs = QueryParserBusinessScenario.PRODUCT_ANALYSIS

        return QueryAnalysisResult(
            original_query=user_query,
            complexity=qc,
            query_type=qt,
            business_scenario=bs,  # ä½¿ç”¨ä¿®æ­£åçš„ bs
            confidence_score=0.3,
            time_requirements={'parsed_by': 'fallback_rule', 'default_period': 'current'},
            date_parse_result=None,
            data_requirements={'apis_guessed': ['/api/sta/system']},
            required_apis=['/api/sta/system'],
            business_parameters={},
            calculation_requirements={},
            execution_plan=[
                QueryParserExecutionStep(  # ä½¿ç”¨ä» query_parser å¯¼å…¥çš„ QueryParserExecutionStep
                    step_id="fallback_fetch_system_data",
                    step_type="data_retrieval",
                    description="Fallback: è·å–åŸºç¡€ç³»ç»Ÿæ¦‚è§ˆæ•°æ®",
                    required_data=["system_overview"],
                    processing_method="direct_api_call_via_smart_fetcher",
                    dependencies=[],
                    estimated_time=1.5,
                    ai_model_preference="any"
                )
            ],
            processing_strategy="direct_response",
            ai_collaboration_plan={'strategy_type': 'none', 'reason': 'fallback_parsing_used'},
            analysis_timestamp=datetime.now().isoformat(),
            estimated_total_time=2.0,
            processing_metadata={"parser_status": "fallback_rule_based_incomplete"}
        )

    def _classify_error(self, error_msg: str) -> str:
        """
        æ ¹æ®é”™è¯¯æ¶ˆæ¯æ–‡æœ¬åˆ†ç±»é”™è¯¯ç±»å‹ã€‚
        è¿™æ˜¯ä¸€ä¸ªåŒæ­¥æ–¹æ³•ã€‚
        """
        s_err = str(error_msg).lower()  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²å¹¶è½¬ä¸ºå°å†™ä»¥ä¾¿åŒ¹é…

        if "timeout" in s_err: return "timeout_error"
        if "api key" in s_err or "authentication" in s_err or "unauthorized" in s_err: return "authentication_error"
        if "rate limit" in s_err or "quota exceeded" in s_err or "too many requests" in s_err: return "rate_limit_error"
        if "connection" in s_err or "network" in s_err or "dns" in s_err or "host not found" in s_err: return "network_error"
        if "database" in s_err or "sql" in s_err or "db query" in s_err: return "database_error"
        if "not initialized" in s_err or "not configured" in s_err: return "initialization_error"
        if any(ai_kw in s_err for ai_kw in
               ["claude", "openai", "gpt", "anthropic", "gemini", "ai model error"]): return "ai_service_error"
        if "attributeerror" in s_err: return "attribute_error_integration_issue"  # é€šå¸¸æ˜¯ä»£ç é›†æˆé—®é¢˜
        if "keyerror" in s_err: return "key_error_data_structure_issue"  # è®¿é—®äº†ä¸å­˜åœ¨çš„å­—å…¸é”®
        if "indexerror" in s_err: return "index_error_data_structure_issue"  # åˆ—è¡¨ç´¢å¼•è¶Šç•Œ
        if "valueerror" in s_err or ("typeerror" in s_err and (
                "parameter" in s_err or "argument" in s_err)): return "validation_or_type_error"
        if "file not found" in s_err: return "file_not_found_error"
        if "permission denied" in s_err: return "permission_error"
        if "memory" in s_err and "out of" in s_err: return "out_of_memory_error"

        # æ›´é€šç”¨çš„é”™è¯¯ç±»å‹
        if "parsing failed" in s_err: return "parsing_error"
        if "data acquisition failed" in s_err: return "data_acquisition_error"
        if "processing failed" in s_err: return "business_processing_error"
        if "insight generation failed" in s_err: return "insight_generation_error"

        return "unknown_processing_error"  # é»˜è®¤çš„æœªçŸ¥é”™è¯¯
    def _extract_raw_numeric_metrics(self, business_result_data: Dict[str, Any]) -> Dict[str, float]:
        raw_metrics: Dict[str, float] = {}
        if not business_result_data or not isinstance(business_result_data, dict): return raw_metrics

        primary_res_content = business_result_data.get('primary_result')

        def _extract_from_single_res_raw(res_content: Any, prefix=""):
            if not res_content: return
            data_to_search = {}
            # Handle both dataclass objects and dictionaries
            if hasattr(res_content, 'to_dict') and callable(res_content.to_dict):
                data_to_search = res_content.to_dict()
            elif hasattr(res_content, '__dict__'):
                data_to_search = res_content.__dict__
            elif isinstance(res_content, dict):
                data_to_search = res_content
            else:
                return

            for metric_key_container in ['key_metrics', 'metrics', 'main_prediction', 'predictions']:
                if metric_key_container in data_to_search and isinstance(data_to_search[metric_key_container], dict):
                    for k, v_original in data_to_search[metric_key_container].items():
                        try:
                            v_str = str(v_original).replace('%', '').replace('Â¥', '').replace(',', '')
                            numeric_v = float(v_str)
                            metric_name_to_store = f"{prefix}{k}".strip('_')
                            if metric_name_to_store in raw_metrics and raw_metrics[metric_name_to_store] != numeric_v:
                                metric_name_to_store = f"{prefix}{metric_key_container}_{k}".strip('_')
                            raw_metrics[metric_name_to_store] = numeric_v
                        except (ValueError, TypeError):
                            continue
                    break

        if isinstance(primary_res_content, dict):
            if 'multi_results' in primary_res_content and isinstance(primary_res_content['multi_results'], dict):
                for proc_name, proc_res_obj_or_dict in primary_res_content['multi_results'].items():
                    if proc_res_obj_or_dict and not (
                            isinstance(proc_res_obj_or_dict, dict) and 'error' in proc_res_obj_or_dict):
                        _extract_from_single_res_raw(proc_res_obj_or_dict, prefix=f"{proc_name}_")
            else:
                _extract_from_single_res_raw(primary_res_content)
        elif primary_res_content:
            _extract_from_single_res_raw(primary_res_content, prefix="main_")
        return raw_metrics

    def _determine_focus_areas(self, query_analysis: Optional[QueryAnalysisResult]) -> List[str]:
        if not query_analysis: return ['general_overview']
        focus_areas = set()
        query_type_val = query_analysis.query_type.value if query_analysis.query_type else ""
        biz_scenario_val = query_analysis.business_scenario.value if query_analysis.business_scenario else ""
        if 'financial' in biz_scenario_val or 'risk' in query_type_val: focus_areas.add('financial_health')
        if 'growth' in biz_scenario_val or 'trend' in query_type_val: focus_areas.add('growth_analysis')
        if 'prediction' in query_type_val or 'scenario' in query_type_val: focus_areas.add(
            'future_outlook_and_opportunities')
        if 'risk' in query_type_val: focus_areas.add('risk_management')
        if 'user' in biz_scenario_val or 'user' in query_type_val: focus_areas.add('user_behavior_insights')
        if 'product' in biz_scenario_val or 'product' in query_type_val: focus_areas.add('product_performance_review')
        return list(focus_areas) if focus_areas else ['general_business_overview']

    def get_orchestrator_stats(self) -> Dict[str, Any]:
        stats = self.orchestrator_stats.copy()
        total_q = stats.get('total_queries', 0)
        if total_q > 0:
            stats['success_rate'] = stats.get('successful_queries', 0) / total_q
            stats['failure_rate'] = stats.get('failed_queries', 0) / total_q
        else:
            stats['success_rate'] = 0.0;
            stats['failure_rate'] = 0.0
        cache_reqs = stats.get('cache_hits', 0) + stats.get('cache_misses', 0)
        stats['cache_hit_rate'] = stats.get('cache_hits', 0) / cache_reqs if cache_reqs > 0 else 0.0
        return stats

    async def health_check(self) -> Dict[str, Any]:
        if not self.initialized:
            try:
                await self.initialize()
            except Exception as e:
                return {'status': 'unhealthy', 'reason': 'Orchestrator initialization failed during health check',
                        'error': str(e), 'timestamp': datetime.now().isoformat()}
        health_status = {
            'status': 'healthy', 'timestamp': datetime.now().isoformat(),
            'orchestrator_initialized': self.initialized, 'components_status': {},
            'ai_clients_status': {
                'claude_available': self.claude_client is not None and hasattr(self.claude_client, 'messages'),
                # Example check
                'gpt_available': self.gpt_client is not None and hasattr(self.gpt_client, 'chat')  # Example check
            },
            'db_connector_status': 'available' if self.db_connector else 'unavailable',
            'conversation_manager_status': 'available' if self.conversation_manager else 'unavailable',
            'statistics_snapshot': self.get_orchestrator_stats(),
            'active_sessions_count': len(self.active_sessions),
            'cache_current_size': len(self.result_cache)
        }
        components_to_check = {
            'QueryParser': self.query_parser, 'DataRequirementsAnalyzer': self.data_requirements_analyzer,
            'SmartDataFetcher': self.data_fetcher, 'FinancialDataAnalyzer': self.financial_data_analyzer,
            'InsightGenerator': self.insight_generator, 'CurrentDataProcessor': self.current_data_processor,
            'HistoricalAnalysisProcessor': self.historical_analysis_processor,
            'PredictionProcessor': self.prediction_processor,
            'APIConnector': getattr(self.data_fetcher, 'api_connector', None) if self.data_fetcher else None
        }
        all_components_healthy = True
        for name, comp_instance in components_to_check.items():
            if comp_instance and hasattr(comp_instance, 'health_check') and callable(
                    getattr(comp_instance, 'health_check')):
                try:
                    comp_health = await comp_instance.health_check()
                    status_from_comp = comp_health.get('status', 'unknown_status_key')
                    health_status['components_status'][name] = status_from_comp
                    if status_from_comp != 'healthy': all_components_healthy = False
                except Exception as e_hc:
                    health_status['components_status'][name] = f'error_calling_hc: {str(e_hc)}'
                    all_components_healthy = False
            elif comp_instance:
                health_status['components_status'][name] = 'available_no_hc_method'
            else:
                health_status['components_status'][name] = 'unavailable'; all_components_healthy = False

        if not self.db_connector and self.conversation_manager and getattr(self.conversation_manager, 'db',
                                                                           None) is None:
            health_status['components_status']['ConversationManager'] = 'running_in_memory_mode'
        elif self.db_connector and self.conversation_manager:
            health_status['components_status']['ConversationManager'] = 'healthy_with_db'

        if not all_components_healthy: health_status['status'] = 'degraded'
        ai_status = health_status['ai_clients_status']
        if not ai_status['claude_available'] and not ai_status['gpt_available']:
            health_status['status'] = 'critical_ai_unavailable'
        elif not ai_status['claude_available'] or not ai_status['gpt_available']:
            health_status['status'] = 'limited_one_ai_unavailable'
        return health_status

    async def close(self):
        logger.info("Closing IntelligentQAOrchestrator and its resources...")
        if self.data_fetcher and hasattr(self.data_fetcher, 'close') and callable(self.data_fetcher.close):
            if asyncio.iscoroutinefunction(self.data_fetcher.close):
                await self.data_fetcher.close()
            else:
                self.data_fetcher.close()
        if self.db_connector and hasattr(self.db_connector, 'close') and callable(self.db_connector.close):
            try:
                if asyncio.iscoroutinefunction(self.db_connector.close):
                    await self.db_connector.close()
                else:
                    self.db_connector.close()
            except Exception as db_close_err:
                logger.error(f"Error closing DatabaseConnector: {db_close_err}")
        self.result_cache.clear()
        self.active_sessions.clear()
        self.initialized = False
        logger.info("IntelligentQAOrchestrator closed successfully.")


# --- å…¨å±€å®ä¾‹ç®¡ç† ---
_orchestrator_instance: Optional[IntelligentQAOrchestrator] = None


def get_orchestrator(claude_client_instance: Optional[ClaudeClient] = None,
                     gpt_client_instance: Optional[OpenAIClient] = None,
                     db_connector_instance: Optional[DatabaseConnector] = None,
                     app_config_instance: Optional[AppConfig] = None) -> IntelligentQAOrchestrator:
    global _orchestrator_instance
    if _orchestrator_instance is None:
        logger.info("Creating new IntelligentQAOrchestrator singleton instance.")
        _orchestrator_instance = IntelligentQAOrchestrator(
            claude_client_instance, gpt_client_instance, db_connector_instance, app_config_instance
        )
    elif (claude_client_instance and _orchestrator_instance.claude_client != claude_client_instance) or \
            (gpt_client_instance and _orchestrator_instance.gpt_client != gpt_client_instance) or \
            (db_connector_instance and _orchestrator_instance.db_connector != db_connector_instance) or \
            (app_config_instance and _orchestrator_instance.app_config != app_config_instance):
        logger.info("Re-configuring existing orchestrator instance.")
        if claude_client_instance is not None: _orchestrator_instance.claude_client = claude_client_instance
        if gpt_client_instance is not None: _orchestrator_instance.gpt_client = gpt_client_instance
        if db_connector_instance is not None: _orchestrator_instance.db_connector = db_connector_instance
        if app_config_instance is not None:
            _orchestrator_instance.app_config = app_config_instance
            _orchestrator_instance.config = _orchestrator_instance._load_orchestrator_config()
        _orchestrator_instance.initialized = False
        logger.info("Orchestrator marked for re-initialization due to new configuration/clients.")

    return _orchestrator_instance

# (async def main() å·²ç§»é™¤)