# core/orchestrator/intelligent_qa_orchestrator.py
"""
üöÄ AIÈ©±Âä®ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁºñÊéíÂô®
Êï¥‰∏™ÈáëËûçAIÂàÜÊûêÁ≥ªÁªüÁöÑÊ†∏ÂøÉÂ§ßËÑëÔºåË¥üË¥£ÂçèË∞ÉÊâÄÊúâÁªÑ‰ª∂ÂçèÂêåÂ∑•‰Ωú
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import asyncio
import json
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import traceback

# Ê†∏ÂøÉÁªÑ‰ª∂ÂØºÂÖ• (‰∏éÊÇ®‰πãÂâçÊèê‰æõÁöÑ‰∏ÄËá¥)
from core.analyzers.query_parser import (
    SmartQueryParser, create_smart_query_parser, QueryAnalysisResult,
    QueryComplexity as QueryParserComplexity,
    QueryType as QueryParserQueryType,
    BusinessScenario as QueryParserBusinessScenario,
    ExecutionStep as QueryParserExecutionStep
)
from core.analyzers.data_requirements_analyzer import (
    DataRequirementsAnalyzer, create_data_requirements_analyzer, DataAcquisitionPlan
)
from core.analyzers.financial_data_analyzer import (
    FinancialDataAnalyzer, create_financial_data_analyzer, AnalysisResult as FinancialAnalysisResultType,
    TrendAnalysis as FinancialTrendAnalysis, AnomalyDetection as FinancialAnomalyDetection
)
from core.analyzers.insight_generator import (
    InsightGenerator, create_insight_generator, BusinessInsight
)
from core.data_orchestration.smart_data_fetcher import (
    SmartDataFetcher, create_smart_data_fetcher, ExecutionResult as FetcherExecutionResult,
    DataQualityLevel as FetcherDataQualityLevel
)
from data.processors.current_data_processor import CurrentDataProcessor, CurrentDataResponse
from data.processors.historical_analysis_processor import HistoricalAnalysisProcessor, HistoricalAnalysisResponse
from data.processors.prediction_processor import PredictionProcessor, PredictionResponse

# Â∑•ÂÖ∑Á±ªÂØºÂÖ• (‰∏éÊÇ®‰πãÂâçÊèê‰æõÁöÑ‰∏ÄËá¥)
from utils.helpers.date_utils import DateUtils, create_date_utils
from utils.formatters.financial_formatter import FinancialFormatter, create_financial_formatter
from utils.formatters.chart_generator import ChartGenerator as UtilsChartGenerator, \
    create_chart_generator as create_utils_chart_generator, ChartType as VisChartType
from utils.formatters.report_generator import ReportGenerator, create_report_generator, Report as ReportObject, \
    ReportSection
from data.models.conversation import ConversationManager, create_conversation_manager, Message as ConversationMessage, \
    Visual as ConversationVisual
from data.connectors.database_connector import DatabaseConnector, create_database_connector
from data.connectors.api_connector import APIConnector

# AI ÂÆ¢Êà∑Á´ØÂØºÂÖ• (‰∏éÊÇ®‰πãÂâçÊèê‰æõÁöÑ‰∏ÄËá¥)
from core.models.claude_client import ClaudeClient, CustomJSONEncoder
from core.models.openai_client import OpenAIClient

# Â∫îÁî®ÈÖçÁΩÆÂØºÂÖ• (‰∏éÊÇ®‰πãÂâçÊèê‰æõÁöÑ‰∏ÄËá¥)
from config import Config as AppConfig

logger = logging.getLogger(__name__)


class OrchestratorProcessingStrategy(Enum):
    DIRECT_RESPONSE = "direct_response"
    SINGLE_PROCESSOR = "single_processor"
    MULTI_PROCESSOR = "multi_processor"
    FULL_PIPELINE = "full_pipeline"
    ERROR_HANDLING = "error_handling"


@dataclass
class ProcessingResult:
    # (‰∏éÊÇ®‰πãÂâçÊèê‰æõÁöÑÂÆö‰πâ‰∏ÄËá¥)
    session_id: str
    query_id: str
    success: bool
    response_text: str
    insights: List[BusinessInsight] = field(default_factory=list)
    key_metrics: Dict[str, Any] = field(default_factory=dict)
    visualizations: List[Dict[str, Any]] = field(default_factory=list)
    processing_strategy: OrchestratorProcessingStrategy = OrchestratorProcessingStrategy.SINGLE_PROCESSOR
    processors_used: List[str] = field(default_factory=list)
    ai_collaboration_summary: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 0.0
    data_quality_score: float = 0.0
    response_completeness: float = 0.0
    total_processing_time: float = 0.0
    ai_processing_time: float = 0.0
    data_fetching_time: float = 0.0
    processing_metadata: Dict[str, Any] = field(default_factory=dict)
    error_info: Optional[Dict[str, Any]] = None
    conversation_id: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    query_analysis_snapshot: Optional[Dict[str, Any]] = None


class IntelligentQAOrchestrator:
    _instance: Optional['IntelligentQAOrchestrator'] = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(IntelligentQAOrchestrator, cls).__new__(cls)
        return cls._instance

    def __init__(self, claude_client_instance: Optional[ClaudeClient] = None,
                 gpt_client_instance: Optional[OpenAIClient] = None,
                 db_connector_instance: Optional[DatabaseConnector] = None,
                 app_config_instance: Optional[AppConfig] = None):

        if hasattr(self, 'initialized') and self.initialized:
            # (Êõ¥Êñ∞‰æùËµñÂπ∂Ê†áËÆ∞ÈáçÂàùÂßãÂåñÁöÑÈÄªËæë)
            needs_reinit = False
            if claude_client_instance and self.claude_client != claude_client_instance: self.claude_client = claude_client_instance; needs_reinit = True
            if gpt_client_instance and self.gpt_client != gpt_client_instance: self.gpt_client = gpt_client_instance; needs_reinit = True
            if db_connector_instance and self.db_connector != db_connector_instance: self.db_connector = db_connector_instance; needs_reinit = True
            if app_config_instance and self.app_config != app_config_instance:
                self.app_config = app_config_instance
                self.config = self._load_orchestrator_config()
                needs_reinit = True
            if needs_reinit:
                self.initialized = False
                logger.info(
                    "Orchestrator dependencies updated, will re-initialize components on next use or explicit call to initialize().")
            return

        self.claude_client = claude_client_instance
        self.gpt_client = gpt_client_instance
        self.db_connector = db_connector_instance
        self.app_config = app_config_instance if app_config_instance is not None else AppConfig()
        self.config = self._load_orchestrator_config()

        self.initialized = False
        self._initialize_component_placeholders()
        self.active_sessions: Dict[str, Any] = {}
        self.orchestrator_stats = self._default_stats()
        self.result_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = self.config.get('cache_ttl_seconds', 1800)
        logger.info("IntelligentQAOrchestrator instance created. Call async initialize() to set up components.")

    def _initialize_component_placeholders(self):
        # (‰∏é‰πãÂâç‰∏ÄËá¥)
        self.query_parser: Optional[SmartQueryParser] = None
        self.data_requirements_analyzer: Optional[DataRequirementsAnalyzer] = None
        self.financial_data_analyzer: Optional[FinancialDataAnalyzer] = None
        self.insight_generator: Optional[InsightGenerator] = None
        self.data_fetcher: Optional[SmartDataFetcher] = None
        self.current_data_processor: Optional[CurrentDataProcessor] = None
        self.historical_analysis_processor: Optional[HistoricalAnalysisProcessor] = None
        self.prediction_processor: Optional[PredictionProcessor] = None
        self.date_utils: Optional[DateUtils] = None
        self.financial_formatter: Optional[FinancialFormatter] = None
        self.chart_generator: Optional[UtilsChartGenerator] = None
        self.report_generator: Optional[ReportGenerator] = None
        self.conversation_manager: Optional[ConversationManager] = None

    def _default_stats(self) -> Dict[str, Any]:
        # (‰∏é‰πãÂâç‰∏ÄËá¥)
        return {
            'total_queries': 0, 'successful_queries': 0, 'failed_queries': 0,
            'avg_processing_time': 0.0, 'avg_confidence_score': 0.0,
            'ai_collaboration_count': 0, 'cache_hits': 0, 'cache_misses': 0,
            'processor_usage': {}, 'error_types': {}
        }

    def _load_orchestrator_config(self) -> Dict[str, Any]:
        # (‰∏é‰πãÂâç‰∏ÄËá¥)
        cfg = {
            'max_processing_time': getattr(self.app_config, 'MAX_PROCESSING_TIME', 120),
            'enable_parallel_processing': getattr(self.app_config, 'ENABLE_PARALLEL_PROCESSING', True),
            'enable_intelligent_caching': getattr(self.app_config, 'ENABLE_INTELLIGENT_CACHING', True),
            'enable_ai_collaboration': getattr(self.app_config, 'ENABLE_AI_COLLABORATION', True),
            'min_confidence_threshold': getattr(self.app_config, 'MIN_CONFIDENCE_THRESHOLD', 0.6),
            'min_confidence_threshold_parsing': 0.7,
            'enable_result_validation': getattr(self.app_config, 'ENABLE_RESULT_VALIDATION', True),
            'enable_quality_monitoring': getattr(self.app_config, 'ENABLE_QUALITY_MONITORING', True),
            'cache_ttl_seconds': getattr(self.app_config, 'CACHE_TTL', 1800),
            'max_cache_size': getattr(self.app_config, 'MAX_CACHE_SIZE', 100),
            'max_concurrent_queries': getattr(self.app_config, 'MAX_CONCURRENT_QUERIES', 50),
            'enable_smart_routing': getattr(self.app_config, 'ENABLE_SMART_ROUTING', True),
            'claude_timeout': getattr(self.app_config, 'CLAUDE_TIMEOUT', 60),
            'gpt_timeout': getattr(self.app_config, 'GPT_TIMEOUT', 40),
            'max_ai_retries': getattr(self.app_config, 'MAX_AI_RETRIES', 2),
            'enable_graceful_degradation': getattr(self.app_config, 'ENABLE_GRACEFUL_DEGRADATION', True),
            'fallback_response_enabled': getattr(self.app_config, 'FALLBACK_RESPONSE_ENABLED', True),
            'DEBUG': getattr(self.app_config, 'DEBUG', False),
            'version': getattr(self.app_config, 'VERSION', '2.1.9-fuller-methods-no-main-fixed-prompts')  # Êõ¥Êñ∞ÁâàÊú¨
        }
        if hasattr(self.app_config, 'CLAUDE_API_KEY'): cfg['CLAUDE_API_KEY'] = self.app_config.CLAUDE_API_KEY
        if hasattr(self.app_config, 'OPENAI_API_KEY'): cfg['OPENAI_API_KEY'] = self.app_config.OPENAI_API_KEY
        if hasattr(self.app_config, 'DATABASE_CONFIG'): cfg['DATABASE_CONFIG'] = self.app_config.DATABASE_CONFIG
        api_connector_cfg = {}
        if hasattr(self.app_config, 'FINANCE_API_BASE_URL'): api_connector_cfg[
            'base_url'] = self.app_config.FINANCE_API_BASE_URL
        if hasattr(self.app_config, 'FINANCE_API_KEY'): api_connector_cfg['api_key'] = self.app_config.FINANCE_API_KEY
        if api_connector_cfg: cfg['api_connector_config'] = api_connector_cfg
        return cfg

    async def initialize(self):
        # (‰∏é‰πãÂâç‰∏ÄËá¥ÔºåÁ°Æ‰øùÊâÄÊúâÁªÑ‰ª∂Ë¢´Ê≠£Á°ÆÂàõÂª∫)
        if self.initialized:
            logger.debug("Orchestrator already initialized.")
            return

        logger.info("üöÄ ÂºÄÂßãÂàùÂßãÂåñÊô∫ËÉΩÁºñÊéíÂô®ÁªÑ‰ª∂...")
        start_init_time = time.time()
        try:
            if not self.claude_client and self.config.get('CLAUDE_API_KEY'):
                self.claude_client = ClaudeClient(api_key=self.config['CLAUDE_API_KEY'])
            if not self.gpt_client and self.config.get('OPENAI_API_KEY'):
                self.gpt_client = OpenAIClient(api_key=self.config['OPENAI_API_KEY'])

            if not self.claude_client: logger.warning("ClaudeClientÊú™ÈÖçÁΩÆ„ÄÇ")
            if not self.gpt_client: logger.warning("OpenAIClientÊú™ÈÖçÁΩÆ„ÄÇ")

            if not self.db_connector and self.config.get('DATABASE_CONFIG'):
                db_cfg_details = self.config['DATABASE_CONFIG']
                if db_cfg_details.get('user') and db_cfg_details.get('password') and db_cfg_details.get(
                        'host') and db_cfg_details.get('database'):
                    self.db_connector = create_database_connector(db_cfg_details)
                    logger.info(f"DatabaseConnector for host '{db_cfg_details.get('host')}' implicitly initialized.")
                else:
                    logger.error("DATABASE_CONFIG‰∏çÂÆåÊï¥„ÄÇ")
            elif not self.db_connector:
                logger.warning("DatabaseConnectorÊú™ÈÖçÁΩÆ„ÄÇÂØπËØùÂéÜÂè≤Á≠âÂäüËÉΩÂ∞Ü‰ΩøÁî®ÂÜÖÂ≠òÊ®°ÂºèÊàñÂèóÈôê„ÄÇ")

            self.query_parser = create_smart_query_parser(self.claude_client, self.gpt_client)
            self.data_requirements_analyzer = create_data_requirements_analyzer(self.claude_client, self.gpt_client)
            fetcher_config = self.config.get('api_connector_config', {})
            self.data_fetcher = create_smart_data_fetcher(self.claude_client, self.gpt_client, fetcher_config)
            self.financial_data_analyzer = create_financial_data_analyzer(self.claude_client, self.gpt_client)
            self.insight_generator = create_insight_generator(self.claude_client, self.gpt_client)
            self.current_data_processor = CurrentDataProcessor(self.claude_client, self.gpt_client)
            self.historical_analysis_processor = HistoricalAnalysisProcessor(self.claude_client, self.gpt_client)
            self.prediction_processor = PredictionProcessor(self.claude_client, self.gpt_client)
            self.date_utils = create_date_utils(self.claude_client)
            self.financial_formatter = create_financial_formatter()
            self.chart_generator = create_utils_chart_generator()
            self.report_generator = create_report_generator()

            if self.db_connector:
                self.conversation_manager = create_conversation_manager(self.db_connector)
                logger.info("ConversationManager initialized with DB backend.")
            else:
                logger.warning("DatabaseConnector not available. ConversationManager using non-persistent in-memory.")
                self.conversation_manager = ConversationManager(database_connector=None)

            self._inject_dependencies()
            self.initialized = True
            init_duration = time.time() - start_init_time
            logger.info(f"‚úÖ Êô∫ËÉΩÁºñÊéíÂô®ÁªÑ‰ª∂ÂàùÂßãÂåñÂÆåÊàê (ËÄóÊó∂: {init_duration:.2f}s)„ÄÇ")
        except Exception as e:
            self.initialized = False
            logger.error(f"‚ùå Êô∫ËÉΩÁºñÊéíÂô®ÂàùÂßãÂåñËøáÁ®ã‰∏≠ÂèëÁîü‰∏•ÈáçÈîôËØØ: {str(e)}\n{traceback.format_exc()}")

    def _inject_dependencies(self):
        # (‰∏é‰πãÂâç‰∏ÄËá¥)
        logger.debug("Injecting dependencies into components...")
        if not self.data_fetcher:
            logger.error("SmartDataFetcher is not initialized, cannot inject APIConnector.")
            return

        api_connector_instance = getattr(self.data_fetcher, 'api_connector', None)
        if not api_connector_instance and self.data_fetcher:
            logger.error("APIConnector is missing from SmartDataFetcher! This is critical.")
            api_conn_config_details = self.config.get('api_connector_config', {})
            if api_conn_config_details and api_conn_config_details.get('base_url'):
                from data.connectors.api_connector import create_enhanced_api_connector
                self.data_fetcher.api_connector = create_enhanced_api_connector(api_conn_config_details,
                                                                                self.claude_client, self.gpt_client)
                api_connector_instance = self.data_fetcher.api_connector
                logger.info("Dynamically re-initialized APIConnector for SmartDataFetcher during DI.")
            else:
                logger.error("Cannot dynamically re-init APIConnector for SmartDataFetcher due to missing config.")

        components_to_inject_into = [
            self.current_data_processor, self.historical_analysis_processor, self.prediction_processor,
            self.financial_data_analyzer, self.insight_generator,
            self.query_parser, self.data_requirements_analyzer, self.data_fetcher
        ]

        for component in components_to_inject_into:
            if not component: continue
            if api_connector_instance and isinstance(component, (
            CurrentDataProcessor, HistoricalAnalysisProcessor, PredictionProcessor, FinancialDataAnalyzer)):
                if not hasattr(component, 'api_connector') or component.api_connector is None:
                    component.api_connector = api_connector_instance

            if self.financial_data_analyzer and isinstance(component, (
            HistoricalAnalysisProcessor, PredictionProcessor, InsightGenerator)):
                if not hasattr(component, 'financial_data_analyzer') or component.financial_data_analyzer is None:
                    component.financial_data_analyzer = self.financial_data_analyzer

            financial_calculator_instance = getattr(self.financial_data_analyzer, 'financial_calculator',
                                                    None) if self.financial_data_analyzer else None
            if financial_calculator_instance and isinstance(component,
                                                            (HistoricalAnalysisProcessor, PredictionProcessor)):
                if not hasattr(component, 'financial_calculator') or component.financial_calculator is None:
                    component.financial_calculator = financial_calculator_instance

            if self.date_utils and (not hasattr(component, 'date_utils') or component.date_utils is None):
                component.date_utils = self.date_utils
        logger.debug("Dependency injection check complete.")

    # ========================================================================
    # ============= Ê†∏ÂøÉÊµÅÁ®ãÊñπÊ≥ï process_intelligent_query (‰øùÊåÅ‰∏çÂèò) ============
    # ========================================================================
    async def process_intelligent_query(self, user_query: str, user_id: int = 0,
                                        conversation_id: Optional[str] = None,
                                        preferences: Optional[Dict[str, Any]] = None) -> ProcessingResult:
        if not self.initialized:
            logger.critical("CRITICAL: Orchestrator not initialized. Attempting to initialize now.")
            await self.initialize()
            if not self.initialized:
                return self._handle_processing_error_sync(  # ‰ΩøÁî®Â∑≤ÂÆö‰πâÁöÑÂêåÊ≠•ÈîôËØØÂ§ÑÁêÜÂô®
                    str(uuid.uuid4()), f"init_fail_{int(time.time())}", user_query,
                    "Á≥ªÁªüÊ†∏ÂøÉÁªÑ‰ª∂ÂàùÂßãÂåñÂ§±Ë¥•ÔºåËØ∑ËÅîÁ≥ªÁÆ°ÁêÜÂëò„ÄÇ", 0.0, conversation_id
                )

        session_id = str(uuid.uuid4())
        query_id = f"q_{datetime.now().strftime('%Y%m%d%H%M%S%f')}_{hashlib.md5(user_query.encode('utf-8')).hexdigest()[:6]}"
        start_time = time.time()

        conversation_id_for_db: Optional[int] = None
        if conversation_id:
            try:
                conversation_id_for_db = int(conversation_id)
            except ValueError:
                logger.warning(f"QueryID: {query_id} - Invalid Conversation ID '{conversation_id}'.")
                conversation_id = None

        logger.info(
            f"üß† QueryID: {query_id} - UserID: {user_id} - ConvStrID: {conversation_id} (DBIntID: {conversation_id_for_db}) - Processing: '{user_query[:100]}...'")
        self.orchestrator_stats['total_queries'] += 1

        if conversation_id_for_db and self.conversation_manager:
            try:
                user_msg_id = self.conversation_manager.add_message(conversation_id_for_db, True, user_query)
                logger.debug(
                    f"QueryID: {query_id} - User query (MsgID: {user_msg_id}) logged to ConvID {conversation_id_for_db}.")
            except Exception as conv_err:
                logger.error(
                    f"QueryID: {query_id} - Failed to log user query for ConvID {conversation_id_for_db}: {conv_err}")

        parsing_time, data_fetching_time, business_processing_time = 0.0, 0.0, 0.0
        insight_processing_time, response_formatting_time, visualization_processing_time = 0.0, 0.0, 0.0
        query_analysis: Optional[QueryAnalysisResult] = None
        data_acquisition_result: Optional[FetcherExecutionResult] = None
        business_result_data: Dict[str, Any] = {}
        insights_list: List[BusinessInsight] = []
        visualizations_data: List[Dict[str, Any]] = []
        response_text: str = ""
        routing_strategy: OrchestratorProcessingStrategy = OrchestratorProcessingStrategy.ERROR_HANDLING
        data_quality: float = 0.1

        try:
            if self.config.get('enable_intelligent_caching', True):
                current_context_for_cache = {}
                if conversation_id_for_db and self.conversation_manager:
                    current_context_for_cache = self.conversation_manager.get_context(conversation_id_for_db)
                cache_key_full_query = f"full_res_{self._generate_query_cache_key(user_query, current_context_for_cache)}"
                cached_full_result = self._get_cached_result(cache_key_full_query)
                if cached_full_result and isinstance(cached_full_result, ProcessingResult):
                    logger.info(f"QueryID: {query_id} - Full result cache HIT.")
                    self.orchestrator_stats['cache_hits'] += 1
                    cached_full_result.total_processing_time = time.time() - start_time
                    self._update_orchestrator_stats(cached_full_result)
                    if conversation_id_for_db and self.conversation_manager:
                        self._log_ai_response_to_conversation(conversation_id_for_db, cached_full_result, query_id)
                    return cached_full_result
            self.orchestrator_stats['cache_misses'] += 1

            parsing_start_time = time.time()
            query_analysis = await self._intelligent_query_parsing(user_query, conversation_id_for_db)
            parsing_time = time.time() - parsing_start_time
            logger.info(
                f"QueryID: {query_id} - 1. Query Parsing ({parsing_time:.3f}s): Complexity='{query_analysis.complexity.value}', Type='{query_analysis.query_type.value}'")

            routing_strategy = await self._determine_processing_strategy(query_analysis, preferences)
            logger.info(f"QueryID: {query_id} - 2. Processing Strategy: {routing_strategy.value}")

            data_acquisition_start_time = time.time()
            data_acquisition_result = await self._orchestrate_data_acquisition(query_analysis, routing_strategy)
            data_fetching_time = time.time() - data_acquisition_start_time
            data_quality = getattr(data_acquisition_result, 'confidence_level', 0.3)
            logger.info(
                f"QueryID: {query_id} - 3. Data Acquisition ({data_fetching_time:.3f}s): Quality={data_quality:.2f}, Status='{getattr(data_acquisition_result, 'execution_status', 'unknown').value if hasattr(getattr(data_acquisition_result, 'execution_status', None), 'value') else 'unknown'}'")

            if not data_acquisition_result or \
                    (not getattr(data_acquisition_result, 'fetched_data', None) and not getattr(data_acquisition_result,
                                                                                                'processed_data',
                                                                                                None)) or \
                    (hasattr(data_acquisition_result,
                             'execution_status') and data_acquisition_result.execution_status.value not in ['completed',
                                                                                                            'partial_success']):
                err_msg = f"Critical data acquisition failed. Status: {getattr(data_acquisition_result, 'execution_status', {}).get('value', 'unknown')}. Error: {getattr(data_acquisition_result, 'error_info', 'N/A')}"
                logger.error(f"QueryID: {query_id} - {err_msg}")
                raise ValueError(err_msg)

            business_processing_start_time = time.time()
            business_result_data = await self._orchestrate_business_processing(query_analysis, data_acquisition_result,
                                                                               routing_strategy)
            business_processing_time = time.time() - business_processing_start_time
            logger.info(
                f"QueryID: {query_id} - 4. Business Processing ({business_processing_time:.3f}s): Processor='{business_result_data.get('processor_used', 'N/A')}'")

            insight_generation_start_time = time.time()
            insights_list = await self._orchestrate_insight_generation(business_result_data, query_analysis,
                                                                       data_acquisition_result)
            insight_processing_time = time.time() - insight_generation_start_time
            logger.info(
                f"QueryID: {query_id} - 5. Insight Generation ({insight_processing_time:.3f}s): Found {len(insights_list)} insights.")

            response_formatting_start_time = time.time()
            response_text = await self._generate_intelligent_response(user_query, query_analysis, business_result_data,
                                                                      insights_list)
            response_formatting_time = time.time() - response_formatting_start_time
            logger.info(f"QueryID: {query_id} - 6. Response Text Generation ({response_formatting_time:.3f}s).")

            visualization_start_time = time.time()
            visualizations_data = await self._generate_visualizations(business_result_data, query_analysis,
                                                                      insights_list)
            visualization_processing_time = time.time() - visualization_start_time
            logger.info(
                f"QueryID: {query_id} - 7. Visualization Data Generation ({visualization_processing_time:.3f}s): Found {len(visualizations_data)} visuals.")

            total_ai_time = parsing_time
            biz_proc_metadata = business_result_data.get('metadata', {})
            total_ai_time += biz_proc_metadata.get('ai_processing_time', 0.0) if isinstance(biz_proc_metadata,
                                                                                            dict) else 0.0
            insight_gen_metadata = business_result_data.get('insight_generation_metadata',
                                                            {}) if business_result_data else {}
            total_ai_time += insight_gen_metadata.get('ai_time', insight_processing_time) if isinstance(
                insight_gen_metadata, dict) else insight_processing_time
            total_ai_time += response_formatting_time

            total_processing_time = time.time() - start_time
            overall_confidence = self._calculate_overall_confidence(query_analysis, business_result_data, insights_list,
                                                                    data_quality)

            final_result = ProcessingResult(
                session_id=session_id, query_id=query_id, success=True,
                response_text=response_text,
                insights=insights_list,
                key_metrics=self._extract_key_metrics(business_result_data),
                visualizations=visualizations_data,
                processing_strategy=routing_strategy,
                processors_used=self._get_processors_used(business_result_data, routing_strategy),
                ai_collaboration_summary=self._get_ai_collaboration_summary(query_analysis, business_result_data,
                                                                            insights_list),
                confidence_score=overall_confidence,
                data_quality_score=data_quality,
                response_completeness=self._calculate_response_completeness(business_result_data, insights_list),
                total_processing_time=total_processing_time,
                ai_processing_time=total_ai_time,
                data_fetching_time=data_fetching_time,
                processing_metadata={
                    'query_complexity': query_analysis.complexity.value,
                    'query_type': query_analysis.query_type.value,
                    'business_scenario': query_analysis.business_scenario.value if query_analysis.business_scenario else "N/A",
                    'data_sources_used': getattr(data_acquisition_result, 'data_sources_used', []),
                    'step_times': {
                        'parsing': round(parsing_time, 3), 'data_fetching': round(data_fetching_time, 3),
                        'business_processing': round(business_processing_time, 3),
                        'insight_generation': round(insight_processing_time, 3),
                        'response_formatting': round(response_formatting_time, 3),
                        'visualization': round(visualization_processing_time, 3)
                    },
                    'ai_models_invoked': self._get_ai_models_invoked_summary(query_analysis, business_result_data,
                                                                             insights_list)
                },
                conversation_id=conversation_id,
                query_analysis_snapshot=query_analysis.to_dict() if hasattr(query_analysis,
                                                                            'to_dict') else query_analysis.__dict__ if query_analysis else None
            )

            self._update_orchestrator_stats(final_result)
            if self.config.get('enable_intelligent_caching', True) and final_result.success:
                self._cache_result(cache_key_full_query, final_result)

            if conversation_id_for_db and self.conversation_manager:
                self._log_ai_response_to_conversation(conversation_id_for_db, final_result, query_id)

            logger.info(
                f"QueryID: {query_id} - ‚úÖ Êô∫ËÉΩÊü•ËØ¢Â§ÑÁêÜÊàêÂäü„ÄÇÊÄªËÄóÊó∂: {total_processing_time:.3f}s, ÁΩÆ‰ø°Â∫¶: {final_result.confidence_score:.2f}")
            return final_result

        except Exception as e:
            total_processing_time_on_error = time.time() - start_time
            logger.error(f"‚ùå QueryID: {query_id} - Êô∫ËÉΩÊü•ËØ¢Â§ÑÁêÜ‰∏ªÊµÅÁ®ãÂ§±Ë¥•: {str(e)}\n{traceback.format_exc()}")
            return await self._handle_processing_error(
                session_id, query_id, user_query, str(e), total_processing_time_on_error, conversation_id
            )

    # ========================================================================
    # ============= ‰ª•‰∏ãÊòØË°•ÂÖ®ÁöÑËæÖÂä©ÊñπÊ≥ï (‰∏≤ËÅîÊ†∏ÂøÉÈÄªËæë) ========================
    # ========================================================================

        # Âú® IntelligentQAOrchestrator Á±ª‰∏≠Ê∑ªÂä†Ëøô‰∫õÊñπÊ≥ïÔºö

    def _generate_basic_response(self, business_result_data: Dict[str, Any],
                                     insights_list: List[BusinessInsight]) -> str:
            """
            Âú®AIÂìçÂ∫îÁîüÊàêÂ§±Ë¥•Êó∂ÔºåÊèê‰æõ‰∏Ä‰∏™Âü∫‰∫éÊ®°ÊùøÁöÑ„ÄÅÊõ¥ÁÆÄÂçïÁöÑ‰∏≠ÊñáÊñáÊú¨ÂìçÂ∫î„ÄÇ
            """
            logger.warning("Generating basic (non-AI) response due to previous errors or lack of AI client.")
            response_parts = ["Ê†πÊçÆÁ≥ªÁªüÂàÜÊûêÔºö\n"]

            key_metrics = self._extract_key_metrics(business_result_data)  # ÂÅáËÆæÊ≠§ÊñπÊ≥ïÂ∑≤ÂÆûÁé∞
            if key_metrics:
                response_parts.append("ÂÖ≥ÈîÆÊåáÊ†áÔºö")
                # Á°Æ‰øù FinancialFormatter Â∑≤ÂàùÂßãÂåñÂπ∂ÂèØÁî®
                # for name, value in list(key_metrics.items())[:5]: # ÊúÄÂ§öÊòæÁ§∫5‰∏™
                #     response_parts.append(f"  - {name}: {value}") # value Â∫îÂ∑≤Áî± _extract_key_metrics Ê†ºÂºèÂåñ
                # ÁÆÄÂåñÁâàÔºå‰∏ç‰æùËµñ _extract_key_metrics ÂÆåÂÖ®Ê≠£Á°ÆÔºö
                metric_items = []
                temp_metrics_to_show = {}
                primary_res_content = business_result_data.get('primary_result', {})
                if isinstance(primary_res_content, dict):
                    if 'key_metrics' in primary_res_content: temp_metrics_to_show.update(
                        primary_res_content['key_metrics'])
                    if 'metrics' in primary_res_content: temp_metrics_to_show.update(primary_res_content['metrics'])
                    if 'main_prediction' in primary_res_content: temp_metrics_to_show.update(
                        primary_res_content['main_prediction'])

                for name, value in list(temp_metrics_to_show.items())[:5]:
                    val_str = str(value)
                    if self.financial_formatter and isinstance(value, (int, float)):  # Â∞ùËØïÊ†ºÂºèÂåñ
                        try:
                            if "rate" in name.lower() or "ratio" in name.lower() or "%" in val_str:
                                val_str = self.financial_formatter.format_percentage(
                                    float(val_str.replace('%', '')) / 100 if "%" in val_str else float(val_str))
                            else:
                                val_str = self.financial_formatter.format_currency(float(value))
                        except:
                            pass  # ‰øùÊåÅÂéüÊ†∑
                    metric_items.append(f"  - {name}: {val_str}")
                if metric_items:
                    response_parts.extend(metric_items)
                else:
                    response_parts.append("  ÊöÇÊó†ÂÖ≥ÈîÆÊåáÊ†áÂèØÁõ¥Êé•Â±ïÁ§∫„ÄÇ")
            else:
                primary_res = business_result_data.get('primary_result')
                main_ans_candidate = "ÂàÜÊûêÂ∑≤ÊâßË°åÔºå‰ΩÜÊó†ÁâπÂÆöÊåáÊ†áËæìÂá∫„ÄÇ"
                if isinstance(primary_res, dict):
                    main_ans_candidate = primary_res.get('main_answer', primary_res.get('summary', main_ans_candidate))
                elif primary_res:
                    main_ans_candidate = getattr(primary_res, 'main_answer',
                                                 getattr(primary_res, 'summary', main_ans_candidate))
                response_parts.append(f"‰∏ªË¶Å‰ø°ÊÅØ: {str(main_ans_candidate)[:300]}")

            if insights_list:
                response_parts.append("\nÈáçË¶ÅÊ¥ûÂØüÔºö")
                for i, insight in enumerate(insights_list[:3], 1):  # ÊúÄÂ§ö3Êù°
                    title = getattr(insight, 'title', f"Ê¥ûÂØü{i}")
                    summary = getattr(insight, 'summary', "ËØ∑Êü•ÁúãËØ¶ÁªÜÂàÜÊûê„ÄÇ")
                    response_parts.append(f"  {i}. **{title}**: {summary}")
            else:
                response_parts.append("\nÊöÇÊó†ÁâπÂà´ÁöÑ‰∏öÂä°Ê¥ûÂØü„ÄÇ")

            confidence = business_result_data.get('confidence_score', 0.5)
            data_quality_src = business_result_data.get('data_quality_score',
                                                        getattr(business_result_data.get('data_acquisition_result', {}),
                                                                'confidence_level', 0.5)
                                                        )
            response_parts.append(f"\nÊú¨Ê¨°ÂàÜÊûêÁöÑÊï¥‰ΩìÁΩÆ‰ø°Â∫¶Á∫¶‰∏∫ {confidence:.0%}ÔºåÊï∞ÊçÆË¥®ÈáèËØÑÂàÜ‰∏∫ {data_quality_src:.0%}")
            if confidence < 0.7 or data_quality_src < 0.7:
                response_parts.append("ËØ∑Ê≥®ÊÑèÔºåÁî±‰∫éÊï∞ÊçÆË¥®ÈáèÊàñÂàÜÊûêÂ§çÊùÇÊÄßÔºåÁªìÊûúÂèØËÉΩÂ≠òÂú®‰∏ÄÂÆö‰∏çÁ°ÆÂÆöÊÄß„ÄÇ")

            return "\n".join(response_parts)

    def _summarize_business_result_for_ai(self, business_result_data: Dict[str, Any]) -> Dict[str, Any]:
            """‰∏∫AIÂáÜÂ§á‰∏öÂä°ÁªìÊûúÁöÑÊëòË¶ÅÔºå‰ª•‰æøAIËÉΩÊõ¥Â•ΩÂú∞Êï¥Âêà‰ø°ÊÅØÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÂõûÂ§ç„ÄÇ"""
            summary = {}
            if not business_result_data or not isinstance(business_result_data, dict):
                logger.warning("_summarize_business_result_for_ai: business_result_data is empty or invalid.")
                return {"ÊëòË¶Å": "‰∏öÂä°Â§ÑÁêÜÈò∂ÊÆµÊú™ËøîÂõûÊúâÊïàÁªìÊûú„ÄÇ"}

            summary['processing_type'] = business_result_data.get('processing_type')
            summary['confidence_score'] = round(business_result_data.get('confidence_score', 0.0), 2)

            primary_res_payload = business_result_data.get('primary_result')  # This is object or dict of objects

            # ËæÖÂä©ÂáΩÊï∞ÔºåÂ∞ÜÂ§ÑÁêÜÂô®ËøîÂõûÁöÑÂØπË±°ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏‰ª•‰æøÊëòË¶Å
            def _convert_to_dict_for_summary(obj: Any) -> Optional[Dict[str, Any]]:
                if hasattr(obj, 'to_dict') and callable(obj.to_dict): return obj.to_dict()
                if hasattr(obj, '__dict__'): return obj.__dict__  # For dataclasses
                if isinstance(obj, dict): return obj
                logger.warning(f"Cannot convert object of type {type(obj)} to dict for summary.")
                return None

            if isinstance(primary_res_payload, dict) and 'multi_results' in primary_res_payload:
                summary['multi_analysis_highlights'] = {}
                for key, res_obj_or_dict in primary_res_payload['multi_results'].items():
                    res_dict = _convert_to_dict_for_summary(res_obj_or_dict)
                    if res_dict and 'error' not in res_dict:
                        single_summary = {}
                        # ‰ªé CurrentDataResponse, HistoricalAnalysisResponse, PredictionResponse ‰∏≠ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØ
                        if 'main_answer' in res_dict: single_summary['answer_snippet'] = str(res_dict['main_answer'])[
                                                                                         :100] + "..."

                        metrics_src = res_dict.get('key_metrics') or res_dict.get('metrics') or res_dict.get(
                            'main_prediction')
                        if isinstance(metrics_src, dict): single_summary['top_metrics_sample'] = dict(
                            list(metrics_src.items())[:2])  # ÊúÄÂ§ö2‰∏™Á§∫‰æãÊåáÊ†á

                        if 'trend_summary' in res_dict and res_dict['trend_summary']: single_summary['trend_snapshot'] = \
                        res_dict['trend_summary']
                        if 'key_findings' in res_dict and isinstance(res_dict['key_findings'], list) and res_dict[
                            'key_findings']:
                            single_summary['top_finding'] = str(res_dict['key_findings'][0])[:150] + "..."

                        if single_summary:  # Âè™Ê∑ªÂä†ÊúâÂÜÖÂÆπÁöÑÊëòË¶Å
                            summary['multi_analysis_highlights'][key] = single_summary
            else:  # Âçï‰∏™Â§ÑÁêÜÂô®ÁöÑÁªìÊûú (ÂØπË±°ÊàñÂÖ∂Â≠óÂÖ∏)
                res_dict = _convert_to_dict_for_summary(primary_res_payload)
                if res_dict:
                    if 'main_answer' in res_dict: summary['main_answer_snippet'] = str(res_dict['main_answer'])[
                                                                                   :200] + "..."
                    metrics_src = res_dict.get('key_metrics') or res_dict.get('metrics') or res_dict.get(
                        'main_prediction')
                    if isinstance(metrics_src, dict): summary['main_metrics_sample'] = dict(
                        list(metrics_src.items())[:3])  # ÊúÄÂ§ö3‰∏™Á§∫‰æãÊåáÊ†á
                    if 'trend_summary' in res_dict and res_dict['trend_summary']: summary['trend_snapshot'] = res_dict[
                        'trend_summary']
                    if 'key_findings' in res_dict and isinstance(res_dict['key_findings'], list) and res_dict[
                        'key_findings']:
                        summary['top_finding'] = str(res_dict['key_findings'][0])[:150] + "..."
                elif primary_res_payload:  # Â¶ÇÊûúËΩ¨Êç¢Â≠óÂÖ∏Â§±Ë¥•Ôºå‰ΩÜÂØπË±°Â≠òÂú®
                    summary['raw_primary_result_type'] = str(type(primary_res_payload))

            if not summary.get('main_answer_snippet') and not summary.get('multi_analysis_highlights'):
                summary['general_summary'] = "Á≥ªÁªüÂ∑≤ÂÆåÊàêÂ§öÈ°πÂàÜÊûêÔºåÂÖ∑‰ΩìÁªÜËäÇËØ∑ÂèÇËÄÉÊåáÊ†áÂíåÊ¥ûÂØüÈÉ®ÂàÜ„ÄÇ"

            return summary

    async def _generate_fallback_response(self, user_query: str, error: str) -> str:
            """Âú®Â§ÑÁêÜÂ§±Ë¥•Êó∂ÁîüÊàê‰∏Ä‰∏™Áî®Êà∑ÂèãÂ•ΩÁöÑ‰∏≠ÊñáÈôçÁ∫ßÂìçÂ∫î„ÄÇ"""
            logger.info(f"‰∏∫Êü•ËØ¢ÁîüÊàêÈôçÁ∫ßÂìçÂ∫î: '{user_query[:50]}...'ÔºåÈîôËØØ: {error[:100]}")

            # Â∞ùËØï‰ΩøÁî®AIÔºàÂ¶ÇÊûú‰∏Ä‰∏™Ê®°ÂûãÂèØÁî®‰∏îÈîôËØØ‰∏çÊòØAIÊúçÂä°Êú¨Ë∫´ÁöÑÈóÆÈ¢òÔºâÁîüÊàêÊõ¥Êô∫ËÉΩÁöÑÈîôËØØÊèêÁ§∫
            ai_client_for_fallback = None
            error_lower = error.lower()

            # ‰ºòÂÖàÈÄâÊã©‰∏éÂØºËá¥ÈîôËØØÁöÑÊ®°Âûã‰∏çÂêåÁöÑÊ®°ÂûãÔºåÊàñËÄÖÂ¶ÇÊûúÈîôËØØ‰∏éAIÊó†ÂÖ≥ÔºåÂàô‰ºòÂÖàClaude
            if "claude" not in error_lower and self.claude_client:
                ai_client_for_fallback = self.claude_client
            elif ("gpt" not in error_lower and "openai" not in error_lower) and self.gpt_client:
                ai_client_for_fallback = self.gpt_client
            elif self.claude_client:  # Â¶ÇÊûú‰∏§‰∏™ÈÉΩÊ≤°Âú®ÈîôËØØ‰ø°ÊÅØ‰∏≠ÔºåÈªòËÆ§Â∞ùËØïClaude
                ai_client_for_fallback = self.claude_client
            elif self.gpt_client:  # ÂÖ∂Ê¨°Â∞ùËØïGPT
                ai_client_for_fallback = self.gpt_client

            if ai_client_for_fallback and self.config.get('fallback_response_enabled', True):
                try:
                    # ‰∏≠ÊñáÊèêÁ§∫ËØç
                    fallback_prompt = f"""
                    Áî®Êà∑‰πãÂâçÁöÑÊü•ËØ¢ÊòØÔºö‚Äú{user_query}‚Äù
                    Á≥ªÁªüÂú®Â∞ùËØïÂ§ÑÁêÜËøô‰∏™Êü•ËØ¢Êó∂ÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÂÜÖÈÉ®ÈîôËØØÔºåÈîôËØØ‰ø°ÊÅØÊëòË¶ÅÂ¶Ç‰∏ãÔºàÊ≠§ÊëòË¶Å‰ªÖ‰æõÊÇ®ÂèÇËÄÉÔºå‰∏çË¶ÅÁõ¥Êé•Â±ïÁ§∫ÁªôÁî®Êà∑ÔºâÔºö
                    ‚Äú{error[:150]}...‚Äù

                    ËØ∑‰Ω†ÊâÆÊºî‰∏Ä‰∏™‰πê‰∫éÂä©‰∫∫‰∏î‰∏ì‰∏öÁöÑAIÂÆ¢ÊúçÔºåÁî®ÁÆÄ‰Ωì‰∏≠ÊñáÁªôÁî®Êà∑ÁîüÊàê‰∏Ä‰∏™ÁÆÄÊ¥Å„ÄÅÂèãÂ•ΩÁöÑÂõûÂ§ç„ÄÇ
                    ÊÇ®ÁöÑÂõûÂ§çÂ∫îËØ•ÂåÖÂê´‰ª•‰∏ãË¶ÅÁÇπÔºö
                    1. ÂØπÁî®Êà∑Ë°®Á§∫Ê≠âÊÑèÔºåËØ¥Êòé‰ªñ‰ª¨ÁöÑËØ∑Ê±ÇÊú™ËÉΩÊàêÂäüÂ§ÑÁêÜ„ÄÇ
                    2. Êó†ÈúÄÂêëÁî®Êà∑Â§çËø∞ÂÖ∑‰ΩìÁöÑÈîôËØØ‰ø°ÊÅØÊëòË¶ÅÔºåÈô§ÈùûÊòØÈùûÂ∏∏ÊòéÁ°ÆÁöÑÁî®Êà∑ËæìÂÖ•ÂèÇÊï∞ÈóÆÈ¢òÔºà‰ΩÜËøôÁßçÊÉÖÂÜµÂæàÂ∞ëÔºâ„ÄÇ
                    3. Âª∫ËÆÆÁî®Êà∑ÂèØ‰ª•Â∞ùËØïÁöÑÊìç‰ΩúÔºå‰æãÂ¶ÇÔºö
                        - Á®çÂêéÈáçËØï„ÄÇ
                        - Â∞ùËØïÁî®‰∏çÂêåÁöÑÊñπÂºèË°®Ëø∞‰ªñ‰ª¨ÁöÑÈóÆÈ¢òÔºåÊàñËÄÖÁÆÄÂåñÊü•ËØ¢„ÄÇ
                        - ÔºàÂ¶ÇÊûúÈÄÇÁî®ÔºâÊ£ÄÊü•‰ªñ‰ª¨ÁöÑËæìÂÖ•ÊòØÂê¶Á¨¶ÂêàÈ¢ÑÊúüÊ†ºÂºè„ÄÇ
                    4. Â¶ÇÊûúÈîôËØØÁúãËµ∑Êù•ÊòØ‰∏¥Êó∂ÁöÑÁ≥ªÁªüÊÄßÈóÆÈ¢òÔºåÂèØ‰ª•ÊöóÁ§∫ÊäÄÊúØÂõ¢ÈòüÂèØËÉΩÂ∑≤Áü•ÊôìÂπ∂Âú®Â§ÑÁêÜ„ÄÇ
                    ËØ∑Á°Æ‰øùÂõûÂ§çËØ≠Ê∞î‰∏ì‰∏ö„ÄÅÂÆâÊäöÁî®Êà∑ÔºåÂπ∂‰∏î‰∏çË¶ÅÊèê‰æõËôöÂÅáÊâøËØ∫„ÄÇÂõûÂ§çÈïøÂ∫¶ÊéßÂà∂Âú®1-2Âè•ËØù„ÄÇ
                    """
                    ai_response_data = None
                    if isinstance(ai_client_for_fallback, ClaudeClient) and hasattr(ai_client_for_fallback,
                                                                                    'generate_text'):
                        ai_response_data = await ai_client_for_fallback.generate_text(fallback_prompt, max_tokens=200)
                    elif isinstance(ai_client_for_fallback, OpenAIClient) and hasattr(ai_client_for_fallback,
                                                                                      'generate_completion'):
                        ai_response_data = await ai_client_for_fallback.generate_completion(fallback_prompt,
                                                                                            max_tokens=200)

                    if ai_response_data and ai_response_data.get('success', False):
                        content = ai_response_data.get('text', ai_response_data.get('response',
                                                                                    ai_response_data.get('completion')))
                        if content and isinstance(content, str) and content.strip():
                            logger.info("AIÊàêÂäüÁîüÊàê‰∫ÜÈôçÁ∫ßÂìçÂ∫î„ÄÇ")
                            return content.strip()
                except Exception as ai_fallback_err:
                    logger.error(f"‰ΩøÁî®AIÁîüÊàêÈôçÁ∫ßÂìçÂ∫îÊó∂‰πüÂèëÁîüÈîôËØØ: {ai_fallback_err}")

            # Â¶ÇÊûúAIÁîüÊàêÈôçÁ∫ßÂìçÂ∫îÂ§±Ë¥•ÔºåÊàñÊ≤°ÊúâAIÂÆ¢Êà∑Á´ØÔºåÂàô‰ΩøÁî®Âü∫‰∫éËßÑÂàôÁöÑÁÆÄÂçï‰∏≠ÊñáÂìçÂ∫î
            query_lower = user_query.lower()
            if any(kw in query_lower for kw in ["‰ΩôÈ¢ù", "balance"]):
                return "Êä±Ê≠âÔºåÁ≥ªÁªüÂΩìÂâçÊó†Ê≥ïÊü•ËØ¢‰ΩôÈ¢ù‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÁöÑÂ∑•Á®ãÂ∏àÊ≠£Âú®Á¥ßÊÄ•Â§ÑÁêÜÔºåËØ∑ÊÇ®Á®çÂêéÈáçËØïÊàñËÅîÁ≥ªÊàë‰ª¨ÁöÑÊîØÊåÅÂõ¢Èòü„ÄÇ"
            elif any(kw in query_lower for kw in ["Ë∂ãÂäø", "trend", "ÂéÜÂè≤", "history"]):
                return "Êä±Ê≠âÔºåÂéÜÂè≤Êï∞ÊçÆÂàÜÊûêÂäüËÉΩÊöÇÊó∂ÈÅáÂà∞‰∏Ä‰∫õÊäÄÊúØÈóÆÈ¢ò„ÄÇÊàë‰ª¨Ê≠£Âú®Âä™Âäõ‰øÆÂ§ç‰∏≠ÔºåËØ∑ÊÇ®Á®çÂêéÂÜçËØï„ÄÇ"
            elif any(kw in query_lower for kw in ["È¢ÑÊµã", "predict", "È¢ÑËÆ°", "forecast"]):
                return "Êä±Ê≠âÔºåÈ¢ÑÊµãÊúçÂä°ÂΩìÂâçÊöÇÊó∂‰∏çÂèØÁî®„ÄÇËØ∑ÊÇ®Á®çÁ≠âÁâáÂàªÂÜçÂ∞ùËØïÊÇ®ÁöÑÈ¢ÑÊµãËØ∑Ê±Ç„ÄÇ"
            return f"Â§ÑÁêÜÊÇ®ÁöÑÊü•ËØ¢ '{user_query[:30]}...' Êó∂ÈÅáÂà∞‰∏Ä‰∏™ÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÊäÄÊúØÂõ¢ÈòüÂ∑≤ÁªèÊ≥®ÊÑèÂà∞Ê≠§ÊÉÖÂÜµÔºåÂπ∂Â∞ÜÂ∞ΩÂø´Ëß£ÂÜ≥„ÄÇÁªôÊÇ®Â∏¶Êù•‰∏ç‰æøÔºåÊàë‰ª¨Ê∑±Ë°®Ê≠âÊÑèÔºåËØ∑Á®çÂêéÈáçËØï„ÄÇ"
    def _calculate_overall_confidence(self, query_analysis: Optional[QueryAnalysisResult],
                                      business_result_data: Dict[str, Any],
                                      insights_list: List[BusinessInsight],
                                      data_quality_score: float) -> float:
        """
        ËÆ°ÁÆóÊï¥‰∏™Êü•ËØ¢Â§ÑÁêÜÊµÅÁ®ãÁöÑÊï¥‰ΩìÁΩÆ‰ø°Â∫¶„ÄÇ
        ÁªºÂêàËÄÉËôëÊü•ËØ¢Ëß£Êûê„ÄÅÊï∞ÊçÆË¥®Èáè„ÄÅ‰∏öÂä°Â§ÑÁêÜÂíåÊ¥ûÂØüÁöÑÁΩÆ‰ø°Â∫¶„ÄÇ
        """
        factors: List[float] = []

        # 1. Êü•ËØ¢Ëß£ÊûêÁΩÆ‰ø°Â∫¶
        if query_analysis and hasattr(query_analysis, 'confidence_score'):
            factors.append(float(query_analysis.confidence_score))
        else:
            factors.append(0.5)  # Â¶ÇÊûúÊ≤°ÊúâÊü•ËØ¢ÂàÜÊûêÁªìÊûúÔºåÁªô‰∏Ä‰∏™ËæÉ‰ΩéÁöÑÈªòËÆ§ÂÄº

        # 2. Êï∞ÊçÆË¥®ÈáèËØÑÂàÜ (Áî± SmartDataFetcher Êèê‰æõ)
        factors.append(float(data_quality_score))

        # 3. ‰∏öÂä°Â§ÑÁêÜÁΩÆ‰ø°Â∫¶
        # business_result_data ÂåÖÂê´‰∫Ü 'confidence_score' ÈîÆ
        if business_result_data and isinstance(business_result_data, dict):
            factors.append(float(business_result_data.get('confidence_score', 0.7)))  # ÈªòËÆ§0.7Â¶ÇÊûú‰∏öÂä°Â§ÑÁêÜÂ±ÇÊú™Êèê‰æõ
        else:
            factors.append(0.5)

        # 4. Ê¥ûÂØüË¥®Èáè (Âü∫‰∫éÊ¥ûÂØüÂàóË°®ÂíåÂçï‰∏™Ê¥ûÂØüÁöÑÁΩÆ‰ø°Â∫¶)
        if insights_list:
            insights_confidences = [
                float(getattr(i, 'confidence_score', 0.7)) for i in insights_list if hasattr(i, 'confidence_score')
            ]
            if insights_confidences:
                factors.append(sum(insights_confidences) / len(insights_confidences))
            else:
                factors.append(0.6)  # ÊúâÊ¥ûÂØü‰ΩÜÊ≤°ÊúâÂÖ∑‰ΩìÁΩÆ‰ø°Â∫¶
        else:
            factors.append(0.5)  # Ê≤°ÊúâÊ¥ûÂØü

        # ÂÆö‰πâÂêÑÂõ†Á¥†ÁöÑÊùÉÈáçÔºåÊÄªÂíå‰∏∫1
        # ÊùÉÈáçÂèØ‰ª•Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµË∞ÉÊï¥Ôºå‰æãÂ¶ÇÔºåÊï∞ÊçÆË¥®ÈáèÂèØËÉΩÊØîÊ¥ûÂØüÁΩÆ‰ø°Â∫¶Êõ¥ÈáçË¶Å
        # È°∫Â∫èÔºöËß£Êûê„ÄÅÊï∞ÊçÆË¥®Èáè„ÄÅ‰∏öÂä°Â§ÑÁêÜ„ÄÅÊ¥ûÂØü
        weights = [0.20, 0.30, 0.30, 0.20]

        if len(factors) == len(weights):
            weighted_sum = sum(factor * weight for factor, weight in zip(factors, weights))
            final_confidence = round(weighted_sum, 3)
        elif factors:  # Â¶ÇÊûúÊùÉÈáçÂíåÂõ†Á¥†Êï∞Èáè‰∏çÂåπÈÖçÔºà‰∏çÂ∫îÂèëÁîüÔºâÔºåÂàôÂèñÁÆÄÂçïÂπ≥Âùá
            logger.warning(
                f"Confidence factors count ({len(factors)}) does not match weights count ({len(weights)}). Using simple average.")
            final_confidence = round(sum(factors) / len(factors), 3)
        else:  # ‰∏çÂ∫îÂèëÁîüÔºå‰ΩÜ‰Ωú‰∏∫‰øùÊä§
            final_confidence = 0.3

        logger.debug(f"Calculated overall confidence: {final_confidence}, based on factors: {factors}")
        return final_confidence

    def _extract_key_metrics(self, business_result_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ‰ªé‰∏öÂä°Â§ÑÁêÜÁªìÊûú‰∏≠ÊèêÂèñÂÖ≥ÈîÆÊåáÊ†áÔºåÂπ∂‰ΩøÁî® FinancialFormatter ËøõË°åÊ†ºÂºèÂåñ„ÄÇ
        ËøîÂõû‰∏Ä‰∏™Â≠óÂÖ∏ÔºåÈîÆÊòØÊåáÊ†áÂêçÁß∞ÔºåÂÄºÊòØÊ†ºÂºèÂåñÂêéÁöÑÊåáÊ†áÂÄºÔºàÂ≠óÁ¨¶‰∏≤Ôºâ„ÄÇ
        """
        raw_metrics: Dict[str, Any] = {}  # Â≠òÂÇ®ÂéüÂßãÊèêÂèñÁöÑÊåáÊ†á
        if not business_result_data or not isinstance(business_result_data, dict):
            logger.debug("_extract_key_metrics: business_result_data is empty or invalid.")
            return {}

        primary_res_content = business_result_data.get('primary_result')
        logger.debug(f"_extract_key_metrics: primary_res_content type is {type(primary_res_content)}")

        def _extract_from_single_source(source_dict: Any, prefix: str = ""):
            """ËæÖÂä©ÂáΩÊï∞Ôºå‰ªéÂçï‰∏™ÁªìÊûúÂØπË±°ÊàñÂ≠óÂÖ∏‰∏≠ÊèêÂèñÊåáÊ†á„ÄÇ"""
            if not source_dict: return

            data_to_search: Optional[Dict[str, Any]] = None
            if hasattr(source_dict, 'to_dict') and callable(source_dict.to_dict):  # Â§ÑÁêÜdataclassÂÆû‰æã
                data_to_search = source_dict.to_dict()
            elif hasattr(source_dict, '__dict__'):  # Â§ÑÁêÜÂÖ∂‰ªñÊôÆÈÄöÂØπË±°ÂÆû‰æã
                data_to_search = source_dict.__dict__
            elif isinstance(source_dict, dict):  # Â¶ÇÊûúÂ∑≤ÁªèÊòØÂ≠óÂÖ∏
                data_to_search = source_dict

            if not data_to_search:
                logger.debug(
                    f"_extract_from_single_source: source_dict (prefix: {prefix}) is not dict-convertible or is None.")
                return

            # Êü•ÊâæÂ∏∏ËßÅÁöÑÊåáÊ†áÂÆπÂô®ÈîÆ
            # ÊÇ®ÁöÑÂ§ÑÁêÜÂô®ÂìçÂ∫îÂØπË±°ÔºàCurrentDataResponse, HistoricalAnalysisResponse, PredictionResponseÔºâ
            # ‰∏≠ÂåÖÂê´ 'key_metrics', 'metrics', 'main_prediction' Á≠âÂ≠óÊÆµ
            metric_container_keys = ['key_metrics', 'metrics', 'main_prediction', 'predictions_summary',
                                     'data']  # 'data' for CurrentDataResponse

            found_metrics_in_container = False
            for container_key in metric_container_keys:
                if container_key in data_to_search and isinstance(data_to_search[container_key], dict):
                    for k, v in data_to_search[container_key].items():
                        metric_name_to_store = f"{prefix}{k}".strip('_')
                        if metric_name_to_store in raw_metrics and raw_metrics[metric_name_to_store] != v:
                            logger.debug(
                                f"Metric conflict for '{metric_name_to_store}'. Current: {raw_metrics[metric_name_to_store]}, New: {v}. Making key unique.")
                            metric_name_to_store = f"{prefix}{container_key}_{k}".strip('_')  # Â¢ûÂä†ÂîØ‰∏ÄÊÄß
                        raw_metrics[metric_name_to_store] = v
                    found_metrics_in_container = True
                    # Â¶ÇÊûú‰∏Ä‰∏™ÂÆπÂô®ÊâæÂà∞‰∫ÜÊåáÊ†áÔºåÂèØËÉΩ‰∏çÈúÄË¶ÅÂÜçÊ£ÄÊü•Ê≠§ÂØπË±°ÁöÑÂÖ∂‰ªñÂÆπÂô®ÈîÆ‰∫ÜÔºå
                    # ‰ΩÜËøôÂèñÂÜ≥‰∫éÊÇ®ÁöÑÊï∞ÊçÆÁªìÊûÑËÆæËÆ°„ÄÇÂ¶ÇÊûúÂ§ö‰∏™ÂÆπÂô®ÈÉΩÂèØËÉΩÂåÖÂê´Áã¨Á´ãÊåáÊ†áÔºåÂàôÁßªÈô§break„ÄÇ
                    # break

            # Â¶ÇÊûúÊ≤°ÊúâÂú®Ê†áÂáÜÂÆπÂô®‰∏≠ÊâæÂà∞ÔºåÂπ∂‰∏îdata_to_searchÊú¨Ë∫´Â∞±ÊòØÈ°∂Â±ÇÊåáÊ†á (‰æãÂ¶ÇCurrentDataResponseÁöÑdataÂ≠óÊÆµ)
            if not found_metrics_in_container and data_to_search and container_key == 'data' and 'key_metrics' not in data_to_search and 'metrics' not in data_to_search:
                for k, v in data_to_search.items():  # ÂÅáËÆæÈ°∂Â±ÇÈîÆÂÄºÂØπÊòØÊåáÊ†á
                    # ÈÅøÂÖçÊèêÂèñÈùûÊåáÊ†áÁöÑÂÖÉÊï∞ÊçÆÁ≠â
                    if isinstance(v, (str, int, float, bool)) or (isinstance(v, list) and len(v) < 5 and all(
                            isinstance(i, (str, int, float)) for i in v)):  # ÁÆÄÂçïÁ±ªÂûãÊàñÁü≠ÂàóË°®
                        metric_name_to_store = f"{prefix}{k}".strip('_')
                        if metric_name_to_store in raw_metrics and raw_metrics[metric_name_to_store] != v:
                            metric_name_to_store = f"{prefix}top_level_{k}".strip('_')
                        raw_metrics[metric_name_to_store] = v

        if isinstance(primary_res_content, dict):
            if 'multi_results' in primary_res_content and isinstance(primary_res_content['multi_results'], dict):
                logger.debug("_extract_key_metrics: Processing multi_results.")
                for proc_name, proc_res_obj_or_dict in primary_res_content['multi_results'].items():
                    if proc_res_obj_or_dict and not (
                            isinstance(proc_res_obj_or_dict, dict) and 'error' in proc_res_obj_or_dict):
                        _extract_from_single_source(proc_res_obj_or_dict, prefix=f"{proc_name}_")
            else:  # Single processor result (ÂèØËÉΩÂ∑≤ÁªèÊòØÂ≠óÂÖ∏Ôºå‰πüÂèØËÉΩÊòØÂØπË±°)
                logger.debug("_extract_key_metrics: Processing single primary_result (dict or unknown obj).")
                _extract_from_single_source(primary_res_content)
        elif primary_res_content:  # It's a single response object (not a dict from multi_results)
            logger.debug(
                f"_extract_key_metrics: Processing single primary_result (object type: {type(primary_res_content)}).")
            _extract_from_single_source(primary_res_content, prefix="main_")

        # Â∫îÁî® FinancialFormatter ËøõË°åÊ†ºÂºèÂåñ
        if self.financial_formatter:
            formatted_metrics: Dict[str, Any] = {}
            logger.debug(f"_extract_key_metrics: Applying financial formatting to {len(raw_metrics)} raw metrics.")
            for k, v_original in raw_metrics.items():
                v_formatted = v_original  # ÈªòËÆ§‰ΩøÁî®ÂéüÂßãÂÄº
                try:
                    # Â∞ùËØïÂ∞ÜÂÄºËΩ¨Êç¢‰∏∫ÂèØÊ†ºÂºèÂåñÁöÑÊï∞ÂÄº
                    numeric_value_for_formatting: Optional[Union[int, float]] = None
                    if isinstance(v_original, (int, float)):
                        numeric_value_for_formatting = v_original
                    elif isinstance(v_original, str):
                        val_str_cleaned = v_original.strip().replace('%', '').replace('¬•', '').replace(',', '').replace(
                            'Ôø•', '')
                        if val_str_cleaned.replace('.', '', 1).replace('-', '', 1).isdigit():
                            numeric_value_for_formatting = float(val_str_cleaned)

                    if numeric_value_for_formatting is not None:
                        key_lower = k.lower()
                        original_str_lower = str(v_original).lower()  # Áî®‰∫éÊ£ÄÊü•ÂéüÂßãÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑ '%' Á≠â

                        if "rate" in key_lower or "ratio" in key_lower or "growth" in key_lower or "change" in key_lower or "%" in original_str_lower:
                            # Â¶ÇÊûúÂéüÂßãÂ≠óÁ¨¶‰∏≤Âê´ '%', ÂÅáËÆæÂÆÉÊòØÁôæÂàÜÁÇπ (‰æãÂ¶Ç '5%')ÔºåÂê¶ÂàôÊòØÂ∞èÊï∞ (‰æãÂ¶Ç 0.05)
                            value_to_format_as_percentage = numeric_value_for_formatting / 100.0 if "%" in original_str_lower else numeric_value_for_formatting
                            v_formatted = self.financial_formatter.format_percentage(value_to_format_as_percentage)
                        elif any(curr_kw in key_lower for curr_kw in
                                 ["balance", "amount", "value", "inflow", "outflow", "ËµÑÈáë", "ÈáëÈ¢ù", "capital", "fund",
                                  "asset", "revenue", "profit", "cost", "‰ΩôÈ¢ù", "ÂÖ•Èáë", "Âá∫Èáë"]):
                            v_formatted = self.financial_formatter.format_currency(numeric_value_for_formatting)
                        else:  # ÂÖ∂‰ªñÊï∞ÂÄºÁöÑÈªòËÆ§Ê†ºÂºèÂåñ
                            v_formatted = f"{numeric_value_for_formatting:,.2f}" if isinstance(
                                numeric_value_for_formatting, float) else f"{numeric_value_for_formatting:,}"
                    # else v_formatted ‰øùÊåÅ v_original (ÈùûÊï∞ÂÄºÁ±ªÂûã)
                    formatted_metrics[k] = v_formatted
                except Exception as fmt_e:
                    logger.warning(
                        f"Financial formatting failed for metric '{k}' (value: '{v_original}'): {fmt_e}. Using original value.")
                    formatted_metrics[k] = v_original  # Âá∫ÈîôÊó∂ÂõûÈÄÄÂà∞ÂéüÂßãÂÄº
            logger.debug(f"_extract_key_metrics: Returning {len(formatted_metrics)} formatted metrics.")
            return formatted_metrics

        logger.debug(
            f"_extract_key_metrics: Returning {len(raw_metrics)} unformatted metrics (FinancialFormatter not available or no numeric values).")
        return raw_metrics  # Â¶ÇÊûúÊ≤°Êúâ formatterÔºåËøîÂõûÂéüÂßãÊèêÂèñÁöÑÊåáÊ†á

    def _get_processors_used(self, business_result_data: Dict[str, Any], strategy: OrchestratorProcessingStrategy) -> \
    List[str]:
        """
        ‰ªé‰∏öÂä°Â§ÑÁêÜÁªìÊûú‰∏≠ÊèêÂèñÂÆûÈôÖ‰ΩøÁî®ÁöÑÂ§ÑÁêÜÂô®ÂêçÁß∞ÂàóË°®„ÄÇ
        """
        if not business_result_data or not isinstance(business_result_data, dict):
            logger.warning("_get_processors_used: business_result_data is empty or invalid.")
            return []

        # 'processor_used' Â≠óÊÆµÂ∫îÁî± _orchestrate_business_processing ÊñπÊ≥ïÂú®ËøîÂõûÊó∂ËÆæÁΩÆ
        # ÂÆÉÈÄöÂ∏∏ÊòØ‰∏Ä‰∏™ÂåÖÂê´Â§ÑÁêÜÂô®Á±ªÂêçÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÁî®ÈÄóÂè∑ÂàÜÈöîÔºåÊàñËÄÖÊòØÁ±ªÂêçÂàóË°®
        processor_info = business_result_data.get('processor_used')  # Ëøô‰∏™ÈîÆÊòØÂú® _orchestrate_business_processing ‰∏≠ËÆæÁΩÆÁöÑ

        if isinstance(processor_info, str) and processor_info:
            # ÂàÜÂâ≤ÈÄóÂè∑ÂàÜÈöîÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÂπ∂ÂéªÈô§‰∏§Á´ØÁ©∫Ê†º
            return [name.strip() for name in processor_info.split(',') if name.strip()]
        elif isinstance(processor_info, list):
            # Á°Æ‰øùÂàóË°®‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†ÈÉΩÊòØÂ≠óÁ¨¶‰∏≤
            return [str(name).strip() for name in processor_info if str(name).strip()]

        # Â¶ÇÊûú 'processor_used' Â≠óÊÆµÁº∫Â§±ÊàñÊ†ºÂºè‰∏çÊ≠£Á°ÆÔºåÂ∞ùËØïÊ†πÊçÆÁ≠ñÁï•Êé®Êñ≠ (ËøôÂè™ÊòØ‰∏Ä‰∏™ÂêéÂ§áÊñπÊ°à)
        logger.warning(
            f"'_get_processors_used' could not determine processors from 'processor_used' field (value: {processor_info}). Falling back to strategy-based inference.")
        if strategy == OrchestratorProcessingStrategy.DIRECT_RESPONSE:
            return [
                self.current_data_processor.__class__.__name__ if self.current_data_processor else "CurrentDataProcessor"]
        if strategy == OrchestratorProcessingStrategy.SINGLE_PROCESSOR:
            # ÂØπ‰∫éÂçïÂ§ÑÁêÜÂô®ÔºåÂ¶ÇÊûúÊ≤°ÊúâÊòéÁ°ÆËÆ∞ÂΩïÔºåÂæàÈöæÁ≤æÁ°ÆÁü•ÈÅìÊòØÂì™‰∏™„ÄÇ
            # _orchestrate_business_processing Â∫îËØ•ËÆæÁΩÆ processor_used„ÄÇ
            # ËøôÈáåËøîÂõû‰∏Ä‰∏™ÈÄöÁî®Âç†‰ΩçÁ¨¶ÊàñÂ∞ùËØï‰ªé primary_result ÁöÑÁ±ªÂûãÊé®Êñ≠„ÄÇ
            primary_res = business_result_data.get('primary_result')
            if isinstance(primary_res, CurrentDataResponse): return [CurrentDataProcessor.__class__.__name__]
            if isinstance(primary_res, HistoricalAnalysisResponse): return [
                HistoricalAnalysisProcessor.__class__.__name__]
            if isinstance(primary_res, PredictionResponse): return [PredictionProcessor.__class__.__name__]
            return ["UnknownSingleProcessor"]
        if strategy == OrchestratorProcessingStrategy.MULTI_PROCESSOR:
            # ÊèêÂèñ multi_results ‰∏≠ÁöÑÈîÆÂêç‰Ωú‰∏∫Â§ÑÁêÜÂô®Ê†áËØÜÁ¨¶
            multi_res = business_result_data.get('primary_result', {}).get('multi_results', {})
            if isinstance(multi_res, dict):
                return list(multi_res.keys())  # ‰æãÂ¶Ç ['current_data_analysis', 'historical_analysis']
            return ["MultipleProcessorsInferred"]
        if strategy == OrchestratorProcessingStrategy.FULL_PIPELINE:
            multi_res = business_result_data.get('primary_result', {}).get('multi_results', {})
            processors = []
            if isinstance(multi_res, dict): processors.extend(list(multi_res.keys()))
            if 'financial_deep_dive' in processors:  # ÈÄöÂ∏∏ financial_deep_dive ÊòØ FinancialDataAnalyzer
                # Remove and add proper class name if available
                if self.financial_data_analyzer:
                    processors = [p for p in processors if p != 'financial_deep_dive']
                    processors.append(self.financial_data_analyzer.__class__.__name__)
            return list(set(processors))  # ÂéªÈáç

        return ["UnknownStrategyOrNoProcessors"]

    def _get_ai_collaboration_summary(self, query_analysis: Optional[QueryAnalysisResult],
                                      business_result_data: Dict[str, Any],
                                      insights_list: List[BusinessInsight]) -> Dict[str, Any]:
        """
        ÊÄªÁªìAIÊ®°ÂûãÔºàÂ¶ÇClaude, GPTÔºâÂú®Êï¥‰∏™Â§ÑÁêÜÊµÅÁ®ã‰∏≠ÁöÑ‰ΩøÁî®ÊÉÖÂÜµ„ÄÇ
        Êõ¥ÂáÜÁ°ÆÁöÑÁªüËÆ°ÈúÄË¶ÅÂêÑÁªÑ‰ª∂Âú®ÂÖ∂ËøîÂõûÁöÑÂÖÉÊï∞ÊçÆ‰∏≠Êä•ÂëäAI‰ΩøÁî®ÊÉÖÂÜµ„ÄÇ
        """
        claude_invoked_count = 0
        gpt_invoked_count = 0
        other_ai_invoked_count = 0  # ‰∏∫Êú™Êù•Êâ©Â±ï

        # 1. Êü•ËØ¢Ëß£ÊûêÈò∂ÊÆµÁöÑAI‰ΩøÁî®
        if query_analysis and hasattr(query_analysis, 'ai_collaboration_plan') and \
                isinstance(query_analysis.ai_collaboration_plan, dict):
            plan = query_analysis.ai_collaboration_plan
            if plan.get('primary_ai', '').lower() == 'claude' or plan.get('secondary_ai', '').lower() == 'claude' or \
                    (isinstance(plan.get('claude_tasks'), list) and len(plan['claude_tasks']) > 0):
                claude_invoked_count += 1
            if plan.get('primary_ai', '').lower() == 'gpt' or plan.get('secondary_ai', '').lower() == 'gpt' or \
                    (isinstance(plan.get('gpt_tasks'), list) and len(plan['gpt_tasks']) > 0):
                gpt_invoked_count += 1
        elif query_analysis:  # Â¶ÇÊûúÊ≤°ÊúâËØ¶ÁªÜËÆ°ÂàíÔºå‰ΩÜËß£ÊûêÁΩÆ‰ø°Â∫¶È´òÔºå‰πüÂèØËÉΩÁî®‰∫ÜAI
            if query_analysis.confidence_score > 0.7 and (self.claude_client or self.gpt_client):
                # Êó†Ê≥ïÂå∫ÂàÜÊòØÂì™‰∏™ÔºåÂÅáËÆæÊòØ‰∏ªAI (e.g., Claude)
                if self.claude_client:
                    claude_invoked_count += 1
                elif self.gpt_client:
                    gpt_invoked_count += 1

        # 2. ‰∏öÂä°Â§ÑÁêÜÈò∂ÊÆµÁöÑAI‰ΩøÁî® (ÈúÄË¶ÅÂ§ÑÁêÜÂô®Âú®ÂÖ∂ËøîÂõûÁöÑÂÖÉÊï∞ÊçÆ‰∏≠Êä•Âëä)
        # ‰æãÂ¶ÇÔºå business_result_data['metadata']['ai_models_used_in_processing'] = ['claude', 'gpt']
        biz_proc_meta = business_result_data.get('metadata', {})
        if isinstance(biz_proc_meta, dict):
            models_in_biz = biz_proc_meta.get('ai_models_used_in_processing', [])  # ÊúüÊúõÊòØÂàóË°®
            if 'claude' in models_in_biz: claude_invoked_count += 1
            if 'gpt' in models_in_biz: gpt_invoked_count += 1
            # ‰πüÂèØ‰ª•‰ªé primary_result.multi_results ÁöÑÊØè‰∏™ÁªìÊûúÁöÑmetadata‰∏≠Á¥ØÂä†

        # 3. Ê¥ûÂØüÁîüÊàêÈò∂ÊÆµÁöÑAI‰ΩøÁî®
        # InsightGenerator ÈÄöÂ∏∏Âº∫‰æùËµñAI
        if insights_list:  # Â¶ÇÊûúÁîüÊàê‰∫ÜÊ¥ûÂØü
            insight_gen_meta = business_result_data.get('insight_generation_metadata', {})
            if isinstance(insight_gen_meta, dict) and insight_gen_meta.get('ai_model_used'):
                model_str = str(insight_gen_meta['ai_model_used']).lower()
                if 'claude' in model_str:
                    claude_invoked_count += 1
                elif 'gpt' in model_str:
                    gpt_invoked_count += 1
            elif self.claude_client:  # ÈªòËÆ§InsightGenerator‰ΩøÁî®Claude
                claude_invoked_count += 1
            elif self.gpt_client:
                gpt_invoked_count += 1

        # 4. ÊúÄÁªàÂìçÂ∫îÊñáÊú¨ÁîüÊàêÈò∂ÊÆµÁöÑAI‰ΩøÁî®
        # _generate_intelligent_response ‰ºöÈÄâÊã©‰∏Ä‰∏™ÂÆ¢Êà∑Á´Ø
        if self.claude_client or self.gpt_client:  # Â¶ÇÊûúÊúâ‰ªª‰ΩïAIÂÆ¢Êà∑Á´ØÁî®‰∫éÂìçÂ∫îÁîüÊàê
            if self.claude_client and not self.gpt_client:
                claude_invoked_count += 1  # ÂÅáËÆæ‰ºòÂÖàClaude
            elif self.gpt_client and not self.claude_client:
                gpt_invoked_count += 1
            elif self.claude_client and self.gpt_client:
                claude_invoked_count += 1  # ÂÅáËÆæ‰ºòÂÖàClaude

        claude_actually_used = claude_invoked_count > 0
        gpt_actually_used = gpt_invoked_count > 0

        collaboration_level = "none"
        if claude_actually_used and gpt_actually_used:
            collaboration_level = "dual_ai_collaboration"
        elif claude_actually_used or gpt_actually_used:
            collaboration_level = "single_ai_assist"

        return {
            'claude_used_in_process': claude_actually_used,
            'gpt_used_in_process': gpt_actually_used,
            'claude_invocation_count_estimate': claude_invoked_count,  # ‰º∞ÁÆóË∞ÉÁî®Ê¨°Êï∞
            'gpt_invocation_count_estimate': gpt_invoked_count,
            'collaboration_level': collaboration_level,
            'ai_enhanced_parsing': getattr(query_analysis, 'confidence_score', 0.0) > 0.75 and \
                                   getattr(query_analysis, 'processing_metadata', {}).get(
                                       'parser_status') != 'fallback_rule_based' if query_analysis else False,
            'ai_generated_insights': bool(insights_list),
        }
    async def _intelligent_query_parsing(self, user_query: str,
                                         conversation_id_for_db: Optional[int] = None) -> QueryAnalysisResult:
        """
        AIÈ©±Âä®ÁöÑÊô∫ËÉΩÊü•ËØ¢Ëß£ÊûêÔºå‰ΩøÁî® SmartQueryParser„ÄÇ
        Ëé∑ÂèñÂØπËØù‰∏ä‰∏ãÊñáÔºåË∞ÉÁî®Êü•ËØ¢Ëß£ÊûêÂô®ÔºåÂπ∂Â§ÑÁêÜÁºìÂ≠ò„ÄÇ
        """
        method_start_time = time.time()
        logger.debug(
            f"Orchestrator: Calling QueryParser for '{user_query[:50]}...' with ConvID_DB: {conversation_id_for_db}")
        if not self.query_parser:
            logger.error("QueryParser not initialized in Orchestrator. Cannot parse query.")
            return await self._fallback_query_parsing(user_query)

        context_for_parser: Dict[str, Any] = {}
        if conversation_id_for_db and self.conversation_manager:
            try:
                context_for_parser = self.conversation_manager.get_context(
                    conversation_id_for_db)  # Implemented in ConversationManager
                logger.debug(
                    f"Context for QueryParser (ConvID {conversation_id_for_db}): Found {len(context_for_parser.get('recent_history', []))} history messages.")
            except Exception as e_ctx:
                logger.error(
                    f"Failed to get context for ConvID {conversation_id_for_db} from ConversationManager: {e_ctx}")

        cache_key = self._generate_query_cache_key(user_query, context_for_parser)  # Implemented
        cached_result = self._get_cached_result(cache_key)  # Implemented
        if cached_result and isinstance(cached_result, QueryAnalysisResult):
            self.orchestrator_stats['cache_hits'] += 1
            logger.info(f"QueryParser cache hit for: '{user_query[:50]}...'")
            return cached_result

        self.orchestrator_stats['cache_misses'] += 1
        logger.info(f"QueryParser cache miss. Executing AI parsing for: '{user_query[:50]}...'")

        try:
            query_analysis_result: QueryAnalysisResult = await self.query_parser.parse_complex_query(user_query,
                                                                                                     context_for_parser)
            if query_analysis_result and query_analysis_result.confidence_score > self.config.get(
                    'min_confidence_threshold_parsing_for_cache', 0.5):
                self._cache_result(cache_key, query_analysis_result)  # Implemented
            elif not query_analysis_result:
                logger.warning(f"QueryParser returned None for query: '{user_query[:50]}...'. Using fallback.")
                query_analysis_result = await self._fallback_query_parsing(user_query)  # Implemented
        except Exception as e_parse:
            logger.error(f"Error during query parsing by SmartQueryParser: {e_parse}\n{traceback.format_exc()}")
            query_analysis_result = await self._fallback_query_parsing(user_query)

        logger.debug(
            f"Query parsing took {time.time() - method_start_time:.3f}s. Complexity: {query_analysis_result.complexity.value if query_analysis_result else 'N/A'}")
        return query_analysis_result

    def _generate_query_cache_key(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        ‰∏∫Êü•ËØ¢ÂíåÂèØÈÄâÁöÑ‰∏ä‰∏ãÊñáÁîüÊàê‰∏Ä‰∏™‰∏ÄËá¥ÁöÑ„ÄÅÂèØÁî®‰∫éÁºìÂ≠òÁöÑÂìàÂ∏åÈîÆ„ÄÇ
        ‰ΩøÁî® CustomJSONEncoder Â§ÑÁêÜ context ‰∏≠ÂèØËÉΩÂ≠òÂú®ÁöÑÂ§çÊùÇÂØπË±°ÔºàÂ¶Ç datetimeÔºâ„ÄÇ
        """
        try:
            # ÂØπ‰∏ä‰∏ãÊñáËøõË°åËßÑËåÉÂåñÊéíÂ∫èÔºå‰ª•Á°Æ‰øùÁõ∏ÂêåÂÜÖÂÆπÁöÑ‰∏ä‰∏ãÊñáÁîüÊàêÁõ∏ÂêåÁöÑÈîÆ
            context_str = json.dumps(context or {}, sort_keys=True, cls=CustomJSONEncoder, ensure_ascii=False)
        except TypeError as te:
            logger.warning(
                f"Failed to serialize context for cache key due to TypeError: {te}. Using empty context for key.")
            context_str = "{}"  # Fallback to empty context string if serialization fails

        cache_data = f"{query}_{context_str}"
        return hashlib.md5(cache_data.encode('utf-8')).hexdigest()

    def _get_cached_result(self, cache_key: str) -> Optional[Union[QueryAnalysisResult, ProcessingResult]]:
        """
        ‰ªéÁºìÂ≠ò‰∏≠Ëé∑ÂèñÁªìÊûú„ÄÇÂ¶ÇÊûúÊâæÂà∞‰∏îÊú™ËøáÊúüÔºåÂàôËøîÂõûÁºìÂ≠òÊï∞ÊçÆ„ÄÇ
        Âê¶ÂàôÔºåÁßªÈô§ËøáÊúüÊù°ÁõÆÂπ∂ËøîÂõû None„ÄÇ
        """
        if not self.config.get('enable_intelligent_caching', False):
            return None

        if cache_key in self.result_cache:
            cache_entry = self.result_cache[cache_key]
            if (time.time() - cache_entry['timestamp']) < self.cache_ttl:
                logger.debug(f"Cache HIT for key: {cache_key[:10]}...")
                self.orchestrator_stats['cache_hits'] = self.orchestrator_stats.get('cache_hits', 0) + 1
                return cache_entry['data']
            else:
                logger.debug(f"Cache EXPIRED for key: {cache_key[:10]}... Removing.")
                del self.result_cache[cache_key]

        logger.debug(f"Cache MISS for key: {cache_key[:10]}...")
        self.orchestrator_stats['cache_misses'] = self.orchestrator_stats.get('cache_misses', 0) + 1
        return None

    def _cache_result(self, cache_key: str, result: Union[QueryAnalysisResult, ProcessingResult]):
        """
        Â∞ÜÁªìÊûúÔºàQueryAnalysisResult Êàñ ProcessingResultÔºâÂ≠òÂÖ•ÁºìÂ≠ò„ÄÇ
        ÁÆ°ÁêÜÁºìÂ≠òÂ§ßÂ∞èÔºåÂ¶ÇÊûúË∂ÖÂá∫ÈôêÂà∂ÂàôÁßªÈô§ÊúÄÊóßÁöÑÊù°ÁõÆ„ÄÇ
        """
        if not self.config.get('enable_intelligent_caching', False):
            return

        max_cache_size = self.config.get('max_cache_size', 100)
        if len(self.result_cache) >= max_cache_size:
            try:
                # ÁÆÄÂçïÁöÑLRUËøë‰ººÔºöÁßªÈô§Êó∂Èó¥Êà≥ÊúÄÊó©ÁöÑÊù°ÁõÆ
                oldest_key = min(self.result_cache, key=lambda k: self.result_cache[k]['timestamp'])
                del self.result_cache[oldest_key]
                logger.debug(f"Cache max size ({max_cache_size}) reached. Removed oldest entry: {oldest_key[:10]}...")
            except ValueError:  # ÁºìÂ≠òÂèØËÉΩÂú®Âπ∂ÂèëÊÉÖÂÜµ‰∏ãÂèòÁ©∫
                pass
            except Exception as e:
                logger.error(f"Error during cache eviction: {e}")

        self.result_cache[cache_key] = {
            'data': result,  # ÂèØ‰ª•ÊòØ QueryAnalysisResult Êàñ ProcessingResult
            'timestamp': time.time()
        }
        logger.debug(f"Cached result for key: {cache_key[:10]}... Cache size: {len(self.result_cache)}")

    def _update_orchestrator_stats(self, result: ProcessingResult):
        """Êõ¥Êñ∞ÁºñÊéíÂô®ÁöÑÊÄßËÉΩÂíå‰ΩøÁî®ÁªüËÆ°„ÄÇ"""
        if not isinstance(result, ProcessingResult):
            logger.warning(f"Attempted to update stats with invalid result type: {type(result)}")
            return

        # total_queries Âú® process_intelligent_query ÂºÄÂßãÊó∂Â∑≤Â¢ûÂä†
        total_queries = self.orchestrator_stats.get('total_queries', 0)
        if total_queries == 0:
            logger.warning(
                "Total queries is 0 in stats, cannot calculate averages. This might happen if called before total_queries is incremented.")
            # Potentially increment here if it's guaranteed this method is called once per query
            # self.orchestrator_stats['total_queries'] = 1
            # total_queries = 1
            return  # Avoid division by zero if total_queries is still 0

        if result.success:
            self.orchestrator_stats['successful_queries'] = self.orchestrator_stats.get('successful_queries', 0) + 1
        else:  # failed_queries Â∑≤ÁªèÂú® _handle_processing_error Êàñ _handle_processing_error_sync ‰∏≠Â¢ûÂä†
            pass
            # self.orchestrator_stats['failed_queries'] = self.orchestrator_stats.get('failed_queries', 0) + 1
            # if result.error_info and isinstance(result.error_info, dict):
            #     error_type = result.error_info.get('error_type', 'unknown_in_stats_update')
            #     self.orchestrator_stats['error_types'][error_type] = \
            #         self.orchestrator_stats['error_types'].get(error_type, 0) + 1

        # ÂÆâÂÖ®Âú∞ËÆ°ÁÆóÂπ≥ÂùáÂÄº
        current_avg_time = float(self.orchestrator_stats.get('avg_processing_time', 0.0))
        self.orchestrator_stats['avg_processing_time'] = \
            (current_avg_time * (total_queries - 1) + float(result.total_processing_time)) / total_queries

        current_avg_confidence = float(self.orchestrator_stats.get('avg_confidence_score', 0.0))
        self.orchestrator_stats['avg_confidence_score'] = \
            (current_avg_confidence * (total_queries - 1) + float(result.confidence_score)) / total_queries

        for processor_name in result.processors_used:  # processors_used is List[str]
            if isinstance(processor_name, str):
                self.orchestrator_stats['processor_usage'][processor_name] = \
                    self.orchestrator_stats['processor_usage'].get(processor_name, 0) + 1

        # AIÂçè‰ΩúÁªüËÆ°
        ai_collab_summary = result.ai_collaboration_summary
        if isinstance(ai_collab_summary, dict) and \
                ai_collab_summary.get('claude_used_in_process') and \
                ai_collab_summary.get('gpt_used_in_process'):
            self.orchestrator_stats['ai_collaboration_count'] = self.orchestrator_stats.get('ai_collaboration_count',
                                                                                            0) + 1

        logger.debug(f"Orchestrator stats updated for QueryID: {result.query_id}. Total queries: {total_queries}")

    def _log_ai_response_to_conversation(self, conversation_id_for_db: int,
                                         final_result: ProcessingResult, query_id: str):
        """ËæÖÂä©ÊñπÊ≥ïÔºöÂ∞ÜAIÂìçÂ∫îÂíåÂèØËßÜÂåñÂÜÖÂÆπËÆ∞ÂΩïÂà∞ÂØπËØùÂéÜÂè≤‰∏≠„ÄÇ"""
        if not self.conversation_manager:
            logger.warning(
                f"QueryID: {query_id} - ConversationManager not available. Skipping logging AI response to ConvID {conversation_id_for_db}.")
            return
        try:
            # Á°Æ‰øù conversation_id_for_db ÊòØÊï¥Êï∞ (ConversationManager ÁöÑÊñπÊ≥ïÊúüÊúõ int)
            if not isinstance(conversation_id_for_db, int):
                logger.error(
                    f"QueryID: {query_id} - Invalid conversation_id_for_db type: {type(conversation_id_for_db)}. Must be int for DB operations with ConversationManager. Skipping log.")
                return

            # Ê∑ªÂä†AIÂìçÂ∫îÊ∂àÊÅØ
            # add_message ËøîÂõûÊñ∞Ê∂àÊÅØÁöÑ ID
            ai_message_id = self.conversation_manager.add_message(
                conversation_id=conversation_id_for_db,
                is_user=False,  # AIÁöÑÂìçÂ∫î
                content=final_result.response_text,
                ai_model_used=str(final_result.processing_metadata.get('ai_models_invoked', [])),  # ‰ªéÂÖÉÊï∞ÊçÆËé∑Âèñ
                ai_strategy=final_result.processing_strategy.value,  # ‰ΩøÁî®Êûö‰∏æÂÄº
                processing_time=final_result.total_processing_time,
                confidence_score=final_result.confidence_score
            )
            logger.debug(
                f"QueryID: {query_id} - AI response (MessageID in DB: {ai_message_id}) logged to ConvID {conversation_id_for_db}.")

            # Ê∑ªÂä†ÂèØËßÜÂåñÂÜÖÂÆπÂà∞ message_visuals Ë°®
            if final_result.visualizations and ai_message_id > 0:  # Á°Æ‰øùÊúâÊ∂àÊÅØID
                for vis_idx, vis_data_dict in enumerate(final_result.visualizations):
                    if not isinstance(vis_data_dict, dict):
                        logger.warning(
                            f"QueryID: {query_id} - Invalid visual data for MsgID {ai_message_id}: {vis_data_dict}. Skipping.")
                        continue
                    try:
                        self.conversation_manager.add_visual(
                            message_id=ai_message_id,
                            visual_type=vis_data_dict.get('type', 'chart'),  # 'type' Âú® visualization dict ‰∏≠
                            visual_order=vis_idx,  # ÁÆÄÂçïÁöÑÈ°∫Â∫è
                            title=vis_data_dict.get('title', 'ÁîüÊàêÁöÑÂõæË°®'),
                            # 'data_payload' Êàñ 'data' ÈîÆÂèØËÉΩÂåÖÂê´ÂõæË°®ÁöÑÂÆûÈôÖÊï∞ÊçÆ/ÈÖçÁΩÆ
                            data=vis_data_dict.get('data_payload', vis_data_dict.get('data', {}))
                        )
                    except Exception as e_vis_add:
                        logger.error(
                            f"QueryID: {query_id} - Failed to add visual to MsgID {ai_message_id} for ConvID {conversation_id_for_db}: {e_vis_add}")
                logger.debug(
                    f"QueryID: {query_id} - {len(final_result.visualizations)} visuals logged to MessageID {ai_message_id} for ConvID {conversation_id_for_db}.")
            elif final_result.visualizations and ai_message_id <= 0:
                logger.warning(
                    f"QueryID: {query_id} - Got visuals but AI message ID ({ai_message_id}) is invalid. Visuals not logged for ConvID {conversation_id_for_db}.")


        except Exception as conv_err:
            # ÊçïËé∑ ConversationManager ÂèØËÉΩÊäõÂá∫ÁöÑ‰ªª‰ΩïÂºÇÂ∏∏
            logger.error(
                f"QueryID: {query_id} - Error logging AI response/visuals to ConvID {conversation_id_for_db}: {conv_err}\n{traceback.format_exc()}")
    def _handle_processing_error_sync(self, session_id: str, query_id: str,
                                      user_query: str, error_msg: str,
                                      total_time_on_error: float,
                                      conversation_id: Optional[str] = None) -> ProcessingResult:
        """
        ÂêåÊ≠•Â§ÑÁêÜÊµÅÁ®ã‰∏≠ÁöÑÂÖ≥ÈîÆÈîôËØØÔºåÁîüÊàêÈîôËØØÁªìÊûúÂØπË±°„ÄÇ
        Áî®‰∫éÂàùÂßãÂåñÂ§±Ë¥•Á≠âÊó†Ê≥ïËøõÂÖ•ÂÆåÊï¥ÂºÇÊ≠•ÊµÅÁ®ãÁöÑÂú∫ÊôØ„ÄÇ
        """
        # Ê≥®ÊÑèÔºöÊ≠§ÊñπÊ≥ïÊòØÂêåÊ≠•ÁöÑÔºå‰∏çÂ∫îË∞ÉÁî®‰ªª‰ΩïÂºÇÊ≠•Êìç‰ΩúÊàñ‰æùËµñAIÁîüÊàêÈôçÁ∫ßÂìçÂ∫î„ÄÇ

        current_stats = getattr(self, 'orchestrator_stats', self._default_stats())
        current_config = getattr(self, 'config', self._load_orchestrator_config())

        current_stats['failed_queries'] = current_stats.get('failed_queries', 0) + 1
        error_type = self._classify_error(error_msg)  # _classify_error ÂøÖÈ°ªÊòØÂêåÊ≠•ÁöÑ
        current_stats['error_types'][error_type] = \
            current_stats['error_types'].get(error_type, 0) + 1

        logger.error(f"QueryID: {query_id} - SYNC Handling CRITICAL processing error: {error_msg}, Type: {error_type}.")

        simple_fallback_text = f"Êä±Ê≠âÔºåÁ≥ªÁªüÂú®Â§ÑÁêÜÊÇ®ÁöÑÊü•ËØ¢ '{user_query[:30]}...' Êó∂ÈÅáÂà∞‰∏Ä‰∏™ÂÖ≥ÈîÆÈîôËØØ„ÄÇ"

        debug_mode = current_config.get("DEBUG", False)

        if debug_mode:
            simple_fallback_text += f" ÈîôËØØËØ¶ÊÉÖ: {error_msg}"
        else:
            simple_fallback_text += " ÊäÄÊúØÂõ¢ÈòüÂ∑≤Ë¢´ÈÄöÁü•ÔºåËØ∑Á®çÂêéÈáçËØïÊàñËÅîÁ≥ªÊîØÊåÅ„ÄÇ"

        error_processing_strategy = OrchestratorProcessingStrategy.ERROR_HANDLING

        error_result = ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            response_text=simple_fallback_text,
            insights=[],
            key_metrics={},
            visualizations=[],
            error_info={
                'error_type': error_type,
                'message': error_msg,
                'original_query': user_query,
                'trace': traceback.format_exc() if debug_mode else "Error trace hidden in sync error handler."
            },
            processing_strategy=error_processing_strategy,
            processors_used=['SyncErrorHandling'],
            total_processing_time=total_time_on_error,
            conversation_id=conversation_id,
            timestamp=datetime.now().isoformat()
        )

        # ÂØπ‰∫éÂàùÂßãÂåñÈò∂ÊÆµÁöÑÈîôËØØÔºåÂÆåÊï¥ÁöÑÁªüËÆ°Êõ¥Êñ∞ÂèØËÉΩ‰∏çÈÄÇÁî®Êàñ‰∏çÂáÜÁ°Æ
        # Ê≠§Â§Ñ‰ªÖÊõ¥Êñ∞‰∫ÜÂ§±Ë¥•ËÆ°Êï∞ÂíåÈîôËØØÁ±ªÂûã„ÄÇ
        # Â¶ÇÊûúÈúÄË¶ÅÔºåÂèØ‰ª•ÊúâÊù°‰ª∂Âú∞Ë∞ÉÁî® _update_orchestrator_statsÔºå‰ΩÜË¶ÅÁ°Æ‰øù total_queries Â∑≤Ê≠£Á°ÆËÆ°Êï∞„ÄÇ
        # self._update_orchestrator_stats(error_result) # ÊöÇÊó∂Ê≥®Èáä‰ª•ÈÅøÂÖçÊΩúÂú®ÈóÆÈ¢ò

        return error_result
    async def _determine_processing_strategy(self, query_analysis: QueryAnalysisResult, preferences: Optional[
        Dict[str, Any]] = None) -> OrchestratorProcessingStrategy:
        """Êô∫ËÉΩÁ°ÆÂÆöÂ§ÑÁêÜÁ≠ñÁï•ÔºåÂü∫‰∫éËßÑÂàôÂíåAIËæÖÂä©„ÄÇ"""
        method_start_time = time.time()
        logger.debug(
            f"Determining processing strategy for query type: {query_analysis.query_type.value}, complexity: {query_analysis.complexity.value}")

        if preferences and 'processing_strategy' in preferences:
            try:
                preferred_strategy = OrchestratorProcessingStrategy(preferences['processing_strategy'])
                logger.info(f"Using user preferred strategy: {preferred_strategy.value}")
                return preferred_strategy
            except ValueError:
                logger.warning(
                    f"Invalid processing_strategy in preferences: {preferences['processing_strategy']}. Defaulting.")

        complexity_from_parser: QueryParserComplexity = query_analysis.complexity
        strategy_map = {
            QueryParserComplexity.SIMPLE: OrchestratorProcessingStrategy.DIRECT_RESPONSE,
            QueryParserComplexity.MEDIUM: OrchestratorProcessingStrategy.SINGLE_PROCESSOR,
            QueryParserComplexity.COMPLEX: OrchestratorProcessingStrategy.MULTI_PROCESSOR,
            QueryParserComplexity.EXPERT: OrchestratorProcessingStrategy.FULL_PIPELINE,
        }
        determined_strategy = strategy_map.get(complexity_from_parser, OrchestratorProcessingStrategy.SINGLE_PROCESSOR)

        # AIËæÖÂä©ÂÜ≥Á≠ñ (‰∏≠ÊñáÊèêÁ§∫ËØç)
        if self.config.get('enable_smart_routing', True) and self.claude_client:
            try:
                ai_prompt_for_strategy = f"""
                ‰Ωú‰∏∫Êô∫ËÉΩÈáëËûçÂàÜÊûêÁ≥ªÁªüÁöÑÂÜ≥Á≠ñÊ†∏ÂøÉÔºåËØ∑‰∏∫‰ª•‰∏ãÁî®Êà∑Êü•ËØ¢ÁöÑÂàÜÊûêÁªìÊûúÈÄâÊã©ÊúÄÂêàÈÄÇÁöÑÂ§ÑÁêÜÁ≠ñÁï•„ÄÇ
                Áî®Êà∑Êü•ËØ¢ÁöÑÂàÜÊûêÊ¶ÇË¶Å:
                - Â§çÊùÇÂ∫¶ËØÑ‰º∞: {query_analysis.complexity.value}
                - Êü•ËØ¢ÊÑèÂõæÁ±ªÂûã: {query_analysis.query_type.value}
                - Ê∂âÂèä‰∏öÂä°Âú∫ÊôØ: {query_analysis.business_scenario.value if query_analysis.business_scenario else 'Êú™ÊòéÁ°Æ'}
                - Á≥ªÁªüËß£ÊûêÁΩÆ‰ø°Â∫¶: {query_analysis.confidence_score:.2f}
                - È¢ÑËÆ°ÊâßË°åÊ≠•È™§Êï∞: {len(query_analysis.execution_plan)}

                ÂèØÈÄâÁöÑÂ§ÑÁêÜÁ≠ñÁï•ÂåÖÊã¨:
                - "direct_response": Áî®‰∫éÁÆÄÂçï‰ø°ÊÅØËé∑ÂèñÊàñÊó†ÈúÄÂ§çÊùÇËÆ°ÁÆóÁöÑÊü•ËØ¢„ÄÇ
                - "single_processor": Áî®‰∫éÈúÄË¶ÅÂçï‰∏ÄÁ±ªÂûãÊ†∏ÂøÉÂ§ÑÁêÜÂô®ÔºàÂ¶Ç‰ªÖÂΩìÂâçÊï∞ÊçÆ„ÄÅ‰ªÖÂéÜÂè≤ÂàÜÊûê„ÄÅÊàñ‰ªÖÈ¢ÑÊµãÔºâÁöÑÊ†áÂáÜÊü•ËØ¢„ÄÇ
                - "multi_processor": Áî®‰∫éÊ∂âÂèäÂ§öÁßçÂàÜÊûêÁª¥Â∫¶ÔºåÂèØËÉΩÈúÄË¶ÅÂ§ö‰∏™Â§ÑÁêÜÂô®Âçè‰ΩúÁöÑÂ§çÊùÇÊü•ËØ¢„ÄÇ
                - "full_pipeline": Áî®‰∫éÈúÄË¶ÅÁ≥ªÁªüËøõË°åÊúÄÂÖ®Èù¢„ÄÅÊúÄÊ∑±Â∫¶ÂàÜÊûêÁöÑ‰∏ìÂÆ∂Á∫ßÊü•ËØ¢ÔºåÂèØËÉΩË∞ÉÂä®ÊâÄÊúâÁõ∏ÂÖ≥Â§ÑÁêÜÂíåÂàÜÊûêÊ®°Âùó„ÄÇ

                ÂΩìÂâçÂü∫‰∫éËßÑÂàôÁöÑÂàùÊ≠•Âª∫ËÆÆÁ≠ñÁï•ÊòØ: {determined_strategy.value}
                ËØ∑ÁªìÂêà‰ª•‰∏ä‰ø°ÊÅØÔºåÁâπÂà´ÊòØÊü•ËØ¢ÁöÑÂ§çÊùÇÂ∫¶ÂíåËß£ÊûêÁΩÆ‰ø°Â∫¶ÔºåÁªôÂá∫ÊÇ®ÁöÑÊúÄÁªàÁ≠ñÁï•ÈÄâÊã©„ÄÇ
                Â¶ÇÊûúËß£ÊûêÁΩÆ‰ø°Â∫¶ËæÉ‰ΩéÔºà‰æãÂ¶Ç‰Ωé‰∫é0.7ÔºâÔºåËØ∑ÂÄæÂêë‰∫éÈÄâÊã©Êõ¥ÁÆÄÂçï„ÄÅÊõ¥Á®≥ÂÅ•ÁöÑÁ≠ñÁï•„ÄÇ
                ËØ∑‰ªÖËøîÂõûÁ≠ñÁï•ÁöÑËã±ÊñáÂêçÂ∞èÂÜôÂ≠óÁ¨¶‰∏≤Ôºå‰æãÂ¶ÇÔºö"single_processor"„ÄÇ
                """
                # ÂÅáËÆæClaudeClientÁöÑanalyze_complex_queryÊñπÊ≥ïÁ¨¨‰∏Ä‰∏™ÂèÇÊï∞ÊòØpromptÔºåÁ¨¨‰∫å‰∏™ÊòØ‰∏ä‰∏ãÊñáÔºàÂèØ‰∏∫Á©∫Ôºâ
                ai_suggestion_result = await self.claude_client.analyze_complex_query(ai_prompt_for_strategy, {})
                if ai_suggestion_result.get('success'):
                    suggested_strategy_str = ai_suggestion_result.get('response', ai_suggestion_result.get('analysis',
                                                                                                           '')).strip().lower()
                    try:
                        ai_determined_strategy = OrchestratorProcessingStrategy(suggested_strategy_str)
                        logger.info(
                            f"AI suggested strategy: {ai_determined_strategy.value} (Rule-based was: {determined_strategy.value})")
                        determined_strategy = ai_determined_strategy  # AI Âª∫ËÆÆË¶ÜÁõñËßÑÂàô
                    except ValueError:
                        logger.warning(f"AIËøîÂõû‰∫ÜÊó†ÊïàÁöÑÁ≠ñÁï•Â≠óÁ¨¶‰∏≤: '{suggested_strategy_str}'")
                else:
                    logger.warning(f"AIÁ≠ñÁï•ÂÜ≥Á≠ñÂ§±Ë¥•: {ai_suggestion_result.get('error', 'Unknown AI error')}")
            except Exception as e_ai_strat:
                logger.error(f"AIÁ≠ñÁï•ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÂèëÁîüÂºÇÂ∏∏: {e_ai_strat}")

        if query_analysis.confidence_score < self.config.get('min_confidence_threshold_parsing', 0.7):
            logger.warning(
                f"Ëß£ÊûêÁΩÆ‰ø°Â∫¶‰Ωé ({query_analysis.confidence_score:.2f}), ÊúÄÁªàÁ≠ñÁï•‰ªé {determined_strategy.value} ÈôçÁ∫ß„ÄÇ")
            if determined_strategy == OrchestratorProcessingStrategy.FULL_PIPELINE:
                determined_strategy = OrchestratorProcessingStrategy.MULTI_PROCESSOR
            elif determined_strategy == OrchestratorProcessingStrategy.MULTI_PROCESSOR:
                determined_strategy = OrchestratorProcessingStrategy.SINGLE_PROCESSOR
            elif determined_strategy == OrchestratorProcessingStrategy.SINGLE_PROCESSOR:
                determined_strategy = OrchestratorProcessingStrategy.DIRECT_RESPONSE

        logger.debug(
            f"Strategy determination took {time.time() - method_start_time:.3f}s. Final strategy: {determined_strategy.value}")
        return determined_strategy

    async def _orchestrate_data_acquisition(self, query_analysis: QueryAnalysisResult,
                                            strategy: OrchestratorProcessingStrategy) -> FetcherExecutionResult:
        """ÁºñÊéíÊï∞ÊçÆËé∑ÂèñÊµÅÁ®ãÔºåË∞ÉÁî® DataRequirementsAnalyzer Âíå SmartDataFetcher„ÄÇ"""
        method_start_time = time.time()
        logger.info(f"DataAcquisition: Analyzing requirements for query type '{query_analysis.query_type.value}'")
        if not self.data_requirements_analyzer: raise SystemError("DataRequirementsAnalyzer not initialized.")
        if not self.data_fetcher: raise SystemError("SmartDataFetcher not initialized.")

        data_plan: DataAcquisitionPlan = await self.data_requirements_analyzer.analyze_data_requirements(query_analysis)

        if not data_plan or not data_plan.api_call_plans:
            logger.warning(
                "DataRequirementsAnalyzer did not produce API call plans. This might be ok for very simple queries or indicate an issue.")
            # Âç≥‰ΩøÊ≤°ÊúâËÆ°ÂàíÔºå‰πüË∞ÉÁî®fetcherÔºåÂÆÉÂ∫îËØ•ËÉΩÂ§ÑÁêÜÁ©∫ËÆ°Âàí
            # ÊàñËÄÖÂ¶ÇÊûúÁªùÂØπÈúÄË¶ÅÊï∞ÊçÆÔºåÂ∞ùËØïËé∑ÂèñÂü∫Á°ÄÊï∞ÊçÆ
            if strategy != OrchestratorProcessingStrategy.DIRECT_RESPONSE or query_analysis.query_type != QueryParserQueryType.GENERAL_KNOWLEDGE:
                logger.info(
                    "No API calls in plan, attempting fallback to fetch basic system data for non-direct/general queries.")
                # (Fallback logic from previous full version can be inserted here if needed)
                # For now, let SmartDataFetcher handle an empty plan if that's its design.
                # If not, we must ensure data_plan always has something or handle this case explicitly.
                # Assume SmartDataFetcher returns an ExecutionResult indicating no data if plan is empty.
                pass  # Allow execution of potentially empty plan

        logger.info(
            f"DataAcquisition: Executing data plan '{data_plan.plan_id if data_plan else 'N/A'}' with {len(data_plan.api_call_plans) if data_plan else 0} API calls.")
        fetch_result: FetcherExecutionResult = await self.data_fetcher.execute_data_acquisition_plan(
            data_plan)  # SmartDataFetcher handles empty plan

        logger.debug(
            f"Data acquisition took {time.time() - method_start_time:.3f}s. Fetch status: {fetch_result.execution_status.value if fetch_result.execution_status else 'N/A'}")
        return fetch_result

    async def _orchestrate_business_processing(self, query_analysis: QueryAnalysisResult,
                                               data_acquisition_result: FetcherExecutionResult,
                                               strategy: OrchestratorProcessingStrategy) -> Dict[str, Any]:
        # ++++++++++++++ Ê∑ªÂä†Ê≠§Ë°å ++++++++++++++
        biz_proc_start_wall_time = time.time()
        # +++++++++++++++++++++++++++++++++++++

        logger.info(
            f"BusinessProcessing: Strategy '{strategy.value}', QueryType from Parser '{query_analysis.query_type.value}'")
        if not all([self.current_data_processor, self.historical_analysis_processor, self.prediction_processor,
                    self.financial_data_analyzer]):
            raise SystemError("One or more core processors/analyzers are not initialized for business processing.")

        processor_context = {'data_acquisition_result': data_acquisition_result, 'query_analysis': query_analysis}
        original_user_query = query_analysis.original_query

        processor_result_payload: Any = None
        processor_used_names: List[str] = []

        sum_of_reported_proc_times: float = 0.0
        confidences_from_procs: List[float] = []
        # biz_proc_metadata Â∑≤ÁßªÂà∞ return ËØ≠Âè•‰∏≠Âä®ÊÄÅÊûÑÂª∫Ôºå‰ª•ÂåÖÂê´ ai_processing_time

        # --- SINGLE_PROCESSOR and DIRECT_RESPONSE ---
        if strategy == OrchestratorProcessingStrategy.DIRECT_RESPONSE:
            # ... (Â§ÑÁêÜÈÄªËæë) ...
            res_obj = await self.current_data_processor.process_current_data_query(original_user_query)
            processor_result_payload = res_obj
            processor_used_names.append(self.current_data_processor.__class__.__name__)
            sum_of_reported_proc_times = getattr(res_obj, 'processing_time', 0.1)
            confidences_from_procs.append(getattr(res_obj, 'response_confidence', 0.7))

        elif strategy == OrchestratorProcessingStrategy.SINGLE_PROCESSOR:
            # ... (Â§ÑÁêÜÈÄªËæë) ...
            qt_parser_enum: QueryParserQueryType = query_analysis.query_type
            selected_processor: Any = None

            if qt_parser_enum in [QueryParserQueryType.DATA_RETRIEVAL, QueryParserQueryType.CALCULATION,
                                  QueryParserQueryType.GENERAL_KNOWLEDGE]:  # ÂÅáËÆæGENERAL_KNOWLEDGEÂ∑≤Ê∑ªÂä†
                selected_processor = self.current_data_processor
                res_obj = await selected_processor.process_current_data_query(original_user_query)
            elif qt_parser_enum in [QueryParserQueryType.TREND_ANALYSIS, QueryParserQueryType.COMPARISON]:
                selected_processor = self.historical_analysis_processor
                res_obj = await selected_processor.process_historical_analysis_query(original_user_query,
                                                                                     processor_context)
            elif qt_parser_enum in [QueryParserQueryType.PREDICTION, QueryParserQueryType.SCENARIO_SIMULATION,
                                    QueryParserQueryType.RISK_ASSESSMENT]:
                selected_processor = self.prediction_processor
                res_obj = await selected_processor.process_prediction_query(original_user_query, processor_context)
            else:
                selected_processor = self.current_data_processor  # Default
                res_obj = await selected_processor.process_current_data_query(original_user_query)

            processor_result_payload = res_obj
            processor_used_names.append(selected_processor.__class__.__name__)
            sum_of_reported_proc_times += getattr(res_obj, 'processing_time', 0.2)
            confidences_from_procs.append(
                getattr(res_obj, 'confidence_score',
                        getattr(res_obj, 'response_confidence',
                                getattr(res_obj, 'analysis_confidence',
                                        getattr(res_obj, 'prediction_confidence', 0.6)))))

        elif strategy in [OrchestratorProcessingStrategy.MULTI_PROCESSOR, OrchestratorProcessingStrategy.FULL_PIPELINE]:
            # ... (Â§öÂ§ÑÁêÜÂô®/ÂÖ®ÊµÅÊ∞¥Á∫øÂ§ÑÁêÜÈÄªËæëÔºå‰∏éÊÇ®‰πãÂâçÊèê‰æõÁöÑÂÆåÊï¥ÁâàÊú¨‰∏ÄËá¥) ...
            multi_results_payload: Dict[str, Any] = {}
            tasks_to_run_coroutines_map: Dict[str, asyncio.Task] = {}  # type: ignore

            tasks_to_run_coroutines_map['current_data_analysis'] = asyncio.create_task(
                self.current_data_processor.process_current_data_query(original_user_query)
            )

            if query_analysis.query_type in [QueryParserQueryType.TREND_ANALYSIS, QueryParserQueryType.COMPARISON,
                                             QueryParserQueryType.PREDICTION, QueryParserQueryType.SCENARIO_SIMULATION,
                                             QueryParserQueryType.RISK_ASSESSMENT] or \
                    any(step.step_type == "trend_analysis" for step in query_analysis.execution_plan):
                tasks_to_run_coroutines_map['historical_analysis'] = asyncio.create_task(
                    self.historical_analysis_processor.process_historical_analysis_query(original_user_query,
                                                                                         processor_context)
                )

            if query_analysis.query_type in [QueryParserQueryType.PREDICTION, QueryParserQueryType.SCENARIO_SIMULATION,
                                             QueryParserQueryType.RISK_ASSESSMENT] or \
                    any(step.step_type == "prediction" for step in query_analysis.execution_plan):
                tasks_to_run_coroutines_map['prediction_analysis'] = asyncio.create_task(
                    self.prediction_processor.process_prediction_query(original_user_query, processor_context)
                )

            if strategy == OrchestratorProcessingStrategy.FULL_PIPELINE and self.financial_data_analyzer:
                scope_for_fda = query_analysis.business_scenario.value if query_analysis.business_scenario else 'financial_overview'
                time_range_days_fda = 30
                if query_analysis.time_requirements and query_analysis.time_requirements.get('time_range_days'):
                    time_range_days_fda = int(query_analysis.time_requirements['time_range_days'])

                tasks_to_run_coroutines_map['financial_deep_dive'] = asyncio.create_task(
                    self.financial_data_analyzer.analyze_business_performance(scope=scope_for_fda,
                                                                              time_range=time_range_days_fda
                                                                             )
                )

            logger.info(
                f"Executing {len(tasks_to_run_coroutines_map)} tasks for strategy {strategy.value}: {list(tasks_to_run_coroutines_map.keys())}")

            for name, task_coro in tasks_to_run_coroutines_map.items():
                try:
                    res_obj = await task_coro
                    multi_results_payload[name] = res_obj
                    processor_used_names.append(getattr(res_obj, '__class__', {}).__name__ or name)
                    sum_of_reported_proc_times += getattr(res_obj, 'processing_time', 0.1)
                    confidences_from_procs.append(
                        getattr(res_obj, 'confidence_score', getattr(res_obj, 'response_confidence', 0.6))
                    )
                except Exception as task_e:
                    multi_results_payload[name] = {"error": str(task_e), "details": traceback.format_exc()}
                    logger.error(f"Error processing task '{name}' in {strategy.value}: {task_e}")
                    confidences_from_procs.append(0.1)
            processor_result_payload = {"multi_results": multi_results_payload}
        else:
            logger.error(f"Unsupported processing strategy: {strategy.value}")
            raise ValueError(f"Unsupported processing strategy: {strategy.value}")

        # --- Âú®ÊâÄÊúâ‰∏öÂä°Â§ÑÁêÜÈÄªËæëÂÆåÊàêÂêéËÆ°ÁÆóÂÆûÈôÖËÄóÊó∂ ---
        combined_processing_time = time.time() - biz_proc_start_wall_time  # Áé∞Âú® biz_proc_start_wall_time Â∑≤ÂÆö‰πâ
        # --- ---

        overall_biz_confidence = sum(confidences_from_procs) / len(
            confidences_from_procs) if confidences_from_procs else 0.5

        # Á¥ØÂä†ÂêÑÂ§ÑÁêÜÂô®Êä•ÂëäÁöÑAIÊó∂Èó¥
        biz_proc_ai_time = 0.0
        if isinstance(processor_result_payload, dict) and 'multi_results' in processor_result_payload:
            for res_obj in processor_result_payload['multi_results'].values():
                if hasattr(res_obj, 'metadata') and isinstance(res_obj.metadata, dict):
                    biz_proc_ai_time += res_obj.metadata.get('ai_processing_time', 0.0)
        elif hasattr(processor_result_payload, 'metadata') and isinstance(processor_result_payload.metadata,
                                                                          dict):  # Âçï‰∏™ÂØπË±°
            biz_proc_ai_time = processor_result_payload.metadata.get('ai_processing_time', 0.0)

        return {
            'processing_type': strategy.value,
            'processor_used': ", ".join(list(set(processor_used_names))),
            'primary_result': processor_result_payload,
            'confidence_score': overall_biz_confidence,
            'processing_time': combined_processing_time,
            'metadata': {
                'data_input_source': 'from_smart_data_fetcher',
                'sum_of_individual_processor_reported_times': sum_of_reported_proc_times,
                'ai_processing_time': biz_proc_ai_time  # ËÆ∞ÂΩï‰∏öÂä°Â§ÑÁêÜÈò∂ÊÆµÁöÑAIËÄóÊó∂
            }
        }

    async def _orchestrate_insight_generation(self, business_result_data: Dict[str, Any],
                                              query_analysis: QueryAnalysisResult,
                                              data_acquisition_result: FetcherExecutionResult) -> List[BusinessInsight]:
        logger.debug("Orchestrator: Calling InsightGenerator...")
        if not self.insight_generator: raise SystemError("InsightGenerator not initialized.")

        analysis_objects_for_insights = []
        primary_res_payload = business_result_data.get('primary_result')  # This is object or dict of objects

        if isinstance(primary_res_payload, dict) and 'multi_results' in primary_res_payload:
            for res_key, res_obj in primary_res_payload['multi_results'].items():
                if res_obj and not (isinstance(res_obj, dict) and 'error' in res_obj):
                    analysis_objects_for_insights.append(res_obj)
        elif primary_res_payload and not (isinstance(primary_res_payload, dict) and 'error' in primary_res_payload):
            analysis_objects_for_insights.append(primary_res_payload)

        if not analysis_objects_for_insights:
            logger.warning("No valid business processing results to generate insights from.")
            return []

        # Prepare context for insight generator
        conv_id_for_db = None
        # conversation_id is from process_intelligent_query's parameters
        # If process_intelligent_query received a valid string conversation_id that was converted to int
        # that int conversation_id_for_db can be used here.
        # For simplicity, let's assume it's available if needed by get_context
        # This context logic needs to be robust based on how conv_id is managed.
        # For now, passing empty context if conv_id not available or invalid.
        parent_conversation_id_str = query_analysis.processing_metadata.get(
            'conversation_id_from_query_parsing') if query_analysis.processing_metadata else None
        if parent_conversation_id_str:
            try:
                conv_id_for_db = int(parent_conversation_id_str)
            except:
                pass

        user_context_for_insights = self.conversation_manager.get_context(
            conv_id_for_db) if conv_id_for_db and self.conversation_manager else {}

        insights, metadata = await self.insight_generator.generate_comprehensive_insights(
            analysis_results=analysis_objects_for_insights,
            user_context=user_context_for_insights,
            focus_areas=self._determine_focus_areas(query_analysis)  # Implemented
        )

        # Store AI time from insight generation
        ai_time_insights = metadata.get('ai_time', 0.0) if isinstance(metadata, dict) else 0.0
        business_result_data.setdefault('metadata', {}).update({'insight_gen_ai_time': ai_time_insights})
        business_result_data['insight_generation_metadata'] = metadata
        return insights

    async def _generate_intelligent_response(self, user_query: str, query_analysis: QueryAnalysisResult,
                                             business_result_data: Dict[str, Any],
                                             insights_list: List[BusinessInsight]) -> str:
        logger.debug("Orchestrator: Generating intelligent response text using CHINESE prompts.")
        if not self.claude_client and not self.gpt_client:
            logger.warning("AIÂÆ¢Êà∑Á´Ø (Claude/GPT) Êú™ÈÖçÁΩÆÔºå‰ΩøÁî®Âü∫Á°ÄÊ®°ÊùøÁîüÊàêÂìçÂ∫î„ÄÇ")
            return self._generate_basic_response(business_result_data, insights_list)  # Implemented

        summarized_analysis = self._summarize_business_result_for_ai(business_result_data)  # Implemented
        summarized_insights = []
        for insight in insights_list[:3]:
            title = getattr(insight, 'title', "ÈáçË¶ÅÊ¥ûÂØü")
            summary = getattr(insight, 'summary', "...")
            summarized_insights.append(f"{title}: {summary}")

        # --- CHINESE PROMPT ---
        prompt = f"""
        ‰Ωú‰∏∫‰∏Ä‰ΩçËµÑÊ∑±ÁöÑAIÈáëËûç‰∏öÂä°È°æÈóÆÔºåËØ∑Ê†πÊçÆ‰ª•‰∏ã‰ø°ÊÅØÔºå‰∏∫Áî®Êà∑ÁîüÊàê‰∏Ä‰ªΩÂÖ®Èù¢„ÄÅ‰∏ì‰∏ö‰∏îÊòì‰∫éÁêÜËß£ÁöÑ‰∏≠ÊñáÂõûÁ≠î„ÄÇ

        Áî®Êà∑ÁöÑÂéüÂßãÊü•ËØ¢:
        "{user_query}"

        Á≥ªÁªüÂØπÊü•ËØ¢ÁöÑÂàÜÊûêÊ¶ÇË¶Å:
        - Áî®Êà∑ÊÑèÂõæÊ†∏ÂøÉ: {query_analysis.query_type.value if query_analysis.query_type else "Êú™Áü•"}
        - Ê∂âÂèä‰∏öÂä°Âú∫ÊôØ: {query_analysis.business_scenario.value if query_analysis.business_scenario else "ÈÄöÁî®"}
        - ÂàÜÊûêÂ§çÊùÇÂ∫¶ËØÑ‰º∞: {query_analysis.complexity.value if query_analysis.complexity else "‰∏≠Á≠â"}

        Ê†∏ÂøÉÂàÜÊûêÁªìÊûúÁ≤æÁÇº (JSONÊ†ºÂºè):
        {json.dumps(summarized_analysis, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}

        ÂÖ≥ÈîÆ‰∏öÂä°Ê¥ûÂØüÊèêË¶Å:
        {chr(10).join([f"  - {si}" for si in summarized_insights]) if summarized_insights else "  - ÊöÇÊú™ÁîüÊàêÁâπÂà´ÁöÑ‰∏öÂä°Ê¥ûÂØü„ÄÇ"}

        ËØ∑‰∏•Ê†ºÊåâÁÖß‰ª•‰∏ãÁªìÊûÑÂíåË¶ÅÊ±ÇÁîüÊàêÊúÄÁªàÁ≠îÂ§çÔºà‰ΩøÁî®MarkdownÊ†ºÂºèÔºâÔºö

        ### **ÂõûÁ≠îÊ†∏ÂøÉÈóÆÈ¢ò**
        [Ê∏ÖÊô∞„ÄÅÁõ¥Êé•Âú∞ÂõûÁ≠îÁî®Êà∑Êü•ËØ¢ÁöÑÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇ]

        ### **ÂÖ≥ÈîÆÊï∞ÊçÆ‰∏éÂèëÁé∞**
        [ÂàóÂá∫Âπ∂Ëß£Èáä1-3‰∏™ÊúÄÈáçË¶ÅÁöÑÂõæË°®ÊàñÂàÜÊûêÂèëÁé∞„ÄÇ‰æãÂ¶ÇÔºö**ÊÄª‰ΩôÈ¢ùË∂ãÂäøÔºöËøáÂéª30Â§©ÂÜÖÂ¢ûÈïø‰∫Ü15%ÔºåËææÂà∞¬•XXX„ÄÇ**]

        ### **‰∏öÂä°Ëß£ËØª‰∏éÊ¥ûÂØü**
        [ÔºàÂ¶ÇÊûúÊ¥ûÂØüÂàóË°®‰∏ç‰∏∫Á©∫ÔºâÁÆÄË¶ÅËß£ÈáäËøô‰∫õÂèëÁé∞ÂØπ‰∏öÂä°ÁöÑÊΩúÂú®ÂΩ±ÂìçÊàñÊÑè‰πâÔºåËá™ÁÑ∂Âú∞ËûçÂÖ•Ê¥ûÂØüÁöÑÁªìËÆ∫„ÄÇ‰æãÂ¶ÇÔºö*Ê¥ûÂØüÊòæÁ§∫ÔºåËøëÊúüÁî®Êà∑Ê¥ªË∑ÉÂ∫¶ÊèêÂçáÂèØËÉΩ‰∏éXXÊ¥ªÂä®ÊúâÂÖ≥ÔºåÂª∫ËÆÆÊåÅÁª≠ÂÖ≥Ê≥®Âπ∂‰ºòÂåñÁõ∏ÂÖ≥Á≠ñÁï•„ÄÇ*]

        ### **Âª∫ËÆÆË°åÂä®**
        [ÔºàÂ¶ÇÊûúÊ¥ûÂØüÂåÖÂê´ÊòéÁ°ÆÂª∫ËÆÆÔºâÊèê‰æõ1-2Êù°ÊúÄÂÖ≥ÈîÆ„ÄÅÊúÄÂÖ∑ÂèØÊìç‰ΩúÊÄßÁöÑÂª∫ËÆÆ„ÄÇ]

        ### **ÊÄªÁªì‰∏éÂ±ïÊúõ** (ÂèØÈÄâ)
        [ÂØπÊï¥‰ΩìÊÉÖÂÜµËøõË°åÁÆÄÁü≠ÊÄªÁªìÊàñÂØπÊú™Êù•Ë∂ãÂäøÂÅöÂàùÊ≠•Â±ïÊúõ„ÄÇ]

        **ËØ∑Ê≥®ÊÑèÔºö**
        - ËØ≠Ë®ÄÈ£éÊ†ºÈúÄ‰∏ì‰∏ö„ÄÅÂÆ¢ËßÇ„ÄÅÊï∞ÊçÆÈ©±Âä®ÔºåÂêåÊó∂Á°Æ‰øù‰∏öÂä°‰∫∫ÂëòËÉΩÂ§üËΩªÊùæÁêÜËß£„ÄÇ
        - ÈÅøÂÖç‰ΩøÁî®Ëøá‰∫éÊäÄÊúØÊÄßÁöÑÊúØËØ≠ÔºåÂøÖË¶ÅÊó∂ËøõË°åËß£Èáä„ÄÇ
        - Â¶ÇÊûúÂàÜÊûêÁªìÊûú‰∏≠Â≠òÂú®‰∏çÁ°ÆÂÆöÊÄß„ÄÅÈ£éÈô©ÊàñÊï∞ÊçÆÂ±ÄÈôêÊÄßÔºåËØ∑Âú®ÂõûÁ≠î‰∏≠ÊòéÁ°Æ„ÄÅÂÆ°ÊÖéÂú∞ÊåáÂá∫„ÄÇ
        - Â¶ÇÊûúÊï∞ÊçÆ‰∏çË∂≥ÊàñÂàÜÊûêÁΩÆ‰ø°Â∫¶ËæÉ‰ΩéÔºåËØ∑Âù¶ËØöËØ¥ÊòéÔºåÂπ∂ÂèØ‰ª•Á§ºË≤åÂú∞Âª∫ËÆÆÁî®Êà∑Êèê‰æõÊõ¥Â§ö‰ø°ÊÅØÊàñË∞ÉÊï¥Êü•ËØ¢ËåÉÂõ¥„ÄÇ
        - ÊÄªÂõûÁ≠îÈïøÂ∫¶Âª∫ËÆÆÊéßÂà∂Âú®300-800Â≠óÂ∑¶Âè≥ÔºåÊ†πÊçÆ‰ø°ÊÅØÈáèÁÅµÊ¥ªË∞ÉÊï¥„ÄÇ
        """

        ai_client_to_use = self.claude_client if self.claude_client else self.gpt_client  # ‰ºòÂÖàClaude
        if not ai_client_to_use: return self._generate_basic_response(business_result_data, insights_list)

        try:
            ai_response_data = None
            logger.info(f"‰ΩøÁî® {ai_client_to_use.__class__.__name__} ÁîüÊàêÊúÄÁªàÂìçÂ∫îÊñáÊú¨„ÄÇ")
            # ÂÅáËÆæAIÂÆ¢Êà∑Á´ØÊúâÈÄöÁî®ÁöÑÊñáÊú¨ÁîüÊàêÊñπÊ≥ïÔºåÊàñËÄÖÈúÄË¶ÅÊ†πÊçÆÁ±ªÂûãÈÄâÊã©
            if isinstance(ai_client_to_use, ClaudeClient) and hasattr(ai_client_to_use,
                                                                      'generate_text'):  # ÂÅáËÆæClaudeÊúâÊ≠§ÊñπÊ≥ï
                ai_response_data = await ai_client_to_use.generate_text(prompt, max_tokens=1500)  # ClaudeÁöÑÂÖ∏ÂûãÊñπÊ≥ï
            elif isinstance(ai_client_to_use, OpenAIClient) and hasattr(ai_client_to_use, 'generate_completion'):
                ai_response_data = await ai_client_to_use.generate_completion(prompt, max_tokens=1500)  # OpenAIÁöÑÂÖ∏ÂûãÊñπÊ≥ï
            else:  # Fallback for unknown client method signature
                logger.warning(f"AIÂÆ¢Êà∑Á´Ø {ai_client_to_use.__class__.__name__} Êú™ÊâæÂà∞Ê†áÂáÜÁöÑÊñáÊú¨ÁîüÊàêÊñπÊ≥ï„ÄÇ")
                return self._generate_basic_response(business_result_data, insights_list)

            # Ëß£ÊûêAIÂìçÂ∫î (‰∏çÂêåÂÆ¢Êà∑Á´ØÁöÑÂìçÂ∫îÁªìÊûÑÂèØËÉΩ‰∏çÂêå)
            if ai_response_data and ai_response_data.get('success', False):  # ÂÅáËÆæÂìçÂ∫î‰∏≠Êúâ 'success' Ê†áÂøó
                response_content = ai_response_data.get('text', ai_response_data.get('response', ai_response_data.get(
                    'completion')))  # Â∞ùËØï‰∏çÂêåÂèØËÉΩÁöÑÈîÆ
                if response_content and isinstance(response_content, str):
                    return response_content.strip()
                else:
                    logger.warning(f"AIÂìçÂ∫îÂÜÖÂÆπ‰∏∫Á©∫ÊàñÊ†ºÂºè‰∏çÊ≠£Á°Æ: {response_content}")
                    return self._generate_basic_response(business_result_data, insights_list)
            else:
                logger.warning(
                    f"AIÁîüÊàêÂìçÂ∫îÊñáÊú¨Â§±Ë¥•: {ai_response_data.get('error', 'Êú™Áü•AIÈîôËØØ') if ai_response_data else 'AIÂÆ¢Êà∑Á´ØËøîÂõûÁ©∫'}")
                return self._generate_basic_response(business_result_data, insights_list)
        except Exception as e:
            logger.error(f"ÁîüÊàêÊô∫ËÉΩÂìçÂ∫îÊó∂ÂèëÁîüÂºÇÂ∏∏: {str(e)}\n{traceback.format_exc()}")
            return self._generate_basic_response(business_result_data, insights_list)

    async def _generate_visualizations(self, business_result_data: Dict[str, Any],
                                       query_analysis: QueryAnalysisResult,
                                       insights_list: List[BusinessInsight]) -> List[Dict[str, Any]]:
        # (‰∏é‰∏ä‰∏ÄÁâàÊú¨ÂÆåÊï¥ÂÆûÁé∞‰∏ÄËá¥ÔºåÁ°Æ‰øù‰ªéÁúüÂÆû‰∏öÂä°ÁªìÊûú‰∏≠ÊèêÂèñÊï∞ÊçÆÂπ∂Ë∞ÉÁî® self.chart_generator)
        logger.debug("Orchestrator: Generating visualizations...")
        visualizations: List[Dict[str, Any]] = []
        if not self.chart_generator:
            logger.warning("ChartGeneratorÊú™ÂàùÂßãÂåñÔºåË∑≥ËøáÂèØËßÜÂåñÁîüÊàê„ÄÇ")
            return visualizations

        try:
            raw_numeric_metrics = self._extract_raw_numeric_metrics(business_result_data)

            if raw_numeric_metrics and len(raw_numeric_metrics) >= 2 and len(raw_numeric_metrics) <= 10:
                chart_labels = list(raw_numeric_metrics.keys())
                chart_values = list(raw_numeric_metrics.values())
                try:
                    bar_chart_result = self.chart_generator.generate_bar_chart(
                        data={'category': chart_labels, 'value': chart_values},
                        title="ÂÖ≥ÈîÆÊåáÊ†áÂø´ÁÖß", x_column='category', y_columns=['value']
                    )
                    if bar_chart_result and not bar_chart_result.get("error"):
                        visualizations.append({
                            "type": VisChartType.BAR.value,
                            "title": bar_chart_result.get('title', "ÂÖ≥ÈîÆÊåáÊ†áÂø´ÁÖß"),
                            "data_payload": bar_chart_result.get('data'),
                            "image_data_b64": bar_chart_result.get('image_data', {}).get('base64')
                        })
                except Exception as e_bar:
                    logger.error(f"Error generating bar chart for key metrics: {e_bar}")

            primary_res_payload = business_result_data.get('primary_result', {})
            # historical_analysis_res_obj should be the direct object from the processor or its dict representation
            historical_analysis_res_obj = None
            if isinstance(primary_res_payload, dict) and 'multi_results' in primary_res_payload:
                historical_analysis_res_obj = primary_res_payload['multi_results'].get('historical_analysis')
            elif isinstance(primary_res_payload, HistoricalAnalysisResponse):
                historical_analysis_res_obj = primary_res_payload
            elif isinstance(primary_res_payload, dict) and primary_res_payload.get(
                    'analysis_type') == QueryParserQueryType.TREND_ANALYSIS.value:
                historical_analysis_res_obj = primary_res_payload

            # Convert to dict if it's an object and has to_dict or __dict__
            historical_analysis_dict = None
            if historical_analysis_res_obj:
                if hasattr(historical_analysis_res_obj, 'to_dict') and callable(historical_analysis_res_obj.to_dict):
                    historical_analysis_dict = historical_analysis_res_obj.to_dict()
                elif hasattr(historical_analysis_res_obj, '__dict__'):
                    historical_analysis_dict = historical_analysis_res_obj.__dict__
                elif isinstance(historical_analysis_res_obj, dict):
                    historical_analysis_dict = historical_analysis_res_obj

            if historical_analysis_dict and isinstance(historical_analysis_dict.get('trends'), list):
                for trend_detail_item in historical_analysis_dict[
                    'trends']:  # trend_detail_item is FinancialTrendAnalysis or its dict
                    metric_name = "Ë∂ãÂäø"
                    data_points = []
                    if isinstance(trend_detail_item, FinancialTrendAnalysis):  # If it's the object
                        metric_name = getattr(trend_detail_item, 'metric_name', 'Ë∂ãÂäø')
                        data_points = getattr(trend_detail_item, 'chart_data', [])
                    elif isinstance(trend_detail_item, dict):  # If it's a dict
                        metric_name = trend_detail_item.get('metric_name', trend_detail_item.get('metric', 'Ë∂ãÂäø'))
                        data_points = trend_detail_item.get('chart_data',
                                                            trend_detail_item.get('data_points_for_chart', []))

                    if data_points and isinstance(data_points, list) and len(data_points) > 1:
                        if not (all(isinstance(dp, dict) and 'date' in dp and 'value' in dp for dp in data_points)):
                            logger.warning(
                                f"Trend data for {metric_name} is not in expected format for charting. Skipping.")
                            continue
                        try:
                            line_chart_result = self.chart_generator.generate_line_chart(
                                data={'dates': [dp['date'] for dp in data_points],
                                      metric_name: [dp['value'] for dp in data_points]},
                                title=f"{metric_name} Ë∂ãÂäøÂàÜÊûê", x_column='dates', y_columns=[metric_name]
                            )
                            if line_chart_result and not line_chart_result.get("error"):
                                visualizations.append({
                                    "type": VisChartType.LINE.value,
                                    "title": line_chart_result.get('title', f"{metric_name} Ë∂ãÂäøÂàÜÊûê"),
                                    "data_payload": line_chart_result.get('data'),
                                    "image_data_b64": line_chart_result.get('image_data', {}).get('base64')
                                })
                        except Exception as e_line:
                            logger.error(f"Error generating line chart for {metric_name}: {e_line}")

            logger.info(f"ÁîüÊàê‰∫Ü {len(visualizations)} ‰∏™ÂèØËßÜÂåñÊï∞ÊçÆÂØπË±°„ÄÇ")
        except Exception as e:
            logger.error(f"ÁîüÊàêÂèØËßÜÂåñÊó∂ÂèëÁîüÈîôËØØ: {str(e)}\n{traceback.format_exc()}")

        return visualizations

    # --- ‰ª•‰∏ãÊòØÊâÄÊúâ‰πãÂâçËÆ®ËÆ∫ÁöÑ„ÄÅÈúÄË¶ÅÁ°Æ‰øùÂ≠òÂú®ÁöÑËæÖÂä©ÊñπÊ≥ï ---
    # _extract_raw_numeric_metrics, _summarize_business_result_for_ai,
    # _determine_focus_areas, _get_processors_used, _get_ai_collaboration_summary,
    # _calculate_overall_confidence, _calculate_response_completeness,
    # _get_ai_models_invoked_summary, _log_ai_response_to_conversation,
    # _fallback_query_parsing, _generate_basic_response

    def _calculate_response_completeness(self, business_result_data: Dict[str, Any],
                                         insights_list: List[BusinessInsight]) -> float:
        """
        ËØÑ‰º∞ÂìçÂ∫îÁöÑÂÆåÊï¥ÊÄß„ÄÇ
        ËÄÉËôëÊòØÂê¶Ë¶ÜÁõñ‰∫ÜÂÖ≥ÈîÆÂàÜÊûêÁªìÊûúÂíåÊ¥ûÂØü„ÄÇ
        ËøîÂõû‰∏Ä‰∏™ 0.0 Âà∞ 1.0 ‰πãÈó¥ÁöÑÂàÜÊï∞„ÄÇ
        """
        score = 0.0
        max_score = 1.0  # ÊÄªÂàÜÂÄº

        # 1. ÊòØÂê¶Êúâ‰∏ªË¶ÅÁöÑ‰∏öÂä°Â§ÑÁêÜÁªìÊûú (Âç†0.6ÂàÜ)
        if business_result_data and business_result_data.get('primary_result'):
            # Ëøõ‰∏ÄÊ≠•Ê£ÄÊü• primary_result ÊòØÂê¶ÁúüÁöÑÊúâÂÜÖÂÆπÔºåËÄå‰∏çÊòØÁ©∫Â£≥ÊàñÈîôËØØ
            primary_res_content = business_result_data.get('primary_result')
            is_error_dict = isinstance(primary_res_content, dict) and 'error' in primary_res_content
            if primary_res_content and not is_error_dict:
                score += 0.6
            elif is_error_dict:  # Â¶ÇÊûúÊòØÈîôËØØÔºåÂÆåÊï¥ÊÄß‰ºö‰Ωé‰∏Ä‰∫õ
                score += 0.1
        else:  # Â¶ÇÊûúËøû primary_result ÈÉΩÊ≤°ÊúâÔºåÂàôÂæóÂàÜÂæà‰Ωé
            score += 0.05

        # 2. ÊòØÂê¶ÁîüÊàê‰∫ÜÊ¥ûÂØü (Âç†0.3ÂàÜ)
        if insights_list and isinstance(insights_list, list) and len(insights_list) > 0:
            score += 0.3

        # 3. ÊòØÂê¶ÊèêÂèñÂá∫ÂÖ≥ÈîÆÊåáÊ†á (Âç†0.1ÂàÜ)
        # _extract_key_metrics Â∑≤ÁªèÂ§ÑÁêÜ‰∫ÜÂêÑÁßçÊÉÖÂÜµÂπ∂ËøîÂõûÊ†ºÂºèÂåñÊåáÊ†á
        key_metrics = self._extract_key_metrics(business_result_data)
        if key_metrics and isinstance(key_metrics, dict) and len(key_metrics) > 0:
            score += 0.1

        # Á°Æ‰øùÂàÜÊï∞Âú®0Âà∞1‰πãÈó¥
        final_completeness = round(min(score, max_score), 3)
        logger.debug(f"Calculated response completeness: {final_completeness}")
        return final_completeness

    def _get_ai_models_invoked_summary(self, query_analysis: Optional[QueryAnalysisResult],
                                       business_result_data: Dict[str, Any],
                                       insights_list: List[BusinessInsight]) -> List[str]:
        """
        Ê†πÊçÆÂàÜÊûêËøáÁ®ãÊÄªÁªìÂÆûÈôÖË∞ÉÁî®ÁöÑAIÊ®°Âûã (‰æãÂ¶Ç ["Claude", "GPT"])„ÄÇ
        Êõ¥ÂáÜÁ°ÆÁöÑÁªüËÆ°ÈúÄË¶ÅÂêÑÁªÑ‰ª∂Âú®ÂÖ∂ËøîÂõûÁöÑÂÖÉÊï∞ÊçÆ‰∏≠ÊòéÁ°ÆÊä•ÂëäAI‰ΩøÁî®ÊÉÖÂÜµ„ÄÇ
        """
        models_invoked = set()

        # 1. ‰ªéAIÂçè‰ΩúÊëòË¶Å‰∏≠Ëé∑ÂèñÔºàÂ¶ÇÊûúËØ•ÊëòË¶ÅÂ∑≤ÂáÜÁ°ÆËÆ∞ÂΩïÔºâ
        ai_collab_summary = self._get_ai_collaboration_summary(query_analysis, business_result_data, insights_list)
        if ai_collab_summary.get('claude_used_in_process'):
            models_invoked.add("Claude")
        if ai_collab_summary.get('gpt_used_in_process'):
            models_invoked.add("GPT")  # ÊàñÊõ¥ÂÖ∑‰ΩìÁöÑÊ®°ÂûãÂêçÂ¶Ç "GPT-4o"

        # 2. (ÂèØÈÄâ) Ëøõ‰∏ÄÊ≠•Ê£ÄÊü•ÂêÑÈò∂ÊÆµÁöÑÂÖÉÊï∞ÊçÆÔºåÂ¶ÇÊûúÂÆÉ‰ª¨ÂåÖÂê´Êõ¥ÁªÜËá¥ÁöÑÊ®°Âûã‰ΩøÁî®‰ø°ÊÅØ
        # ‰æãÂ¶ÇÔºåquery_analysis.processing_metadata.get('parser_ai_model')
        # business_result_data.get('metadata', {}).get('processor_ai_model')
        # insights_list[0].metadata.get('generator_ai_model') Á≠â

        if not models_invoked:
            # Â¶ÇÊûú‰∏äËø∞ÈÉΩÊú™ÊòéÁ°ÆÔºå‰ΩÜAIÂÆ¢Êà∑Á´ØÂ≠òÂú®ÔºåÂÅö‰∏Ä‰∏™Âü∫Êú¨ÂÅáËÆæ
            if self.claude_client: models_invoked.add("Claude (assumed)")
            if self.gpt_client: models_invoked.add("GPT (assumed)")

        return list(models_invoked) if models_invoked else ["None_Specifically_Reported"]

    async def _handle_processing_error(self, session_id: str, query_id: str,
                                       user_query: str, error_msg: str,
                                       total_time_on_error: float,
                                       conversation_id: Optional[str] = None) -> ProcessingResult:
        """
        Áªü‰∏ÄÂ§ÑÁêÜÊµÅÁ®ã‰∏≠ÁöÑÈîôËØØÔºåÁîüÊàêÈîôËØØÁªìÊûúÂØπË±°„ÄÇÊ≠§‰∏∫ÂºÇÊ≠•ÁâàÊú¨„ÄÇ
        """
        # total_queries Â∑≤ÁªèÂú® process_intelligent_query ÂºÄÂßãÊó∂Â¢ûÂä†
        # ËøôÈáå‰∏ªË¶ÅÊõ¥Êñ∞Â§±Ë¥•ËÆ°Êï∞ÂíåÈîôËØØÁ±ªÂûã
        error_type = self._classify_error(error_msg)  # _classify_error ÊòØÂêåÊ≠•ÁöÑ

        logger.error(
            f"QueryID: {query_id} - ASYNC Handling processing error: {error_msg}, Type: {error_type}. Full Traceback (if DEBUG):\n{traceback.format_exc() if self.config.get('DEBUG') else 'Traceback hidden in production.'}")

        fallback_text = f"Êä±Ê≠âÔºåÂú®Â§ÑÁêÜÊÇ®ÁöÑÊü•ËØ¢ '{user_query[:30]}...' Êó∂Á≥ªÁªüÈÅáÂà∞‰∫Ü‰∏Ä‰∏™È¢ÑÊúü‰πãÂ§ñÁöÑÈóÆÈ¢ò„ÄÇ"
        if self.config.get('fallback_response_enabled', True):
            try:
                # _generate_fallback_response ÊòØÂºÇÊ≠•ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂèØËÉΩË∞ÉÁî®AI
                fallback_text = await self._generate_fallback_response(user_query, error_msg)
            except Exception as fallback_e:
                logger.error(f"QueryID: {query_id} - Generating fallback response itself also failed: {fallback_e}")
                # Â¶ÇÊûúAIÁîüÊàêÈôçÁ∫ßÂìçÂ∫î‰πüÂ§±Ë¥•Ôºå‰ΩøÁî®Êõ¥ÁÆÄÂçï„ÄÅÁ°¨ÁºñÁ†ÅÁöÑÊñáÊú¨
                fallback_text = f"Â§ÑÁêÜËØ∑Ê±ÇÊó∂ÂèëÁîüÈîôËØØ„ÄÇÈîôËØØËØ¶ÊÉÖ: {error_msg if self.config.get('DEBUG') else 'ÂÜÖÈÉ®Á≥ªÁªüÈîôËØØÔºåËØ∑ËÅîÁ≥ªÊäÄÊúØÊîØÊåÅ„ÄÇ'}"

        error_processing_result = ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            response_text=fallback_text,
            insights=[],  # ÈîôËØØÊÉÖÂÜµ‰∏ãÔºåÊ¥ûÂØüÁ≠â‰∏∫Á©∫
            key_metrics={},
            visualizations=[],
            error_info={
                'error_type': error_type,
                'message': error_msg,
                'original_query': user_query,
                'trace': traceback.format_exc() if self.config.get("DEBUG") else "Error details hidden for security."
            },
            processing_strategy=OrchestratorProcessingStrategy.ERROR_HANDLING,
            processors_used=['AsyncErrorHandling'],  # Ê†áËÆ∞‰∏∫ÂºÇÊ≠•ÈîôËØØÂ§ÑÁêÜÂô®
            total_processing_time=total_time_on_error,
            conversation_id=conversation_id,  # ‰øùÊåÅ‰º†ÂÖ•ÁöÑ str ID
            timestamp=datetime.now().isoformat()
        )

        # Êõ¥Êñ∞ÁºñÊéíÂô®ÁªüËÆ°ÔºàÂåÖÊã¨Â§±Ë¥•ËÆ°Êï∞ÂíåÈîôËØØÁ±ªÂûãÔºâ
        self._update_orchestrator_stats(error_processing_result)

        # Â∞ùËØïËÆ∞ÂΩïÈîôËØØÂìçÂ∫îÂà∞ÂØπËØùÂéÜÂè≤
        if conversation_id and self.conversation_manager:
            conv_id_for_db_err: Optional[int] = None
            try:
                conv_id_for_db_err = int(conversation_id)
            except ValueError:
                logger.warning(
                    f"QueryID: {query_id} - Cannot log error to conversation due to invalid conversation_id format: {conversation_id}")

            if conv_id_for_db_err is not None:
                self._log_ai_response_to_conversation(conv_id_for_db_err, error_processing_result, query_id)

        return error_processing_result

        # Âú® IntelligentQAOrchestrator Á±ª‰∏≠‰øÆÊîπÊ≠§ÊñπÊ≥ïÔºö
    async def _fallback_query_parsing(self, user_query: str) -> QueryAnalysisResult:
        """
        Âú®AIÊü•ËØ¢Ëß£ÊûêÂ§±Ë¥•Êó∂ÔºåÊèê‰æõ‰∏Ä‰∏™Âü∫‰∫éËßÑÂàôÁöÑ„ÄÅÂü∫Á°ÄÁöÑ QueryAnalysisResult„ÄÇ
        """
        logger.warning(f"Executing fallback query parsing for: '{user_query[:50]}...'")

        query_lower = user_query.lower()
        # ‰ΩøÁî® query_parser.py ‰∏≠ÂÆö‰πâÁöÑ QueryParserQueryType Âíå QueryParserBusinessScenario
        qt = QueryParserQueryType.GENERAL_KNOWLEDGE if hasattr(QueryParserQueryType,
                                                               'GENERAL_KNOWLEDGE') else QueryParserQueryType.DEFINITION_EXPLANATION  # ÂÅáËÆæÊúâ‰∏Ä‰∏™ÈÄöÁî®Á±ªÂûãÊàñËß£ÈáäÁ±ªÂûã
        bs = QueryParserBusinessScenario.UNKNOWN  # ÈªòËÆ§‰ΩøÁî® UNKNOWN
        qc = QueryParserComplexity.SIMPLE

        if any(kw in query_lower for kw in ["‰ΩôÈ¢ù", "balance", "Â§öÂ∞ëÈí±", "ËµÑÈáëÁé∞Áä∂"]):
            qt = QueryParserQueryType.DATA_RETRIEVAL
            bs = QueryParserBusinessScenario.FINANCIAL_OVERVIEW  # ‰ΩøÁî®Â∑≤ÂÆö‰πâÁöÑ FINANCIAL_OVERVIEW
            qc = QueryParserComplexity.SIMPLE
        elif any(kw in query_lower for kw in ["Ë∂ãÂäø", "ÂéÜÂè≤", "trend", "history", "ËøáÂéª", "ÂØπÊØî"]):
            qt = QueryParserQueryType.TREND_ANALYSIS
            bs = QueryParserBusinessScenario.HISTORICAL_PERFORMANCE  # ‰ΩøÁî®Â∑≤ÂÆö‰πâÁöÑ HISTORICAL_PERFORMANCE
            qc = QueryParserComplexity.MEDIUM
        elif any(kw in query_lower for kw in
                 ["È¢ÑÊµã", "È¢ÑËÆ°", "Êú™Êù•", "scenario", "Â¶ÇÊûú", "what if"]):  # scenario Ëã±ÊñáÂ∞èÂÜô
            qt = QueryParserQueryType.PREDICTION
            bs = QueryParserBusinessScenario.FUTURE_PROJECTION  # ‰ΩøÁî®Â∑≤ÂÆö‰πâÁöÑ FUTURE_PROJECTION
            qc = QueryParserComplexity.COMPLEX
        elif any(kw in query_lower for kw in ["Áî®Êà∑", "‰ºöÂëò", "user", "member"]):
            bs = QueryParserBusinessScenario.USER_ANALYSIS
        elif any(kw in query_lower for kw in ["‰∫ßÂìÅ", "product"]):
            bs = QueryParserBusinessScenario.PRODUCT_ANALYSIS

        return QueryAnalysisResult(
            original_query=user_query,
            complexity=qc,
            query_type=qt,
            business_scenario=bs,  # ‰ΩøÁî®‰øÆÊ≠£ÂêéÁöÑ bs
            confidence_score=0.3,
            time_requirements={'parsed_by': 'fallback_rule', 'default_period': 'current'},
            date_parse_result=None,
            data_requirements={'apis_guessed': ['/api/sta/system']},
            required_apis=['/api/sta/system'],
            business_parameters={},
            calculation_requirements={},
            execution_plan=[
                QueryParserExecutionStep(  # ‰ΩøÁî®‰ªé query_parser ÂØºÂÖ•ÁöÑ QueryParserExecutionStep
                    step_id="fallback_fetch_system_data",
                    step_type="data_retrieval",
                    description="Fallback: Ëé∑ÂèñÂü∫Á°ÄÁ≥ªÁªüÊ¶ÇËßàÊï∞ÊçÆ",
                    required_data=["system_overview"],
                    processing_method="direct_api_call_via_smart_fetcher",
                    dependencies=[],
                    estimated_time=1.5,
                    ai_model_preference="any"
                )
            ],
            processing_strategy="direct_response",
            ai_collaboration_plan={'strategy_type': 'none', 'reason': 'fallback_parsing_used'},
            analysis_timestamp=datetime.now().isoformat(),
            estimated_total_time=2.0,
            processing_metadata={"parser_status": "fallback_rule_based_incomplete"}
        )

    def _classify_error(self, error_msg: str) -> str:
        """
        Ê†πÊçÆÈîôËØØÊ∂àÊÅØÊñáÊú¨ÂàÜÁ±ªÈîôËØØÁ±ªÂûã„ÄÇ
        ËøôÊòØ‰∏Ä‰∏™ÂêåÊ≠•ÊñπÊ≥ï„ÄÇ
        """
        s_err = str(error_msg).lower()  # Á°Æ‰øùÊòØÂ≠óÁ¨¶‰∏≤Âπ∂ËΩ¨‰∏∫Â∞èÂÜô‰ª•‰æøÂåπÈÖç

        if "timeout" in s_err: return "timeout_error"
        if "api key" in s_err or "authentication" in s_err or "unauthorized" in s_err: return "authentication_error"
        if "rate limit" in s_err or "quota exceeded" in s_err or "too many requests" in s_err: return "rate_limit_error"
        if "connection" in s_err or "network" in s_err or "dns" in s_err or "host not found" in s_err: return "network_error"
        if "database" in s_err or "sql" in s_err or "db query" in s_err: return "database_error"
        if "not initialized" in s_err or "not configured" in s_err: return "initialization_error"
        if any(ai_kw in s_err for ai_kw in
               ["claude", "openai", "gpt", "anthropic", "gemini", "ai model error"]): return "ai_service_error"
        if "attributeerror" in s_err: return "attribute_error_integration_issue"  # ÈÄöÂ∏∏ÊòØ‰ª£Á†ÅÈõÜÊàêÈóÆÈ¢ò
        if "keyerror" in s_err: return "key_error_data_structure_issue"  # ËÆøÈóÆ‰∫Ü‰∏çÂ≠òÂú®ÁöÑÂ≠óÂÖ∏ÈîÆ
        if "indexerror" in s_err: return "index_error_data_structure_issue"  # ÂàóË°®Á¥¢ÂºïË∂äÁïå
        if "valueerror" in s_err or ("typeerror" in s_err and (
                "parameter" in s_err or "argument" in s_err)): return "validation_or_type_error"
        if "file not found" in s_err: return "file_not_found_error"
        if "permission denied" in s_err: return "permission_error"
        if "memory" in s_err and "out of" in s_err: return "out_of_memory_error"

        # Êõ¥ÈÄöÁî®ÁöÑÈîôËØØÁ±ªÂûã
        if "parsing failed" in s_err: return "parsing_error"
        if "data acquisition failed" in s_err: return "data_acquisition_error"
        if "processing failed" in s_err: return "business_processing_error"
        if "insight generation failed" in s_err: return "insight_generation_error"

        return "unknown_processing_error"  # ÈªòËÆ§ÁöÑÊú™Áü•ÈîôËØØ
    def _extract_raw_numeric_metrics(self, business_result_data: Dict[str, Any]) -> Dict[str, float]:
        raw_metrics: Dict[str, float] = {}
        if not business_result_data or not isinstance(business_result_data, dict): return raw_metrics

        primary_res_content = business_result_data.get('primary_result')

        def _extract_from_single_res_raw(res_content: Any, prefix=""):
            if not res_content: return
            data_to_search = {}
            # Handle both dataclass objects and dictionaries
            if hasattr(res_content, 'to_dict') and callable(res_content.to_dict):
                data_to_search = res_content.to_dict()
            elif hasattr(res_content, '__dict__'):
                data_to_search = res_content.__dict__
            elif isinstance(res_content, dict):
                data_to_search = res_content
            else:
                return

            for metric_key_container in ['key_metrics', 'metrics', 'main_prediction', 'predictions']:
                if metric_key_container in data_to_search and isinstance(data_to_search[metric_key_container], dict):
                    for k, v_original in data_to_search[metric_key_container].items():
                        try:
                            v_str = str(v_original).replace('%', '').replace('¬•', '').replace(',', '')
                            numeric_v = float(v_str)
                            metric_name_to_store = f"{prefix}{k}".strip('_')
                            if metric_name_to_store in raw_metrics and raw_metrics[metric_name_to_store] != numeric_v:
                                metric_name_to_store = f"{prefix}{metric_key_container}_{k}".strip('_')
                            raw_metrics[metric_name_to_store] = numeric_v
                        except (ValueError, TypeError):
                            continue
                    break

        if isinstance(primary_res_content, dict):
            if 'multi_results' in primary_res_content and isinstance(primary_res_content['multi_results'], dict):
                for proc_name, proc_res_obj_or_dict in primary_res_content['multi_results'].items():
                    if proc_res_obj_or_dict and not (
                            isinstance(proc_res_obj_or_dict, dict) and 'error' in proc_res_obj_or_dict):
                        _extract_from_single_res_raw(proc_res_obj_or_dict, prefix=f"{proc_name}_")
            else:
                _extract_from_single_res_raw(primary_res_content)
        elif primary_res_content:
            _extract_from_single_res_raw(primary_res_content, prefix="main_")
        return raw_metrics

    def _determine_focus_areas(self, query_analysis: Optional[QueryAnalysisResult]) -> List[str]:
        if not query_analysis: return ['general_overview']
        focus_areas = set()
        query_type_val = query_analysis.query_type.value if query_analysis.query_type else ""
        biz_scenario_val = query_analysis.business_scenario.value if query_analysis.business_scenario else ""
        if 'financial' in biz_scenario_val or 'risk' in query_type_val: focus_areas.add('financial_health')
        if 'growth' in biz_scenario_val or 'trend' in query_type_val: focus_areas.add('growth_analysis')
        if 'prediction' in query_type_val or 'scenario' in query_type_val: focus_areas.add(
            'future_outlook_and_opportunities')
        if 'risk' in query_type_val: focus_areas.add('risk_management')
        if 'user' in biz_scenario_val or 'user' in query_type_val: focus_areas.add('user_behavior_insights')
        if 'product' in biz_scenario_val or 'product' in query_type_val: focus_areas.add('product_performance_review')
        return list(focus_areas) if focus_areas else ['general_business_overview']

    def get_orchestrator_stats(self) -> Dict[str, Any]:
        stats = self.orchestrator_stats.copy()
        total_q = stats.get('total_queries', 0)
        if total_q > 0:
            stats['success_rate'] = stats.get('successful_queries', 0) / total_q
            stats['failure_rate'] = stats.get('failed_queries', 0) / total_q
        else:
            stats['success_rate'] = 0.0;
            stats['failure_rate'] = 0.0
        cache_reqs = stats.get('cache_hits', 0) + stats.get('cache_misses', 0)
        stats['cache_hit_rate'] = stats.get('cache_hits', 0) / cache_reqs if cache_reqs > 0 else 0.0
        return stats

    async def health_check(self) -> Dict[str, Any]:
        if not self.initialized:
            try:
                await self.initialize()
            except Exception as e:
                return {'status': 'unhealthy', 'reason': 'Orchestrator initialization failed during health check',
                        'error': str(e), 'timestamp': datetime.now().isoformat()}
        health_status = {
            'status': 'healthy', 'timestamp': datetime.now().isoformat(),
            'orchestrator_initialized': self.initialized, 'components_status': {},
            'ai_clients_status': {
                'claude_available': self.claude_client is not None and hasattr(self.claude_client, 'messages'),
                # Example check
                'gpt_available': self.gpt_client is not None and hasattr(self.gpt_client, 'chat')  # Example check
            },
            'db_connector_status': 'available' if self.db_connector else 'unavailable',
            'conversation_manager_status': 'available' if self.conversation_manager else 'unavailable',
            'statistics_snapshot': self.get_orchestrator_stats(),
            'active_sessions_count': len(self.active_sessions),
            'cache_current_size': len(self.result_cache)
        }
        components_to_check = {
            'QueryParser': self.query_parser, 'DataRequirementsAnalyzer': self.data_requirements_analyzer,
            'SmartDataFetcher': self.data_fetcher, 'FinancialDataAnalyzer': self.financial_data_analyzer,
            'InsightGenerator': self.insight_generator, 'CurrentDataProcessor': self.current_data_processor,
            'HistoricalAnalysisProcessor': self.historical_analysis_processor,
            'PredictionProcessor': self.prediction_processor,
            'APIConnector': getattr(self.data_fetcher, 'api_connector', None) if self.data_fetcher else None
        }
        all_components_healthy = True
        for name, comp_instance in components_to_check.items():
            if comp_instance and hasattr(comp_instance, 'health_check') and callable(
                    getattr(comp_instance, 'health_check')):
                try:
                    comp_health = await comp_instance.health_check()
                    status_from_comp = comp_health.get('status', 'unknown_status_key')
                    health_status['components_status'][name] = status_from_comp
                    if status_from_comp != 'healthy': all_components_healthy = False
                except Exception as e_hc:
                    health_status['components_status'][name] = f'error_calling_hc: {str(e_hc)}'
                    all_components_healthy = False
            elif comp_instance:
                health_status['components_status'][name] = 'available_no_hc_method'
            else:
                health_status['components_status'][name] = 'unavailable'; all_components_healthy = False

        if not self.db_connector and self.conversation_manager and getattr(self.conversation_manager, 'db',
                                                                           None) is None:
            health_status['components_status']['ConversationManager'] = 'running_in_memory_mode'
        elif self.db_connector and self.conversation_manager:
            health_status['components_status']['ConversationManager'] = 'healthy_with_db'

        if not all_components_healthy: health_status['status'] = 'degraded'
        ai_status = health_status['ai_clients_status']
        if not ai_status['claude_available'] and not ai_status['gpt_available']:
            health_status['status'] = 'critical_ai_unavailable'
        elif not ai_status['claude_available'] or not ai_status['gpt_available']:
            health_status['status'] = 'limited_one_ai_unavailable'
        return health_status

    async def close(self):
        logger.info("Closing IntelligentQAOrchestrator and its resources...")
        if self.data_fetcher and hasattr(self.data_fetcher, 'close') and callable(self.data_fetcher.close):
            if asyncio.iscoroutinefunction(self.data_fetcher.close):
                await self.data_fetcher.close()
            else:
                self.data_fetcher.close()
        if self.db_connector and hasattr(self.db_connector, 'close') and callable(self.db_connector.close):
            try:
                if asyncio.iscoroutinefunction(self.db_connector.close):
                    await self.db_connector.close()
                else:
                    self.db_connector.close()
            except Exception as db_close_err:
                logger.error(f"Error closing DatabaseConnector: {db_close_err}")
        self.result_cache.clear()
        self.active_sessions.clear()
        self.initialized = False
        logger.info("IntelligentQAOrchestrator closed successfully.")


# --- ÂÖ®Â±ÄÂÆû‰æãÁÆ°ÁêÜ ---
_orchestrator_instance: Optional[IntelligentQAOrchestrator] = None


def get_orchestrator(claude_client_instance: Optional[ClaudeClient] = None,
                     gpt_client_instance: Optional[OpenAIClient] = None,
                     db_connector_instance: Optional[DatabaseConnector] = None,
                     app_config_instance: Optional[AppConfig] = None) -> IntelligentQAOrchestrator:
    global _orchestrator_instance
    if _orchestrator_instance is None:
        logger.info("Creating new IntelligentQAOrchestrator singleton instance.")
        _orchestrator_instance = IntelligentQAOrchestrator(
            claude_client_instance, gpt_client_instance, db_connector_instance, app_config_instance
        )
    elif (claude_client_instance and _orchestrator_instance.claude_client != claude_client_instance) or \
            (gpt_client_instance and _orchestrator_instance.gpt_client != gpt_client_instance) or \
            (db_connector_instance and _orchestrator_instance.db_connector != db_connector_instance) or \
            (app_config_instance and _orchestrator_instance.app_config != app_config_instance):
        logger.info("Re-configuring existing orchestrator instance.")
        if claude_client_instance is not None: _orchestrator_instance.claude_client = claude_client_instance
        if gpt_client_instance is not None: _orchestrator_instance.gpt_client = gpt_client_instance
        if db_connector_instance is not None: _orchestrator_instance.db_connector = db_connector_instance
        if app_config_instance is not None:
            _orchestrator_instance.app_config = app_config_instance
            _orchestrator_instance.config = _orchestrator_instance._load_orchestrator_config()
        _orchestrator_instance.initialized = False
        logger.info("Orchestrator marked for re-initialization due to new configuration/clients.")

    return _orchestrator_instance

# (async def main() Â∑≤ÁßªÈô§)