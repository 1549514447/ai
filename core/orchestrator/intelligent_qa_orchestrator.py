"""
ğŸ§  Claudeé©±åŠ¨çš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨ - æ™ºèƒ½å¢å¼ºç‰ˆ
æ ¸å¿ƒæ”¹è¿›ï¼š
- æ™ºèƒ½æ•°æ®æå–ï¼šæå–å…·ä½“æ•°å€¼è€Œéç®€å•æè¿°
- å¤æ‚è´¢åŠ¡è®¡ç®—æ”¯æŒï¼šå¤æŠ•ã€ç°é‡‘è·‘é“ã€å¢é•¿é¢„æµ‹ç­‰
- å¢å¼ºçš„Claudeç†è§£å’Œå›ç­”ç”Ÿæˆ
- æ™ºèƒ½æŸ¥è¯¢è·¯ç”±å’ŒAPIç»„åˆ
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import asyncio
import json
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import traceback
import re

# ğŸ¯ æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from core.analyzers.query_parser import (
    SmartQueryParser, create_smart_query_parser, QueryAnalysisResult,
    QueryComplexity, QueryType, BusinessScenario
)
from core.data_orchestration.smart_data_fetcher import (
    SmartDataFetcher, create_smart_data_fetcher,
    ExecutionResult as FetcherExecutionResult,
    DataQualityLevel as FetcherDataQualityLevel,
    ExecutionStatus as FetcherExecutionStatus
)
from utils.calculators.statistical_calculator import (
    UnifiedCalculator, create_unified_calculator, UnifiedCalculationResult,
    CalculationType
)

# å·¥å…·ç±»å¯¼å…¥
from utils.helpers.date_utils import DateUtils, create_date_utils
from utils.formatters.financial_formatter import FinancialFormatter, create_financial_formatter
from utils.formatters.chart_generator import ChartGenerator, create_chart_generator, ChartType
from utils.formatters.report_generator import ReportGenerator, create_report_generator
from data.models.conversation import ConversationManager, create_conversation_manager
from data.connectors.database_connector import DatabaseConnector, create_database_connector

# AI å®¢æˆ·ç«¯å¯¼å…¥
from core.models.claude_client import ClaudeClient, CustomJSONEncoder
from core.models.openai_client import OpenAIClient
from config import Config as AppConfig

logger = logging.getLogger(__name__)


# ============= æ•°æ®ç±»å®šä¹‰ =============

@dataclass
class BusinessInsight:
    """ä¸šåŠ¡æ´å¯Ÿç±»"""
    title: str
    summary: str
    confidence_score: float = 0.8
    insight_type: str = "general"
    recommendations: List[str] = field(default_factory=list)
    supporting_data: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if self.recommendations is None:
            self.recommendations = []
        if self.supporting_data is None:
            self.supporting_data = {}


class ProcessingStrategy(Enum):
    """å¤„ç†ç­–ç•¥"""
    SIMPLE_DATA = "simple_data"
    DATA_WITH_CALC = "data_with_calc"
    COMPREHENSIVE = "comprehensive"
    QUICK_RESPONSE = "quick_response"
    ERROR_HANDLING = "error_handling"


class DataExtractionStrategy(Enum):
    """æ•°æ®æå–ç­–ç•¥"""
    FINANCIAL_OVERVIEW = "financial_overview"
    EXPIRY_ANALYSIS = "expiry_analysis"
    DAILY_ANALYSIS = "daily_analysis"
    REINVESTMENT_ANALYSIS = "reinvestment_analysis"
    TREND_ANALYSIS = "trend_analysis"
    USER_ANALYSIS = "user_analysis"
    CASH_FLOW_ANALYSIS = "cash_flow_analysis"
    COMPREHENSIVE = "comprehensive"


@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ"""
    session_id: str
    query_id: str
    success: bool
    response_text: str
    insights: List[BusinessInsight] = field(default_factory=list)
    key_metrics: Dict[str, Any] = field(default_factory=dict)
    visualizations: List[Dict[str, Any]] = field(default_factory=list)
    processing_strategy: ProcessingStrategy = ProcessingStrategy.SIMPLE_DATA
    processors_used: List[str] = field(default_factory=list)
    ai_collaboration_summary: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 0.0
    data_quality_score: float = 0.0
    response_completeness: float = 0.0
    total_processing_time: float = 0.0
    ai_processing_time: float = 0.0
    data_fetching_time: float = 0.0
    processing_metadata: Dict[str, Any] = field(default_factory=dict)
    error_info: Optional[Dict[str, Any]] = None
    conversation_id: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    query_analysis_snapshot: Optional[Dict[str, Any]] = None


# ============= ä¸»ç¼–æ’å™¨ç±» =============

class IntelligentQAOrchestrator:
    """ğŸ§  æ™ºèƒ½é—®ç­”ç¼–æ’å™¨ - æ™ºèƒ½å¢å¼ºç‰ˆ"""

    _instance: Optional['IntelligentQAOrchestrator'] = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(IntelligentQAOrchestrator, cls).__new__(cls)
        return cls._instance

    def __init__(self, claude_client_instance: Optional[ClaudeClient] = None,
                 gpt_client_instance: Optional[OpenAIClient] = None,
                 db_connector_instance: Optional[DatabaseConnector] = None,
                 app_config_instance: Optional[AppConfig] = None):

        if hasattr(self, 'initialized') and self.initialized:
            return

        # åŸºç¡€é…ç½®
        self.claude_client = claude_client_instance
        self.gpt_client = gpt_client_instance
        self.db_connector = db_connector_instance
        self.app_config = app_config_instance if app_config_instance is not None else AppConfig()
        self.config = self._load_orchestrator_config()

        self.initialized = False
        self._initialize_component_placeholders()

        # ç»Ÿè®¡å’Œç¼“å­˜
        self.orchestrator_stats = self._default_stats()
        self.result_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = self.config.get('cache_ttl_seconds', 1800)

        # ğŸ†• æ™ºèƒ½æå–ç¼“å­˜
        self.extraction_cache: Dict[str, Dict[str, Any]] = {}

        logger.info("æ™ºèƒ½å¢å¼ºç‰ˆ IntelligentQAOrchestrator åˆ›å»ºå®Œæˆ")

    def _initialize_component_placeholders(self):
        """ç»„ä»¶åˆå§‹åŒ–"""
        # æ ¸å¿ƒç»„ä»¶
        self.query_parser: Optional[SmartQueryParser] = None
        self.data_fetcher: Optional[SmartDataFetcher] = None
        self.statistical_calculator: Optional[UnifiedCalculator] = None

        # å·¥å…·ç»„ä»¶
        self.date_utils: Optional[DateUtils] = None
        self.financial_formatter: Optional[FinancialFormatter] = None
        self.chart_generator: Optional[ChartGenerator] = None
        self.report_generator: Optional[ReportGenerator] = None
        self.conversation_manager: Optional[ConversationManager] = None

    def _load_orchestrator_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        cfg = {
            'max_processing_time': getattr(self.app_config, 'MAX_PROCESSING_TIME', 120),
            'enable_intelligent_caching': getattr(self.app_config, 'ENABLE_INTELLIGENT_CACHING', True),
            'min_confidence_threshold': getattr(self.app_config, 'MIN_CONFIDENCE_THRESHOLD', 0.6),
            'cache_ttl_seconds': getattr(self.app_config, 'CACHE_TTL', 1800),
            'claude_timeout': getattr(self.app_config, 'CLAUDE_TIMEOUT', 60),
            'gpt_timeout': getattr(self.app_config, 'GPT_TIMEOUT', 40),
            'DEBUG': getattr(self.app_config, 'DEBUG', False),
            'version': getattr(self.app_config, 'VERSION', '3.0.0-intelligent')
        }

        # APIé…ç½®
        if hasattr(self.app_config, 'CLAUDE_API_KEY'):
            cfg['CLAUDE_API_KEY'] = self.app_config.CLAUDE_API_KEY
        if hasattr(self.app_config, 'OPENAI_API_KEY'):
            cfg['OPENAI_API_KEY'] = self.app_config.OPENAI_API_KEY
        if hasattr(self.app_config, 'DATABASE_CONFIG'):
            cfg['DATABASE_CONFIG'] = self.app_config.DATABASE_CONFIG

        # APIè¿æ¥å™¨é…ç½®
        api_connector_cfg = {}
        if hasattr(self.app_config, 'FINANCE_API_BASE_URL'):
            api_connector_cfg['base_url'] = self.app_config.FINANCE_API_BASE_URL
        if hasattr(self.app_config, 'FINANCE_API_KEY'):
            api_connector_cfg['api_key'] = self.app_config.FINANCE_API_KEY
        if api_connector_cfg:
            cfg['api_connector_config'] = api_connector_cfg

        return cfg

    async def initialize(self):
        """æ™ºèƒ½åˆå§‹åŒ–"""
        if self.initialized:
            logger.debug("Orchestrator already initialized.")
            return

        logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ™ºèƒ½å¢å¼ºç‰ˆç¼–æ’å™¨...")
        start_init_time = time.time()

        try:
            # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
            if not self.claude_client and self.config.get('CLAUDE_API_KEY'):
                self.claude_client = ClaudeClient(api_key=self.config['CLAUDE_API_KEY'])
            if not self.gpt_client and self.config.get('OPENAI_API_KEY'):
                self.gpt_client = OpenAIClient(api_key=self.config['OPENAI_API_KEY'])

            # æ•°æ®åº“è¿æ¥å™¨
            if not self.db_connector and self.config.get('DATABASE_CONFIG'):
                db_cfg = self.config['DATABASE_CONFIG']
                if all(key in db_cfg for key in ['user', 'password', 'host', 'database']):
                    self.db_connector = create_database_connector(db_cfg)
                    logger.info("DatabaseConnector åˆå§‹åŒ–å®Œæˆ")

            # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
            self.query_parser = create_smart_query_parser(self.claude_client, self.gpt_client)

            fetcher_config = self.config.get('api_connector_config', {})
            self.data_fetcher = create_smart_data_fetcher(self.claude_client, self.gpt_client, fetcher_config)

            # ğŸ†• ä½¿ç”¨ç»Ÿä¸€è®¡ç®—å™¨
            self.statistical_calculator = create_unified_calculator(self.gpt_client, precision=6)

            # å·¥å…·ç»„ä»¶
            self.date_utils = create_date_utils(self.claude_client)
            self.financial_formatter = create_financial_formatter()
            self.chart_generator = create_chart_generator()
            self.report_generator = create_report_generator()

            # å¯¹è¯ç®¡ç†å™¨
            if self.db_connector:
                self.conversation_manager = create_conversation_manager(self.db_connector)
                logger.info("ConversationManager åˆå§‹åŒ–å®Œæˆ")
            else:
                logger.warning("æ•°æ®åº“è¿æ¥å™¨ä¸å¯ç”¨ï¼ŒConversationManager ä½¿ç”¨å†…å­˜æ¨¡å¼")
                self.conversation_manager = ConversationManager(database_connector=None)

            self.initialized = True
            init_duration = time.time() - start_init_time
            logger.info(f"âœ… æ™ºèƒ½å¢å¼ºç‰ˆç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_duration:.2f}s)")

        except Exception as e:
            self.initialized = False
            logger.error(f"âŒ æ™ºèƒ½ç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}\n{traceback.format_exc()}")

    # ================================================================
    # ğŸ¯ æ ¸å¿ƒæ–¹æ³•ï¼šæ™ºèƒ½æŸ¥è¯¢å¤„ç†
    # ================================================================

    async def process_intelligent_query(self, user_query: str, user_id: int = 0,
                                        conversation_id: Optional[str] = None,
                                        preferences: Optional[Dict[str, Any]] = None) -> ProcessingResult:
        """ğŸ¯ æ™ºèƒ½æŸ¥è¯¢å¤„ç† - å¢å¼ºç‰ˆ"""

        if not self.initialized:
            await self.initialize()
            if not self.initialized:
                return self._create_error_result("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥", user_query)

        session_id = str(uuid.uuid4())
        query_id = f"q_{datetime.now().strftime('%Y%m%d%H%M%S%f')}_{hashlib.md5(user_query.encode('utf-8')).hexdigest()[:6]}"
        start_time = time.time()

        logger.info(f"ğŸ¯ QueryID: {query_id} - å¼€å§‹æ™ºèƒ½å¤„ç†: '{user_query[:50]}...'")
        self.orchestrator_stats['total_queries'] += 1

        # å¤„ç†å¯¹è¯ID
        conversation_id_for_db = self._parse_conversation_id(conversation_id)

        # ä¿å­˜ç”¨æˆ·è¾“å…¥
        user_message_saved = await self._save_user_message_if_needed(
            conversation_id_for_db, user_query, query_id)

        try:
            # ğŸš€ æ™ºèƒ½å¿«é€Ÿå“åº”æ£€æµ‹
            quick_response = await self._try_intelligent_quick_response(user_query, query_id)
            if quick_response:
                return await self._build_quick_response_result(
                    quick_response, session_id, query_id, conversation_id, start_time, user_message_saved, conversation_id_for_db)

            timing = {}

            # 1ï¸âƒ£ æ™ºèƒ½æŸ¥è¯¢ç†è§£
            start_t = time.time()
            query_analysis = await self._intelligent_query_understanding(user_query, conversation_id_for_db)
            timing['parsing'] = time.time() - start_t

            # 2ï¸âƒ£ æ™ºèƒ½æ•°æ®è·å–
            start_t = time.time()
            data_result = await self._intelligent_data_acquisition(query_analysis)
            timing['data_fetching'] = time.time() - start_t

            # 3ï¸âƒ£ æ™ºèƒ½æ•°æ®æå–
            start_t = time.time()
            extracted_data = await self._intelligent_data_extraction(data_result, user_query, query_analysis)
            timing['data_extraction'] = time.time() - start_t

            # 4ï¸âƒ£ æ™ºèƒ½è®¡ç®—å¤„ç†
            start_t = time.time()
            calculation_result = None
            if query_analysis.needs_calculation:
                calculation_result = await self._intelligent_calculation_processing(
                    query_analysis, extracted_data, user_query)
            timing['calculation'] = time.time() - start_t

            # 5ï¸âƒ£ æ™ºèƒ½å›ç­”ç”Ÿæˆ
            start_t = time.time()
            response_text = await self._intelligent_response_generation(
                user_query, query_analysis, extracted_data, calculation_result)
            timing['response_generation'] = time.time() - start_t

            # 6ï¸âƒ£ æ™ºèƒ½æ´å¯Ÿç”Ÿæˆ
            start_t = time.time()
            insights = await self._intelligent_insights_generation(
                extracted_data, calculation_result, query_analysis, user_query)
            timing['insights'] = time.time() - start_t

            # 7ï¸âƒ£ æ™ºèƒ½å¯è§†åŒ–ç”Ÿæˆ
            start_t = time.time()
            visualizations = await self._intelligent_visualization_generation(
                extracted_data, calculation_result, query_analysis)
            timing['visualization'] = time.time() - start_t

            # æ„å»ºæ™ºèƒ½ç»“æœ
            total_processing_time = time.time() - start_time
            confidence = self._calculate_intelligent_confidence(
                query_analysis, extracted_data, calculation_result, insights)

            result = ProcessingResult(
                session_id=session_id,
                query_id=query_id,
                success=True,
                response_text=response_text,
                insights=insights,
                key_metrics=self._extract_intelligent_metrics(extracted_data, calculation_result),
                visualizations=visualizations,
                processing_strategy=self._determine_intelligent_strategy(query_analysis),
                processors_used=self._get_intelligent_processors_used(query_analysis, calculation_result),
                ai_collaboration_summary=self._get_intelligent_ai_summary(timing),
                confidence_score=confidence,
                data_quality_score=self._calculate_data_quality_score(extracted_data),
                response_completeness=self._calculate_intelligent_completeness(
                    extracted_data, calculation_result, insights),
                total_processing_time=total_processing_time,
                ai_processing_time=timing.get('parsing', 0) + timing.get('response_generation', 0),
                data_fetching_time=timing.get('data_fetching', 0),
                processing_metadata={
                    'query_complexity': query_analysis.complexity.value,
                    'query_type': query_analysis.query_type.value,
                    'extraction_strategy': extracted_data.get('extraction_strategy', 'unknown'),
                    'step_times': timing,
                    'intelligent_architecture': True
                },
                conversation_id=conversation_id,
                query_analysis_snapshot=query_analysis.to_dict()
            )

            # æ›´æ–°ç»Ÿè®¡å’Œç¼“å­˜
            self._update_stats(result)
            await self._cache_result_if_appropriate(result)

            # ä¿å­˜AIå“åº”
            if conversation_id_for_db and user_message_saved:
                await self._save_ai_response_if_needed(conversation_id_for_db, result, query_id)

            logger.info(f"âœ… QueryID: {query_id} - æ™ºèƒ½å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {total_processing_time:.2f}s")
            return result

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"âŒ QueryID: {query_id} - æ™ºèƒ½å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            return await self._handle_error(session_id, query_id, user_query, str(e), error_time, conversation_id)

    # ================================================================
    # ğŸ§  æ™ºèƒ½å¤„ç†æ–¹æ³•
    # ================================================================

    async def _try_intelligent_quick_response(self, user_query: str, query_id: str) -> Optional[Dict[str, Any]]:
        """ğŸš€ æ™ºèƒ½å¿«é€Ÿå“åº” - å¢å¼ºç‰ˆ"""
        try:
            query_lower = user_query.lower()

            # ğŸ§  æ™ºèƒ½æ¨¡å¼æ£€æµ‹ - æ›´ç²¾ç¡®çš„æ¨¡å¼åŒ¹é…
            quick_patterns = self._detect_intelligent_quick_patterns(query_lower, user_query)

            if not quick_patterns:
                return None

            logger.info(f"âš¡ æ£€æµ‹åˆ°æ™ºèƒ½å¿«é€Ÿæ¨¡å¼: {quick_patterns['pattern_type']}")

            # ğŸš€ æ‰§è¡Œæ™ºèƒ½APIè°ƒç”¨
            start_fetch_time = time.time()
            api_result = await self._execute_intelligent_quick_api(quick_patterns)
            fetch_time = time.time() - start_fetch_time

            if not api_result or not api_result.get('success'):
                logger.warning(f"å¿«é€ŸAPIè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´æµç¨‹")
                return None

            # ğŸ¯ æ™ºèƒ½æ•°æ®æå–å’Œæ ¼å¼åŒ–
            intelligent_response = await self._format_intelligent_quick_response(
                api_result, quick_patterns, user_query)

            return {
                'response': intelligent_response['text'],
                'metrics': intelligent_response.get('metrics', {}),
                'insights': intelligent_response.get('insights', []),
                'data_quality': api_result.get('validation', {}).get('confidence', 0.9),
                'fetch_time': fetch_time,
                'api_method': quick_patterns['api_method'],
                'pattern_type': quick_patterns['pattern_type']
            }

        except Exception as e:
            logger.error(f"æ™ºèƒ½å¿«é€Ÿå“åº”å¤„ç†å¤±è´¥: {e}")
            return None

    def _detect_intelligent_quick_patterns(self, query_lower: str, original_query: str) -> Optional[Dict[str, Any]]:
        """ğŸ§  æ™ºèƒ½å¿«é€Ÿæ¨¡å¼æ£€æµ‹ - å¤§å¹…å¢å¼º"""

        # ğŸ¯ æ™ºèƒ½æ—¥æœŸæå–
        dates_info = self._extract_dates_from_query(original_query)

        patterns = [
            # ğŸ’° è´¢åŠ¡æ¦‚è§ˆæŸ¥è¯¢
            {
                'pattern_type': 'financial_overview',
                'keywords': ['æ€»èµ„é‡‘', 'æ€»ä½™é¢', 'èµ„é‡‘', 'ä½™é¢', 'æ´»è·ƒä¼šå‘˜', 'ç”¨æˆ·æ•°'],
                'exclude_keywords': ['å…¥é‡‘', 'å‡ºé‡‘', 'åˆ°æœŸ', 'è¶‹åŠ¿'],
                'api_method': 'get_system_data',
                'extraction_strategy': DataExtractionStrategy.FINANCIAL_OVERVIEW,
                'description': 'è´¢åŠ¡æ¦‚è§ˆæŸ¥è¯¢'
            },

            # ğŸ“… ç‰¹å®šæ—¥æœŸæŸ¥è¯¢
            {
                'pattern_type': 'specific_date_query',
                'date_required': True,
                'keywords': ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œ', 'æ•°æ®'],
                'api_method': 'get_daily_data',
                'extraction_strategy': DataExtractionStrategy.DAILY_ANALYSIS,
                'description': 'ç‰¹å®šæ—¥æœŸæ•°æ®æŸ¥è¯¢'
            },

            # â° åˆ°æœŸäº§å“æŸ¥è¯¢
            {
                'pattern_type': 'expiry_query',
                'keywords': ['åˆ°æœŸ', 'è¿‡æœŸ', 'äº§å“åˆ°æœŸ'],
                'exclude_keywords': ['å¤æŠ•', 'è®¡ç®—', 'é¢„è®¡', 'è¶‹åŠ¿'],
                'api_method': 'auto_detect_expiry',  # æ ¹æ®æ—¥æœŸè‡ªåŠ¨é€‰æ‹©
                'extraction_strategy': DataExtractionStrategy.EXPIRY_ANALYSIS,
                'description': 'äº§å“åˆ°æœŸæŸ¥è¯¢'
            },

            # ğŸ‘¥ ç”¨æˆ·åˆ†ææŸ¥è¯¢
            {
                'pattern_type': 'user_analysis',
                'keywords': ['ç”¨æˆ·', 'ä¼šå‘˜', 'æ³¨å†Œ', 'æ´»è·ƒ'],
                'exclude_keywords': ['åˆ°æœŸ', 'å…¥é‡‘', 'å‡ºé‡‘'],
                'api_method': 'get_system_data',  # ä»ç³»ç»Ÿæ•°æ®è·å–ç”¨æˆ·ç»Ÿè®¡
                'extraction_strategy': DataExtractionStrategy.USER_ANALYSIS,
                'description': 'ç”¨æˆ·åˆ†ææŸ¥è¯¢'
            }
        ]

        # ğŸ” æ™ºèƒ½åŒ¹é…
        for pattern in patterns:
            if self._match_intelligent_pattern(query_lower, pattern, dates_info):
                matched_pattern = pattern.copy()
                matched_pattern.update({
                    'dates_info': dates_info,
                    'original_query': original_query,
                    'params': self._extract_pattern_params(query_lower, pattern, dates_info)
                })
                return matched_pattern

        return None

    def _extract_dates_from_query(self, query: str) -> Dict[str, Any]:
        """ğŸ§  æ™ºèƒ½æ—¥æœŸæå–"""
        import re
        from datetime import datetime

        dates_info = {
            'has_dates': False,
            'date_type': 'none',  # single, range, relative
            'dates': [],
            'api_format_dates': []
        }

        # ğŸ¯ å¤šç§æ—¥æœŸæ ¼å¼åŒ¹é…
        patterns = [
            # XæœˆXæ—¥æ ¼å¼
            (r'(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]', 'chinese_month_day'),
            # YYYYMMDDæ ¼å¼
            (r'(\d{4})(\d{2})(\d{2})', 'yyyymmdd'),
            # YYYY-MM-DDæ ¼å¼
            (r'(\d{4})-(\d{2})-(\d{2})', 'yyyy_mm_dd'),
            # ç›¸å¯¹æ—¥æœŸ
            (r'ä»Š[å¤©æ—¥]|å½“[å¤©æ—¥]', 'today'),
            (r'æ˜[å¤©æ—¥]', 'tomorrow'),
            (r'æœ¬å‘¨', 'this_week'),
            (r'ä¸Šå‘¨|ä¸Šä¸ªæ˜ŸæœŸ', 'last_week')
        ]

        current_year = datetime.now().year

        for pattern, date_type in patterns:
            matches = re.findall(pattern, query)

            if date_type == 'chinese_month_day':
                for match in matches:
                    month, day = int(match[0]), int(match[1])
                    try:
                        date_obj = datetime(current_year, month, day)
                        api_format = date_obj.strftime('%Y%m%d')
                        dates_info['dates'].append(date_obj)
                        dates_info['api_format_dates'].append(api_format)
                        dates_info['has_dates'] = True
                    except ValueError:
                        continue

            elif date_type == 'yyyymmdd':
                for match in matches:
                    try:
                        date_str = ''.join(match)
                        date_obj = datetime.strptime(date_str, '%Y%m%d')
                        dates_info['dates'].append(date_obj)
                        dates_info['api_format_dates'].append(date_str)
                        dates_info['has_dates'] = True
                    except ValueError:
                        continue

            elif date_type == 'today':
                today = datetime.now()
                dates_info['dates'].append(today)
                dates_info['api_format_dates'].append(today.strftime('%Y%m%d'))
                dates_info['has_dates'] = True

            # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æ—¥æœŸç±»å‹çš„å¤„ç†...

        # ğŸ¯ åˆ¤æ–­æ—¥æœŸç±»å‹
        if len(dates_info['dates']) == 1:
            dates_info['date_type'] = 'single'
        elif len(dates_info['dates']) == 2:
            dates_info['date_type'] = 'range'
        elif len(dates_info['dates']) > 2:
            dates_info['date_type'] = 'multiple'

        return dates_info

    def _match_intelligent_pattern(self, query_lower: str, pattern: Dict[str, Any],
                                   dates_info: Dict[str, Any]) -> bool:
        """æ™ºèƒ½æ¨¡å¼åŒ¹é…"""

        # æ£€æŸ¥æ—¥æœŸè¦æ±‚
        if pattern.get('date_required', False) and not dates_info['has_dates']:
            return False

        # æ£€æŸ¥æ’é™¤å…³é”®è¯
        if 'exclude_keywords' in pattern:
            if any(exc in query_lower for exc in pattern['exclude_keywords']):
                return False

        # æ£€æŸ¥å¿…é¡»å…³é”®è¯
        if 'keywords' in pattern:
            if not any(kw in query_lower for kw in pattern['keywords']):
                return False

        return True

    def _extract_pattern_params(self, query_lower: str, pattern: Dict[str, Any],
                                dates_info: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ¨¡å¼å‚æ•°"""
        params = {}

        # ğŸ¯ æ ¹æ®ä¸åŒæ¨¡å¼æå–å‚æ•°
        if pattern['pattern_type'] == 'specific_date_query':
            if dates_info['has_dates'] and dates_info['api_format_dates']:
                params['date'] = dates_info['api_format_dates'][0]

        elif pattern['pattern_type'] == 'expiry_query':
            if dates_info['date_type'] == 'single' and dates_info['api_format_dates']:
                params['date'] = dates_info['api_format_dates'][0]
                pattern['api_method'] = 'get_product_end_data'
            elif dates_info['date_type'] == 'range' and len(dates_info['api_format_dates']) >= 2:
                params['start_date'] = dates_info['api_format_dates'][0]
                params['end_date'] = dates_info['api_format_dates'][1]
                pattern['api_method'] = 'get_product_end_interval'
            elif 'ä»Š' in query_lower or 'ä»Šå¤©' in query_lower:
                pattern['api_method'] = 'get_expiring_products_today'
            elif 'æœ¬å‘¨' in query_lower:
                pattern['api_method'] = 'get_expiring_products_week'

        return params

    async def _execute_intelligent_quick_api(self, pattern_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ğŸš€ æ‰§è¡Œæ™ºèƒ½å¿«é€ŸAPIè°ƒç”¨"""
        try:
            api_method = pattern_info['api_method']
            params = pattern_info['params']

            if not self.data_fetcher or not self.data_fetcher.api_connector:
                logger.error("APIè¿æ¥å™¨ä¸å¯ç”¨")
                return None

            api_connector = self.data_fetcher.api_connector

            logger.info(f"ğŸš€ æ‰§è¡Œæ™ºèƒ½å¿«é€ŸAPI: {api_method}, å‚æ•°: {params}")

            # ğŸ¯ æ™ºèƒ½APIè·¯ç”±
            if api_method == 'get_daily_data':
                if 'date' in params:
                    result = await api_connector.get_daily_data(params['date'])
                else:
                    result = await api_connector.get_daily_data()

            elif api_method == 'get_system_data':
                result = await api_connector.get_system_data()

            elif api_method == 'get_expiring_products_today':
                result = await api_connector.get_expiring_products_today()

            elif api_method == 'get_expiring_products_week':
                result = await api_connector.get_expiring_products_week()

            elif api_method == 'get_product_end_data':
                if 'date' in params:
                    result = await api_connector.get_product_end_data(params['date'])
                else:
                    logger.error("get_product_end_data éœ€è¦dateå‚æ•°")
                    return None

            elif api_method == 'get_product_end_interval':
                if 'start_date' in params and 'end_date' in params:
                    result = await api_connector.get_product_end_interval(
                        params['start_date'], params['end_date'])
                else:
                    logger.error("get_product_end_interval éœ€è¦start_dateå’Œend_dateå‚æ•°")
                    return None

            else:
                logger.error(f"æœªçŸ¥çš„APIæ–¹æ³•: {api_method}")
                return None

            logger.info(f"ğŸ” APIè°ƒç”¨ç»“æœ: success={result.get('success')}")
            return result

        except Exception as e:
            logger.error(f"æ™ºèƒ½å¿«é€ŸAPIè°ƒç”¨å¤±è´¥: {e}")
            return None

    async def _format_intelligent_quick_response(self, api_result: Dict[str, Any],
                                                 pattern_info: Dict[str, Any],
                                                 original_query: str) -> Dict[str, Any]:
        """ğŸ¯ æ™ºèƒ½å¿«é€Ÿå“åº”æ ¼å¼åŒ–"""
        try:
            data = api_result.get('data', {})
            if not data:
                return {
                    'text': f"è·å–åˆ°{pattern_info['description']}ï¼Œä½†æ•°æ®ä¸ºç©ºã€‚",
                    'metrics': {}
                }

            extraction_strategy = pattern_info.get('extraction_strategy')

            # ğŸ§  æ ¹æ®æå–ç­–ç•¥æ™ºèƒ½æ ¼å¼åŒ–
            if extraction_strategy == DataExtractionStrategy.FINANCIAL_OVERVIEW:
                return await self._format_financial_overview_response(data, original_query)
            elif extraction_strategy == DataExtractionStrategy.DAILY_ANALYSIS:
                return await self._format_daily_analysis_response(data, original_query, pattern_info)
            elif extraction_strategy == DataExtractionStrategy.EXPIRY_ANALYSIS:
                return await self._format_expiry_analysis_response(data, original_query, pattern_info)
            elif extraction_strategy == DataExtractionStrategy.USER_ANALYSIS:
                return await self._format_user_analysis_response(data, original_query)
            else:
                return {
                    'text': f"å·²è·å–{pattern_info['description']}æ•°æ®ã€‚",
                    'metrics': self._extract_basic_metrics(data)
                }

        except Exception as e:
            logger.error(f"æ™ºèƒ½å“åº”æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"æ•°æ®è·å–æˆåŠŸï¼Œä½†æ ¼å¼åŒ–æ—¶é‡åˆ°é—®é¢˜ï¼š{str(e)}",
                'metrics': {}
            }

    async def _format_financial_overview_response(self, data: Dict[str, Any],
                                                  query: str) -> Dict[str, Any]:
        """ğŸ’° æ™ºèƒ½è´¢åŠ¡æ¦‚è§ˆå“åº”æ ¼å¼åŒ–"""
        try:
            # ğŸ¯ æ™ºèƒ½æ•°æ®æå–
            total_balance = float(data.get('æ€»ä½™é¢', 0))
            total_inflow = float(data.get('æ€»å…¥é‡‘', 0))
            total_outflow = float(data.get('æ€»å‡ºé‡‘', 0))
            total_investment = float(data.get('æ€»æŠ•èµ„é‡‘é¢', 0))

            user_stats = data.get('ç”¨æˆ·ç»Ÿè®¡', {})
            total_users = int(user_stats.get('æ€»ç”¨æˆ·æ•°', 0))
            active_users = int(user_stats.get('æ´»è·ƒç”¨æˆ·æ•°', 0))

            # ğŸ§  æ ¹æ®æŸ¥è¯¢å†…å®¹æ™ºèƒ½é€‰æ‹©å›ç­”é‡ç‚¹
            query_lower = query.lower()
            response_parts = ["ğŸ’° è´¢åŠ¡æ¦‚è§ˆï¼š"]

            if 'æ´»è·ƒä¼šå‘˜' in query_lower or 'æ´»è·ƒç”¨æˆ·' in query_lower:
                response_parts.append(f"ğŸ”¥ æ´»è·ƒä¼šå‘˜ï¼š{active_users:,}äºº")
                if total_users > 0:
                    activity_rate = (active_users / total_users) * 100
                    response_parts.append(f"ğŸ“Š æ´»è·ƒç‡ï¼š{activity_rate:.1f}%")
                response_parts.append(f"ğŸ‘¥ æ€»ç”¨æˆ·ï¼š{total_users:,}äºº")

            elif 'æ€»èµ„é‡‘' in query_lower or 'ä½™é¢' in query_lower:
                if self.financial_formatter:
                    response_parts.append(f"ğŸ’µ æ€»ä½™é¢ï¼š{self.financial_formatter.format_currency(total_balance)}")
                else:
                    response_parts.append(f"ğŸ’µ æ€»ä½™é¢ï¼šÂ¥{total_balance:,.2f}")

                # ğŸ’¡ æ™ºèƒ½æ´å¯Ÿ
                net_flow = total_inflow - total_outflow
                if net_flow > 0:
                    if self.financial_formatter:
                        response_parts.append(f"ğŸ“ˆ å‡€æµå…¥ï¼š{self.financial_formatter.format_currency(net_flow)}")
                    else:
                        response_parts.append(f"ğŸ“ˆ å‡€æµå…¥ï¼šÂ¥{net_flow:,.2f}")
                    response_parts.append("ğŸ’ª èµ„é‡‘çŠ¶å†µè‰¯å¥½")

            else:
                # ç»¼åˆå±•ç¤º
                if self.financial_formatter:
                    response_parts.extend([
                        f"ğŸ’µ æ€»ä½™é¢ï¼š{self.financial_formatter.format_currency(total_balance)}",
                        f"ğŸ‘¥ æ´»è·ƒä¼šå‘˜ï¼š{active_users:,}äººï¼ˆæ€»ç”¨æˆ·ï¼š{total_users:,}äººï¼‰"
                    ])
                else:
                    response_parts.extend([
                        f"ğŸ’µ æ€»ä½™é¢ï¼šÂ¥{total_balance:,.2f}",
                        f"ğŸ‘¥ æ´»è·ƒä¼šå‘˜ï¼š{active_users:,}äººï¼ˆæ€»ç”¨æˆ·ï¼š{total_users:,}äººï¼‰"
                    ])

            # ğŸ¯ æ™ºèƒ½æ´å¯Ÿç”Ÿæˆ
            insights = []
            if total_users > 0:
                activity_rate = active_users / total_users
                if activity_rate > 0.8:
                    insights.append("ç”¨æˆ·æ´»è·ƒåº¦å¾ˆé«˜ï¼Œå¹³å°å¸å¼•åŠ›å¼º")
                elif activity_rate < 0.3:
                    insights.append("ç”¨æˆ·æ´»è·ƒåº¦è¾ƒä½ï¼Œå»ºè®®åŠ å¼ºç”¨æˆ·è¿è¥")

            if total_balance > 0 and total_inflow > 0:
                balance_ratio = total_balance / total_inflow
                if balance_ratio > 0.8:
                    insights.append("èµ„é‡‘ç•™å­˜ç‡é«˜ï¼Œç”¨æˆ·ä¿¡ä»»åº¦è‰¯å¥½")

            return {
                'text': '\n'.join(response_parts),
                'metrics': {
                    'æ€»ä½™é¢': total_balance,
                    'æ€»å…¥é‡‘': total_inflow,
                    'æ€»å‡ºé‡‘': total_outflow,
                    'æ´»è·ƒç”¨æˆ·æ•°': active_users,
                    'æ€»ç”¨æˆ·æ•°': total_users,
                    'æ´»è·ƒç‡': (active_users / total_users) if total_users > 0 else 0
                },
                'insights': insights
            }

        except Exception as e:
            logger.error(f"è´¢åŠ¡æ¦‚è§ˆæ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"è´¢åŠ¡æ•°æ®è·å–æˆåŠŸï¼ŒåŸå§‹æ•°æ®ï¼š{str(data)[:200]}...",
                'metrics': {}
            }

    async def _format_daily_analysis_response(self, data: Dict[str, Any], query: str,
                                              pattern_info: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ“… æ™ºèƒ½æ¯æ—¥æ•°æ®å“åº”æ ¼å¼åŒ–"""
        try:
            date = data.get('æ—¥æœŸ', pattern_info.get('params', {}).get('date', 'æœªçŸ¥æ—¥æœŸ'))
            inflow = float(data.get('å…¥é‡‘', 0))
            outflow = float(data.get('å‡ºé‡‘', 0))
            registrations = int(data.get('æ³¨å†Œäººæ•°', 0))
            purchases = int(data.get('è´­ä¹°äº§å“æ•°é‡', 0))
            holdings = int(data.get('æŒä»“äººæ•°', 0))

            query_lower = query.lower()
            response_parts = [f"ğŸ“… {date} æ•°æ®åˆ†æï¼š"]

            # ğŸ§  æ™ºèƒ½é‡ç‚¹çªå‡º
            if 'å…¥é‡‘' in query_lower:
                if self.financial_formatter:
                    response_parts.append(f"ğŸ’° å…¥é‡‘ï¼š{self.financial_formatter.format_currency(inflow)}")
                else:
                    response_parts.append(f"ğŸ’° å…¥é‡‘ï¼šÂ¥{inflow:,.2f}")

                if inflow > 100000:  # 10ä¸‡ä»¥ä¸Š
                    response_parts.append("ğŸ“ˆ å…¥é‡‘è¡¨ç°ä¼˜ç§€")
                elif inflow > 50000:  # 5ä¸‡ä»¥ä¸Š
                    response_parts.append("âœ… å…¥é‡‘è¡¨ç°è‰¯å¥½")

            elif 'å‡ºé‡‘' in query_lower:
                if self.financial_formatter:
                    response_parts.append(f"ğŸ’¸ å‡ºé‡‘ï¼š{self.financial_formatter.format_currency(outflow)}")
                else:
                    response_parts.append(f"ğŸ’¸ å‡ºé‡‘ï¼šÂ¥{outflow:,.2f}")

            elif 'æ³¨å†Œ' in query_lower:
                response_parts.append(f"ğŸ‘¥ æ–°å¢æ³¨å†Œï¼š{registrations}äºº")
                if registrations > 100:
                    response_parts.append("ğŸš€ æ³¨å†Œé‡è¡¨ç°å‡ºè‰²")

            else:
                # å®Œæ•´å±•ç¤º
                if self.financial_formatter:
                    response_parts.extend([
                        f"ğŸ’° å…¥é‡‘ï¼š{self.financial_formatter.format_currency(inflow)}",
                        f"ğŸ’¸ å‡ºé‡‘ï¼š{self.financial_formatter.format_currency(outflow)}",
                        f"ğŸ‘¥ æ–°å¢æ³¨å†Œï¼š{registrations}äºº",
                        f"ğŸ›ï¸ äº§å“è´­ä¹°ï¼š{purchases}ç¬”"
                    ])
                else:
                    response_parts.extend([
                        f"ğŸ’° å…¥é‡‘ï¼šÂ¥{inflow:,.2f}",
                        f"ğŸ’¸ å‡ºé‡‘ï¼šÂ¥{outflow:,.2f}",
                        f"ğŸ‘¥ æ–°å¢æ³¨å†Œï¼š{registrations}äºº",
                        f"ğŸ›ï¸ äº§å“è´­ä¹°ï¼š{purchases}ç¬”"
                    ])

            # ğŸ¯ æ™ºèƒ½åˆ†æ
            net_flow = inflow - outflow
            insights = []

            if net_flow > 0:
                if self.financial_formatter:
                    response_parts.append(f"ğŸ“ˆ å‡€æµå…¥ï¼š{self.financial_formatter.format_currency(net_flow)}")
                else:
                    response_parts.append(f"ğŸ“ˆ å‡€æµå…¥ï¼šÂ¥{net_flow:,.2f}")
                insights.append("èµ„é‡‘å‡€æµå…¥ï¼Œè¡¨ç°ç§¯æ")
            elif net_flow < 0:
                if self.financial_formatter:
                    response_parts.append(f"ğŸ“‰ å‡€æµå‡ºï¼š{self.financial_formatter.format_currency(abs(net_flow))}")
                else:
                    response_parts.append(f"ğŸ“‰ å‡€æµå‡ºï¼šÂ¥{abs(net_flow):,.2f}")

            if registrations > 0 and purchases > 0:
                conversion_rate = purchases / registrations
                if conversion_rate > 0.5:
                    insights.append("æ³¨å†Œè½¬åŒ–ç‡é«˜ï¼Œäº§å“å¸å¼•åŠ›å¼º")

            return {
                'text': '\n'.join(response_parts),
                'metrics': {
                    'å…¥é‡‘': inflow,
                    'å‡ºé‡‘': outflow,
                    'å‡€æµå…¥': net_flow,
                    'æ³¨å†Œäººæ•°': registrations,
                    'è´­ä¹°äº§å“æ•°é‡': purchases,
                    'æŒä»“äººæ•°': holdings
                },
                'insights': insights
            }

        except Exception as e:
            logger.error(f"æ¯æ—¥æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"æ¯æ—¥æ•°æ®è·å–æˆåŠŸï¼ŒåŸå§‹æ•°æ®ï¼š{str(data)[:200]}...",
                'metrics': {}
            }

    async def _format_expiry_analysis_response(self, data: Dict[str, Any], query: str,
                                               pattern_info: Dict[str, Any]) -> Dict[str, Any]:
        """â° æ™ºèƒ½åˆ°æœŸæ•°æ®å“åº”æ ¼å¼åŒ–"""
        try:
            # ğŸ§  æ™ºèƒ½è¯†åˆ«æ•°æ®ç»“æ„
            if 'interval_stats' in data:
                # åŒºé—´åˆ°æœŸæ•°æ®
                return await self._format_interval_expiry_response(data, query, pattern_info)
            else:
                # å•æ—¥åˆ°æœŸæ•°æ®
                return await self._format_single_day_expiry_response(data, query, pattern_info)

        except Exception as e:
            logger.error(f"åˆ°æœŸæ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"åˆ°æœŸæ•°æ®è·å–æˆåŠŸï¼ŒåŸå§‹æ•°æ®ï¼š{str(data)[:200]}...",
                'metrics': {}
            }

    async def _format_interval_expiry_response(self, data: Dict[str, Any], query: str,
                                               pattern_info: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ“Š åŒºé—´åˆ°æœŸæ•°æ®æ ¼å¼åŒ–"""
        try:
            date_range = data.get('æ—¥æœŸ', 'æœªçŸ¥æ—¶é—´èŒƒå›´')
            total_count = int(data.get('åˆ°æœŸæ•°é‡', 0))
            total_amount = float(data.get('åˆ°æœŸé‡‘é¢', 0))

            interval_stats = data.get('interval_stats', {})
            total_days = interval_stats.get('total_days', 1)

            response_parts = [f"ğŸ“Š {date_range} äº§å“åˆ°æœŸåˆ†æï¼š"]

            # ğŸ’° æ ¸å¿ƒæ•°æ®å±•ç¤º
            if self.financial_formatter:
                response_parts.extend([
                    f"ğŸ’° æ€»åˆ°æœŸé‡‘é¢ï¼š{self.financial_formatter.format_currency(total_amount)}",
                    f"ğŸ“¦ æ€»åˆ°æœŸæ•°é‡ï¼š{total_count:,}ç¬”"
                ])
            else:
                response_parts.extend([
                    f"ğŸ’° æ€»åˆ°æœŸé‡‘é¢ï¼šÂ¥{total_amount:,.2f}",
                    f"ğŸ“¦ æ€»åˆ°æœŸæ•°é‡ï¼š{total_count:,}ç¬”"
                ])

            # ğŸ“ˆ ç»Ÿè®¡åˆ†æ
            if total_count > 0:
                avg_amount = total_amount / total_count
                if self.financial_formatter:
                    response_parts.append(f"ğŸ“Š å¹³å‡é‡‘é¢ï¼š{self.financial_formatter.format_currency(avg_amount)}")
                else:
                    response_parts.append(f"ğŸ“Š å¹³å‡é‡‘é¢ï¼šÂ¥{avg_amount:,.2f}")

            if total_days > 1:
                daily_avg_amount = total_amount / total_days
                daily_avg_count = total_count / total_days
                if self.financial_formatter:
                    response_parts.append(f"ğŸ“… æ—¥å‡åˆ°æœŸï¼š{self.financial_formatter.format_currency(daily_avg_amount)}ï¼ˆ{daily_avg_count:.1f}ç¬”ï¼‰")
                else:
                    response_parts.append(f"ğŸ“… æ—¥å‡åˆ°æœŸï¼šÂ¥{daily_avg_amount:,.2f}ï¼ˆ{daily_avg_count:.1f}ç¬”ï¼‰")

            # ğŸ” äº§å“è¯¦æƒ…
            product_list = data.get('äº§å“åˆ—è¡¨', [])
            if product_list:
                response_parts.append(f"\nğŸ” ä¸»è¦äº§å“ï¼ˆå‰3åï¼‰ï¼š")
                sorted_products = sorted(product_list, key=lambda x: float(x.get('åˆ°æœŸé‡‘é¢', 0)), reverse=True)

                for i, product in enumerate(sorted_products[:3], 1):
                    name = product.get('äº§å“åç§°', f'äº§å“{i}')
                    amount = float(product.get('åˆ°æœŸé‡‘é¢', 0))
                    if amount > 0:
                        if self.financial_formatter:
                            response_parts.append(f"  {i}. {name}ï¼š{self.financial_formatter.format_currency(amount)}")
                        else:
                            response_parts.append(f"  {i}. {name}ï¼šÂ¥{amount:,.2f}")

            # ğŸ¯ æ™ºèƒ½æ´å¯Ÿ
            insights = []
            if total_amount > 5000000:  # 500ä¸‡ä»¥ä¸Š
                insights.append("å¤§é¢åˆ°æœŸé¢„è­¦ï¼šéœ€è¦å……è¶³çš„æµåŠ¨æ€§å‡†å¤‡")
            elif total_amount > 1000000:  # 100ä¸‡ä»¥ä¸Š
                insights.append("ä¸­ç­‰è§„æ¨¡åˆ°æœŸï¼šå»ºè®®æå‰å‡†å¤‡èµ„é‡‘")

            if total_days > 1:
                concentration = (max([float(p.get('åˆ°æœŸé‡‘é¢', 0)) for p in product_list[:5]]) / total_amount) if product_list else 0
                if concentration > 0.3:
                    insights.append("åˆ°æœŸé›†ä¸­åº¦è¾ƒé«˜ï¼Œå­˜åœ¨æµåŠ¨æ€§é£é™©")

            return {
                'text': '\n'.join(response_parts),
                'metrics': {
                    'åˆ°æœŸé‡‘é¢': total_amount,
                    'åˆ°æœŸæ•°é‡': total_count,
                    'åˆ†æå¤©æ•°': total_days,
                    'æ—¥å‡é‡‘é¢': total_amount / total_days if total_days > 0 else 0,
                    'æ—¥å‡ç¬”æ•°': total_count / total_days if total_days > 0 else 0
                },
                'insights': insights
            }

        except Exception as e:
            logger.error(f"åŒºé—´åˆ°æœŸæ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"åŒºé—´åˆ°æœŸæ•°æ®å¤„ç†å‡ºé”™ï¼š{str(e)}",
                'metrics': {}
            }

    async def _format_single_day_expiry_response(self, data: Dict[str, Any], query: str,
                                                 pattern_info: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ“… å•æ—¥åˆ°æœŸæ•°æ®æ ¼å¼åŒ–"""
        try:
            date = data.get('æ—¥æœŸ', 'æœªçŸ¥æ—¥æœŸ')
            expiry_count = int(data.get('åˆ°æœŸæ•°é‡', 0))
            expiry_amount = float(data.get('åˆ°æœŸé‡‘é¢', 0))

            response_parts = [f"â° {date} äº§å“åˆ°æœŸæƒ…å†µï¼š"]

            if self.financial_formatter:
                response_parts.extend([
                    f"ğŸ’° åˆ°æœŸé‡‘é¢ï¼š{self.financial_formatter.format_currency(expiry_amount)}",
                    f"ğŸ“¦ åˆ°æœŸæ•°é‡ï¼š{expiry_count}ç¬”"
                ])
            else:
                response_parts.extend([
                    f"ğŸ’° åˆ°æœŸé‡‘é¢ï¼šÂ¥{expiry_amount:,.2f}",
                    f"ğŸ“¦ åˆ°æœŸæ•°é‡ï¼š{expiry_count}ç¬”"
                ])

            if expiry_count > 0:
                avg_amount = expiry_amount / expiry_count
                if self.financial_formatter:
                    response_parts.append(f"ğŸ“Š å¹³å‡é‡‘é¢ï¼š{self.financial_formatter.format_currency(avg_amount)}")
                else:
                    response_parts.append(f"ğŸ“Š å¹³å‡é‡‘é¢ï¼šÂ¥{avg_amount:,.2f}")

            # ğŸ¯ æ™ºèƒ½è¯„ä¼°
            insights = []
            if expiry_amount > 1000000:  # 100ä¸‡ä»¥ä¸Š
                insights.append("é«˜é¢åˆ°æœŸæé†’ï¼šè¯·ç¡®ä¿æµåŠ¨æ€§å……è¶³")
            elif expiry_amount == 0:
                insights.append("å½“æ—¥æ— äº§å“åˆ°æœŸ")
            else:
                insights.append("åˆ°æœŸé‡‘é¢åœ¨æ­£å¸¸èŒƒå›´å†…")

            return {
                'text': '\n'.join(response_parts),
                'metrics': {
                    'åˆ°æœŸæ•°é‡': expiry_count,
                    'åˆ°æœŸé‡‘é¢': expiry_amount,
                    'å¹³å‡é‡‘é¢': expiry_amount / expiry_count if expiry_count > 0 else 0
                },
                'insights': insights
            }

        except Exception as e:
            logger.error(f"å•æ—¥åˆ°æœŸæ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"å•æ—¥åˆ°æœŸæ•°æ®å¤„ç†å‡ºé”™ï¼š{str(e)}",
                'metrics': {}
            }

    async def _format_user_analysis_response(self, data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """ğŸ‘¥ æ™ºèƒ½ç”¨æˆ·åˆ†æå“åº”æ ¼å¼åŒ–"""
        try:
            user_stats = data.get('ç”¨æˆ·ç»Ÿè®¡', {})
            total_users = int(user_stats.get('æ€»ç”¨æˆ·æ•°', 0))
            active_users = int(user_stats.get('æ´»è·ƒç”¨æˆ·æ•°', 0))

            response_parts = ["ğŸ‘¥ ç”¨æˆ·åˆ†æï¼š"]

            # ğŸ§  æ™ºèƒ½é‡ç‚¹åˆ†æ
            query_lower = query.lower()

            if 'æ´»è·ƒ' in query_lower:
                response_parts.append(f"ğŸ”¥ æ´»è·ƒç”¨æˆ·ï¼š{active_users:,}äºº")
                if total_users > 0:
                    activity_rate = (active_users / total_users) * 100
                    response_parts.append(f"ğŸ“Š æ´»è·ƒç‡ï¼š{activity_rate:.1f}%")

                    if activity_rate > 80:
                        response_parts.append("ğŸ’ª æ´»è·ƒåº¦è¡¨ç°ä¼˜ç§€")
                    elif activity_rate > 60:
                        response_parts.append("âœ… æ´»è·ƒåº¦è¡¨ç°è‰¯å¥½")
                    else:
                        response_parts.append("âš ï¸ æ´»è·ƒåº¦æœ‰å¾…æå‡")

            response_parts.append(f"ğŸ‘¥ æ€»ç”¨æˆ·æ•°ï¼š{total_users:,}äºº")

            # ğŸ¯ æ™ºèƒ½æ´å¯Ÿ
            insights = []
            if total_users > 0:
                activity_rate = active_users / total_users
                if activity_rate > 0.8:
                    insights.append("ç”¨æˆ·æ´»è·ƒåº¦å¾ˆé«˜ï¼Œå¹³å°ç²˜æ€§å¼º")
                elif activity_rate > 0.6:
                    insights.append("ç”¨æˆ·æ´»è·ƒåº¦è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")
                elif activity_rate > 0.4:
                    insights.append("ç”¨æˆ·æ´»è·ƒåº¦ä¸€èˆ¬ï¼Œå»ºè®®åŠ å¼ºç”¨æˆ·è¿è¥")
                else:
                    insights.append("ç”¨æˆ·æ´»è·ƒåº¦è¾ƒä½ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ç”¨æˆ·ç•™å­˜")

                if total_users > 10000:
                    insights.append("ç”¨æˆ·è§„æ¨¡å·²è¾¾åˆ°ä¸€å®šä½“é‡")

            return {
                'text': '\n'.join(response_parts),
                'metrics': {
                    'æ€»ç”¨æˆ·æ•°': total_users,
                    'æ´»è·ƒç”¨æˆ·æ•°': active_users,
                    'æ´»è·ƒç‡': (active_users / total_users) if total_users > 0 else 0
                },
                'insights': insights
            }

        except Exception as e:
            logger.error(f"ç”¨æˆ·åˆ†ææ ¼å¼åŒ–å¤±è´¥: {e}")
            return {
                'text': f"ç”¨æˆ·æ•°æ®è·å–æˆåŠŸï¼ŒåŸå§‹æ•°æ®ï¼š{str(data)[:200]}...",
                'metrics': {}
            }

    def _extract_basic_metrics(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–åŸºç¡€æŒ‡æ ‡"""
        metrics = {}

        # å°è¯•æå–å¸¸è§çš„æ•°å€¼å­—æ®µ
        common_fields = ['æ€»ä½™é¢', 'å…¥é‡‘', 'å‡ºé‡‘', 'åˆ°æœŸé‡‘é¢', 'åˆ°æœŸæ•°é‡', 'æ³¨å†Œäººæ•°', 'æ€»ç”¨æˆ·æ•°', 'æ´»è·ƒç”¨æˆ·æ•°']

        for field in common_fields:
            if field in data:
                try:
                    metrics[field] = float(data[field])
                except (ValueError, TypeError):
                    pass

        return metrics

    async def _build_quick_response_result(self, quick_response: Dict[str, Any], session_id: str,
                                           query_id: str, conversation_id: Optional[str],
                                           start_time: float, user_message_saved: bool,
                                           conversation_id_for_db: Optional[int]) -> ProcessingResult:
        """æ„å»ºå¿«é€Ÿå“åº”ç»“æœ"""
        total_processing_time = time.time() - start_time

        # ğŸ§  è½¬æ¢æ´å¯Ÿæ ¼å¼
        insights = []
        for insight_text in quick_response.get('insights', []):
            insights.append(BusinessInsight(
                title="å¿«é€Ÿåˆ†ææ´å¯Ÿ",
                summary=insight_text,
                confidence_score=0.9,
                insight_type="quick_analysis"
            ))

        result = ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=True,
            response_text=quick_response['response'],
            insights=insights,
            key_metrics=quick_response.get('metrics', {}),
            processing_strategy=ProcessingStrategy.QUICK_RESPONSE,
            processors_used=["QuickResponse", "APIConnector", "IntelligentExtractor"],
            confidence_score=0.95,
            data_quality_score=quick_response.get('data_quality', 0.9),
            response_completeness=1.0,
            total_processing_time=total_processing_time,
            ai_processing_time=0.0,
            data_fetching_time=quick_response.get('fetch_time', 0.0),
            processing_metadata={
                'query_type': 'intelligent_quick_query',
                'api_used': quick_response.get('api_method', ''),
                'pattern_type': quick_response.get('pattern_type', ''),
                'quick_response': True
            },
            conversation_id=conversation_id
        )

        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(result)

        # ä¿å­˜AIå“åº”
        if conversation_id_for_db and user_message_saved:
            await self._save_ai_response_if_needed(conversation_id_for_db, result, query_id)

        logger.info(f"âš¡ QueryID: {query_id} - æ™ºèƒ½å¿«é€Ÿå¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_processing_time:.2f}s")
        return result

    async def _intelligent_query_understanding(self, user_query: str,
                                               conversation_id_for_db: Optional[int]) -> QueryAnalysisResult:
        """ğŸ§  æ™ºèƒ½æŸ¥è¯¢ç†è§£ - å¢å¼ºç‰ˆ"""
        logger.debug(f"ğŸ§  æ™ºèƒ½æŸ¥è¯¢ç†è§£: {user_query[:50]}...")

        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        context = {}
        if conversation_id_for_db and self.conversation_manager:
            try:
                context = self.conversation_manager.get_context(conversation_id_for_db)
            except Exception as e:
                logger.warning(f"è·å–å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}")

        # ğŸ§  å¢å¼ºçš„Claudeç†è§£æç¤º
        enhanced_prompt = f"""
        ä½œä¸ºä¸“ä¸šçš„é‡‘èAIç³»ç»Ÿï¼Œè¯·æ·±åº¦åˆ†æè¿™ä¸ªæŸ¥è¯¢å¹¶åˆ¶å®šç²¾ç¡®çš„æ‰§è¡Œè®¡åˆ’ï¼š

        ç”¨æˆ·æŸ¥è¯¢: "{user_query}"
        å¯¹è¯å†å²: {json.dumps(context or {}, ensure_ascii=False)[:300]}

        ç‰¹åˆ«æ³¨æ„è¯†åˆ«ï¼š
        1. ğŸ“… æ—¶é—´ä¿¡æ¯ï¼š
           - å…·ä½“æ—¥æœŸï¼ˆå¦‚"6æœˆ1æ—¥"ã€"5æœˆ28æ—¥"ï¼‰
           - æ—¥æœŸåŒºé—´ï¼ˆå¦‚"6æœˆ1æ—¥è‡³6æœˆ30æ—¥"ï¼‰
           - ç›¸å¯¹æ—¶é—´ï¼ˆå¦‚"ä»Šå¤©"ã€"æœ¬å‘¨"ã€"ä¸Šä¸ªæ˜ŸæœŸ"ï¼‰

        2. ğŸ’° å¤æŠ•/æŠ•èµ„åˆ†æï¼š
           - å¤æŠ•æ¯”ä¾‹ï¼ˆå¦‚"25%"ã€"ä¸€åŠ"ã€"50%"ã€"ç™¾åˆ†ä¹‹äº”å"ï¼‰
           - æç°åˆ†æï¼ˆå¦‚"ç™¾åˆ†ä¹‹äº”åæç°"ï¼‰
           - èµ„é‡‘è¿è¥è®¡ç®—ï¼ˆå¦‚"å…¬å¸è¿˜èƒ½è¿è¡Œå¤šä¹…"ï¼‰

        3. ğŸ“Š æ•°æ®éœ€æ±‚ï¼š
           - åˆ°æœŸäº§å“æ•°æ®ï¼ˆåŒ…å«åˆ©æ¯ä¿¡æ¯ï¼‰
           - æ¯æ—¥å…¥é‡‘/å‡ºé‡‘æ•°æ®
           - ç”¨æˆ·/ä¼šå‘˜æ•°æ®
           - ç³»ç»Ÿè´¢åŠ¡æ¦‚è§ˆ

        4. ğŸ§® è®¡ç®—éœ€æ±‚ï¼š
           - å¤æŠ•åœºæ™¯åˆ†æ
           - ç°é‡‘è·‘é“è®¡ç®—
           - è¶‹åŠ¿å¢é•¿åˆ†æ
           - åˆ©æ¯æ”¶ç›Šè®¡ç®—
           - èµ„é‡‘é¢„æµ‹

        è¯·è¿”å›è¯¦ç»†çš„JSONæ‰§è¡Œè®¡åˆ’ï¼Œå¿…é¡»åŒ…å«ï¼š
        {{
            "query_understanding": {{
                "complexity": "simple|medium|complex|expert",
                "query_type": "data_retrieval|calculation|trend_analysis|prediction|reinvestment_analysis|cash_flow_analysis",
                "business_scenario": "financial_overview|daily_operations|expiry_management|reinvestment_planning|risk_assessment",
                "user_intent": "ç”¨æˆ·å…·ä½“æƒ³äº†è§£ä»€ä¹ˆ",
                "confidence": 0.9,
                "key_entities": {{
                    "dates": ["2024-06-01"],
                    "amounts": ["25%"],
                    "products": [],
                    "calculations": ["reinvestment"]
                }}
            }},
            "execution_plan": {{
                "api_calls": [
                    {{
                        "api_method": "get_product_end_interval",
                        "params": {{"start_date": "20240601", "end_date": "20240630"}},
                        "reason": "è·å–6æœˆä»½äº§å“åˆ°æœŸæ•°æ®",
                        "priority": 1
                    }}
                ],
                "needs_calculation": true,
                "calculation_type": "reinvestment_analysis|cash_runway|trend_analysis|compound_interest|financial_ratios",
                "calculation_params": {{
                    "reinvest_rate": 0.25,
                    "analysis_period": 30
                }},
                "calculation_description": "éœ€è¦GPTè®¡ç®—çš„å…·ä½“å†…å®¹"
            }},
            "time_analysis": {{
                "has_time_requirement": true,
                "start_date": "20240601",
                "end_date": "20240630",
                "time_description": "6æœˆ1æ—¥è‡³6æœˆ30æ—¥"
            }}
        }}

        ğŸ¯ APIæ–¹æ³•æ˜ å°„ï¼š
        - get_system_data(): ç³»ç»Ÿæ¦‚è§ˆï¼ˆä½™é¢ã€ç”¨æˆ·ç»Ÿè®¡ï¼‰
        - get_daily_data(date): ç‰¹å®šæ—¥æœŸæ•°æ®ï¼ˆå…¥é‡‘ã€å‡ºé‡‘ã€æ³¨å†Œï¼‰
        - get_product_end_data(date): å•æ—¥åˆ°æœŸäº§å“
        - get_product_end_interval(start_date, end_date): åŒºé—´åˆ°æœŸäº§å“
        - get_product_data(): äº§å“è¯¦æƒ…å’ŒæŒæœ‰æƒ…å†µ
        - get_user_daily_data(date): ç”¨æˆ·æ¯æ—¥ç»Ÿè®¡
        - get_user_data(page): è¯¦ç»†ç”¨æˆ·æ•°æ®

        ğŸ§® è®¡ç®—ç±»å‹è¯´æ˜ï¼š
        - reinvestment_analysis: å¤æŠ•åˆ†æï¼ˆåŒ…å«å¤æŠ•ç‡ã€æç°é‡‘é¢è®¡ç®—ï¼‰
        - cash_runway: ç°é‡‘è·‘é“åˆ†æï¼ˆå…¬å¸èƒ½è¿è¡Œå¤šä¹…ï¼‰
        - trend_analysis: è¶‹åŠ¿åˆ†æï¼ˆå¢é•¿ç‡ã€å˜åŒ–è¶‹åŠ¿ï¼‰
        - compound_interest: å¤åˆ©è®¡ç®—ï¼ˆåˆ©æ¯ç´¯ç§¯ï¼‰
        - financial_ratios: è´¢åŠ¡æ¯”ç‡åˆ†æ
        - growth_prediction: å¢é•¿é¢„æµ‹
        - withdrawal_analysis: æç°åˆ†æ

        ç‰¹åˆ«æ³¨æ„ï¼š
        - æ—¥æœŸæ ¼å¼å¿…é¡»æ˜¯YYYYMMDDï¼ˆå¦‚ï¼š20240601ï¼‰
        - å¤æŠ•æ¯”ä¾‹è½¬æ¢ä¸ºå°æ•°ï¼ˆå¦‚ï¼š25% â†’ 0.25ï¼‰
        - å¤æ‚æŸ¥è¯¢éœ€è¦å¤šä¸ªAPIç»„åˆ
        - è®¡ç®—å‚æ•°è¦ç²¾ç¡®æå–
        """

        # è°ƒç”¨Claudeè¿›è¡Œæ™ºèƒ½åˆ†æ
        try:
            result = await asyncio.wait_for(
                self.claude_client.analyze_complex_query(enhanced_prompt, {
                    "query": user_query,
                    "context": context,
                    "current_date": datetime.now().strftime("%Y-%m-%d")
                }),
                timeout=30.0
            )

            if result.get("success"):
                analysis_text = result.get("analysis", "{}")
                analysis = self._extract_json_from_response(analysis_text)

                if analysis and self._validate_claude_response(analysis):
                    query_analysis = self._build_enhanced_analysis_result(user_query, analysis)
                    logger.info(f"ğŸ§  æ™ºèƒ½æŸ¥è¯¢ç†è§£æˆåŠŸ: {query_analysis.query_type.value}")
                    return query_analysis

        except Exception as e:
            logger.error(f"æ™ºèƒ½æŸ¥è¯¢ç†è§£å¤±è´¥: {e}")

        # é™çº§åˆ°åŸºç¡€è§£æ
        logger.warning("Claudeç†è§£å¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½é™çº§è§£æ")
        return await self._intelligent_fallback_analysis(user_query)

    def _build_enhanced_analysis_result(self, original_query: str, claude_plan: Dict[str, Any]) -> QueryAnalysisResult:
        """æ„å»ºå¢å¼ºåˆ†æç»“æœ"""
        try:
            understanding = claude_plan["query_understanding"]
            execution = claude_plan["execution_plan"]
            time_info = claude_plan.get("time_analysis", {})

            # ğŸ¯ å¢å¼ºçš„APIè°ƒç”¨å‚æ•°å¤„ç†
            api_calls = []
            for api_call in execution.get("api_calls", []):
                method = api_call.get("api_method", "get_system_data")
                params = api_call.get("params", {})

                # ğŸ§  æ™ºèƒ½å‚æ•°å¤„ç†å’ŒéªŒè¯
                params = self._process_enhanced_api_params(params)

                api_calls.append({
                    "method": method,
                    "params": params,
                    "reason": api_call.get("reason", "æ•°æ®è·å–"),
                    "priority": api_call.get("priority", 1)
                })

            # ğŸ§  æ™ºèƒ½æ—¶é—´èŒƒå›´å¤„ç†
            time_range = None
            if time_info.get("has_time_requirement"):
                time_range = {
                    "start_date": time_info.get("start_date"),
                    "end_date": time_info.get("end_date"),
                    "description": time_info.get("time_description", "")
                }

            # ğŸ§  æ™ºèƒ½è®¡ç®—å‚æ•°æå–
            calculation_params = execution.get("calculation_params", {})

            # å®‰å…¨åœ°è·å–æšä¸¾å€¼
            complexity = self._safe_get_enum(QueryComplexity, understanding.get("complexity", "medium"))
            query_type = self._safe_get_enum(QueryType, understanding.get("query_type", "data_retrieval"))
            business_scenario = self._safe_get_enum(BusinessScenario,
                                                    understanding.get("business_scenario", "daily_operations"))

            return QueryAnalysisResult(
                original_query=original_query,
                complexity=complexity,
                query_type=query_type,
                business_scenario=business_scenario,
                confidence_score=float(understanding.get("confidence", 0.85)),

                # ğŸ§  å¢å¼ºçš„æ‰§è¡Œä¿¡æ¯
                api_calls_needed=api_calls,
                needs_calculation=execution.get("needs_calculation", False),
                calculation_type=execution.get("calculation_type") if execution.get("needs_calculation") else None,

                # æ—¶é—´ä¿¡æ¯
                time_range=time_range,

                processing_metadata={
                    "user_intent": understanding.get("user_intent", ""),
                    "key_entities": understanding.get("key_entities", {}),
                    "api_count": len(api_calls),
                    "processing_method": "intelligent_claude_enhanced",
                    "calculation_description": execution.get("calculation_description", ""),
                    "calculation_params": calculation_params
                }
            )
        except Exception as e:
            logger.error(f"æ„å»ºå¢å¼ºåˆ†æç»“æœå¤±è´¥: {e}\n{traceback.format_exc()}")
            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç»“æœ
            return QueryAnalysisResult(
                original_query=original_query,
                complexity=QueryComplexity.MEDIUM,
                query_type=QueryType.DATA_RETRIEVAL,
                business_scenario=BusinessScenario.DAILY_OPERATIONS,
                confidence_score=0.5,
                api_calls_needed=[{"method": "get_system_data", "params": {}, "reason": "é™çº§æ•°æ®è·å–"}],
                processing_metadata={"error": str(e), "fallback": True}
            )

    def _process_enhanced_api_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ§  å¢å¼ºçš„APIå‚æ•°å¤„ç†"""
        processed_params = params.copy()

        # ğŸ¯ æ™ºèƒ½æ—¥æœŸæ ¼å¼éªŒè¯å’Œè½¬æ¢
        date_fields = ["date", "start_date", "end_date"]
        for field in date_fields:
            if field in processed_params and processed_params[field]:
                processed_date = self._intelligent_date_conversion(processed_params[field])
                if processed_date:
                    processed_params[field] = processed_date
                else:
                    logger.warning(f"æ—¥æœŸå‚æ•° {field}={processed_params[field]} è½¬æ¢å¤±è´¥")

        return processed_params

    def _intelligent_date_conversion(self, date_str: str) -> Optional[str]:
        """ğŸ§  æ™ºèƒ½æ—¥æœŸè½¬æ¢"""
        if not date_str:
            return datetime.now().strftime('%Y%m%d')

        try:
            # å¦‚æœå·²ç»æ˜¯YYYYMMDDæ ¼å¼
            if len(date_str) == 8 and date_str.isdigit():
                # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                datetime.strptime(date_str, '%Y%m%d')
                return date_str

            # å¦‚æœæ˜¯YYYY-MM-DDæ ¼å¼
            if len(date_str) == 10 and '-' in date_str:
                dt = datetime.strptime(date_str, '%Y-%m-%d')
                return dt.strftime('%Y%m%d')

            # å¦‚æœä½¿ç”¨date_utilsè¿›è¡Œæ™ºèƒ½è½¬æ¢
            if self.date_utils:
                try:
                    parsed_date = self.date_utils.api_format_to_date(date_str)
                    return parsed_date.strftime('%Y%m%d')
                except:
                    pass

            # é»˜è®¤è¿”å›ä»Šå¤©
            logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}, ä½¿ç”¨ä»Šå¤©æ—¥æœŸ")
            return datetime.now().strftime('%Y%m%d')

        except Exception as e:
            logger.error(f"æ™ºèƒ½æ—¥æœŸè½¬æ¢å¤±è´¥: {date_str}, é”™è¯¯: {e}")
            return datetime.now().strftime('%Y%m%d')

    async def _intelligent_fallback_analysis(self, query: str) -> QueryAnalysisResult:
        """ğŸ§  æ™ºèƒ½é™çº§åˆ†æ - å¢å¼ºç‰ˆ"""
        logger.info(f"æ‰§è¡Œæ™ºèƒ½é™çº§è§£æ: {query[:50]}...")

        query_lower = query.lower()

        # ğŸ§  æ™ºèƒ½æ¨¡å¼åŒ¹é…
        if self._detect_reinvestment_pattern(query_lower):
            return self._build_reinvestment_analysis_result(query)
        elif self._detect_cash_runway_pattern(query_lower):
            return self._build_cash_runway_analysis_result(query)
        elif self._detect_expiry_pattern(query_lower):
            return self._build_expiry_analysis_result(query)
        elif self._detect_trend_pattern(query_lower):
            return self._build_trend_analysis_result(query)
        elif self._detect_daily_pattern(query_lower):
            return self._build_daily_analysis_result(query)
        else:
            return self._build_default_analysis_result(query)

    def _detect_reinvestment_pattern(self, query_lower: str) -> bool:
        """æ£€æµ‹å¤æŠ•æ¨¡å¼"""
        reinvestment_keywords = ['å¤æŠ•', 'å†æŠ•èµ„', '25%', '50%', 'ä¸€åŠ', 'ç™¾åˆ†ä¹‹', 'æç°', 'å‰©ä½™èµ„é‡‘']
        return any(kw in query_lower for kw in reinvestment_keywords)

    def _detect_cash_runway_pattern(self, query_lower: str) -> bool:
        """æ£€æµ‹ç°é‡‘è·‘é“æ¨¡å¼"""
        runway_keywords = ['è¿˜èƒ½è¿è¡Œ', 'è¿è¡Œå¤šä¹…', 'èµ„é‡‘è€—å°½', 'ç°é‡‘æµ', 'æ²¡å…¥é‡‘']
        return any(kw in query_lower for kw in runway_keywords)

    def _detect_expiry_pattern(self, query_lower: str) -> bool:
        """æ£€æµ‹åˆ°æœŸæ¨¡å¼"""
        expiry_keywords = ['åˆ°æœŸ', 'è¿‡æœŸ', 'äº§å“åˆ°æœŸ']
        return any(kw in query_lower for kw in expiry_keywords)

    def _detect_trend_pattern(self, query_lower: str) -> bool:
        """æ£€æµ‹è¶‹åŠ¿æ¨¡å¼"""
        trend_keywords = ['è¶‹åŠ¿', 'å¢é•¿', 'å˜åŒ–', 'å¹³å‡', 'å†å²']
        return any(kw in query_lower for kw in trend_keywords)

    def _detect_daily_pattern(self, query_lower: str) -> bool:
        """æ£€æµ‹æ¯æ—¥æ•°æ®æ¨¡å¼"""
        daily_keywords = ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œ', 'æœˆ', 'æ—¥']
        return any(kw in query_lower for kw in daily_keywords)

    def _build_reinvestment_analysis_result(self, query: str) -> QueryAnalysisResult:
        """æ„å»ºå¤æŠ•åˆ†æç»“æœ"""
        # ğŸ§  æ™ºèƒ½æå–å¤æŠ•æ¯”ä¾‹
        reinvest_rate = self._extract_reinvestment_rate(query)

        # ğŸ§  æ™ºèƒ½æå–æ—¥æœŸèŒƒå›´
        dates_info = self._extract_dates_from_query(query)

        api_calls = [{"method": "get_system_data", "params": {}, "reason": "è·å–å½“å‰èµ„é‡‘çŠ¶å†µ"}]

        if dates_info['has_dates'] and dates_info['date_type'] == 'range':
            api_calls.append({
                "method": "get_product_end_interval",
                "params": {
                    "start_date": dates_info['api_format_dates'][0],
                    "end_date": dates_info['api_format_dates'][1]
                },
                "reason": "è·å–åŒºé—´åˆ°æœŸæ•°æ®"
            })
        elif dates_info['has_dates']:
            api_calls.append({
                "method": "get_product_end_data",
                "params": {"date": dates_info['api_format_dates'][0]},
                "reason": "è·å–åˆ°æœŸæ•°æ®"
            })

        return QueryAnalysisResult(
            original_query=query,
            complexity=QueryComplexity.COMPLEX,
            query_type=QueryType.CALCULATION,
            business_scenario=BusinessScenario.FUTURE_PROJECTION,
            confidence_score=0.8,
            api_calls_needed=api_calls,
            needs_calculation=True,
            calculation_type="reinvestment_analysis",
            processing_metadata={
                "parsing_method": "intelligent_fallback_reinvestment",
                "reinvestment_rate": reinvest_rate,
                "extracted_dates": dates_info
            }
        )

    def _extract_reinvestment_rate(self, query: str) -> float:
        """ğŸ§  æ™ºèƒ½æå–å¤æŠ•æ¯”ä¾‹"""
        import re

        # åŒ¹é…ç™¾åˆ†æ¯”
        percent_patterns = [
            r'(\d+)%',
            r'ç™¾åˆ†ä¹‹(\d+)',
            r'(\d+)æˆ'
        ]

        for pattern in percent_patterns:
            matches = re.findall(pattern, query)
            if matches:
                try:
                    rate = float(matches[0]) / 100
                    return min(rate, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡100%
                except ValueError:
                    continue

        # åŒ¹é…æ–‡å­—æè¿°
        if 'ä¸€åŠ' in query or '50%' in query:
            return 0.5
        elif 'å››åˆ†ä¹‹ä¸€' in query or '25%' in query:
            return 0.25
        elif 'ä¸‰åˆ†ä¹‹ä¸€' in query:
            return 0.33
        elif 'å››åˆ†ä¹‹ä¸‰' in query or '75%' in query:
            return 0.75

        # é»˜è®¤50%
        return 0.5

    def _build_cash_runway_analysis_result(self, query: str) -> QueryAnalysisResult:
        """æ„å»ºç°é‡‘è·‘é“åˆ†æç»“æœ"""
        api_calls = [
            {"method": "get_system_data", "params": {}, "reason": "è·å–å½“å‰èµ„é‡‘çŠ¶å†µ"},
            {"method": "get_daily_data", "params": {}, "reason": "è·å–æœ€è¿‘æ”¯å‡ºæ•°æ®"}
        ]

        return QueryAnalysisResult(
            original_query=query,
            complexity=QueryComplexity.COMPLEX,
            query_type=QueryType.PREDICTION,
            business_scenario=BusinessScenario.RISK_MANAGEMENT,
            confidence_score=0.8,
            api_calls_needed=api_calls,
            needs_calculation=True,
            calculation_type="cash_runway",
            processing_metadata={
                "parsing_method": "intelligent_fallback_cash_runway",
                "analysis_type": "financial_sustainability"
            }
        )

    def _build_expiry_analysis_result(self, query: str) -> QueryAnalysisResult:
        """æ„å»ºåˆ°æœŸåˆ†æç»“æœ"""
        dates_info = self._extract_dates_from_query(query)

        if dates_info['date_type'] == 'range' and len(dates_info['api_format_dates']) >= 2:
            api_calls = [{
                "method": "get_product_end_interval",
                "params": {
                    "start_date": dates_info['api_format_dates'][0],
                    "end_date": dates_info['api_format_dates'][1]
                },
                "reason": "è·å–åŒºé—´åˆ°æœŸæ•°æ®"
            }]
        elif dates_info['has_dates']:
            api_calls = [{
                "method": "get_product_end_data",
                "params": {"date": dates_info['api_format_dates'][0]},
                "reason": "è·å–å•æ—¥åˆ°æœŸæ•°æ®"
            }]
        else:
            api_calls = [{"method": "get_expiring_products_today", "params": {}, "reason": "è·å–ä»Šæ—¥åˆ°æœŸæ•°æ®"}]

        return QueryAnalysisResult(
            original_query=query,
            complexity=QueryComplexity.MEDIUM,
            query_type=QueryType.DATA_RETRIEVAL,
            business_scenario=BusinessScenario.DAILY_OPERATIONS,
            confidence_score=0.8,
            api_calls_needed=api_calls,
            processing_metadata={
                "parsing_method": "intelligent_fallback_expiry",
                "extracted_dates": dates_info
            }
        )

    def _build_trend_analysis_result(self, query: str) -> QueryAnalysisResult:
        """æ„å»ºè¶‹åŠ¿åˆ†æç»“æœ"""
        api_calls = [
            {"method": "get_system_data", "params": {}, "reason": "è·å–å½“å‰çŠ¶æ€"},
            {"method": "get_daily_data", "params": {}, "reason": "è·å–å†å²æ•°æ®"}
        ]

        return QueryAnalysisResult(
            original_query=query,
            complexity=QueryComplexity.COMPLEX,
            query_type=QueryType.TREND_ANALYSIS,
            business_scenario=BusinessScenario.HISTORICAL_PERFORMANCE,
            confidence_score=0.75,
            api_calls_needed=api_calls,
            needs_calculation=True,
            calculation_type="trend_analysis",
            processing_metadata={
                "parsing_method": "intelligent_fallback_trend",
                "analysis_scope": "historical_performance"
            }
        )

    def _build_daily_analysis_result(self, query: str) -> QueryAnalysisResult:
        """æ„å»ºæ¯æ—¥åˆ†æç»“æœ"""
        dates_info = self._extract_dates_from_query(query)

        if dates_info['has_dates']:
            api_calls = [{
                "method": "get_daily_data",
                "params": {"date": dates_info['api_format_dates'][0]},
                "reason": "è·å–ç‰¹å®šæ—¥æœŸæ•°æ®"
            }]
        else:
            api_calls = [{"method": "get_daily_data", "params": {}, "reason": "è·å–æœ€æ–°æ¯æ—¥æ•°æ®"}]

        return QueryAnalysisResult(
            original_query=query,
            complexity=QueryComplexity.SIMPLE,
            query_type=QueryType.DATA_RETRIEVAL,
            business_scenario=BusinessScenario.DAILY_OPERATIONS,
            confidence_score=0.8,
            api_calls_needed=api_calls,
            processing_metadata={
                "parsing_method": "intelligent_fallback_daily",
                "extracted_dates": dates_info
            }
        )

    def _build_default_analysis_result(self, query: str) -> QueryAnalysisResult:
        """æ„å»ºé»˜è®¤åˆ†æç»“æœ"""
        return QueryAnalysisResult(
            original_query=query,
            complexity=QueryComplexity.SIMPLE,
            query_type=QueryType.DATA_RETRIEVAL,
            business_scenario=BusinessScenario.DAILY_OPERATIONS,
            confidence_score=0.6,
            api_calls_needed=[{"method": "get_system_data", "params": {}, "reason": "è·å–ç³»ç»Ÿæ¦‚è§ˆ"}],
            processing_metadata={
                "parsing_method": "intelligent_fallback_default",
                "note": "ä½¿ç”¨é»˜è®¤æ•°æ®è·å–ç­–ç•¥"
            }
        )

    async def _intelligent_data_acquisition(self, query_analysis: QueryAnalysisResult) -> Optional[FetcherExecutionResult]:
        """ğŸ§  æ™ºèƒ½æ•°æ®è·å–"""
        logger.debug(f"ğŸ“Š æ‰§è¡Œæ™ºèƒ½æ•°æ®è·å–")

        if not self.data_fetcher:
            raise RuntimeError("SmartDataFetcher æœªåˆå§‹åŒ–")

        try:
            # ğŸ§  ä½¿ç”¨å¢å¼ºç‰ˆæ™ºèƒ½æ•°æ®è·å–
            if hasattr(self.data_fetcher.api_connector, 'intelligent_data_fetch_enhanced'):
                execution_result = await self.data_fetcher.api_connector.intelligent_data_fetch_enhanced(
                    query_analysis.to_dict()
                )

                # ğŸ¯ è½¬æ¢å¢å¼ºæ•°æ®ä¸ºæ ‡å‡†ExecutionResultæ ¼å¼
                if execution_result.get("success"):
                    # åˆ›å»ºæ¨¡æ‹Ÿçš„ExecutionResultå¯¹è±¡
                    mock_result = type('ExecutionResult', (), {
                        'success': True,
                        'execution_status': FetcherExecutionStatus.COMPLETED,
                        'confidence_level': FetcherDataQualityLevel.HIGH,
                        'processed_data': execution_result.get("organized_data", {}),
                        'fetched_data': execution_result.get("organized_data", {}),
                        'processing_metadata': execution_result.get("execution_summary", {})
                    })()

                    return mock_result
                else:
                    return None
            else:
                # é™çº§åˆ°åŸæœ‰é€»è¾‘
                execution_result = await self._legacy_data_fetch(query_analysis)
                return execution_result

        except Exception as e:
            logger.error(f"æ™ºèƒ½æ•°æ®è·å–å¤±è´¥: {e}")
            return None

    async def _intelligent_data_extraction(self, data_result: Optional[FetcherExecutionResult],
                                           user_query: str,
                                           query_analysis: QueryAnalysisResult) -> Dict[str, Any]:
        """ğŸ§  æ™ºèƒ½æ•°æ®æå– - æ ¸å¿ƒæ”¹è¿›"""
        logger.debug(f"ğŸ§  æ‰§è¡Œæ™ºèƒ½æ•°æ®æå–")

        if not data_result:
            return {"status": "æ— æ•°æ®", "extraction_strategy": "none"}

        # ğŸ¯ ç¡®å®šæå–ç­–ç•¥
        extraction_strategy = self._determine_extraction_strategy(user_query, query_analysis)

        extracted_data = {
            "status": "æ•°æ®æå–æˆåŠŸ",
            "extraction_strategy": extraction_strategy.value,
            "data_quality": getattr(data_result, 'confidence_level', 0.8),
            "raw_data_available": True
        }

        # ğŸ§  è·å–å¤„ç†åçš„æ•°æ®
        if hasattr(data_result, 'processed_data') and data_result.processed_data:
            processed = data_result.processed_data

            # ğŸ¯ æ ¹æ®ç­–ç•¥è¿›è¡Œæ™ºèƒ½æå–
            if extraction_strategy == DataExtractionStrategy.FINANCIAL_OVERVIEW:
                extracted_data.update(await self._extract_financial_overview_data(processed, user_query))
            elif extraction_strategy == DataExtractionStrategy.EXPIRY_ANALYSIS:
                extracted_data.update(await self._extract_expiry_analysis_data(processed, user_query))
            elif extraction_strategy == DataExtractionStrategy.DAILY_ANALYSIS:
                extracted_data.update(await self._extract_daily_analysis_data(processed, user_query))
            elif extraction_strategy == DataExtractionStrategy.REINVESTMENT_ANALYSIS:
                extracted_data.update(await self._extract_reinvestment_analysis_data(processed, user_query, query_analysis))
            elif extraction_strategy == DataExtractionStrategy.TREND_ANALYSIS:
                extracted_data.update(await self._extract_trend_analysis_data(processed, user_query))
            elif extraction_strategy == DataExtractionStrategy.USER_ANALYSIS:
                extracted_data.update(await self._extract_user_analysis_data(processed, user_query))
            elif extraction_strategy == DataExtractionStrategy.CASH_FLOW_ANALYSIS:
                extracted_data.update(await self._extract_cash_flow_analysis_data(processed, user_query))
            else:
                # ç»¼åˆæå–
                extracted_data.update(await self._extract_comprehensive_data(processed, user_query))

        # ğŸ§  ç¼“å­˜æå–ç»“æœ
        cache_key = self._generate_extraction_cache_key(user_query, extraction_strategy)
        self.extraction_cache[cache_key] = {
            'data': extracted_data,
            'timestamp': time.time(),
            'strategy': extraction_strategy.value
        }

        logger.info(f"ğŸ§  æ™ºèƒ½æ•°æ®æå–å®Œæˆ: {extraction_strategy.value}")
        return extracted_data

    def _determine_extraction_strategy(self, user_query: str,
                                       query_analysis: QueryAnalysisResult) -> DataExtractionStrategy:
        """ğŸ¯ ç¡®å®šæ•°æ®æå–ç­–ç•¥"""
        query_lower = user_query.lower()

        # ğŸ§  æ™ºèƒ½ç­–ç•¥åŒ¹é…
        if any(kw in query_lower for kw in ["å¤æŠ•", "reinvest", "25%", "ä¸€åŠ", "50%", "æç°"]):
            return DataExtractionStrategy.REINVESTMENT_ANALYSIS
        elif any(kw in query_lower for kw in ["åˆ°æœŸ", "expiry", "product_end"]):
            return DataExtractionStrategy.EXPIRY_ANALYSIS
        elif any(kw in query_lower for kw in ["å…¥é‡‘", "å‡ºé‡‘", "daily", "æ¯æ—¥", "æœˆ", "æ—¥"]):
            return DataExtractionStrategy.DAILY_ANALYSIS
        elif any(kw in query_lower for kw in ["è¶‹åŠ¿", "å¢é•¿", "å˜åŒ–", "trend", "å†å²"]):
            return DataExtractionStrategy.TREND_ANALYSIS
        elif any(kw in query_lower for kw in ["ç”¨æˆ·", "ä¼šå‘˜", "æ´»è·ƒ", "æ³¨å†Œ"]):
            return DataExtractionStrategy.USER_ANALYSIS
        elif any(kw in query_lower for kw in ["ç°é‡‘æµ", "è¿è¡Œ", "å¤šä¹…", "èµ„é‡‘"]):
            return DataExtractionStrategy.CASH_FLOW_ANALYSIS
        elif any(kw in query_lower for kw in ["æ€»èµ„é‡‘", "ä½™é¢", "æ¦‚è§ˆ"]):
            return DataExtractionStrategy.FINANCIAL_OVERVIEW
        else:
            return DataExtractionStrategy.COMPREHENSIVE

    async def _extract_financial_overview_data(self, processed_data: Dict[str, Any],
                                               user_query: str) -> Dict[str, Any]:
        """ğŸ’° æ™ºèƒ½æå–è´¢åŠ¡æ¦‚è§ˆæ•°æ®"""
        extracted = {"financial_overview": {}}

        for key, data in processed_data.items():
            if isinstance(data, dict):
                # ğŸ¯ ç³»ç»Ÿè´¢åŠ¡æ•°æ®
                if any(field in data for field in ["æ€»ä½™é¢", "æ€»å…¥é‡‘", "æ€»å‡ºé‡‘"]):
                    financial_data = {
                        "total_balance": float(data.get("æ€»ä½™é¢", 0)),
                        "total_inflow": float(data.get("æ€»å…¥é‡‘", 0)),
                        "total_outflow": float(data.get("æ€»å‡ºé‡‘", 0)),
                        "total_investment": float(data.get("æ€»æŠ•èµ„é‡‘é¢", 0)),
                        "total_rewards": float(data.get("æ€»å¥–åŠ±å‘æ”¾", 0))
                    }

                    # ğŸ§® è®¡ç®—è¡ç”ŸæŒ‡æ ‡
                    financial_data["net_flow"] = financial_data["total_inflow"] - financial_data["total_outflow"]
                    financial_data["outflow_ratio"] = (
                        financial_data["total_outflow"] / financial_data["total_inflow"]
                        if financial_data["total_inflow"] > 0 else 0
                    )
                    financial_data["balance_utilization"] = (
                        financial_data["total_investment"] / financial_data["total_balance"]
                        if financial_data["total_balance"] > 0 else 0
                    )

                    extracted["financial_overview"].update(financial_data)

                # ğŸ¯ ç”¨æˆ·ç»Ÿè®¡æ•°æ®
                if "ç”¨æˆ·ç»Ÿè®¡" in data:
                    user_stats = data["ç”¨æˆ·ç»Ÿè®¡"]
                    user_data = {
                        "total_users": int(user_stats.get("æ€»ç”¨æˆ·æ•°", 0)),
                        "active_users": int(user_stats.get("æ´»è·ƒç”¨æˆ·æ•°", 0))
                    }

                    if user_data["total_users"] > 0:
                        user_data["activity_rate"] = user_data["active_users"] / user_data["total_users"]
                        user_data["avg_balance_per_user"] = (
                            extracted["financial_overview"].get("total_balance", 0) / user_data["total_users"]
                        )

                    extracted["financial_overview"].update(user_data)

        return extracted

    async def _extract_expiry_analysis_data(self, processed_data: Dict[str, Any],
                                            user_query: str) -> Dict[str, Any]:
        """â° æ™ºèƒ½æå–åˆ°æœŸåˆ†ææ•°æ®"""
        extracted = {"expiry_analysis": {}}

        for key, data in processed_data.items():
            if isinstance(data, dict) and ("åˆ°æœŸ" in str(data) or "expiry" in str(key).lower()):
                expiry_data = {
                    "expiry_amount": float(data.get("åˆ°æœŸé‡‘é¢", 0)),
                    "expiry_count": int(data.get("åˆ°æœŸæ•°é‡", 0)),
                    "expiry_date": data.get("æ—¥æœŸ", ""),
                    "product_details": []
                }

                # ğŸ¯ äº§å“è¯¦æƒ…æå–
                if "äº§å“åˆ—è¡¨" in data:
                    for product in data["äº§å“åˆ—è¡¨"]:
                        if isinstance(product, dict):
                            product_info = {
                                "name": product.get("äº§å“åç§°", ""),
                                "amount": float(product.get("åˆ°æœŸé‡‘é¢", 0)),
                                "count": int(product.get("åˆ°æœŸæ•°é‡", 0)),
                                "daily_rate": float(product.get("æ¯æ—¥åˆ©ç‡", 0)) / 100 if product.get("æ¯æ—¥åˆ©ç‡") else 0,
                                "period_days": int(product.get("æœŸé™å¤©æ•°", 0))
                            }

                            # ğŸ§® è®¡ç®—é¢„æœŸåˆ©æ¯
                            if product_info["daily_rate"] > 0 and product_info["period_days"] > 0:
                                product_info["expected_interest"] = (
                                    product_info["amount"] * product_info["daily_rate"] * product_info["period_days"]
                                )

                            expiry_data["product_details"].append(product_info)

                # ğŸ§® è®¡ç®—åŒºé—´ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
                if "interval_stats" in data:
                    interval_stats = data["interval_stats"]
                    expiry_data.update({
                        "total_days": interval_stats.get("total_days", 1),
                        "daily_average_amount": interval_stats.get("daily_average_amount", 0),
                        "daily_average_quantity": interval_stats.get("daily_average_quantity", 0)
                    })

                extracted["expiry_analysis"].update(expiry_data)

        return extracted

    async def _extract_daily_analysis_data(self, processed_data: Dict[str, Any],
                                           user_query: str) -> Dict[str, Any]:
        """ğŸ“… æ™ºèƒ½æå–æ¯æ—¥åˆ†ææ•°æ®"""
        extracted = {"daily_analysis": {}}

        for key, data in processed_data.items():
            if isinstance(data, dict):
                # ğŸ¯ æ¯æ—¥æ•°æ®å­—æ®µæ£€æµ‹
                if any(field in data for field in ["å…¥é‡‘", "å‡ºé‡‘", "æ³¨å†Œäººæ•°", "æ—¥æœŸ"]):
                    daily_data = {
                        "date": data.get("æ—¥æœŸ", ""),
                        "inflow": float(data.get("å…¥é‡‘", 0)),
                        "outflow": float(data.get("å‡ºé‡‘", 0)),
                        "registrations": int(data.get("æ³¨å†Œäººæ•°", 0)),
                        "purchases": int(data.get("è´­ä¹°äº§å“æ•°é‡", 0)),
                        "holdings": int(data.get("æŒä»“äººæ•°", 0)),
                        "expired_products": int(data.get("åˆ°æœŸäº§å“æ•°é‡", 0))
                    }

                    # ğŸ§® è®¡ç®—æ¯æ—¥è¡ç”ŸæŒ‡æ ‡
                    daily_data["net_flow"] = daily_data["inflow"] - daily_data["outflow"]
                    daily_data["flow_ratio"] = (
                        daily_data["inflow"] / daily_data["outflow"]
                        if daily_data["outflow"] > 0 else float('inf')
                    )
                    daily_data["conversion_rate"] = (
                        daily_data["purchases"] / daily_data["registrations"]
                        if daily_data["registrations"] > 0 else 0
                    )
                    daily_data["activity_score"] = (
                        daily_data["purchases"] + daily_data["expired_products"]
                    ) / 2

                    # ğŸ¯ æ—¥æœŸå…ƒæ•°æ®
                    if daily_data["date"] and self.date_utils:
                        try:
                            date_obj = datetime.strptime(daily_data["date"], "%Y%m%d")
                            daily_data["date_metadata"] = {
                                "weekday": date_obj.strftime("%A"),
                                "formatted_date": date_obj.strftime("%Y-%m-%d"),
                                "is_weekend": date_obj.weekday() >= 5
                            }
                        except:
                            pass

                    extracted["daily_analysis"].update(daily_data)

        return extracted

    async def _extract_reinvestment_analysis_data(self, processed_data: Dict[str, Any],
                                                  user_query: str,
                                                  query_analysis: QueryAnalysisResult) -> Dict[str, Any]:
        """ğŸ’° æ™ºèƒ½æå–å¤æŠ•åˆ†ææ•°æ®"""
        extracted = {"reinvestment_analysis": {}}

        # ğŸ¯ ä»æŸ¥è¯¢åˆ†æä¸­è·å–å¤æŠ•å‚æ•°
        reinvest_rate = query_analysis.processing_metadata.get("reinvestment_rate", 0.5)

        # ğŸ§  æå–åˆ°æœŸæ•°æ®
        expiry_data = await self._extract_expiry_analysis_data(processed_data, user_query)
        if "expiry_analysis" in expiry_data:
            expiry_info = expiry_data["expiry_analysis"]

            extracted["reinvestment_analysis"].update({
                "base_expiry_amount": expiry_info.get("expiry_amount", 0),
                "base_expiry_count": expiry_info.get("expiry_count", 0),
                "reinvestment_rate": reinvest_rate,
                "product_details": expiry_info.get("product_details", [])
            })

        # ğŸ§  æå–å½“å‰è´¢åŠ¡çŠ¶å†µ
        financial_data = await self._extract_financial_overview_data(processed_data, user_query)
        if "financial_overview" in financial_data:
            financial_info = financial_data["financial_overview"]

            extracted["reinvestment_analysis"].update({
                "current_balance": financial_info.get("total_balance", 0),
                "current_inflow": financial_info.get("total_inflow", 0),
                "current_outflow": financial_info.get("total_outflow", 0)
            })

        # ğŸ§® é¢„è®¡ç®—å¤æŠ•æŒ‡æ ‡
        base_amount = extracted["reinvestment_analysis"].get("base_expiry_amount", 0)
        if base_amount > 0:
            extracted["reinvestment_analysis"].update({
                "estimated_reinvest_amount": base_amount * reinvest_rate,
                "estimated_withdrawal_amount": base_amount * (1 - reinvest_rate),
                "liquidity_impact": base_amount * (1 - reinvest_rate)
            })

        return extracted

    async def _extract_trend_analysis_data(self, processed_data: Dict[str, Any],
                                           user_query: str) -> Dict[str, Any]:
        """ğŸ“ˆ æ™ºèƒ½æå–è¶‹åŠ¿åˆ†ææ•°æ®"""
        extracted = {"trend_analysis": {}}

        # ğŸ¯ æŸ¥æ‰¾æ—¶é—´åºåˆ—æ•°æ®
        time_series_data = {}

        for key, data in processed_data.items():
            if isinstance(data, dict):
                # æ£€æµ‹æ¯æ—¥æ•°æ®åºåˆ—
                if "by_date" in data or "by_type" in data:
                    time_series_data[key] = data
                elif isinstance(data, list):
                    # å¤„ç†åˆ—è¡¨æ ¼å¼çš„æ—¶é—´åºåˆ—
                    if all(isinstance(item, dict) and "æ—¥æœŸ" in item for item in data):
                        time_series_data[key] = data

        # ğŸ§® æå–è¶‹åŠ¿æŒ‡æ ‡
        if time_series_data:
            extracted["trend_analysis"]["time_series_available"] = True
            extracted["trend_analysis"]["data_sources"] = list(time_series_data.keys())

            # ğŸ¯ æå–æ•°å€¼åºåˆ—ç”¨äºè®¡ç®—
            numeric_series = {}
            for source_key, series_data in time_series_data.items():
                if isinstance(series_data, list):
                    # ä»åˆ—è¡¨ä¸­æå–æ•°å€¼
                    for field in ["å…¥é‡‘", "å‡ºé‡‘", "æ³¨å†Œäººæ•°"]:
                        values = []
                        for item in series_data:
                            if isinstance(item, dict) and field in item:
                                try:
                                    values.append(float(item[field]))
                                except (ValueError, TypeError):
                                    continue
                        if values and len(values) >= 3:  # è‡³å°‘3ä¸ªæ•°æ®ç‚¹
                            numeric_series[f"{source_key}_{field}"] = values

            extracted["trend_analysis"]["numeric_series"] = numeric_series
        else:
            extracted["trend_analysis"]["time_series_available"] = False

        return extracted

    async def _extract_user_analysis_data(self, processed_data: Dict[str, Any],
                                          user_query: str) -> Dict[str, Any]:
        """ğŸ‘¥ æ™ºèƒ½æå–ç”¨æˆ·åˆ†ææ•°æ®"""
        extracted = {"user_analysis": {}}

        for key, data in processed_data.items():
            if isinstance(data, dict):
                # ğŸ¯ ç”¨æˆ·ç»Ÿè®¡æ•°æ®
                if "ç”¨æˆ·ç»Ÿè®¡" in data:
                    user_stats = data["ç”¨æˆ·ç»Ÿè®¡"]
                    extracted["user_analysis"].update({
                        "total_users": int(user_stats.get("æ€»ç”¨æˆ·æ•°", 0)),
                        "active_users": int(user_stats.get("æ´»è·ƒç”¨æˆ·æ•°", 0))
                    })

                # ğŸ¯ VIPåˆ†å¸ƒæ•°æ®
                vip_data = {}
                for i in range(11):  # VIP0-VIP10
                    vip_key = f"vip{i}çš„äººæ•°"
                    if vip_key in data:
                        vip_data[f"vip{i}"] = int(data.get(vip_key, 0))

                if vip_data:
                    total_vip_users = sum(vip_data.values())
                    vip_distribution = {}
                    for vip_level, count in vip_data.items():
                        vip_distribution[vip_level] = {
                            "count": count,
                            "percentage": (count / total_vip_users * 100) if total_vip_users > 0 else 0
                        }

                    extracted["user_analysis"]["vip_distribution"] = vip_distribution
                    extracted["user_analysis"]["total_vip_users"] = total_vip_users

                # ğŸ¯ ç”¨æˆ·è¯¦ç»†åˆ—è¡¨æ•°æ®
                if "ç”¨æˆ·åˆ—è¡¨" in data:
                    user_list = data["ç”¨æˆ·åˆ—è¡¨"]
                    if isinstance(user_list, list):
                        user_investments = []
                        user_rewards = []
                        user_rois = []

                        for user in user_list:
                            if isinstance(user, dict):
                                investment = float(user.get("æ€»æŠ•å…¥", 0))
                                reward = float(user.get("ç´¯è®¡è·å¾—å¥–åŠ±é‡‘é¢", 0))
                                roi = float(user.get("æŠ•æŠ¥æ¯”", 0))

                                if investment > 0:
                                    user_investments.append(investment)
                                if reward > 0:
                                    user_rewards.append(reward)
                                if roi > 0:
                                    user_rois.append(roi)

                        if user_investments:
                            extracted["user_analysis"]["investment_stats"] = {
                                "total_users_with_investment": len(user_investments),
                                "avg_investment": sum(user_investments) / len(user_investments),
                                "total_investment": sum(user_investments),
                                "max_investment": max(user_investments),
                                "min_investment": min(user_investments)
                            }

                        if user_rois:
                            extracted["user_analysis"]["roi_stats"] = {
                                "avg_roi": sum(user_rois) / len(user_rois),
                                "max_roi": max(user_rois),
                                "min_roi": min(user_rois)
                            }

        # ğŸ§® è®¡ç®—ç”¨æˆ·è¡ç”ŸæŒ‡æ ‡
        if "total_users" in extracted["user_analysis"] and "active_users" in extracted["user_analysis"]:
            total = extracted["user_analysis"]["total_users"]
            active = extracted["user_analysis"]["active_users"]

            if total > 0:
                extracted["user_analysis"]["activity_rate"] = active / total
                extracted["user_analysis"]["inactive_users"] = total - active

        return extracted

    async def _extract_cash_flow_analysis_data(self, processed_data: Dict[str, Any],
                                               user_query: str) -> Dict[str, Any]:
        """ğŸ’¸ æ™ºèƒ½æå–ç°é‡‘æµåˆ†ææ•°æ®"""
        extracted = {"cash_flow_analysis": {}}

        # ğŸ¯ æå–è´¢åŠ¡æ•°æ®
        financial_data = await self._extract_financial_overview_data(processed_data, user_query)
        if "financial_overview" in financial_data:
            financial_info = financial_data["financial_overview"]

            extracted["cash_flow_analysis"].update({
                "current_balance": financial_info.get("total_balance", 0),
                "total_inflow": financial_info.get("total_inflow", 0),
                "total_outflow": financial_info.get("total_outflow", 0),
                "net_flow": financial_info.get("net_flow", 0),
                "outflow_ratio": financial_info.get("outflow_ratio", 0)
            })

        # ğŸ¯ æå–æ¯æ—¥ç°é‡‘æµæ•°æ®
        daily_data = await self._extract_daily_analysis_data(processed_data, user_query)
        if "daily_analysis" in daily_data:
            daily_info = daily_data["daily_analysis"]

            extracted["cash_flow_analysis"]["daily_flows"] = {
                "daily_inflow": daily_info.get("inflow", 0),
                "daily_outflow": daily_info.get("outflow", 0),
                "daily_net_flow": daily_info.get("net_flow", 0)
            }

        # ğŸ§® ç°é‡‘æµå¥åº·åº¦è¯„ä¼°
        current_balance = extracted["cash_flow_analysis"].get("current_balance", 0)
        daily_outflow = extracted["cash_flow_analysis"].get("daily_flows", {}).get("daily_outflow", 0)

        if current_balance > 0 and daily_outflow > 0:
            runway_days = current_balance / daily_outflow
            extracted["cash_flow_analysis"]["estimated_runway_days"] = runway_days

            if runway_days > 365:
                extracted["cash_flow_analysis"]["liquidity_status"] = "excellent"
            elif runway_days > 180:
                extracted["cash_flow_analysis"]["liquidity_status"] = "good"
            elif runway_days > 90:
                extracted["cash_flow_analysis"]["liquidity_status"] = "moderate"
            else:
                extracted["cash_flow_analysis"]["liquidity_status"] = "concerning"

        return extracted

    async def _extract_comprehensive_data(self, processed_data: Dict[str, Any],
                                          user_query: str) -> Dict[str, Any]:
        """ğŸ¯ ç»¼åˆæ•°æ®æå–"""
        extracted = {"comprehensive_analysis": {}}

        # ğŸ§  å¹¶è¡Œæå–å„ç±»æ•°æ®
        extraction_tasks = [
            self._extract_financial_overview_data(processed_data, user_query),
            self._extract_daily_analysis_data(processed_data, user_query),
            self._extract_user_analysis_data(processed_data, user_query)
        ]

        results = await asyncio.gather(*extraction_tasks, return_exceptions=True)

        # ğŸ¯ åˆå¹¶æå–ç»“æœ
        for result in results:
            if isinstance(result, dict) and not isinstance(result, Exception):
                extracted["comprehensive_analysis"].update(result)

        return extracted

    def _generate_extraction_cache_key(self, user_query: str,
                                       extraction_strategy: DataExtractionStrategy) -> str:
        """ç”Ÿæˆæå–ç¼“å­˜é”®"""
        key_data = f"{user_query}_{extraction_strategy.value}"
        return hashlib.md5(key_data.encode()).hexdigest()

    async def _intelligent_calculation_processing(self, query_analysis: QueryAnalysisResult,
                                                  extracted_data: Dict[str, Any],
                                                  user_query: str) -> Optional[Dict[str, Any]]:
        """ğŸ§® æ™ºèƒ½è®¡ç®—å¤„ç† - å¢å¼ºç‰ˆ"""
        if not query_analysis.needs_calculation:
            return None

        logger.debug(f"ğŸ§® æ‰§è¡Œæ™ºèƒ½è®¡ç®—: {query_analysis.calculation_type}")

        if not self.statistical_calculator:
            logger.error("ç»Ÿä¸€è®¡ç®—å™¨æœªåˆå§‹åŒ–")
            return None

        try:
            # ğŸ§  æ™ºèƒ½è®¡ç®—å‚æ•°æå–
            calc_params = self._extract_intelligent_calculation_params(
                query_analysis, extracted_data, user_query)

            # ğŸ§® æ‰§è¡Œè®¡ç®—
            calc_result = await self.statistical_calculator.calculate(
                calculation_type=query_analysis.calculation_type,
                data=extracted_data,
                params=calc_params
            )

            return {
                'calculation_result': calc_result,
                'calculation_type': query_analysis.calculation_type,
                'success': calc_result.success if calc_result else False,
                'confidence': calc_result.confidence if calc_result else 0.5,
                'calculation_params': calc_params
            }

        except Exception as e:
            logger.error(f"æ™ºèƒ½è®¡ç®—æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'calculation_result': None,
                'calculation_type': query_analysis.calculation_type,
                'success': False,
                'error': str(e)
            }

    def _extract_intelligent_calculation_params(self, query_analysis: QueryAnalysisResult,
                                                extracted_data: Dict[str, Any],
                                                user_query: str) -> Dict[str, Any]:
        """ğŸ§  æ™ºèƒ½è®¡ç®—å‚æ•°æå–"""
        params = {}

        # ğŸ¯ æ ¹æ®è®¡ç®—ç±»å‹æå–å‚æ•°
        calc_type = query_analysis.calculation_type

        if calc_type == "reinvestment_analysis":
            reinvest_data = extracted_data.get("reinvestment_analysis", {})
            params.update({
                "reinvest_rate": reinvest_data.get("reinvestment_rate", 0.5),
                "base_amount": reinvest_data.get("base_expiry_amount", 0),
                "current_balance": reinvest_data.get("current_balance", 0)
            })

        elif calc_type == "cash_runway":
            cash_flow_data = extracted_data.get("cash_flow_analysis", {})
            params.update({
                "current_balance": cash_flow_data.get("current_balance", 0),
                "daily_outflow": cash_flow_data.get("daily_flows", {}).get("daily_outflow", 0)
            })

        elif calc_type == "trend_analysis":
            trend_data = extracted_data.get("trend_analysis", {})
            params.update({
                "time_series_data": trend_data.get("numeric_series", {}),
                "analysis_period": 30
            })

        elif calc_type == "compound_interest":
            # ğŸ§  ä»åˆ°æœŸäº§å“ä¸­æå–åˆ©ç‡ä¿¡æ¯
            expiry_data = extracted_data.get("expiry_analysis", {})
            product_details = expiry_data.get("product_details", [])

            if product_details:
                avg_daily_rate = sum(p.get("daily_rate", 0) for p in product_details) / len(product_details)
                params.update({
                    "principal": expiry_data.get("expiry_amount", 10000),
                    "rate": avg_daily_rate * 365,  # å¹´åŒ–åˆ©ç‡
                    "periods": 365,  # ä¸€å¹´
                    "frequency": 365  # æ¯æ—¥å¤åˆ©
                })

        # ğŸ¯ ä»æŸ¥è¯¢åˆ†æå…ƒæ•°æ®ä¸­è·å–é¢å¤–å‚æ•°
        metadata_params = query_analysis.processing_metadata.get("calculation_params", {})
        params.update(metadata_params)

        # ğŸ¯ æ—¶é—´èŒƒå›´å‚æ•°
        if query_analysis.time_range:
            params.update({
                "start_date": query_analysis.time_range.get("start_date"),
                "end_date": query_analysis.time_range.get("end_date")
            })

        return params

    async def _intelligent_response_generation(self, user_query: str,
                                               query_analysis: QueryAnalysisResult,
                                               extracted_data: Dict[str, Any],
                                               calculation_result: Optional[Dict[str, Any]]) -> str:
        """ğŸ§  æ™ºèƒ½å›ç­”ç”Ÿæˆ - å¢å¼ºç‰ˆ"""
        try:
            if not self.claude_client:
                return self._generate_intelligent_fallback_response(
                    user_query, extracted_data, calculation_result)

            # ğŸ§  æ™ºèƒ½æ•°æ®æ‘˜è¦ç”Ÿæˆ
            intelligent_data_summary = self._generate_intelligent_data_summary(
                extracted_data, user_query, query_analysis)

            intelligent_calc_summary = self._generate_intelligent_calc_summary(calculation_result)

            # ğŸ§  å¢å¼ºçš„Claudeå›ç­”ç”Ÿæˆæç¤º
            enhanced_response_prompt = f"""
            ä½œä¸ºä¸“ä¸šçš„é‡‘èAIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹åˆ†æç»“æœä¸ºç”¨æˆ·ç”Ÿæˆä¸“ä¸šã€å‡†ç¡®ã€æ˜“æ‡‚çš„å›ç­”ï¼š

            ç”¨æˆ·æŸ¥è¯¢ï¼š"{user_query}"

            æŸ¥è¯¢åˆ†æï¼š
            - æŸ¥è¯¢ç±»å‹ï¼š{query_analysis.query_type.value}
            - ä¸šåŠ¡åœºæ™¯ï¼š{query_analysis.business_scenario.value}
            - å¤æ‚åº¦ï¼š{query_analysis.complexity.value}

            æ™ºèƒ½æ•°æ®æ‘˜è¦ï¼š
            {json.dumps(intelligent_data_summary, ensure_ascii=False, indent=2)}

            è®¡ç®—ç»“æœæ‘˜è¦ï¼š
            {json.dumps(intelligent_calc_summary, ensure_ascii=False, indent=2)}

            å›ç­”è¦æ±‚ï¼š
            1. ğŸ¯ ç›´æ¥å›ç­”ç”¨æˆ·çš„å…·ä½“é—®é¢˜
            2. ğŸ“Š ä½¿ç”¨å…·ä½“æ•°æ®æ”¯æŒç­”æ¡ˆ
            3. ğŸ’¡ æä¾›ä¸“ä¸šçš„åˆ†ææ´å¯Ÿ
            4. ğŸš€ ç»™å‡ºå¯è¡Œçš„å»ºè®®ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            5. ğŸ’° æ¶‰åŠé‡‘é¢æ—¶ä½¿ç”¨æ˜“è¯»çš„æ ¼å¼ï¼ˆå¦‚ï¼š1,000,000 æˆ– 100ä¸‡ï¼‰

            ç‰¹åˆ«æ³¨æ„ï¼š
            - å¦‚æœæ˜¯å¤æŠ•åˆ†æï¼Œè¯¦ç»†è¯´æ˜å¤æŠ•é‡‘é¢ã€æç°é‡‘é¢å’Œå½±å“
            - å¦‚æœæ˜¯ç°é‡‘è·‘é“ï¼Œæ˜ç¡®è¯´æ˜èƒ½è¿è¡Œçš„æ—¶é—´å’Œé£é™©æç¤º
            - å¦‚æœæ˜¯è¶‹åŠ¿åˆ†æï¼Œæè¿°å˜åŒ–æ–¹å‘å’Œå…³é”®æŒ‡æ ‡
            - å¦‚æœæ˜¯åˆ°æœŸåˆ†æï¼Œè¯´æ˜åˆ°æœŸæ—¶é—´ã€é‡‘é¢å’Œäº§å“åˆ†å¸ƒ

            å›ç­”é£æ ¼ï¼šä¸“ä¸šä½†æ˜“æ‡‚ï¼Œæ•°æ®ç²¾å‡†ï¼Œé€»è¾‘æ¸…æ™°ã€‚
            """

            # è°ƒç”¨Claudeç”Ÿæˆå›ç­”
            result = await asyncio.wait_for(
                self.claude_client.analyze_complex_query(enhanced_response_prompt, {
                    "user_query": user_query,
                    "data_summary": intelligent_data_summary,
                    "calculation_summary": intelligent_calc_summary
                }),
                timeout=45.0
            )

            if result.get("success"):
                response_text = result.get("analysis", "")

                # ğŸ§  åå¤„ç†ï¼šæ·»åŠ æ ¼å¼åŒ–å’ŒéªŒè¯
                formatted_response = self._post_process_claude_response(
                    response_text, extracted_data, calculation_result)

                return formatted_response
            else:
                logger.warning("Claudeå›ç­”ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½é™çº§å›ç­”")
                return self._generate_intelligent_fallback_response(
                    user_query, extracted_data, calculation_result)

        except asyncio.TimeoutError:
            logger.error("Claudeå›ç­”ç”Ÿæˆè¶…æ—¶")
            return self._generate_intelligent_fallback_response(
                user_query, extracted_data, calculation_result)
        except Exception as e:
            logger.error(f"æ™ºèƒ½å›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_intelligent_fallback_response(
                user_query, extracted_data, calculation_result)

    def _generate_intelligent_data_summary(self, extracted_data: Dict[str, Any],
                                           user_query: str,
                                           query_analysis: QueryAnalysisResult) -> Dict[str, Any]:
        """ğŸ§  ç”Ÿæˆæ™ºèƒ½æ•°æ®æ‘˜è¦"""
        summary = {
            "extraction_strategy": extracted_data.get("extraction_strategy", "unknown"),
            "data_quality": extracted_data.get("data_quality", 0.8),
            "core_metrics": {}
        }

        # ğŸ¯ æ ¹æ®æå–ç­–ç•¥ç”Ÿæˆä¸åŒçš„æ‘˜è¦
        strategy = extracted_data.get("extraction_strategy", "")

        if strategy == "financial_overview":
            financial_data = extracted_data.get("financial_overview", {})
            summary["core_metrics"] = {
                "æ€»ä½™é¢": financial_data.get("total_balance", 0),
                "å‡€æµå…¥": financial_data.get("net_flow", 0),
                "æ´»è·ƒç”¨æˆ·": financial_data.get("active_users", 0),
                "æ´»è·ƒç‡": financial_data.get("activity_rate", 0)
            }

        elif strategy == "expiry_analysis":
            expiry_data = extracted_data.get("expiry_analysis", {})
            summary["core_metrics"] = {
                "åˆ°æœŸé‡‘é¢": expiry_data.get("expiry_amount", 0),
                "åˆ°æœŸæ•°é‡": expiry_data.get("expiry_count", 0),
                "äº§å“ç§ç±»": len(expiry_data.get("product_details", []))
            }

        elif strategy == "reinvestment_analysis":
            reinvest_data = extracted_data.get("reinvestment_analysis", {})
            summary["core_metrics"] = {
                "åŸºç¡€åˆ°æœŸé‡‘é¢": reinvest_data.get("base_expiry_amount", 0),
                "å¤æŠ•æ¯”ä¾‹": reinvest_data.get("reinvestment_rate", 0),
                "é¢„è®¡å¤æŠ•é‡‘é¢": reinvest_data.get("estimated_reinvest_amount", 0),
                "é¢„è®¡æç°é‡‘é¢": reinvest_data.get("estimated_withdrawal_amount", 0)
            }

        elif strategy == "daily_analysis":
            daily_data = extracted_data.get("daily_analysis", {})
            summary["core_metrics"] = {
                "å…¥é‡‘": daily_data.get("inflow", 0),
                "å‡ºé‡‘": daily_data.get("outflow", 0),
                "å‡€æµå…¥": daily_data.get("net_flow", 0),
                "æ³¨å†Œäººæ•°": daily_data.get("registrations", 0)
            }

        elif strategy == "cash_flow_analysis":
            cash_data = extracted_data.get("cash_flow_analysis", {})
            summary["core_metrics"] = {
                "å½“å‰ä½™é¢": cash_data.get("current_balance", 0),
                "é¢„è®¡è¿è¡Œå¤©æ•°": cash_data.get("estimated_runway_days", 0),
                "æµåŠ¨æ€§çŠ¶æ€": cash_data.get("liquidity_status", "unknown")
            }

        # ğŸ§  æ·»åŠ ä¸Šä¸‹æ–‡ç›¸å…³ä¿¡æ¯
        summary["query_context"] = {
            "mentions_reinvestment": any(kw in user_query.lower() for kw in ["å¤æŠ•", "å†æŠ•èµ„"]),
            "mentions_timeframe": query_analysis.time_range is not None,
            "calculation_needed": query_analysis.needs_calculation
        }

        return summary

    def _generate_intelligent_calc_summary(self, calculation_result: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ğŸ§  ç”Ÿæˆæ™ºèƒ½è®¡ç®—æ‘˜è¦"""
        if not calculation_result or not calculation_result.get('success'):
            return {"status": "æ— è®¡ç®—ç»“æœ"}

        calc_res = calculation_result.get('calculation_result')
        if not calc_res:
            return {"status": "è®¡ç®—ç»“æœä¸ºç©º"}

        summary = {
            "status": "è®¡ç®—æˆåŠŸ",
            "calculation_type": calculation_result.get('calculation_type'),
            "confidence": calculation_result.get('confidence', 0.5),
            "primary_result": getattr(calc_res, 'primary_result', None),
            "key_results": {}
        }

        # ğŸ¯ æ ¹æ®è®¡ç®—ç±»å‹æå–å…³é”®ç»“æœ
        calc_type = calculation_result.get('calculation_type', '')
        detailed_results = getattr(calc_res, 'detailed_results', {})

        if calc_type == "reinvestment_analysis":
            summary["key_results"] = {
                "å¤æŠ•é‡‘é¢": detailed_results.get("estimated_reinvest_amount", 0),
                "æç°é‡‘é¢": detailed_results.get("estimated_withdrawal_amount", 0),
                "ä½™é¢å½±å“": detailed_results.get("final_balance_impact", 0)
            }

        elif calc_type == "cash_runway":
            runway_analysis = detailed_results.get("runway_analysis", {})
            summary["key_results"] = {
                "è¿è¡Œå¤©æ•°": runway_analysis.get("runway_days", 0),
                "è¿è¡Œæœˆæ•°": runway_analysis.get("runway_months", 0),
                "é£é™©ç­‰çº§": runway_analysis.get("risk_level", "unknown")
            }

        elif calc_type == "trend_analysis":
            summary["key_results"] = {
                "è¶‹åŠ¿æ–¹å‘": "è¯¦è§è¶‹åŠ¿åˆ†æç»“æœ",
                "åˆ†ææŒ‡æ ‡æ•°": len(detailed_results) if detailed_results else 0
            }

        return summary

    def _post_process_claude_response(self, response_text: str,
                                      extracted_data: Dict[str, Any],
                                      calculation_result: Optional[Dict[str, Any]]) -> str:
        """ğŸ§  Claudeå›ç­”åå¤„ç†"""
        try:
            # ğŸ¯ æ•°å€¼æ ¼å¼åŒ–
            if self.financial_formatter:
                # æŸ¥æ‰¾å¹¶æ ¼å¼åŒ–æ•°å€¼
                import re

                # åŒ¹é…è´§å¸æ•°å€¼æ¨¡å¼
                money_pattern = r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)'

                def format_money(match):
                    try:
                        amount = float(match.group(1).replace(',', ''))
                        return self.financial_formatter.format_currency(amount)
                    except:
                        return match.group(1)

                response_text = re.sub(money_pattern, format_money, response_text)

            # ğŸ¯ æ·»åŠ æ•°æ®æ¥æºå£°æ˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if "åŸºäºç³»ç»Ÿæ•°æ®åˆ†æ" not in response_text:
                response_text += "\n\nğŸ“Š *åŸºäºæœ€æ–°ç³»ç»Ÿæ•°æ®åˆ†æ*"

            return response_text

        except Exception as e:
            logger.error(f"å›ç­”åå¤„ç†å¤±è´¥: {e}")
            return response_text

    def _generate_intelligent_fallback_response(self, user_query: str,
                                                extracted_data: Dict[str, Any],
                                                calculation_result: Optional[Dict[str, Any]]) -> str:
        """ğŸ§  æ™ºèƒ½é™çº§å›ç­”ç”Ÿæˆ"""
        response_parts = ["åŸºäºç³»ç»Ÿåˆ†æï¼š\n"]

        # ğŸ¯ æ ¹æ®æå–çš„æ•°æ®ç±»å‹ç”Ÿæˆå›ç­”
        strategy = extracted_data.get("extraction_strategy", "")

        if strategy == "financial_overview":
            financial_data = extracted_data.get("financial_overview", {})
            total_balance = financial_data.get("total_balance", 0)
            active_users = financial_data.get("active_users", 0)

            if self.financial_formatter:
                response_parts.append(f"ğŸ’° å½“å‰æ€»ä½™é¢ï¼š{self.financial_formatter.format_currency(total_balance)}")
            else:
                response_parts.append(f"ğŸ’° å½“å‰æ€»ä½™é¢ï¼šÂ¥{total_balance:,.2f}")

            response_parts.append(f"ğŸ‘¥ æ´»è·ƒç”¨æˆ·ï¼š{active_users:,}äºº")

        elif strategy == "expiry_analysis":
            expiry_data = extracted_data.get("expiry_analysis", {})
            expiry_amount = expiry_data.get("expiry_amount", 0)
            expiry_count = expiry_data.get("expiry_count", 0)

            if self.financial_formatter:
                response_parts.append(f"â° åˆ°æœŸé‡‘é¢ï¼š{self.financial_formatter.format_currency(expiry_amount)}")
            else:
                response_parts.append(f"â° åˆ°æœŸé‡‘é¢ï¼šÂ¥{expiry_amount:,.2f}")

            response_parts.append(f"ğŸ“¦ åˆ°æœŸæ•°é‡ï¼š{expiry_count}ç¬”")

        elif strategy == "reinvestment_analysis":
            reinvest_data = extracted_data.get("reinvestment_analysis", {})
            reinvest_amount = reinvest_data.get("estimated_reinvest_amount", 0)
            withdraw_amount = reinvest_data.get("estimated_withdrawal_amount", 0)

            if self.financial_formatter:
                response_parts.extend([
                    f"ğŸ’° é¢„è®¡å¤æŠ•é‡‘é¢ï¼š{self.financial_formatter.format_currency(reinvest_amount)}",
                    f"ğŸ’¸ é¢„è®¡æç°é‡‘é¢ï¼š{self.financial_formatter.format_currency(withdraw_amount)}"
                ])
            else:
                response_parts.extend([
                    f"ğŸ’° é¢„è®¡å¤æŠ•é‡‘é¢ï¼šÂ¥{reinvest_amount:,.2f}",
                    f"ğŸ’¸ é¢„è®¡æç°é‡‘é¢ï¼šÂ¥{withdraw_amount:,.2f}"
                ])

        # ğŸ¯ æ·»åŠ è®¡ç®—ç»“æœ
        if calculation_result and calculation_result.get('success'):
            calc_res = calculation_result.get('calculation_result')
            calc_type = calculation_result.get('calculation_type', '')

            if calc_type == "cash_runway" and calc_res:
                detailed_results = getattr(calc_res, 'detailed_results', {})
                runway_analysis = detailed_results.get("runway_analysis", {})
                runway_days = runway_analysis.get("runway_days", 0)

                if runway_days > 0:
                    response_parts.append(f"\nğŸ“Š æ ¹æ®å½“å‰èµ„é‡‘çŠ¶å†µï¼Œé¢„è®¡å¯è¿è¡Œ {runway_days:.0f} å¤©")

        if len(response_parts) == 1:  # åªæœ‰å¼€å¤´
            response_parts.append("æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•æä¾›è¯¦ç»†åˆ†æã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚")

        return "\n".join(response_parts)

    async def _intelligent_insights_generation(self, extracted_data: Dict[str, Any],
                                               calculation_result: Optional[Dict[str, Any]],
                                               query_analysis: QueryAnalysisResult,
                                               user_query: str) -> List[BusinessInsight]:
        """ğŸ’¡ æ™ºèƒ½æ´å¯Ÿç”Ÿæˆ - å¢å¼ºç‰ˆ"""
        logger.debug("ğŸ’¡ ç”Ÿæˆæ™ºèƒ½ä¸šåŠ¡æ´å¯Ÿ")

        insights = []

        try:
            # ğŸ§  æ ¹æ®æ•°æ®ç±»å‹ç”Ÿæˆé’ˆå¯¹æ€§æ´å¯Ÿ
            strategy = extracted_data.get("extraction_strategy", "")

            if strategy == "financial_overview":
                insights.extend(await self._generate_financial_insights(extracted_data))
            elif strategy == "expiry_analysis":
                insights.extend(await self._generate_expiry_insights(extracted_data))
            elif strategy == "reinvestment_analysis":
                insights.extend(await self._generate_reinvestment_insights(extracted_data, calculation_result))
            elif strategy == "cash_flow_analysis":
                insights.extend(await self._generate_cash_flow_insights(extracted_data, calculation_result))
            elif strategy == "trend_analysis":
                insights.extend(await self._generate_trend_insights(extracted_data, calculation_result))

            # ğŸ§  åŸºäºè®¡ç®—ç»“æœç”Ÿæˆé¢å¤–æ´å¯Ÿ
            if calculation_result and calculation_result.get('success'):
                insights.extend(await self._generate_calculation_insights(calculation_result))

        except Exception as e:
            logger.error(f"æ™ºèƒ½æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
            insights.append(BusinessInsight(
                title="ç³»ç»Ÿæç¤º",
                summary="æ•°æ®åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•",
                confidence_score=0.3,
                insight_type="system_message"
            ))

        return insights

    async def _generate_financial_insights(self, extracted_data: Dict[str, Any]) -> List[BusinessInsight]:
        """ğŸ’° ç”Ÿæˆè´¢åŠ¡æ´å¯Ÿ"""
        insights = []
        financial_data = extracted_data.get("financial_overview", {})

        total_balance = financial_data.get("total_balance", 0)
        net_flow = financial_data.get("net_flow", 0)
        outflow_ratio = financial_data.get("outflow_ratio", 0)
        activity_rate = financial_data.get("activity_rate", 0)

        # ğŸ¯ ç°é‡‘æµæ´å¯Ÿ
        if net_flow > 0:
            insights.append(BusinessInsight(
                title="æ­£å‘ç°é‡‘æµ",
                summary=f"å½“å‰å‡€æµå…¥è‰¯å¥½ï¼Œèµ„é‡‘å¢é•¿ç¨³å¥",
                confidence_score=0.9,
                insight_type="cash_flow_positive",
                recommendations=["ç»§ç»­ä¿æŒè‰¯å¥½çš„èµ„é‡‘ç®¡ç†", "è€ƒè™‘æ‰©å¤§ä¸šåŠ¡è§„æ¨¡"],
                supporting_data={"net_flow": net_flow}
            ))
        elif net_flow < 0:
            insights.append(BusinessInsight(
                title="ç°é‡‘æµå‡ºè­¦ç¤º",
                summary=f"å½“å‰å­˜åœ¨èµ„é‡‘å‡€æµå‡ºï¼Œéœ€è¦å…³æ³¨ç°é‡‘æµç®¡ç†",
                confidence_score=0.9,
                insight_type="cash_flow_negative",
                recommendations=["åˆ†ææ”¯å‡ºç»“æ„", "åˆ¶å®šç°é‡‘æµä¼˜åŒ–ç­–ç•¥", "è€ƒè™‘å¢åŠ æ”¶å…¥æ¥æº"],
                supporting_data={"net_flow": net_flow}
            ))

        # ğŸ¯ æ”¯å‡ºæ¯”ä¾‹æ´å¯Ÿ
        if outflow_ratio > 0.8:
            insights.append(BusinessInsight(
                title="é«˜æ”¯å‡ºæ¯”ä¾‹é¢„è­¦",
                summary=f"æ”¯å‡ºå å…¥é‡‘æ¯”ä¾‹è¾¾ {outflow_ratio:.1%}ï¼Œå»ºè®®ä¼˜åŒ–æ”¯å‡ºç»“æ„",
                confidence_score=0.85,
                insight_type="high_outflow_warning",
                recommendations=["å®¡æ ¸å¤§é¢æ”¯å‡ºé¡¹ç›®", "åˆ¶å®šæ”¯å‡ºæ§åˆ¶æªæ–½", "æå‡èµ„é‡‘ä½¿ç”¨æ•ˆç‡"],
                supporting_data={"outflow_ratio": outflow_ratio}
            ))

        # ğŸ¯ ç”¨æˆ·æ´»è·ƒåº¦æ´å¯Ÿ
        if activity_rate > 0:
            if activity_rate > 0.8:
                insights.append(BusinessInsight(
                    title="ç”¨æˆ·æ´»è·ƒåº¦ä¼˜ç§€",
                    summary=f"ç”¨æˆ·æ´»è·ƒç‡è¾¾ {activity_rate:.1%}ï¼Œç”¨æˆ·ç²˜æ€§å¾ˆå¼º",
                    confidence_score=0.9,
                    insight_type="high_user_engagement",
                    recommendations=["ç»´æŒç°æœ‰ç”¨æˆ·ä½“éªŒ", "è€ƒè™‘æ¨å‡ºæ›´å¤šäº§å“"],
                    supporting_data={"activity_rate": activity_rate}
                ))
            elif activity_rate < 0.4:
                insights.append(BusinessInsight(
                    title="ç”¨æˆ·æ´»è·ƒåº¦å¾…æå‡",
                    summary=f"ç”¨æˆ·æ´»è·ƒç‡ä»… {activity_rate:.1%}ï¼Œéœ€è¦åŠ å¼ºç”¨æˆ·è¿è¥",
                    confidence_score=0.8,
                    insight_type="low_user_engagement",
                    recommendations=["åˆ†æç”¨æˆ·æµå¤±åŸå› ", "ä¼˜åŒ–äº§å“ä½“éªŒ", "åŠ å¼ºç”¨æˆ·äº’åŠ¨"],
                    supporting_data={"activity_rate": activity_rate}
                ))

        return insights

    async def _generate_expiry_insights(self, extracted_data: Dict[str, Any]) -> List[BusinessInsight]:
        """â° ç”Ÿæˆåˆ°æœŸæ´å¯Ÿ"""
        insights = []
        expiry_data = extracted_data.get("expiry_analysis", {})

        expiry_amount = expiry_data.get("expiry_amount", 0)
        expiry_count = expiry_data.get("expiry_count", 0)
        product_details = expiry_data.get("product_details", [])

        # ğŸ¯ åˆ°æœŸè§„æ¨¡è¯„ä¼°
        if expiry_amount > 5000000:  # 500ä¸‡ä»¥ä¸Š
            insights.append(BusinessInsight(
                title="å¤§é¢åˆ°æœŸé¢„è­¦",
                summary=f"å³å°†åˆ°æœŸé‡‘é¢è¾¾ {expiry_amount:,.0f}ï¼Œéœ€è¦å……è¶³çš„æµåŠ¨æ€§å‡†å¤‡",
                confidence_score=0.95,
                insight_type="large_expiry_warning",
                recommendations=["ç¡®ä¿å……è¶³çš„ç°é‡‘å‚¨å¤‡", "åˆ¶å®šæµåŠ¨æ€§ç®¡ç†è®¡åˆ’", "è€ƒè™‘åˆ†æœŸæ”¯ä»˜æ–¹æ¡ˆ"],
                supporting_data={"expiry_amount": expiry_amount}
            ))
        elif expiry_amount > 1000000:  # 100ä¸‡ä»¥ä¸Š
            insights.append(BusinessInsight(
                title="ä¸­ç­‰è§„æ¨¡åˆ°æœŸæé†’",
                summary=f"åˆ°æœŸé‡‘é¢ {expiry_amount:,.0f}ï¼Œå»ºè®®æå‰å‡†å¤‡èµ„é‡‘",
                confidence_score=0.8,
                insight_type="medium_expiry_notice",
                recommendations=["æ£€æŸ¥ç°é‡‘æµçŠ¶å†µ", "é¢„ç•™è¶³å¤Ÿçš„æ”¯ä»˜èµ„é‡‘"],
                supporting_data={"expiry_amount": expiry_amount}
            ))

        # ğŸ¯ äº§å“é›†ä¸­åº¦åˆ†æ
        if len(product_details) > 0:
            max_product_amount = max([p.get("amount", 0) for p in product_details])
            concentration_ratio = max_product_amount / expiry_amount if expiry_amount > 0 else 0

            if concentration_ratio > 0.4:
                insights.append(BusinessInsight(
                    title="äº§å“é›†ä¸­åº¦è¾ƒé«˜",
                    summary=f"å•ä¸€äº§å“å åˆ°æœŸé‡‘é¢ {concentration_ratio:.1%}ï¼Œå­˜åœ¨é›†ä¸­åº¦é£é™©",
                    confidence_score=0.8,
                    insight_type="product_concentration_risk",
                    recommendations=["åˆ†æ•£äº§å“ç»“æ„", "é™ä½å•ä¸€äº§å“ä¾èµ–", "ä¼˜åŒ–äº§å“ç»„åˆ"],
                    supporting_data={"concentration_ratio": concentration_ratio}
                ))

        return insights

    async def _generate_reinvestment_insights(self, extracted_data: Dict[str, Any],
                                              calculation_result: Optional[Dict[str, Any]]) -> List[BusinessInsight]:
        """ğŸ’° ç”Ÿæˆå¤æŠ•æ´å¯Ÿ"""
        insights = []
        reinvest_data = extracted_data.get("reinvestment_analysis", {})

        reinvest_rate = reinvest_data.get("reinvestment_rate", 0)
        estimated_withdraw = reinvest_data.get("estimated_withdrawal_amount", 0)
        current_balance = reinvest_data.get("current_balance", 0)

        # ğŸ¯ å¤æŠ•ç­–ç•¥è¯„ä¼°
        if reinvest_rate > 0.7:
            insights.append(BusinessInsight(
                title="é«˜å¤æŠ•ç‡ç­–ç•¥",
                summary=f"å¤æŠ•ç‡ {reinvest_rate:.1%} æœ‰åˆ©äºèµ„é‡‘å¢é•¿ï¼Œä½†éœ€æ³¨æ„æµåŠ¨æ€§",
                confidence_score=0.8,
                insight_type="high_reinvestment_strategy",
                recommendations=["ä¿æŒé€‚åº¦ç°é‡‘å‚¨å¤‡", "ç›‘æ§æµåŠ¨æ€§çŠ¶å†µ", "è®¾ç½®æµåŠ¨æ€§é¢„è­¦"],
                supporting_data={"reinvest_rate": reinvest_rate}
            ))
        elif reinvest_rate < 0.3:
            insights.append(BusinessInsight(
                title="ä¿å®ˆå¤æŠ•ç­–ç•¥",
                summary=f"å¤æŠ•ç‡ {reinvest_rate:.1%} è¾ƒä¸ºä¿å®ˆï¼ŒæµåŠ¨æ€§å……è¶³ä½†å¢é•¿æœ‰é™",
                confidence_score=0.8,
                insight_type="conservative_reinvestment",
                recommendations=["è¯„ä¼°æé«˜å¤æŠ•æ¯”ä¾‹çš„å¯èƒ½æ€§", "ä¼˜åŒ–èµ„é‡‘åˆ©ç”¨æ•ˆç‡"],
                supporting_data={"reinvest_rate": reinvest_rate}
            ))

        # ğŸ¯ æµåŠ¨æ€§å½±å“è¯„ä¼°
        if estimated_withdraw > 0 and current_balance > 0:
            withdraw_impact = estimated_withdraw / current_balance
            if withdraw_impact > 0.3:
                insights.append(BusinessInsight(
                    title="æ˜¾è‘—æµåŠ¨æ€§å½±å“",
                    summary=f"é¢„è®¡æç°å°†å½±å“ {withdraw_impact:.1%} çš„èµ„é‡‘ï¼Œéœ€è¦è°¨æ…ç®¡ç†",
                    confidence_score=0.85,
                    insight_type="liquidity_impact_significant",
                    recommendations=["åˆ¶å®šèµ„é‡‘è°ƒé…è®¡åˆ’", "ç¡®ä¿æ ¸å¿ƒä¸šåŠ¡èµ„é‡‘å……è¶³"],
                    supporting_data={"withdraw_impact": withdraw_impact}
                ))

        return insights

    async def _generate_cash_flow_insights(self, extracted_data: Dict[str, Any],
                                           calculation_result: Optional[Dict[str, Any]]) -> List[BusinessInsight]:
        """ğŸ’¸ ç”Ÿæˆç°é‡‘æµæ´å¯Ÿ"""
        insights = []
        cash_data = extracted_data.get("cash_flow_analysis", {})

        runway_days = cash_data.get("estimated_runway_days", 0)
        liquidity_status = cash_data.get("liquidity_status", "unknown")

        # ğŸ¯ ç°é‡‘è·‘é“è¯„ä¼°
        if runway_days > 0:
            if runway_days < 30:
                insights.append(BusinessInsight(
                    title="ç°é‡‘è·‘é“ç´§æ€¥é¢„è­¦",
                    summary=f"æŒ‰å½“å‰æ”¯å‡ºæ°´å¹³ï¼Œèµ„é‡‘ä»…èƒ½ç»´æŒ {runway_days:.0f} å¤©",
                    confidence_score=0.95,
                    insight_type="critical_cash_runway",
                    recommendations=["ç«‹å³åˆ¶å®šç´§æ€¥èµ„é‡‘è®¡åˆ’", "å‰Šå‡éå¿…è¦æ”¯å‡º", "å¯»æ‰¾å¿«é€Ÿèèµ„æ¸ é“"],
                    supporting_data={"runway_days": runway_days}
                ))
            elif runway_days < 90:
                insights.append(BusinessInsight(
                    title="ç°é‡‘è·‘é“é¢„è­¦",
                    summary=f"èµ„é‡‘å¯ç»´æŒçº¦ {runway_days:.0f} å¤©ï¼Œå»ºè®®æå‰å‡†å¤‡",
                    confidence_score=0.9,
                    insight_type="cash_runway_warning",
                    recommendations=["åˆ¶å®šèµ„é‡‘ç­¹æªè®¡åˆ’", "ä¼˜åŒ–ç°é‡‘æµç®¡ç†", "è€ƒè™‘å»¶æœŸæ”¯ä»˜å®‰æ’"],
                    supporting_data={"runway_days": runway_days}
                ))
            elif runway_days > 365:
                insights.append(BusinessInsight(
                    title="å……è¶³çš„èµ„é‡‘å‚¨å¤‡",
                    summary=f"èµ„é‡‘å¯ç»´æŒè¶…è¿‡ä¸€å¹´ï¼ŒæµåŠ¨æ€§çŠ¶å†µä¼˜ç§€",
                    confidence_score=0.9,
                    insight_type="excellent_liquidity",
                    recommendations=["è€ƒè™‘æŠ•èµ„å¢å€¼æœºä¼š", "ä¼˜åŒ–èµ„é‡‘é…ç½®", "åˆ¶å®šé•¿æœŸå‘å±•è§„åˆ’"],
                    supporting_data={"runway_days": runway_days}
                ))

        return insights

    async def _generate_trend_insights(self, extracted_data: Dict[str, Any],
                                       calculation_result: Optional[Dict[str, Any]]) -> List[BusinessInsight]:
        """ğŸ“ˆ ç”Ÿæˆè¶‹åŠ¿æ´å¯Ÿ"""
        insights = []

        # ğŸ¯ åŸºäºè®¡ç®—ç»“æœçš„è¶‹åŠ¿æ´å¯Ÿ
        if calculation_result and calculation_result.get('success'):
            calc_res = calculation_result.get('calculation_result')
            if calc_res and hasattr(calc_res, 'detailed_results'):
                detailed_results = getattr(calc_res, 'detailed_results', {})

                for metric_name, trend_data in detailed_results.items():
                    if isinstance(trend_data, dict) and 'trend_direction' in trend_data:
                        direction = trend_data.get('trend_direction', 'stable')
                        confidence = trend_data.get('confidence', 0.5)

                        if direction == "ä¸Šå‡" and confidence > 0.7:
                            insights.append(BusinessInsight(
                                title=f"{metric_name}å‘ˆä¸Šå‡è¶‹åŠ¿",
                                summary=f"{metric_name}æ˜¾ç¤ºæ˜æ˜¾çš„ä¸Šå‡è¶‹åŠ¿ï¼Œå‘å±•æ€åŠ¿è‰¯å¥½",
                                confidence_score=confidence,
                                insight_type="positive_trend",
                                recommendations=["ç»§ç»­ä¿æŒå½“å‰ç­–ç•¥", "é€‚åº¦æ‰©å¤§è§„æ¨¡"],
                                supporting_data=trend_data
                            ))
                        elif direction == "ä¸‹é™" and confidence > 0.7:
                            insights.append(BusinessInsight(
                                title=f"{metric_name}å‘ˆä¸‹é™è¶‹åŠ¿",
                                summary=f"{metric_name}æ˜¾ç¤ºä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦å…³æ³¨å¹¶é‡‡å–æªæ–½",
                                confidence_score=confidence,
                                insight_type="negative_trend",
                                recommendations=["åˆ†æä¸‹é™åŸå› ", "åˆ¶å®šæ”¹è¿›æªæ–½", "åŠ å¼ºç›‘æ§"],
                                supporting_data=trend_data
                            ))

        return insights

    async def _generate_calculation_insights(self, calculation_result: Dict[str, Any]) -> List[BusinessInsight]:
        """ğŸ§® ç”Ÿæˆè®¡ç®—æ´å¯Ÿ"""
        insights = []

        calc_type = calculation_result.get('calculation_type', '')
        calc_res = calculation_result.get('calculation_result')

        if not calc_res:
            return insights

        if calc_type == "reinvestment_analysis":
            detailed_results = getattr(calc_res, 'detailed_results', {})
            balance_impact = detailed_results.get("final_balance_impact", 0)

            if balance_impact > 0:
                insights.append(BusinessInsight(
                    title="å¤æŠ•ç­–ç•¥æ•ˆæœç§¯æ",
                    summary=f"å½“å‰å¤æŠ•ç­–ç•¥å°†äº§ç”Ÿç§¯æçš„èµ„é‡‘å¢é•¿æ•ˆæœ",
                    confidence_score=calc_res.confidence,
                    insight_type="positive_reinvestment_impact",
                    recommendations=["ä¿æŒå½“å‰å¤æŠ•æ¯”ä¾‹", "ç›‘æ§å¸‚åœºå˜åŒ–"],
                    supporting_data=detailed_results
                ))

        elif calc_type == "cash_runway":
            detailed_results = getattr(calc_res, 'detailed_results', {})
            runway_analysis = detailed_results.get("runway_analysis", {})
            risk_level = runway_analysis.get("risk_level", "unknown")

            if risk_level == "critical":
                insights.append(BusinessInsight(
                    title="ç°é‡‘æµé£é™©ä¸´ç•Œ",
                    summary="æ ¹æ®ç°é‡‘è·‘é“åˆ†æï¼Œå½“å‰ç°é‡‘æµçŠ¶å†µéœ€è¦ç«‹å³å…³æ³¨",
                    confidence_score=calc_res.confidence,
                    insight_type="critical_cash_flow_risk",
                    recommendations=["ç«‹å³åˆ¶å®šåº”æ€¥é¢„æ¡ˆ", "å¯»æ±‚å¤–éƒ¨èµ„é‡‘æ”¯æŒ"],
                    supporting_data=runway_analysis
                ))

        return insights

    async def _intelligent_visualization_generation(self, extracted_data: Dict[str, Any],
                                                    calculation_result: Optional[Dict[str, Any]],
                                                    query_analysis: QueryAnalysisResult) -> List[Dict[str, Any]]:
        """ğŸ¨ æ™ºèƒ½å¯è§†åŒ–ç”Ÿæˆ - å¢å¼ºç‰ˆ"""
        visualizations = []

        if not self.chart_generator:
            return visualizations

        try:
            strategy = extracted_data.get("extraction_strategy", "")

            # ğŸ¯ æ ¹æ®æ•°æ®ç±»å‹ç”Ÿæˆæ™ºèƒ½å›¾è¡¨
            if strategy == "financial_overview":
                visualizations.extend(await self._generate_financial_visualizations(extracted_data))
            elif strategy == "expiry_analysis":
                visualizations.extend(await self._generate_expiry_visualizations(extracted_data))
            elif strategy == "daily_analysis":
                visualizations.extend(await self._generate_daily_visualizations(extracted_data))
            elif strategy == "trend_analysis":
                visualizations.extend(await self._generate_trend_visualizations(extracted_data, calculation_result))
            elif strategy == "reinvestment_analysis":
                visualizations.extend(await self._generate_reinvestment_visualizations(extracted_data, calculation_result))

        except Exception as e:
            logger.error(f"æ™ºèƒ½å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

        return visualizations

    async def _generate_financial_visualizations(self, extracted_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ğŸ’° ç”Ÿæˆè´¢åŠ¡å¯è§†åŒ–"""
        visualizations = []
        financial_data = extracted_data.get("financial_overview", {})

        # ğŸ¯ èµ„é‡‘æ¦‚è§ˆé¥¼å›¾
        total_balance = financial_data.get("total_balance", 0)
        total_inflow = financial_data.get("total_inflow", 0)
        total_outflow = financial_data.get("total_outflow", 0)

        if total_inflow > 0 and total_outflow > 0:
            chart_data = {
                'labels': ['æ€»ä½™é¢', 'å†å²å…¥é‡‘', 'å†å²å‡ºé‡‘'],
                'values': [total_balance, total_inflow, total_outflow],
                'colors': ['#4CAF50', '#2196F3', '#FF9800']
            }

            visualizations.append({
                'type': 'pie',
                'title': 'èµ„é‡‘æ¦‚è§ˆåˆ†å¸ƒ',
                'data_payload': chart_data,
                'description': 'å½“å‰èµ„é‡‘çŠ¶å†µçš„æ•´ä½“åˆ†å¸ƒ'
            })

        # ğŸ¯ ç”¨æˆ·æ´»è·ƒåº¦æ¡å½¢å›¾
        total_users = financial_data.get("total_users", 0)
        active_users = financial_data.get("active_users", 0)

        if total_users > 0:
            inactive_users = total_users - active_users
            user_chart_data = {
                'labels': ['æ´»è·ƒç”¨æˆ·', 'éæ´»è·ƒç”¨æˆ·'],
                'values': [active_users, inactive_users],
                'colors': ['#4CAF50', '#E0E0E0']
            }

            visualizations.append({
                'type': 'bar',
                'title': 'ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒ',
                'data_payload': user_chart_data,
                'description': 'ç”¨æˆ·æ´»è·ƒçŠ¶å†µåˆ†æ'
            })

        return visualizations

    async def _generate_expiry_visualizations(self, extracted_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """â° ç”Ÿæˆåˆ°æœŸå¯è§†åŒ–"""
        visualizations = []
        expiry_data = extracted_data.get("expiry_analysis", {})
        product_details = expiry_data.get("product_details", [])

        # ğŸ¯ äº§å“åˆ°æœŸé‡‘é¢åˆ†å¸ƒ
        if len(product_details) > 0:
            # æŒ‰é‡‘é¢æ’åºå¹¶å–å‰5å
            sorted_products = sorted(product_details,
                                     key=lambda x: x.get("amount", 0), reverse=True)[:5]

            if sorted_products:
                chart_data = {
                    'labels': [p.get("name", f"äº§å“{i+1}") for i, p in enumerate(sorted_products)],
                    'values': [p.get("amount", 0) for p in sorted_products]
                }

                visualizations.append({
                    'type': 'bar',
                    'title': 'ä¸»è¦äº§å“åˆ°æœŸé‡‘é¢åˆ†å¸ƒ',
                    'data_payload': chart_data,
                    'description': 'æŒ‰åˆ°æœŸé‡‘é¢æ’åºçš„ä¸»è¦äº§å“åˆ†å¸ƒ'
                })

        return visualizations

    async def _generate_reinvestment_visualizations(self, extracted_data: Dict[str, Any],
                                                    calculation_result: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ğŸ’° ç”Ÿæˆå¤æŠ•å¯è§†åŒ–"""
        visualizations = []
        reinvest_data = extracted_data.get("reinvestment_analysis", {})

        # ğŸ¯ å¤æŠ•åˆ†é…é¥¼å›¾
        reinvest_amount = reinvest_data.get("estimated_reinvest_amount", 0)
        withdraw_amount = reinvest_data.get("estimated_withdrawal_amount", 0)

        if reinvest_amount > 0 or withdraw_amount > 0:
            chart_data = {
                'labels': ['å¤æŠ•é‡‘é¢', 'æç°é‡‘é¢'],
                'values': [reinvest_amount, withdraw_amount],
                'colors': ['#4CAF50', '#FF9800']
            }

            visualizations.append({
                'type': 'pie',
                'title': 'å¤æŠ•èµ„é‡‘åˆ†é…',
                'data_payload': chart_data,
                'description': 'å¤æŠ•ç­–ç•¥ä¸‹çš„èµ„é‡‘åˆ†é…æƒ…å†µ'
            })

        return visualizations

    async def _generate_daily_visualizations(self, extracted_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ğŸ“… ç”Ÿæˆæ¯æ—¥æ•°æ®å¯è§†åŒ–"""
        visualizations = []
        daily_data = extracted_data.get("daily_analysis", {})

        # ğŸ¯ å½“æ—¥èµ„é‡‘æµå‘
        inflow = daily_data.get("inflow", 0)
        outflow = daily_data.get("outflow", 0)

        if inflow > 0 or outflow > 0:
            chart_data = {
                'labels': ['å…¥é‡‘', 'å‡ºé‡‘'],
                'values': [inflow, outflow],
                'colors': ['#4CAF50', '#F44336']
            }

            visualizations.append({
                'type': 'bar',
                'title': 'å½“æ—¥èµ„é‡‘æµå‘',
                'data_payload': chart_data,
                'description': 'å½“æ—¥å…¥é‡‘å’Œå‡ºé‡‘å¯¹æ¯”'
            })

        return visualizations

    async def _generate_trend_visualizations(self, extracted_data: Dict[str, Any],
                                             calculation_result: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ğŸ“ˆ ç”Ÿæˆè¶‹åŠ¿å¯è§†åŒ–"""
        visualizations = []

        # ğŸ¯ åŸºäºè®¡ç®—ç»“æœç”Ÿæˆè¶‹åŠ¿å›¾
        if calculation_result and calculation_result.get('success'):
            calc_res = calculation_result.get('calculation_result')
            if calc_res and hasattr(calc_res, 'detailed_results'):
                detailed_results = getattr(calc_res, 'detailed_results', {})

                # æŸ¥æ‰¾æœ‰è¶‹åŠ¿æ•°æ®çš„æŒ‡æ ‡
                for metric_name, trend_data in detailed_results.items():
                    if isinstance(trend_data, dict) and 'data_points' in trend_data:
                        data_points = trend_data.get('data_points', 0)
                        if data_points > 3:  # æœ‰è¶³å¤Ÿæ•°æ®ç‚¹
                            # ç”Ÿæˆè¶‹åŠ¿çº¿å›¾ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                            visualizations.append({
                                'type': 'line',
                                'title': f'{metric_name}è¶‹åŠ¿åˆ†æ',
                                'data_payload': {'message': f'{metric_name}è¶‹åŠ¿åˆ†æå›¾è¡¨'},
                                'description': f'{metric_name}çš„å†å²è¶‹åŠ¿å˜åŒ–'
                            })

        return visualizations

    # ================================================================
    # ğŸ› ï¸ è¾…åŠ©æ–¹æ³•å’Œå·¥å…·å‡½æ•°
    # ================================================================

    def _safe_get_enum(self, enum_class, value: str):
        """å®‰å…¨åœ°è·å–æšä¸¾å€¼"""
        try:
            return enum_class(value)
        except ValueError:
            logger.warning(f"æ— æ•ˆçš„æšä¸¾å€¼ {value} for {enum_class.__name__}, ä½¿ç”¨é»˜è®¤å€¼")
            return list(enum_class)[0]

    def _extract_json_from_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """ä»Claudeå“åº”ä¸­æå–JSON"""
        try:
            return json.loads(response_text.strip())
        except json.JSONDecodeError:
            try:
                import re
                json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group(1))

                brace_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if brace_match:
                    return json.loads(brace_match.group())
            except json.JSONDecodeError:
                pass

        logger.error(f"æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆJSON: {response_text[:300]}")
        return None

    def _validate_claude_response(self, analysis: Dict[str, Any]) -> bool:
        """éªŒè¯Claudeå“åº”çš„å®Œæ•´æ€§"""
        try:
            required_top_fields = ["query_understanding", "execution_plan"]
            for field in required_top_fields:
                if field not in analysis:
                    return False

            understanding = analysis["query_understanding"]
            required_understanding_fields = ["complexity", "query_type", "confidence"]
            for field in required_understanding_fields:
                if field not in understanding:
                    return False

            execution = analysis["execution_plan"]
            required_execution_fields = ["api_calls", "needs_calculation"]
            for field in required_execution_fields:
                if field not in execution:
                    return False

            if not isinstance(execution["api_calls"], list):
                return False

            return True
        except Exception:
            return False

    async def _legacy_data_fetch(self, query_analysis: QueryAnalysisResult) -> Optional[FetcherExecutionResult]:
        """é™çº§æ•°æ®è·å–é€»è¾‘"""
        try:
            api_calls = query_analysis.api_calls_needed

            if not api_calls:
                api_calls = [{"method": "get_system_data", "params": {}, "reason": "é»˜è®¤æ•°æ®è·å–"}]

            @dataclass
            class SimpleAcquisitionPlan:
                plan_id: str = f"plan_{int(time.time())}"
                api_call_plans: List = field(default_factory=list)
                parallel_groups: List = field(default_factory=list)
                total_estimated_time: float = 30.0
                data_requirements: List = field(default_factory=list)

            acquisition_plan = SimpleAcquisitionPlan()

            for api_call in api_calls:
                call_plan = type('CallPlan', (), {
                    'call_method': api_call.get('method', 'get_system_data'),
                    'parameters': api_call.get('params', {}),
                    'reason': api_call.get('reason', 'æ•°æ®è·å–'),
                    'priority': type('Priority', (), {'value': 'normal'})(),
                    'retry_strategy': 'default'
                })()

                acquisition_plan.api_call_plans.append(call_plan)

            execution_result = await self.data_fetcher.execute_data_acquisition_plan(acquisition_plan)
            return execution_result

        except Exception as e:
            logger.error(f"é™çº§æ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def _determine_intelligent_strategy(self, query_analysis: QueryAnalysisResult) -> ProcessingStrategy:
        """ç¡®å®šæ™ºèƒ½å¤„ç†ç­–ç•¥"""
        if query_analysis.needs_calculation:
            return ProcessingStrategy.DATA_WITH_CALC
        elif query_analysis.complexity in [QueryComplexity.COMPLEX, QueryComplexity.EXPERT]:
            return ProcessingStrategy.COMPREHENSIVE
        else:
            return ProcessingStrategy.SIMPLE_DATA

    def _get_intelligent_processors_used(self, query_analysis: QueryAnalysisResult,
                                         calculation_result: Optional[Dict[str, Any]]) -> List[str]:
        """è·å–ä½¿ç”¨çš„æ™ºèƒ½å¤„ç†å™¨"""
        processors = ["IntelligentClaude", "SmartDataFetcher", "IntelligentExtractor"]

        if query_analysis.needs_calculation and calculation_result:
            processors.append("UnifiedCalculator")

        return processors

    def _calculate_intelligent_confidence(self, query_analysis: QueryAnalysisResult,
                                          extracted_data: Dict[str, Any],
                                          calculation_result: Optional[Dict[str, Any]],
                                          insights: List[BusinessInsight]) -> float:
        """è®¡ç®—æ™ºèƒ½ç½®ä¿¡åº¦"""
        confidence_factors = []

        # æŸ¥è¯¢ç†è§£ç½®ä¿¡åº¦
        confidence_factors.append(query_analysis.confidence_score)

        # æ•°æ®æå–è´¨é‡
        data_quality = extracted_data.get("data_quality", 0.5)
        confidence_factors.append(data_quality)

        # è®¡ç®—ç½®ä¿¡åº¦
        if calculation_result and calculation_result.get('success'):
            calc_confidence = calculation_result.get('confidence', 0.5)
            confidence_factors.append(calc_confidence)

        # æ´å¯Ÿè´¨é‡
        if insights:
            insight_confidence = sum(insight.confidence_score for insight in insights) / len(insights)
            confidence_factors.append(insight_confidence)

        # ç­–ç•¥åŒ¹é…åº¦
        strategy = extracted_data.get("extraction_strategy", "")
        if strategy != "comprehensive":  # ç²¾ç¡®ç­–ç•¥åŒ¹é…
            confidence_factors.append(0.9)
        else:
            confidence_factors.append(0.7)

        return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5

    def _calculate_data_quality_score(self, extracted_data: Dict[str, Any]) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        base_quality = extracted_data.get("data_quality", 0.8)

        # æ ¹æ®æå–ç­–ç•¥è°ƒæ•´è´¨é‡åˆ†æ•°
        strategy = extracted_data.get("extraction_strategy", "")
        if strategy in ["financial_overview", "expiry_analysis", "reinvestment_analysis"]:
            # è¿™äº›ç­–ç•¥é€šå¸¸æœ‰æ›´å‡†ç¡®çš„æ•°æ®æå–
            return min(base_quality + 0.1, 1.0)

        return base_quality

    def _calculate_intelligent_completeness(self, extracted_data: Dict[str, Any],
                                            calculation_result: Optional[Dict[str, Any]],
                                            insights: List[BusinessInsight]) -> float:
        """è®¡ç®—æ™ºèƒ½å®Œæ•´æ€§"""
        completeness = 0.0

        # æ•°æ®æå–å®Œæ•´æ€§
        if extracted_data.get("status") == "æ•°æ®æå–æˆåŠŸ":
            completeness += 0.4

        # è®¡ç®—å®Œæ•´æ€§
        if calculation_result and calculation_result.get('success'):
            completeness += 0.3

        # æ´å¯Ÿå®Œæ•´æ€§
        if insights:
            completeness += 0.3

        return min(completeness, 1.0)

    def _extract_intelligent_metrics(self, extracted_data: Dict[str, Any],
                                     calculation_result: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–æ™ºèƒ½å…³é”®æŒ‡æ ‡"""
        metrics = {}

        # ğŸ¯ ä»æå–æ•°æ®ä¸­è·å–æ ¸å¿ƒæŒ‡æ ‡
        strategy = extracted_data.get("extraction_strategy", "")

        if strategy == "financial_overview":
            financial_data = extracted_data.get("financial_overview", {})
            metrics.update({
                'æ€»ä½™é¢': financial_data.get("total_balance", 0),
                'å‡€æµå…¥': financial_data.get("net_flow", 0),
                'æ´»è·ƒç”¨æˆ·æ•°': financial_data.get("active_users", 0)
            })

        elif strategy == "expiry_analysis":
            expiry_data = extracted_data.get("expiry_analysis", {})
            metrics.update({
                'åˆ°æœŸé‡‘é¢': expiry_data.get("expiry_amount", 0),
                'åˆ°æœŸæ•°é‡': expiry_data.get("expiry_count", 0)
            })

        elif strategy == "reinvestment_analysis":
            reinvest_data = extracted_data.get("reinvestment_analysis", {})
            metrics.update({
                'å¤æŠ•é‡‘é¢': reinvest_data.get("estimated_reinvest_amount", 0),
                'æç°é‡‘é¢': reinvest_data.get("estimated_withdrawal_amount", 0)
            })

        # ğŸ¯ ä»è®¡ç®—ç»“æœä¸­è·å–å…³é”®æŒ‡æ ‡
        if calculation_result and calculation_result.get('success'):
            calc_res = calculation_result.get('calculation_result')
            if calc_res:
                if hasattr(calc_res, 'primary_result'):
                    metrics['è®¡ç®—ä¸»ç»“æœ'] = calc_res.primary_result
                if hasattr(calc_res, 'detailed_results'):
                    detailed = calc_res.detailed_results
                    if isinstance(detailed, dict):
                        metrics.update(detailed)

        return metrics

    def _get_intelligent_ai_summary(self, timing: Dict[str, float]) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½AIåä½œæ‘˜è¦"""
        return {
            'claude_used': self.claude_client is not None,
            'gpt_used': self.gpt_client is not None,
            'total_ai_time': timing.get('parsing', 0) + timing.get('response_generation', 0),
            'data_extraction_time': timing.get('data_extraction', 0),
            'intelligence_level': 'enhanced',
            'extraction_strategy_applied': True,
            'architecture': 'intelligent_claude_dominant'
        }

    # ================================================================
    # ğŸ¯ ä¿ç•™çš„å¿…è¦æ–¹æ³• (ç»§æ‰¿è‡ªåŸç‰ˆæœ¬)
    # ================================================================

    def _parse_conversation_id(self, conversation_id: Optional[str]) -> Optional[int]:
        """è§£æå¯¹è¯ID"""
        if not conversation_id:
            return None
        try:
            return int(conversation_id)
        except ValueError:
            logger.warning(f"æ— æ•ˆçš„å¯¹è¯ID: {conversation_id}")
            return None

    async def _save_user_message_if_needed(self, conversation_id_for_db: Optional[int],
                                           user_query: str, query_id: str) -> bool:
        """ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if not conversation_id_for_db or not self.conversation_manager:
            return False

        try:
            recent_messages = getattr(self.conversation_manager, 'get_recent_messages', lambda *args: [])(
                conversation_id_for_db, 2)

            for msg in recent_messages:
                if isinstance(msg, dict):
                    is_user = msg.get('is_user', False)
                    content = msg.get('content', '')
                    if is_user and content.strip() == user_query.strip():
                        return False

            self.conversation_manager.add_message(conversation_id_for_db, True, user_query)
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
            return False

    async def _save_ai_response_if_needed(self, conversation_id_for_db: int,
                                          result: ProcessingResult, query_id: str):
        """ä¿å­˜AIå“åº”ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if not self.conversation_manager:
            return

        try:
            ai_message_id = self.conversation_manager.add_message(
                conversation_id_for_db, False, result.response_text)

            if result.visualizations and hasattr(self.conversation_manager, 'add_visual'):
                for i, vis in enumerate(result.visualizations):
                    if isinstance(vis, dict):
                        self.conversation_manager.add_visual(
                            message_id=ai_message_id,
                            visual_type=vis.get('type', 'chart'),
                            visual_order=i,
                            title=vis.get('title', 'å›¾è¡¨'),
                            data=vis.get('data_payload', {})
                        )
        except Exception as e:
            logger.error(f"ä¿å­˜AIå“åº”å¤±è´¥: {e}")

    def _default_stats(self) -> Dict[str, Any]:
        """é»˜è®¤ç»Ÿè®¡"""
        return {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'quick_response_queries': 0,
            'complex_calculation_queries': 0,
            'avg_processing_time': 0.0,
            'avg_confidence_score': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'intelligent_extractions': 0
        }

    def _update_stats(self, result: ProcessingResult):
        """æ›´æ–°ç»Ÿè®¡"""
        if result.success:
            self.orchestrator_stats['successful_queries'] += 1
        else:
            self.orchestrator_stats['failed_queries'] += 1

        # ç‰¹æ®Šç»Ÿè®¡
        if result.processing_strategy == ProcessingStrategy.QUICK_RESPONSE:
            self.orchestrator_stats['quick_response_queries'] += 1
        elif result.processing_strategy == ProcessingStrategy.DATA_WITH_CALC:
            self.orchestrator_stats['complex_calculation_queries'] += 1

        if 'extraction_strategy' in result.processing_metadata:
            self.orchestrator_stats['intelligent_extractions'] += 1

        # æ›´æ–°å¹³å‡å€¼
        total = self.orchestrator_stats['total_queries']
        if total > 0:
            current_avg_time = self.orchestrator_stats['avg_processing_time']
            self.orchestrator_stats['avg_processing_time'] = (
                    (current_avg_time * (total - 1) + result.total_processing_time) / total)

            current_avg_conf = self.orchestrator_stats['avg_confidence_score']
            self.orchestrator_stats['avg_confidence_score'] = (
                    (current_avg_conf * (total - 1) + result.confidence_score) / total)

    async def _cache_result_if_appropriate(self, result: ProcessingResult):
        """ç¼“å­˜ç»“æœï¼ˆå¦‚æœåˆé€‚ï¼‰"""
        if (self.config.get('enable_intelligent_caching', True) and
                result.success and result.confidence_score > 0.7):
            cache_key = f"result_{result.query_id}"
            self.result_cache[cache_key] = {
                'data': result,
                'timestamp': time.time()
            }

    def _create_error_result(self, error_msg: str, user_query: str,
                             query_id: str = None) -> ProcessingResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return ProcessingResult(
            session_id=str(uuid.uuid4()),
            query_id=query_id or f"error_{int(time.time())}",
            success=False,
            response_text=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶é‡åˆ°é—®é¢˜ï¼š{error_msg}",
            processing_strategy=ProcessingStrategy.ERROR_HANDLING,
            processors_used=["ErrorHandler"],
            error_info={"message": error_msg, "query": user_query}
        )

    async def _handle_error(self, session_id: str, query_id: str, user_query: str,
                            error_msg: str, processing_time: float,
                            conversation_id: Optional[str]) -> ProcessingResult:
        """å¤„ç†é”™è¯¯"""
        self.orchestrator_stats['failed_queries'] += 1

        return ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            response_text=f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯ã€‚æˆ‘ä»¬å·²è®°å½•æ­¤é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            processing_strategy=ProcessingStrategy.ERROR_HANDLING,
            processors_used=["ErrorHandler"],
            total_processing_time=processing_time,
            error_info={"message": error_msg, "query": user_query},
            conversation_id=conversation_id
        )

    # ================================================================
    # ğŸ”§ æ¥å£æ–¹æ³• (ä¿æŒå…¼å®¹æ€§)
    # ================================================================

    def get_orchestrator_stats(self) -> Dict[str, Any]:
        """è·å–ç¼–æ’å™¨ç»Ÿè®¡"""
        stats = self.orchestrator_stats.copy()
        total = stats.get('total_queries', 0)
        if total > 0:
            stats['success_rate'] = stats.get('successful_queries', 0) / total
            stats['failure_rate'] = stats.get('failed_queries', 0) / total
            stats['quick_response_rate'] = stats.get('quick_response_queries', 0) / total
            stats['intelligent_extraction_rate'] = stats.get('intelligent_extractions', 0) / total
        return stats

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        if not self.initialized:
            try:
                await self.initialize()
            except Exception as e:
                return {
                    'status': 'unhealthy',
                    'reason': 'Initialization failed',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }

        return {
            'status': 'healthy',
            'architecture': 'intelligent_enhanced',
            'version': self.config.get('version', '3.0.0-intelligent'),
            'components': {
                'claude_available': self.claude_client is not None,
                'gpt_available': self.gpt_client is not None,
                'query_parser_ready': self.query_parser is not None,
                'data_fetcher_ready': self.data_fetcher is not None,
                'calculator_ready': self.statistical_calculator is not None,
                'date_utils_ready': self.date_utils is not None,
                'formatter_ready': self.financial_formatter is not None
            },
            'intelligent_features': {
                'smart_extraction': True,
                'intelligent_calculation': True,
                'enhanced_insights': True,
                'adaptive_visualization': True
            },
            'statistics': self.get_orchestrator_stats(),
            'timestamp': datetime.now().isoformat()
        }

    async def close(self):
        """å…³é—­ç¼–æ’å™¨"""
        logger.info("å…³é—­æ™ºèƒ½å¢å¼ºç‰ˆç¼–æ’å™¨...")

        if self.data_fetcher and hasattr(self.data_fetcher, 'close'):
            await self.data_fetcher.close()

        if self.db_connector and hasattr(self.db_connector, 'close'):
            await self.db_connector.close()

        self.result_cache.clear()
        self.extraction_cache.clear()
        self.initialized = False

        logger.info("æ™ºèƒ½å¢å¼ºç‰ˆç¼–æ’å™¨å·²å…³é—­")


# ================================================================
# ğŸ­ å·¥å‚å‡½æ•°å’Œå…¨å±€å®ä¾‹ç®¡ç†
# ================================================================

_orchestrator_instance: Optional[IntelligentQAOrchestrator] = None


def get_orchestrator(claude_client_instance: Optional[ClaudeClient] = None,
                     gpt_client_instance: Optional[OpenAIClient] = None,
                     db_connector_instance: Optional[DatabaseConnector] = None,
                     app_config_instance: Optional[AppConfig] = None) -> IntelligentQAOrchestrator:
    """è·å–æ™ºèƒ½ç¼–æ’å™¨å®ä¾‹"""
    global _orchestrator_instance

    if _orchestrator_instance is None:
        logger.info("åˆ›å»ºæ–°çš„æ™ºèƒ½å¢å¼ºç‰ˆç¼–æ’å™¨å®ä¾‹")
        _orchestrator_instance = IntelligentQAOrchestrator(
            claude_client_instance, gpt_client_instance,
            db_connector_instance, app_config_instance
        )

    return _orchestrator_instance


# ================================================================
# ğŸ¯ ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
# ================================================================

async def test_intelligent_orchestrator():
    """æµ‹è¯•æ™ºèƒ½ç¼–æ’å™¨"""
    try:
        orchestrator = get_orchestrator()
        await orchestrator.initialize()

        test_queries = [
            "ä»Šå¤©æ€»èµ„é‡‘å¤šå°‘",  # å¿«é€Ÿå“åº”
            "6æœˆ1æ—¥çš„æœ‰å¤šå°‘äº§å“åˆ°æœŸï¼Œæ€»èµ„é‡‘å¤šå°‘",  # æ™ºèƒ½ç»„åˆ
            "6æœˆ1æ—¥è‡³6æœˆ30æ—¥äº§å“åˆ°æœŸé‡‘é¢æ˜¯å¤šå°‘ï¼Œå¦‚æœä½¿ç”¨25%å¤æŠ•ï¼Œ7æœˆ1æ—¥å‰©ä½™èµ„é‡‘æœ‰å¤šå°‘",  # å¤æ‚è®¡ç®—
            "å‡è®¾ç°åœ¨æ²¡å…¥é‡‘çš„æƒ…å†µå…¬å¸è¿˜èƒ½è¿è¡Œå¤šä¹…"  # ç°é‡‘è·‘é“åˆ†æ
        ]

        for query in test_queries:
            print(f"\nğŸ§  æµ‹è¯•æŸ¥è¯¢: {query}")
            result = await orchestrator.process_intelligent_query(query)
            print(f"âœ… æˆåŠŸ: {result.success}")
            print(f"ğŸ“Š ç­–ç•¥: {result.processing_strategy.value}")
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
            print(f"ğŸ’¬ å›ç­”: {result.response_text[:200]}...")

        await orchestrator.close()

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(test_intelligent_orchestrator())