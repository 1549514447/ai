# core/orchestrator/intelligent_qa_orchestrator.py
"""
ğŸš€ AIé©±åŠ¨çš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨
æ•´ä¸ªé‡‘èAIåˆ†æç³»ç»Ÿçš„æ ¸å¿ƒå¤§è„‘ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰ç»„ä»¶ååŒå·¥ä½œ

æ ¸å¿ƒç‰¹ç‚¹:
- ğŸ§  AIä¼˜å…ˆçš„æ™ºèƒ½è·¯ç”±ä¸å†³ç­–
- âš¡ åŒAIåä½œçš„æ·±åº¦åˆ†ææµç¨‹  
- ğŸ”„ æ™ºèƒ½é™çº§ä¸å®¹é”™å¤„ç†
- ğŸ“Š å®Œæ•´çš„æ€§èƒ½ç›‘æ§ä¸ç»Ÿè®¡
- ğŸ¯ ä¸šåŠ¡å¯¼å‘çš„ç»“æœè¾“å‡º
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import asyncio
import json
import time
import uuid
from dataclasses import dataclass
from enum import Enum
from functools import lru_cache
import hashlib

# å¯¼å…¥æ‰€æœ‰å·²å®Œæˆçš„æ ¸å¿ƒç»„ä»¶
from core.analyzers.query_parser import SmartQueryParser, create_smart_query_parser
from core.analyzers.data_requirements_analyzer import DataRequirementsAnalyzer, create_data_requirements_analyzer
from core.analyzers.financial_data_analyzer import FinancialDataAnalyzer, create_financial_data_analyzer
from core.analyzers.insight_generator import InsightGenerator, create_insight_generator
from core.data_orchestration.smart_data_fetcher import SmartDataFetcher, create_smart_data_fetcher
from core.processors.current_data_processor import CurrentDataProcessor
from core.processors.historical_analysis_processor import HistoricalAnalysisProcessor
from core.processors.prediction_processor import PredictionProcessor

logger = logging.getLogger(__name__)


class QueryComplexity(Enum):
    """æŸ¥è¯¢å¤æ‚åº¦ç­‰çº§"""
    SIMPLE = "simple"
    MEDIUM = "medium" 
    COMPLEX = "complex"
    EXPERT = "expert"


class ProcessingStrategy(Enum):
    """å¤„ç†ç­–ç•¥"""
    DIRECT_RESPONSE = "direct_response"  # ç›´æ¥å“åº”
    SINGLE_PROCESSOR = "single_processor"  # å•å¤„ç†å™¨
    MULTI_PROCESSOR = "multi_processor"  # å¤šå¤„ç†å™¨åä½œ
    FULL_PIPELINE = "full_pipeline"  # å®Œæ•´æµæ°´çº¿


@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœæ•°æ®ç±»"""
    session_id: str  # ä¼šè¯ID
    query_id: str  # æŸ¥è¯¢ID
    success: bool  # å¤„ç†æ˜¯å¦æˆåŠŸ
    
    # æ ¸å¿ƒç»“æœ
    response_text: str  # ä¸»è¦å“åº”æ–‡æœ¬
    insights: List[Dict[str, Any]]  # ä¸šåŠ¡æ´å¯Ÿ
    key_metrics: Dict[str, Any]  # å…³é”®æŒ‡æ ‡
    visualizations: List[Dict[str, Any]]  # å¯è§†åŒ–æ•°æ®
    
    # å¤„ç†ä¿¡æ¯
    processing_strategy: ProcessingStrategy  # ä½¿ç”¨çš„å¤„ç†ç­–ç•¥
    processors_used: List[str]  # ä½¿ç”¨çš„å¤„ç†å™¨
    ai_collaboration_summary: Dict[str, Any]  # AIåä½œæ‘˜è¦
    
    # è´¨é‡æŒ‡æ ‡
    confidence_score: float  # æ•´ä½“ç½®ä¿¡åº¦
    data_quality_score: float  # æ•°æ®è´¨é‡è¯„åˆ†
    response_completeness: float  # å“åº”å®Œæ•´æ€§
    
    # æ€§èƒ½æŒ‡æ ‡
    total_processing_time: float  # æ€»å¤„ç†æ—¶é—´
    ai_processing_time: float  # AIå¤„ç†æ—¶é—´
    data_fetching_time: float  # æ•°æ®è·å–æ—¶é—´
    
    # å…ƒæ•°æ®
    processing_metadata: Dict[str, Any]  # å¤„ç†å…ƒæ•°æ®
    error_info: Optional[Dict[str, Any]]  # é”™è¯¯ä¿¡æ¯
    conversation_id: Optional[str]  # å¯¹è¯ID
    timestamp: str  # å¤„ç†æ—¶é—´æˆ³


class IntelligentQAOrchestrator:
    """
    ğŸš€ AIé©±åŠ¨çš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨
    
    ç³»ç»Ÿæ ¸å¿ƒå¤§è„‘ï¼Œåè°ƒæ‰€æœ‰AIç»„ä»¶ååŒå·¥ä½œï¼Œ
    æä¾›æ™ºèƒ½çš„æŸ¥è¯¢ç†è§£ã€æ•°æ®è·å–ã€åˆ†æå¤„ç†å’Œæ´å¯Ÿç”Ÿæˆ
    """
    
    def __init__(self, claude_client=None, gpt_client=None, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½ç¼–æ’å™¨
        
        Args:
            claude_client: Claudeå®¢æˆ·ç«¯
            gpt_client: GPTå®¢æˆ·ç«¯  
            config: é…ç½®å‚æ•°
        """
        self.claude_client = claude_client
        self.gpt_client = gpt_client
        self.config = config or self._load_default_config()
        
        # åˆå§‹åŒ–æ ‡å¿—
        self.initialized = False
        
        # æ ¸å¿ƒç»„ä»¶ - å»¶è¿Ÿåˆå§‹åŒ–
        self.query_parser = None
        self.data_requirements_analyzer = None
        self.financial_data_analyzer = None
        self.insight_generator = None
        self.data_fetcher = None
        
        # ä¸šåŠ¡å¤„ç†å™¨
        self.current_data_processor = None
        self.historical_analysis_processor = None
        self.prediction_processor = None
        
        # ä¼šè¯ç®¡ç†
        self.active_sessions = {}
        self.conversation_manager = ConversationManager()
        
        # æ€§èƒ½ç›‘æ§
        self.orchestrator_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'avg_processing_time': 0.0,
            'avg_confidence_score': 0.0,
            'ai_collaboration_count': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'processor_usage': {},
            'error_types': {}
        }
        
        # æ™ºèƒ½ç¼“å­˜
        self.result_cache = {}
        self.cache_ttl = self.config.get('cache_ttl', 1800)  # 30åˆ†é’Ÿ
        
        logger.info("IntelligentQAOrchestrator created, waiting for initialization")
    
    def _load_default_config(self) -> Dict[str, Any]:
        """åŠ è½½é»˜è®¤é…ç½®"""
        return {
            # å¤„ç†æ§åˆ¶
            'max_processing_time': 120,  # æœ€å¤§å¤„ç†æ—¶é—´(ç§’)
            'enable_parallel_processing': True,
            'enable_intelligent_caching': True,
            'enable_ai_collaboration': True,
            
            # è´¨é‡æ§åˆ¶
            'min_confidence_threshold': 0.6,
            'enable_result_validation': True,
            'enable_quality_monitoring': True,
            
            # æ€§èƒ½ä¼˜åŒ–
            'cache_ttl': 1800,  # ç¼“å­˜æ—¶é—´
            'max_concurrent_queries': 50,
            'enable_smart_routing': True,
            
            # AIé…ç½®
            'claude_timeout': 30,
            'gpt_timeout': 20,
            'max_ai_retries': 2,
            
            # é™çº§æ§åˆ¶
            'enable_graceful_degradation': True,
            'fallback_response_enabled': True
        }
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        if self.initialized:
            return
            
        try:
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ™ºèƒ½ç¼–æ’å™¨...")
            
            # åˆå§‹åŒ–æ ¸å¿ƒåˆ†æç»„ä»¶
            self.query_parser = create_smart_query_parser(self.claude_client, self.gpt_client)
            self.data_requirements_analyzer = create_data_requirements_analyzer(self.claude_client, self.gpt_client)
            self.financial_data_analyzer = create_financial_data_analyzer(self.claude_client, self.gpt_client)
            self.insight_generator = create_insight_generator(self.claude_client, self.gpt_client)
            self.data_fetcher = create_smart_data_fetcher(self.claude_client, self.gpt_client, self.config)
            
            # åˆå§‹åŒ–ä¸šåŠ¡å¤„ç†å™¨
            self.current_data_processor = CurrentDataProcessor(self.claude_client, self.gpt_client)
            self.historical_analysis_processor = HistoricalAnalysisProcessor(self.claude_client, self.gpt_client)
            self.prediction_processor = PredictionProcessor(self.claude_client, self.gpt_client)
            
            # ä¸ºå¤„ç†å™¨æ³¨å…¥ä¾èµ–
            self._inject_dependencies()
            
            self.initialized = True
            logger.info("âœ… æ™ºèƒ½ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½ç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _inject_dependencies(self):
        """ä¸ºå„å¤„ç†å™¨æ³¨å…¥ä¾èµ–ç»„ä»¶"""
        # ä¸ºå¤„ç†å™¨æ³¨å…¥APIè¿æ¥å™¨
        api_connector = getattr(self.data_fetcher, 'api_connector', None)
        if api_connector:
            self.current_data_processor.api_connector = api_connector
            self.historical_analysis_processor.api_connector = api_connector  
            self.prediction_processor.api_connector = api_connector
    
    # ============= æ ¸å¿ƒç¼–æ’æ–¹æ³• =============
    
    async def process_intelligent_query(self, user_query: str, user_id: Optional[int] = None,
                                       conversation_id: Optional[str] = None,
                                       preferences: Dict[str, Any] = None) -> ProcessingResult:
        """
        ğŸ¯ æ™ºèƒ½æŸ¥è¯¢å¤„ç† - æ ¸å¿ƒå…¥å£æ–¹æ³•
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            user_id: ç”¨æˆ·ID
            conversation_id: å¯¹è¯ID
            preferences: ç”¨æˆ·åå¥½è®¾ç½®
            
        Returns:
            ProcessingResult: å®Œæ•´çš„å¤„ç†ç»“æœ
        """
        # ç¡®ä¿å·²åˆå§‹åŒ–
        if not self.initialized:
            await self.initialize()
        
        session_id = str(uuid.uuid4())
        query_id = f"query_{int(time.time())}"
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ§  å¼€å§‹æ™ºèƒ½æŸ¥è¯¢å¤„ç†: {user_query}")
            self.orchestrator_stats['total_queries'] += 1
            
            # ğŸ” Step 1: æ™ºèƒ½æŸ¥è¯¢è§£æä¸ç†è§£
            parsing_start = time.time()
            query_analysis = await self._intelligent_query_parsing(user_query, conversation_id)
            parsing_time = time.time() - parsing_start
            
            # ğŸ¯ Step 2: æ™ºèƒ½è·¯ç”±å†³ç­–
            routing_strategy = await self._determine_processing_strategy(query_analysis, preferences)
            
            # ğŸ“Š Step 3: æ•°æ®éœ€æ±‚åˆ†æä¸è·å–
            data_start = time.time()
            data_acquisition_result = await self._orchestrate_data_acquisition(query_analysis, routing_strategy)
            data_time = time.time() - data_start
            
            # ğŸ”¬ Step 4: æ™ºèƒ½ä¸šåŠ¡å¤„ç†
            processing_start = time.time()
            business_result = await self._orchestrate_business_processing(
                query_analysis, data_acquisition_result, routing_strategy
            )
            processing_time = time.time() - processing_start
            
            # ğŸ’¡ Step 5: AIæ´å¯Ÿç”Ÿæˆä¸æ•´åˆ
            insight_start = time.time()
            insights = await self._orchestrate_insight_generation(
                business_result, query_analysis, data_acquisition_result
            )
            insight_time = time.time() - insight_start
            
            # ğŸ“ Step 6: å“åº”æ ¼å¼åŒ–ä¸ä¼˜åŒ–
            response_text = await self._generate_intelligent_response(
                user_query, query_analysis, business_result, insights
            )
            
            # ğŸ“Š Step 7: å¯è§†åŒ–æ•°æ®ç”Ÿæˆ
            visualizations = await self._generate_visualizations(business_result, query_analysis)
            
            total_time = time.time() - start_time
            
            # æ„å»ºæˆåŠŸç»“æœ
            result = ProcessingResult(
                session_id=session_id,
                query_id=query_id,
                success=True,
                
                response_text=response_text,
                insights=[insight.__dict__ if hasattr(insight, '__dict__') else insight for insight in insights],
                key_metrics=self._extract_key_metrics(business_result),
                visualizations=visualizations,
                
                processing_strategy=routing_strategy,
                processors_used=self._get_processors_used(routing_strategy),
                ai_collaboration_summary=self._get_ai_collaboration_summary(query_analysis, business_result),
                
                confidence_score=self._calculate_overall_confidence(query_analysis, business_result, insights),
                data_quality_score=data_acquisition_result.get('data_quality_score', 0.8),
                response_completeness=self._calculate_response_completeness(business_result, insights),
                
                total_processing_time=total_time,
                ai_processing_time=parsing_time + insight_time,
                data_fetching_time=data_time,
                
                processing_metadata={
                    'query_complexity': query_analysis.complexity.value,
                    'query_type': query_analysis.query_type.value,
                    'business_scenario': query_analysis.business_scenario.value,
                    'data_sources_used': data_acquisition_result.get('data_sources_used', []),
                    'processing_steps': ['parsing', 'routing', 'data_acquisition', 'business_processing', 'insight_generation', 'response_formatting'],
                    'ai_models_used': ['claude', 'gpt'] if self.claude_client and self.gpt_client else []
                },
                error_info=None,
                conversation_id=conversation_id,
                timestamp=datetime.now().isoformat()
            )
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_orchestrator_stats(result)
            
            # ç¼“å­˜ç»“æœ
            if self.config.get('enable_intelligent_caching', True):
                await self._cache_result(user_query, result)
            
            # æ›´æ–°å¯¹è¯å†å²
            if conversation_id:
                self.conversation_manager.add_exchange(conversation_id, user_query, result)
            
            logger.info(f"âœ… æ™ºèƒ½æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’ï¼Œç½®ä¿¡åº¦: {result.confidence_score:.2f}")
            return result
            
        except Exception as e:
            # é”™è¯¯å¤„ç†å’Œé™çº§
            logger.error(f"âŒ æ™ºèƒ½æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
            return await self._handle_processing_error(
                session_id, query_id, user_query, str(e), time.time() - start_time
            )
    
    # ============= æ™ºèƒ½è§£æä¸è·¯ç”± =============
    
    async def _intelligent_query_parsing(self, user_query: str, conversation_id: Optional[str] = None):
        """AIé©±åŠ¨çš„æ™ºèƒ½æŸ¥è¯¢è§£æ"""
        try:
            # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
            context = {}
            if conversation_id:
                context = self.conversation_manager.get_context(conversation_id)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_query_cache_key(user_query, context)
            cached_result = self._get_cached_parsing_result(cache_key)
            if cached_result:
                self.orchestrator_stats['cache_hits'] += 1
                return cached_result
            
            # AIè§£æ
            query_analysis = await self.query_parser.parse_complex_query(user_query, context)
            
            # ç¼“å­˜è§£æç»“æœ
            self._cache_parsing_result(cache_key, query_analysis)
            self.orchestrator_stats['cache_misses'] += 1
            
            return query_analysis
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è§£æå¤±è´¥: {str(e)}")
            # é™çº§åˆ°åŸºç¡€è§£æ
            return await self._fallback_query_parsing(user_query)
    
    async def _determine_processing_strategy(self, query_analysis, preferences: Dict[str, Any] = None) -> ProcessingStrategy:
        """æ™ºèƒ½ç¡®å®šå¤„ç†ç­–ç•¥"""
        try:
            complexity = query_analysis.complexity
            query_type = query_analysis.query_type
            
            # ğŸ§  ä½¿ç”¨AIå†³å®šå¤„ç†ç­–ç•¥
            if self.claude_client and self.config.get('enable_ai_collaboration', True):
                strategy = await self._ai_determine_strategy(query_analysis, preferences)
                if strategy:
                    return strategy
            
            # åŸºäºè§„åˆ™çš„ç­–ç•¥å†³ç­–
            if complexity.value == "simple":
                return ProcessingStrategy.DIRECT_RESPONSE
            elif complexity.value == "medium":
                return ProcessingStrategy.SINGLE_PROCESSOR
            elif complexity.value == "complex":
                return ProcessingStrategy.MULTI_PROCESSOR
            else:  # expert
                return ProcessingStrategy.FULL_PIPELINE
                
        except Exception as e:
            logger.error(f"ç­–ç•¥å†³ç­–å¤±è´¥: {str(e)}")
            return ProcessingStrategy.SINGLE_PROCESSOR
    
    async def _ai_determine_strategy(self, query_analysis, preferences) -> Optional[ProcessingStrategy]:
        """AIè¾…åŠ©ç­–ç•¥å†³ç­–"""
        try:
            prompt = f"""
            ä½œä¸ºæ™ºèƒ½ç³»ç»Ÿæ¶æ„å¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢åˆ†æé€‰æ‹©æœ€ä½³å¤„ç†ç­–ç•¥ï¼š

            æŸ¥è¯¢åˆ†æç»“æœ:
            - å¤æ‚åº¦: {query_analysis.complexity.value}
            - æŸ¥è¯¢ç±»å‹: {query_analysis.query_type.value}
            - ä¸šåŠ¡åœºæ™¯: {query_analysis.business_scenario.value}
            - é¢„ä¼°æ—¶é—´: {query_analysis.estimated_total_time}ç§’
            - ç½®ä¿¡åº¦: {query_analysis.confidence_score}

            ç”¨æˆ·åå¥½: {json.dumps(preferences or {}, ensure_ascii=False)}

            å¯é€‰ç­–ç•¥:
            1. direct_response - ç›´æ¥å“åº”(ç®€å•æŸ¥è¯¢)
            2. single_processor - å•å¤„ç†å™¨(æ ‡å‡†åˆ†æ)  
            3. multi_processor - å¤šå¤„ç†å™¨åä½œ(å¤æ‚åˆ†æ)
            4. full_pipeline - å®Œæ•´æµæ°´çº¿(ä¸“å®¶çº§åˆ†æ)

            è¯·é€‰æ‹©æœ€ä¼˜ç­–ç•¥ï¼Œè€ƒè™‘æ•ˆç‡ã€å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒã€‚
            åªè¿”å›ç­–ç•¥åç§°ï¼Œå¦‚: "single_processor"
            """
            
            result = await self.claude_client.analyze_complex_query(prompt, {
                "query_analysis": query_analysis.__dict__ if hasattr(query_analysis, '__dict__') else query_analysis,
                "preferences": preferences
            })
            
            if result.get('success'):
                strategy_name = result.get('response', '').strip().lower()
                strategy_map = {
                    'direct_response': ProcessingStrategy.DIRECT_RESPONSE,
                    'single_processor': ProcessingStrategy.SINGLE_PROCESSOR,
                    'multi_processor': ProcessingStrategy.MULTI_PROCESSOR,
                    'full_pipeline': ProcessingStrategy.FULL_PIPELINE
                }
                return strategy_map.get(strategy_name)
                
        except Exception as e:
            logger.warning(f"AIç­–ç•¥å†³ç­–å¤±è´¥: {str(e)}")
            
        return None
    
    # ============= æ•°æ®ç¼–æ’ =============
    
    async def _orchestrate_data_acquisition(self, query_analysis, strategy: ProcessingStrategy) -> Dict[str, Any]:
        """ç¼–æ’æ•°æ®è·å–æµç¨‹"""
        try:
            # ğŸ¯ Step 1: åˆ†ææ•°æ®éœ€æ±‚
            data_plan = await self.data_requirements_analyzer.analyze_data_requirements(query_analysis)
            
            # ğŸš€ Step 2: æ‰§è¡Œæ•°æ®è·å–
            data_result = await self.data_fetcher.execute_data_acquisition_plan(data_plan)
            
            return {
                'success': True,
                'data_plan': data_plan,
                'execution_result': data_result,
                'data_quality_score': data_result.confidence_level,
                'data_sources_used': data_result.data_sources_used,
                'fetched_data': data_result.fetched_data,
                'processed_data': data_result.processed_data,
                'time_series_data': data_result.time_series_data
            }
            
        except Exception as e:
            logger.error(f"æ•°æ®è·å–ç¼–æ’å¤±è´¥: {str(e)}")
            # é™çº§åˆ°åŸºç¡€æ•°æ®è·å–
            return await self._fallback_data_acquisition(query_analysis)
    
    async def _fallback_data_acquisition(self, query_analysis) -> Dict[str, Any]:
        """é™çº§æ•°æ®è·å–"""
        try:
            # åªè·å–åŸºç¡€ç³»ç»Ÿæ•°æ®
            api_connector = getattr(self.data_fetcher, 'api_connector', None)
            if api_connector:
                system_data = await api_connector.get_system_data()
                return {
                    'success': True,
                    'fallback': True,
                    'data_quality_score': 0.6,
                    'data_sources_used': ['system'],
                    'fetched_data': {'system': system_data},
                    'processed_data': {'system_data': system_data.get('data', {})},
                    'time_series_data': {}
                }
        except Exception as e:
            logger.error(f"é™çº§æ•°æ®è·å–ä¹Ÿå¤±è´¥: {str(e)}")
            
        return {
            'success': False,
            'error': 'æ•°æ®è·å–å¤±è´¥',
            'data_quality_score': 0.0,
            'data_sources_used': [],
            'fetched_data': {},
            'processed_data': {},
            'time_series_data': {}
        }
    
    # ============= ä¸šåŠ¡å¤„ç†ç¼–æ’ =============
    
    async def _orchestrate_business_processing(self, query_analysis, data_result, strategy: ProcessingStrategy) -> Dict[str, Any]:
        """ç¼–æ’ä¸šåŠ¡å¤„ç†æµç¨‹"""
        try:
            complexity = query_analysis.complexity
            query_type = query_analysis.query_type
            
            # æ ¹æ®ç­–ç•¥é€‰æ‹©å¤„ç†æ–¹å¼
            if strategy == ProcessingStrategy.DIRECT_RESPONSE:
                return await self._direct_response_processing(query_analysis, data_result)
            elif strategy == ProcessingStrategy.SINGLE_PROCESSOR:
                return await self._single_processor_processing(query_analysis, data_result)
            elif strategy == ProcessingStrategy.MULTI_PROCESSOR:
                return await self._multi_processor_processing(query_analysis, data_result)
            else:  # FULL_PIPELINE
                return await self._full_pipeline_processing(query_analysis, data_result)
                
        except Exception as e:
            logger.error(f"ä¸šåŠ¡å¤„ç†ç¼–æ’å¤±è´¥: {str(e)}")
            return await self._fallback_business_processing(query_analysis, data_result)
    
    async def _direct_response_processing(self, query_analysis, data_result) -> Dict[str, Any]:
        """ç›´æ¥å“åº”å¤„ç†"""
        # ç®€å•æŸ¥è¯¢ç›´æ¥ä½¿ç”¨å½“å‰æ•°æ®å¤„ç†å™¨
        result = await self.current_data_processor.process_current_data_query(query_analysis.original_query)
        return {
            'processing_type': 'direct_response',
            'primary_result': result,
            'confidence_score': result.response_confidence,
            'processing_time': result.processing_time
        }
    
    async def _single_processor_processing(self, query_analysis, data_result) -> Dict[str, Any]:
        """å•å¤„ç†å™¨å¤„ç†"""
        query_type = query_analysis.query_type
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©åˆé€‚çš„å¤„ç†å™¨
        if 'trend' in query_type.value or 'historical' in query_type.value:
            processor = self.historical_analysis_processor
            result = await processor.process_historical_analysis_query(
                query_analysis.original_query, {'data_result': data_result}
            )
        elif 'prediction' in query_type.value:
            processor = self.prediction_processor  
            result = await processor.process_prediction_query(
                query_analysis.original_query, {'data_result': data_result}
            )
        else:
            processor = self.current_data_processor
            result = await processor.process_current_data_query(query_analysis.original_query)
        
        return {
            'processing_type': 'single_processor',
            'processor_used': processor.__class__.__name__,
            'primary_result': result,
            'confidence_score': getattr(result, 'confidence_score', 0.8),
            'processing_time': getattr(result, 'processing_time', 0.0)
        }
    
    async def _multi_processor_processing(self, query_analysis, data_result) -> Dict[str, Any]:
        """å¤šå¤„ç†å™¨åä½œå¤„ç†"""
        try:
            # å¹¶è¡Œè¿è¡Œå¤šä¸ªå¤„ç†å™¨
            tasks = []
            
            # å½“å‰æ•°æ®å¤„ç†
            tasks.append(self.current_data_processor.process_current_data_query(query_analysis.original_query))
            
            # å†å²åˆ†æå¤„ç†  
            tasks.append(self.historical_analysis_processor.process_historical_analysis_query(
                query_analysis.original_query, {'data_result': data_result}
            ))
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹å†³å®šæ˜¯å¦åŒ…å«é¢„æµ‹
            if 'prediction' in query_analysis.query_type.value:
                tasks.append(self.prediction_processor.process_prediction_query(
                    query_analysis.original_query, {'data_result': data_result}
                ))
            
            # å¹¶è¡Œæ‰§è¡Œ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ•´åˆç»“æœ
            successful_results = [r for r in results if not isinstance(r, Exception)]
            
            return {
                'processing_type': 'multi_processor',
                'processors_used': ['CurrentDataProcessor', 'HistoricalAnalysisProcessor', 'PredictionProcessor'],
                'current_data_result': successful_results[0] if len(successful_results) > 0 else None,
                'historical_result': successful_results[1] if len(successful_results) > 1 else None,
                'prediction_result': successful_results[2] if len(successful_results) > 2 else None,
                'confidence_score': sum(getattr(r, 'confidence_score', 0.8) for r in successful_results) / max(len(successful_results), 1),
                'processing_time': max(getattr(r, 'processing_time', 0.0) for r in successful_results) if successful_results else 0.0
            }
            
        except Exception as e:
            logger.error(f"å¤šå¤„ç†å™¨åä½œå¤±è´¥: {str(e)}")
            # é™çº§åˆ°å•å¤„ç†å™¨
            return await self._single_processor_processing(query_analysis, data_result)
    
    async def _full_pipeline_processing(self, query_analysis, data_result) -> Dict[str, Any]:
        """å®Œæ•´æµæ°´çº¿å¤„ç†"""
        # ä¸“å®¶çº§æŸ¥è¯¢çš„å®Œæ•´å¤„ç†æµç¨‹
        pipeline_results = {}
        
        try:
            # Step 1: å½“å‰çŠ¶æ€åˆ†æ
            current_result = await self.current_data_processor.process_current_data_query(query_analysis.original_query)
            pipeline_results['current_analysis'] = current_result
            
            # Step 2: å†å²æ·±åº¦åˆ†æ
            historical_result = await self.historical_analysis_processor.process_historical_analysis_query(
                query_analysis.original_query, {'data_result': data_result}
            )
            pipeline_results['historical_analysis'] = historical_result
            
            # Step 3: é¢„æµ‹ä¸åœºæ™¯åˆ†æ
            prediction_result = await self.prediction_processor.process_prediction_query(
                query_analysis.original_query, {'data_result': data_result}
            )
            pipeline_results['prediction_analysis'] = prediction_result
            
            # Step 4: æ·±åº¦é‡‘èåˆ†æ
            financial_analysis = await self.financial_data_analyzer.analyze_business_performance('comprehensive', 90)
            pipeline_results['financial_analysis'] = financial_analysis
            
            return {
                'processing_type': 'full_pipeline',
                'processors_used': ['CurrentDataProcessor', 'HistoricalAnalysisProcessor', 'PredictionProcessor', 'FinancialDataAnalyzer'],
                'pipeline_results': pipeline_results,
                'confidence_score': 0.9,  # å®Œæ•´æµæ°´çº¿çš„é«˜ç½®ä¿¡åº¦
                'processing_time': sum(getattr(r, 'processing_time', 1.0) for r in pipeline_results.values())
            }
            
        except Exception as e:
            logger.error(f"å®Œæ•´æµæ°´çº¿å¤„ç†å¤±è´¥: {str(e)}")
            # é™çº§åˆ°å¤šå¤„ç†å™¨
            return await self._multi_processor_processing(query_analysis, data_result)
    
    async def _fallback_business_processing(self, query_analysis, data_result) -> Dict[str, Any]:
        """ä¸šåŠ¡å¤„ç†é™çº§æ–¹æ¡ˆ"""
        try:
            # é™çº§åˆ°æœ€åŸºç¡€çš„å½“å‰æ•°æ®å¤„ç†
            result = await self.current_data_processor.process_current_data_query(query_analysis.original_query)
            return {
                'processing_type': 'fallback',
                'primary_result': result,
                'confidence_score': 0.6,
                'processing_time': getattr(result, 'processing_time', 0.0),
                'fallback_reason': 'ä¸šåŠ¡å¤„ç†å™¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å¤„ç†'
            }
        except Exception as e:
            logger.error(f"é™çº§ä¸šåŠ¡å¤„ç†ä¹Ÿå¤±è´¥: {str(e)}")
            return {
                'processing_type': 'error',
                'error': str(e),
                'confidence_score': 0.0,
                'processing_time': 0.0
            }
    
    # ============= æ´å¯Ÿç”Ÿæˆç¼–æ’ =============
    
    async def _orchestrate_insight_generation(self, business_result, query_analysis, data_result) -> List[Any]:
        """ç¼–æ’æ´å¯Ÿç”Ÿæˆæµç¨‹"""
        try:
            # å‡†å¤‡åˆ†æç»“æœç”¨äºæ´å¯Ÿç”Ÿæˆ
            analysis_results = []
            
            # ä»ä¸šåŠ¡å¤„ç†ç»“æœä¸­æå–åˆ†æç»“æœ
            if 'primary_result' in business_result:
                analysis_results.append(business_result['primary_result'])
            
            if 'pipeline_results' in business_result:
                analysis_results.extend(business_result['pipeline_results'].values())
            
            # ç”Ÿæˆç»¼åˆæ´å¯Ÿ
            insights, metadata = await self.insight_generator.generate_comprehensive_insights(
                analysis_results=analysis_results,
                user_context=None,
                focus_areas=self._determine_focus_areas(query_analysis)
            )
            
            return insights
            
        except Exception as e:
            logger.error(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
            return []
    
    def _determine_focus_areas(self, query_analysis) -> List[str]:
        """æ ¹æ®æŸ¥è¯¢åˆ†æç¡®å®šæ´å¯Ÿé‡ç‚¹é¢†åŸŸ"""
        focus_areas = []
        
        query_type = query_analysis.query_type.value
        business_scenario = query_analysis.business_scenario.value
        
        if 'financial' in business_scenario or 'risk' in query_type:
            focus_areas.append('financial_health')
        if 'growth' in business_scenario or 'trend' in query_type:
            focus_areas.append('growth_analysis')
        if 'prediction' in query_type:
            focus_areas.append('opportunity')
        if 'risk' in query_type:
            focus_areas.append('risk_management')
            
        return focus_areas or ['financial_health']
    
    # ============= å“åº”ç”Ÿæˆä¸æ ¼å¼åŒ– =============
    
    async def _generate_intelligent_response(self, user_query: str, query_analysis, 
                                           business_result: Dict[str, Any], 
                                           insights: List[Any]) -> str:
        """AIé©±åŠ¨çš„æ™ºèƒ½å“åº”ç”Ÿæˆ"""
        try:
            if not self.claude_client:
                return self._generate_basic_response(business_result, insights)
            
            # ä½¿ç”¨Claudeç”Ÿæˆä¸“ä¸šå“åº”
            prompt = f"""
            ä½œä¸ºä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹åˆ†æç»“æœç”Ÿæˆç”¨æˆ·å‹å¥½çš„å›ç­”ï¼š

            ç”¨æˆ·é—®é¢˜: "{user_query}"

            æŸ¥è¯¢åˆ†æ:
            - æŸ¥è¯¢ç±»å‹: {query_analysis.query_type.value}
            - å¤æ‚åº¦: {query_analysis.complexity.value}
            - ä¸šåŠ¡åœºæ™¯: {query_analysis.business_scenario.value}

            åˆ†æç»“æœæ‘˜è¦:
            {json.dumps(self._summarize_business_result(business_result), ensure_ascii=False, indent=2)}

            ä¸šåŠ¡æ´å¯Ÿ:
            {json.dumps([insight.get('title', 'æ´å¯Ÿ') + ': ' + insight.get('summary', '') for insight in insights[:3]], ensure_ascii=False)}

            è¯·ç”Ÿæˆï¼š
            1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
            2. çªå‡ºå…³é”®æ•°æ®å’Œå‘ç°
            3. æä¾›ä¸šåŠ¡æ´å¯Ÿå’Œå»ºè®®
            4. è¯­è¨€ä¸“ä¸šä½†æ˜“æ‡‚

            è¦æ±‚ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºï¼Œä¸è¶…è¿‡500å­—ã€‚
            """
            
            result = await self.claude_client.analyze_complex_query(prompt, {
                'query_analysis': query_analysis.__dict__ if hasattr(query_analysis, '__dict__') else query_analysis,
                'business_result': business_result,
                'insights': insights[:3]
            })
            
            if result.get('success'):
                return result.get('response', 'åˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†ç»“æœã€‚')
            
        except Exception as e:
            logger.error(f"AIå“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # é™çº§åˆ°åŸºç¡€å“åº”
        return self._generate_basic_response(business_result, insights)
    
    def _generate_basic_response(self, business_result: Dict[str, Any], insights: List[Any]) -> str:
        """ç”ŸæˆåŸºç¡€å“åº”"""
        response_parts = ["æ ¹æ®æ‚¨çš„æŸ¥è¯¢ï¼Œæˆ‘å·²å®Œæˆåˆ†æï¼š\n"]
        
        # æ·»åŠ å¤„ç†ç±»å‹ä¿¡æ¯
        processing_type = business_result.get('processing_type', 'unknown')
        response_parts.append(f"å¤„ç†æ–¹å¼: {processing_type}")
        
        # æ·»åŠ å…³é”®æŒ‡æ ‡
        if 'primary_result' in business_result:
            result = business_result['primary_result']
            if hasattr(result, 'main_answer'):
                response_parts.append(f"\nä¸»è¦å‘ç°: {result.main_answer}")
        
        # æ·»åŠ æ´å¯Ÿ
        if insights:
            response_parts.append(f"\nä¸šåŠ¡æ´å¯Ÿ: å‘ç°{len(insights)}æ¡é‡è¦æ´å¯Ÿ")
            for i, insight in enumerate(insights[:3], 1):
                title = insight.get('title', f'æ´å¯Ÿ{i}')
                summary = insight.get('summary', 'é‡è¦å‘ç°')
                response_parts.append(f"  {i}. {title}: {summary}")
        
        # æ·»åŠ ç½®ä¿¡åº¦
        confidence = business_result.get('confidence_score', 0.8)
        response_parts.append(f"\nåˆ†æç½®ä¿¡åº¦: {confidence:.1%}")
        
        return "\n".join(response_parts)
    
    def _summarize_business_result(self, business_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ‘˜è¦ä¸šåŠ¡ç»“æœç”¨äºAIå¤„ç†"""
        summary = {
            'processing_type': business_result.get('processing_type'),
            'confidence_score': business_result.get('confidence_score'),
            'processing_time': business_result.get('processing_time')
        }
        
        # æ·»åŠ ä¸»è¦ç»“æœæ‘˜è¦
        if 'primary_result' in business_result:
            result = business_result['primary_result']
            if hasattr(result, 'main_answer'):
                summary['main_answer'] = result.main_answer
            if hasattr(result, 'key_metrics'):
                summary['key_metrics'] = result.key_metrics
        
        return summary
    
    async def _generate_visualizations(self, business_result: Dict[str, Any], 
                                     query_analysis) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¯è§†åŒ–æ•°æ®"""
        visualizations = []
        
        try:
            # åŸºäºåˆ†æç»“æœç”Ÿæˆåˆé€‚çš„å¯è§†åŒ–
            processing_type = business_result.get('processing_type')
            
            if processing_type in ['multi_processor', 'full_pipeline']:
                # å¤šç»´åº¦æ•°æ®å¯è§†åŒ–
                if 'historical_result' in business_result:
                    visualizations.append({
                        'type': 'line_chart',
                        'title': 'å†å²è¶‹åŠ¿åˆ†æ',
                        'data': self._extract_trend_data(business_result['historical_result']),
                        'config': {'x_axis': 'date', 'y_axis': 'value'}
                    })
                
                if 'prediction_result' in business_result:
                    visualizations.append({
                        'type': 'forecast_chart', 
                        'title': 'é¢„æµ‹åˆ†æ',
                        'data': self._extract_prediction_data(business_result['prediction_result']),
                        'config': {'show_confidence_interval': True}
                    })
            
            # åŸºç¡€æŒ‡æ ‡å›¾è¡¨
            if 'primary_result' in business_result:
                result = business_result['primary_result']
                if hasattr(result, 'key_metrics') and result.key_metrics:
                    visualizations.append({
                        'type': 'metrics_dashboard',
                        'title': 'å…³é”®æŒ‡æ ‡',
                        'data': result.key_metrics,
                        'config': {'layout': 'grid'}
                    })
                    
        except Exception as e:
            logger.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        return visualizations
    
    def _extract_trend_data(self, historical_result) -> Dict[str, Any]:
        """ä»å†å²ç»“æœä¸­æå–è¶‹åŠ¿æ•°æ®"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…ä¸­ä¼šæ ¹æ®å…·ä½“ç»“æœç»“æ„æå–
        return {
            'dates': ['2024-05-01', '2024-05-15', '2024-05-30'],
            'values': [100, 120, 115],
            'trend': 'increasing'
        }
    
    def _extract_prediction_data(self, prediction_result) -> Dict[str, Any]:
        """ä»é¢„æµ‹ç»“æœä¸­æå–é¢„æµ‹æ•°æ®"""
        # ç®€åŒ–å®ç°
        return {
            'historical': [100, 120, 115],
            'predicted': [125, 130, 140],
            'confidence_upper': [135, 145, 160],
            'confidence_lower': [115, 120, 125]
        }
    
    # ============= å·¥å…·æ–¹æ³• =============
    
    def _extract_key_metrics(self, business_result: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å…³é”®æŒ‡æ ‡"""
        metrics = {}
        
        try:
            if 'primary_result' in business_result:
                result = business_result['primary_result']
                if hasattr(result, 'key_metrics'):
                    metrics.update(result.key_metrics)
            
            if 'pipeline_results' in business_result:
                for name, result in business_result['pipeline_results'].items():
                    if hasattr(result, 'key_metrics'):
                        metrics[f'{name}_metrics'] = result.key_metrics
                        
        except Exception as e:
            logger.warning(f"å…³é”®æŒ‡æ ‡æå–å¤±è´¥: {str(e)}")
        
        return metrics
    
    def _get_processors_used(self, strategy: ProcessingStrategy) -> List[str]:
        """è·å–ä½¿ç”¨çš„å¤„ç†å™¨åˆ—è¡¨"""
        if strategy == ProcessingStrategy.DIRECT_RESPONSE:
            return ['CurrentDataProcessor']
        elif strategy == ProcessingStrategy.SINGLE_PROCESSOR:
            return ['CurrentDataProcessor']  # æ ¹æ®å®é™…ä½¿ç”¨çš„å¤„ç†å™¨åŠ¨æ€ç¡®å®š
        elif strategy == ProcessingStrategy.MULTI_PROCESSOR:
            return ['CurrentDataProcessor', 'HistoricalAnalysisProcessor', 'PredictionProcessor']
        else:  # FULL_PIPELINE
            return ['CurrentDataProcessor', 'HistoricalAnalysisProcessor', 'PredictionProcessor', 'FinancialDataAnalyzer']
    
    def _get_ai_collaboration_summary(self, query_analysis, business_result) -> Dict[str, Any]:
        """è·å–AIåä½œæ‘˜è¦"""
        return {
            'claude_used': self.claude_client is not None,
            'gpt_used': self.gpt_client is not None,
            'ai_enhanced_parsing': query_analysis.confidence_score > 0.8,
            'ai_enhanced_insights': True,
            'collaboration_level': 'high' if self.claude_client and self.gpt_client else 'basic'
        }
    
    def _calculate_overall_confidence(self, query_analysis, business_result, insights) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # æŸ¥è¯¢è§£æç½®ä¿¡åº¦
        confidence_factors.append(query_analysis.confidence_score)
        
        # ä¸šåŠ¡å¤„ç†ç½®ä¿¡åº¦
        business_confidence = business_result.get('confidence_score', 0.8)
        confidence_factors.append(business_confidence)
        
        # æ´å¯Ÿè´¨é‡
        insight_confidence = 0.8 if insights else 0.6
        confidence_factors.append(insight_confidence)
        
        return sum(confidence_factors) / len(confidence_factors)
    
    def _calculate_response_completeness(self, business_result, insights) -> float:
        """è®¡ç®—å“åº”å®Œæ•´æ€§"""
        completeness_score = 0.5  # åŸºç¡€åˆ†
        
        if business_result.get('primary_result'):
            completeness_score += 0.2
        
        if insights:
            completeness_score += 0.2
        
        if business_result.get('processing_type') in ['multi_processor', 'full_pipeline']:
            completeness_score += 0.1
        
        return min(completeness_score, 1.0)
    
    # ============= ç¼“å­˜ç®¡ç† =============
    
    def _generate_query_cache_key(self, query: str, context: Dict[str, Any] = None) -> str:
        """ç”ŸæˆæŸ¥è¯¢ç¼“å­˜é”®"""
        cache_data = f"{query}_{json.dumps(context or {}, sort_keys=True)}"
        return hashlib.md5(cache_data.encode()).hexdigest()
    
    def _get_cached_parsing_result(self, cache_key: str):
        """è·å–ç¼“å­˜çš„è§£æç»“æœ"""
        if cache_key in self.result_cache:
            cache_entry = self.result_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                return cache_entry['data']
            else:
                del self.result_cache[cache_key]
        return None
    
    def _cache_parsing_result(self, cache_key: str, result):
        """ç¼“å­˜è§£æç»“æœ"""
        self.result_cache[cache_key] = {
            'data': result,
            'timestamp': time.time()
        }
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.result_cache) > 1000:
            oldest_key = min(self.result_cache.keys(), 
                           key=lambda k: self.result_cache[k]['timestamp'])
            del self.result_cache[oldest_key]
    
    async def _cache_result(self, query: str, result: ProcessingResult):
        """ç¼“å­˜å®Œæ•´ç»“æœ"""
        cache_key = f"result_{self._generate_query_cache_key(query)}"
        self.result_cache[cache_key] = {
            'data': result,
            'timestamp': time.time()
        }
    
    # ============= é™çº§å¤„ç† =============
    
    async def _fallback_query_parsing(self, user_query: str):
        """é™çº§æŸ¥è¯¢è§£æ"""
        # åˆ›å»ºåŸºç¡€æŸ¥è¯¢åˆ†æç»“æœ
        from core.analyzers.query_parser import QueryComplexity, QueryType, BusinessScenario
        
        class MockQueryResult:
            def __init__(self):
                self.original_query = user_query
                self.complexity = QueryComplexity.MEDIUM
                self.query_type = QueryType.DATA_RETRIEVAL
                self.business_scenario = BusinessScenario.DAILY_OPERATIONS
                self.confidence_score = 0.6
                self.estimated_total_time = 10.0
                
        return MockQueryResult()
    
    async def _handle_processing_error(self, session_id: str, query_id: str, 
                                     user_query: str, error: str, 
                                     processing_time: float) -> ProcessingResult:
        """å¤„ç†é”™è¯¯å¹¶è¿”å›é™çº§ç»“æœ"""
        self.orchestrator_stats['failed_queries'] += 1
        
        error_type = self._classify_error(error)
        self.orchestrator_stats['error_types'][error_type] = self.orchestrator_stats['error_types'].get(error_type, 0) + 1
        
        # å°è¯•é™çº§å“åº”
        fallback_response = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚"
        
        if self.config.get('fallback_response_enabled', True):
            try:
                # å°è¯•æä¾›åŸºç¡€å“åº”
                fallback_response = await self._generate_fallback_response(user_query, error)
            except:
                pass
        
        return ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            
            response_text=fallback_response,
            insights=[],
            key_metrics={},
            visualizations=[],
            
            processing_strategy=ProcessingStrategy.DIRECT_RESPONSE,
            processors_used=[],
            ai_collaboration_summary={'error': True},
            
            confidence_score=0.0,
            data_quality_score=0.0,
            response_completeness=0.2,
            
            total_processing_time=processing_time,
            ai_processing_time=0.0,
            data_fetching_time=0.0,
            
            processing_metadata={'error_occurred': True},
            error_info={'error_message': error, 'error_type': error_type},
            conversation_id=None,
            timestamp=datetime.now().isoformat()
        )
    
    def _classify_error(self, error: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_lower = error.lower()
        
        if 'timeout' in error_lower:
            return 'timeout'
        elif 'connection' in error_lower:
            return 'connection'
        elif 'validation' in error_lower:
            return 'validation'
        elif 'ai' in error_lower or 'claude' in error_lower or 'gpt' in error_lower:
            return 'ai_service'
        else:
            return 'unknown'
    
    async def _generate_fallback_response(self, user_query: str, error: str) -> str:
        """ç”Ÿæˆé™çº§å“åº”"""
        if 'balance' in user_query.lower() or 'ä½™é¢' in user_query:
            return "ç³»ç»Ÿæš‚æ—¶æ— æ³•è·å–æœ€æ–°ä½™é¢ä¿¡æ¯ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»å®¢æœã€‚"
        elif 'trend' in user_query.lower() or 'è¶‹åŠ¿' in user_query:
            return "è¶‹åŠ¿åˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†æŠ€æœ¯é—®é¢˜ã€‚"
        else:
            return "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„æŸ¥è¯¢ã€‚ç³»ç»Ÿæ­£åœ¨æ¢å¤ä¸­ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    
    # ============= ç»Ÿè®¡ä¸ç›‘æ§ =============
    
    def _update_orchestrator_stats(self, result: ProcessingResult):
        """æ›´æ–°ç¼–æ’å™¨ç»Ÿè®¡ä¿¡æ¯"""
        if result.success:
            self.orchestrator_stats['successful_queries'] += 1
        
        # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        total_queries = self.orchestrator_stats['total_queries']
        current_avg_time = self.orchestrator_stats['avg_processing_time']
        new_avg_time = ((current_avg_time * (total_queries - 1)) + result.total_processing_time) / total_queries
        self.orchestrator_stats['avg_processing_time'] = new_avg_time
        
        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        current_avg_confidence = self.orchestrator_stats['avg_confidence_score']
        new_avg_confidence = ((current_avg_confidence * (total_queries - 1)) + result.confidence_score) / total_queries
        self.orchestrator_stats['avg_confidence_score'] = new_avg_confidence
        
        # æ›´æ–°å¤„ç†å™¨ä½¿ç”¨ç»Ÿè®¡
        for processor in result.processors_used:
            self.orchestrator_stats['processor_usage'][processor] = \
                self.orchestrator_stats['processor_usage'].get(processor, 0) + 1
        
        # AIåä½œç»Ÿè®¡
        if result.ai_collaboration_summary.get('claude_used') and result.ai_collaboration_summary.get('gpt_used'):
            self.orchestrator_stats['ai_collaboration_count'] += 1
    
    def get_orchestrator_stats(self) -> Dict[str, Any]:
        """è·å–ç¼–æ’å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.orchestrator_stats.copy()
        
        # è®¡ç®—æˆåŠŸç‡
        if stats['total_queries'] > 0:
            stats['success_rate'] = stats['successful_queries'] / stats['total_queries']
            stats['failure_rate'] = stats['failed_queries'] / stats['total_queries']
        else:
            stats['success_rate'] = 0.0
            stats['failure_rate'] = 0.0
        
        # ç¼“å­˜å‘½ä¸­ç‡
        cache_requests = stats['cache_hits'] + stats['cache_misses']
        if cache_requests > 0:
            stats['cache_hit_rate'] = stats['cache_hits'] / cache_requests
        else:
            stats['cache_hit_rate'] = 0.0
        
        return stats
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        health_status = {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'orchestrator_initialized': self.initialized,
            'components_status': {},
            'ai_clients_status': {
                'claude_available': self.claude_client is not None,
                'gpt_available': self.gpt_client is not None
            },
            'statistics': self.get_orchestrator_stats(),
            'active_sessions': len(self.active_sessions),
            'cache_size': len(self.result_cache)
        }
        
        # æ£€æŸ¥å„ç»„ä»¶çŠ¶æ€
        if self.initialized:
            components = {
                'query_parser': self.query_parser,
                'data_requirements_analyzer': self.data_requirements_analyzer,
                'financial_data_analyzer': self.financial_data_analyzer,
                'insight_generator': self.insight_generator,
                'data_fetcher': self.data_fetcher,
                'current_data_processor': self.current_data_processor,
                'historical_analysis_processor': self.historical_analysis_processor,
                'prediction_processor': self.prediction_processor
            }
            
            for name, component in components.items():
                try:
                    if hasattr(component, 'health_check'):
                        component_health = await component.health_check()
                        health_status['components_status'][name] = component_health.get('status', 'unknown')
                    else:
                        health_status['components_status'][name] = 'available' if component else 'unavailable'
                except Exception as e:
                    health_status['components_status'][name] = f'error: {str(e)}'
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if not self.initialized:
            health_status['status'] = 'initializing'
        elif any(status == 'error' for status in health_status['components_status'].values()):
            health_status['status'] = 'degraded'
        elif not health_status['ai_clients_status']['claude_available']:
            health_status['status'] = 'limited'
        
        return health_status
    
    # ============= ä¼šè¯ç®¡ç† =============
    
    async def close(self):
        """å…³é—­ç¼–æ’å™¨ï¼Œæ¸…ç†èµ„æº"""
        try:
            if self.data_fetcher and hasattr(self.data_fetcher, 'close'):
                await self.data_fetcher.close()
            
            self.result_cache.clear()
            self.active_sessions.clear()
            
            logger.info("IntelligentQAOrchestrator closed successfully")
            
        except Exception as e:
            logger.error(f"å…³é—­ç¼–æ’å™¨æ—¶å‡ºé”™: {str(e)}")


class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.conversations = {}
        self.max_history = 50
    
    def add_exchange(self, conversation_id: str, query: str, result: ProcessingResult):
        """æ·»åŠ å¯¹è¯è®°å½•"""
        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = []
        
        self.conversations[conversation_id].append({
            'query': query,
            'result': result,
            'timestamp': datetime.now().isoformat()
        })
        
        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(self.conversations[conversation_id]) > self.max_history:
            self.conversations[conversation_id] = self.conversations[conversation_id][-self.max_history:]
    
    def get_context(self, conversation_id: str) -> Dict[str, Any]:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡"""
        if conversation_id not in self.conversations:
            return {}
        
        history = self.conversations[conversation_id]
        recent_queries = [exchange['query'] for exchange in history[-5:]]
        
        return {
            'conversation_id': conversation_id,
            'recent_queries': recent_queries,
            'query_count': len(history)
        }
    
    def get_user_conversations(self, user_id: int, limit: int = 20, offset: int = 0) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…ä¸­ä¼šæ ¹æ®user_idè¿‡æ»¤
        all_conversations = []
        for conv_id, exchanges in self.conversations.items():
            if exchanges:
                last_exchange = exchanges[-1]
                all_conversations.append({
                    'conversation_id': conv_id,
                    'last_query': last_exchange['query'],
                    'last_timestamp': last_exchange['timestamp'],
                    'exchange_count': len(exchanges)
                })
        
        return all_conversations[offset:offset+limit]


# ============= å…¨å±€ç¼–æ’å™¨å®ä¾‹ç®¡ç† =============

_orchestrator_instance = None

def get_orchestrator(claude_client=None, gpt_client=None, config: Dict[str, Any] = None) -> IntelligentQAOrchestrator:
    """
    è·å–å…¨å±€ç¼–æ’å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        claude_client: Claudeå®¢æˆ·ç«¯
        gpt_client: GPTå®¢æˆ·ç«¯
        config: é…ç½®å‚æ•°
        
    Returns:
        IntelligentQAOrchestrator: ç¼–æ’å™¨å®ä¾‹
    """
    global _orchestrator_instance
    
    if _orchestrator_instance is None:
        _orchestrator_instance = IntelligentQAOrchestrator(claude_client, gpt_client, config)
    
    return _orchestrator_instance

def create_orchestrator(claude_client=None, gpt_client=None, config: Dict[str, Any] = None) -> IntelligentQAOrchestrator:
    """
    åˆ›å»ºæ–°çš„ç¼–æ’å™¨å®ä¾‹
    
    Args:
        claude_client: Claudeå®¢æˆ·ç«¯
        gpt_client: GPTå®¢æˆ·ç«¯
        config: é…ç½®å‚æ•°
        
    Returns:
        IntelligentQAOrchestrator: æ–°çš„ç¼–æ’å™¨å®ä¾‹
    """
    return IntelligentQAOrchestrator(claude_client, gpt_client, config)


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    print("=== AIé©±åŠ¨çš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨æµ‹è¯• ===")
    
    # è·å–å…¨å±€ç¼–æ’å™¨å®ä¾‹
    orchestrator = get_orchestrator()
    
    try:
        # åˆå§‹åŒ–ç¼–æ’å™¨
        await orchestrator.initialize()
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ä»Šå¤©ç³»ç»Ÿæ€»ä½™é¢æ˜¯å¤šå°‘ï¼Ÿ",
            "è¿‡å»30å¤©çš„å…¥é‡‘è¶‹åŠ¿å¦‚ä½•ï¼Ÿ", 
            "æ ¹æ®å†å²æ•°æ®é¢„æµ‹ä¸‹æœˆèµ„é‡‘æƒ…å†µ",
            "å‡è®¾50%å¤æŠ•50%æç°å¯¹èµ„é‡‘çš„å½±å“"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n--- æµ‹è¯•æŸ¥è¯¢ {i} ---")
            print(f"æŸ¥è¯¢: {query}")
            
            # å¤„ç†æŸ¥è¯¢
            result = await orchestrator.process_intelligent_query(
                user_query=query,
                user_id=1,
                conversation_id=f"test_conv_{i}"
            )
            
            print(f"æˆåŠŸ: {'æ˜¯' if result.success else 'å¦'}")
            print(f"ç­–ç•¥: {result.processing_strategy.value}")
            print(f"ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
            print(f"å¤„ç†æ—¶é—´: {result.total_processing_time:.2f}ç§’")
            print(f"æ´å¯Ÿæ•°é‡: {len(result.insights)}")
            
            if result.response_text:
                print(f"å“åº”: {result.response_text[:200]}...")
        
        # ç³»ç»Ÿå¥åº·æ£€æŸ¥
        print(f"\n=== ç³»ç»Ÿå¥åº·æ£€æŸ¥ ===")
        health = await orchestrator.health_check()
        print(f"çŠ¶æ€: {health['status']}")
        print(f"AIå®¢æˆ·ç«¯: Claude={health['ai_clients_status']['claude_available']}, GPT={health['ai_clients_status']['gpt_available']}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = orchestrator.get_orchestrator_stats()
        print(f"\n=== ç»Ÿè®¡ä¿¡æ¯ ===")
        print(f"æ€»æŸ¥è¯¢: {stats['total_queries']}")
        print(f"æˆåŠŸç‡: {stats.get('success_rate', 0):.1%}")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.2f}ç§’")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence_score']:.2f}")
        
    finally:
        await orchestrator.close()

if __name__ == "__main__":
    asyncio.run(main())