"""
ç²¾ç®€é‡æ„åçš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨
æ ¸å¿ƒæ”¹è¿›ï¼š
1. ä¸“æ³¨äºæµç¨‹ç¼–æ’ï¼Œä¸å†å¤„ç†promptæ„å»ºå’ŒæŸ¥è¯¢ç±»å‹æ£€æµ‹
2. é›†æˆQueryTypeDetectorã€PromptManagerã€EnhancedAPIStrategyExtractor
3. ä¿æŒæ ¸å¿ƒæ•°æ®å¤„ç†å’Œå“åº”ç”ŸæˆåŠŸèƒ½
4. ç®€åŒ–ä»£ç ç»“æ„ï¼Œæé«˜ç»´æŠ¤æ€§
"""
import json
import logging
import os
import time
import uuid
import asyncio
import traceback
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from core.data_extraction.semantic_collector import SemanticDataCollector
from core.data_extraction.claude_extractor import ClaudeIntelligentExtractor
from core.data_orchestration.smart_data_fetcher import SmartDataFetcher, create_smart_data_fetcher
from utils.calculators.statistical_calculator import UnifiedCalculator, create_unified_calculator
from utils.formatters.financial_formatter import FinancialFormatter, create_financial_formatter
from data.models.conversation import ConversationManager, create_conversation_manager
from data.connectors.database_connector import DatabaseConnector, create_database_connector

# ğŸ†• æ–°ç»„ä»¶å¯¼å…¥
from core.detectors.query_type_detector import QueryTypeDetector, QueryType, QueryTypeResult, create_query_type_detector
from core.prompts.prompt_manager import PromptManager, create_prompt_manager
from core.analyzers.ai_strategy_extractor import EnhancedAPIStrategyExtractor, ExtractedStrategy, create_enhanced_strategy_extractor

# AI å®¢æˆ·ç«¯å¯¼å…¥
from core.models.claude_client import ClaudeClient
from core.models.openai_client import OpenAIClient
from config import Config as AppConfig

logger = logging.getLogger(__name__)

# å•ä¾‹å®ä¾‹
_orchestrator_instance = None

def get_orchestrator():
    """è·å–æ™ºèƒ½é—®ç­”ç¼–æ’å™¨çš„å•ä¾‹å®ä¾‹"""
    global _orchestrator_instance
    if _orchestrator_instance is None:
        from core.models.claude_client import ClaudeClient
        from core.models.openai_client import OpenAIClient
        from config import Config
        from data.connectors.database_connector import create_database_connector

        config = Config()
        claude_client = ClaudeClient()
        gpt_client = OpenAIClient()
        db_connector = create_database_connector()

        _orchestrator_instance = IntelligentQAOrchestrator(
            claude_client=claude_client,
            gpt_client=gpt_client,
            db_connector=db_connector
        )

    return _orchestrator_instance

# ============= æ•°æ®ç±»å®šä¹‰ =============

class QueryComplexity(Enum):
    """æŸ¥è¯¢å¤æ‚åº¦"""
    QUICK_RESPONSE = "quick_response"
    SIMPLE = "simple"
    COMPLEX = "complex"

class ProcessingStrategy(Enum):
    """å¤„ç†ç­–ç•¥"""
    QUICK_RESPONSE = "quick_response"
    STANDARD = "standard"
    FALLBACK = "fallback"
    COMPREHENSIVE = "comprehensive"

@dataclass
class QueryAnalysis:
    """æŸ¥è¯¢åˆ†æç»“æœï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
    original_query: str
    complexity: QueryComplexity
    is_quick_response: bool
    intent: str
    confidence: float
    api_calls: List[Dict[str, Any]] = field(default_factory=list)
    needs_calculation: bool = False
    calculation_type: Optional[str] = None
    calculation_params: Dict[str, Any] = field(default_factory=dict)
    query_type_info: Optional[QueryTypeResult] = None  # ğŸ†• æ–°å¢

# åœ¨ intelligent_qa_orchestrator.py ä¸­æ›´æ–°ProcessingResult
@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ"""
    session_id: str
    query_id: str
    success: bool
    response_text: str

    # æ ¸å¿ƒæ•°æ®
    extracted_data: Dict[str, Any] = field(default_factory=dict)
    calculation_results: Dict[str, Any] = field(default_factory=dict)

    # ğŸ†• å›¾è¡¨ç›¸å…³å­—æ®µ
    charts_data: Dict[str, Any] = field(default_factory=dict)
    has_charts: bool = False
    chart_generation_time: float = 0.0
    chart_generation_method: str = "none"  # ai_intelligent, rule_based, failed, none

    # å¤„ç†ä¿¡æ¯
    complexity: QueryComplexity = QueryComplexity.SIMPLE
    processing_path: str = "standard"
    confidence_score: float = 0.0
    total_processing_time: float = 0.0
    processing_strategy: ProcessingStrategy = ProcessingStrategy.STANDARD

    # é”™è¯¯ä¿¡æ¯
    error_info: Optional[Dict[str, Any]] = None

    # ä¼šè¯ä¿¡æ¯
    conversation_id: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    # å…¼å®¹æ—§ç‰ˆAPI
    key_metrics: Dict[str, Any] = field(default_factory=dict)
    insights: List[Any] = field(default_factory=list)
    visualizations: List[Dict[str, Any]] = field(default_factory=list)  # ğŸ†• è¿™é‡Œå°†åŒ…å«å›¾è¡¨æ•°æ®
    data_quality_score: float = 0.0
    response_completeness: float = 0.0
    ai_processing_time: float = 0.0
    data_fetching_time: float = 0.0
    processors_used: List[str] = field(default_factory=list)
    ai_collaboration_summary: Dict[str, Any] = field(default_factory=dict)
    processing_metadata: Dict[str, Any] = field(default_factory=dict)

# ============= ä¸»ç¼–æ’å™¨ç±» =============

class IntelligentQAOrchestrator:
    """ç²¾ç®€é‡æ„åçš„æ™ºèƒ½é—®ç­”ç¼–æ’å™¨ - ä¸“æ³¨äºæµç¨‹ç¼–æ’"""

    def __init__(self,
                 claude_client: Optional[ClaudeClient] = None,
                 gpt_client: Optional[OpenAIClient] = None,
                 db_connector: Optional[DatabaseConnector] = None,
                 app_config: Optional[AppConfig] = None):

        # AIå®¢æˆ·ç«¯
        self.claude_client = claude_client
        self.gpt_client = gpt_client
        self.db_connector = db_connector
        self.app_config = app_config or AppConfig()

        # æ ¸å¿ƒç»„ä»¶
        self.data_fetcher: Optional[SmartDataFetcher] = None
        self.calculator: Optional[UnifiedCalculator] = None
        self.financial_formatter: Optional[FinancialFormatter] = None
        self.conversation_manager: Optional[ConversationManager] = None

        # ğŸ†• æ–°æ¶æ„ç»„ä»¶
        self.query_type_detector: Optional[QueryTypeDetector] = None
        self.prompt_manager: Optional[PromptManager] = None
        self.strategy_extractor: Optional[EnhancedAPIStrategyExtractor] = None

        # æ•°æ®æå–ç»„ä»¶
        self.semantic_collector = SemanticDataCollector()
        self.claude_extractor = ClaudeIntelligentExtractor(self.claude_client)

        # é…ç½®
        self.config = self._load_config()
        self.current_date = datetime.now()

        # ç»Ÿè®¡è®¡æ•°å™¨
        self.stats = {
            'total_queries': 0,
            'quick_responses': 0,
            'comprehensive_analyses': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'calculation_calls': 0,
            'claude_generations': 0,
            'processing_time_total': 0.0
        }

        self.initialized = False
        logger.info("ç²¾ç®€é‡æ„åçš„æ™ºèƒ½ç¼–æ’å™¨åˆ›å»ºå®Œæˆ")

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        return {
            'max_processing_time': 120,
            'quick_response_timeout': 15,
            'claude_timeout': 30,
            'gpt_timeout': 25,
            'enable_dual_judgment': True,
            'current_date': '2025-06-01'
        }

    async def initialize(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        if self.initialized:
            return

        logger.info("åˆå§‹åŒ–ç²¾ç®€é‡æ„åçš„æ™ºèƒ½ç¼–æ’å™¨...")

        try:
            # ğŸ†• åˆå§‹åŒ–æ–°æ¶æ„ç»„ä»¶
            self.query_type_detector = create_query_type_detector()
            self.prompt_manager = create_prompt_manager()
            self.strategy_extractor = create_enhanced_strategy_extractor(
                self.claude_client, self.query_type_detector, self.prompt_manager
            )

            # æ•°æ®è·å–å™¨
            fetcher_config = {
                'base_url': 'https://api2.3foxzthtdgfy.com',
                'api_key': 'f22bf0ec9c61dce227d8f5d64998e883'
            }
            self.data_fetcher = create_smart_data_fetcher(
                self.claude_client, self.gpt_client, fetcher_config)

            # ç»Ÿä¸€è®¡ç®—å™¨
            self.calculator = create_unified_calculator(self.gpt_client, precision=6)

            # æ ¼å¼åŒ–å™¨
            self.financial_formatter = create_financial_formatter()

            # ä¼šè¯ç®¡ç†å™¨
            if self.db_connector:
                self.conversation_manager = create_conversation_manager(self.db_connector)

            # ğŸ†• æ·»åŠ å›¾è¡¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨
            from utils.formatters.chart_generator import create_chart_generator, ChartType, ChartTheme
            from utils.formatters.report_generator import create_report_generator, ReportFormat

            self.chart_generator = create_chart_generator(
                theme=ChartTheme.FINANCIAL,
                claude_client=self.claude_client  # ğŸ”§ ä¿®å¤ï¼šä¼ é€’Claudeå®¢æˆ·ç«¯
            )
            self.report_generator = create_report_generator()

            logger.info("å›¾è¡¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

            self.initialized = True
            logger.info("æ™ºèƒ½ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise


    # ================================================================
    # æ ¸å¿ƒæ–¹æ³•ï¼šæ™ºèƒ½æŸ¥è¯¢å¤„ç†
    # ================================================================

    async def process_intelligent_query(self,
                                        user_query: str,
                                        session_id: str = "",
                                        user_id: int = 0,
                                        conversation_id: Optional[str] = None,
                                        preferences: Optional[Dict[str, Any]] = None) -> ProcessingResult:
        """æ™ºèƒ½æŸ¥è¯¢å¤„ç† - ç²¾ç®€ç‰ˆ"""

        if not self.initialized:
            await self.initialize()

        query_id = f"q_{int(time.time())}_{hash(user_query) % 10000}"
        start_time = time.time()

        logger.info(f"å¤„ç†æŸ¥è¯¢ [{query_id}]: {user_query}")
        self.stats['total_queries'] += 1

        try:
            # ğŸ†• é˜¶æ®µ1: ä½¿ç”¨æ–°æ¶æ„çš„åŒé‡å¿«é€Ÿå“åº”åˆ¤æ–­
            quick_decision = await self._enhanced_quick_response_decision(user_query, query_id)
            print(quick_decision)
            if quick_decision['is_quick_response']:
                logger.info(f"å¿«é€Ÿå“åº”è·¯å¾„: {quick_decision['reason']}")
                return await self._execute_quick_response_path(
                    user_query, quick_decision, session_id, query_id, start_time, conversation_id)

            # ğŸ†• é˜¶æ®µ2: ä½¿ç”¨å¢å¼ºç­–ç•¥æå–å™¨è¿›è¡Œå®Œæ•´åˆ†æ
            logger.info("æ‰§è¡Œå®Œæ•´åˆ†æè·¯å¾„")
            query_analysis = await self._enhanced_comprehensive_analysis(user_query, query_id, quick_decision)

            # é˜¶æ®µ3: æ•°æ®è·å–
            raw_data = await self._intelligent_data_acquisition(query_analysis)

            # é˜¶æ®µ4: ä¸‰å±‚æ•°æ®æå–
            extracted_data = await self._claude_three_layer_data_extraction(
                raw_data, user_query, query_analysis)

            # é˜¶æ®µ5: è®¡ç®—å¤„ç†
            calculation_results = await self._statistical_calculation_processing(
                query_analysis, extracted_data)

            # ğŸ†• é˜¶æ®µ6: å¢å¼ºå“åº”ç”Ÿæˆï¼ˆåŒ…å«å›¾è¡¨ç”Ÿæˆï¼‰
            response_start_time = time.time()
            response_text = await self._enhanced_response_generation(
                user_query, query_analysis, extracted_data, calculation_results)
            response_generation_time = time.time() - response_start_time

            # ğŸ†• è·å–ç¼“å­˜çš„å›¾è¡¨æ•°æ®
            chart_data = getattr(self, '_last_chart_results', {
                'success': False,
                'chart_count': 0,
                'generation_method': 'none',
                'generated_charts': []
            })

            has_charts = chart_data.get('success', False) and chart_data.get('chart_count', 0) > 0
            chart_generation_time = chart_data.get('processing_time', 0.0)

            # ğŸ†• å¤„ç†å›¾è¡¨æ•°æ®ç”¨äºAPIå“åº”
            processed_charts = []
            if has_charts:
                processed_charts = self._process_chart_results_for_api(chart_data)

            total_time = time.time() - start_time
            self.stats['processing_time_total'] += total_time

            result = ProcessingResult(
                session_id=session_id,
                query_id=query_id,
                success=True,
                response_text=response_text,
                extracted_data=extracted_data,
                calculation_results=calculation_results,

                # ğŸ†• å›¾è¡¨ç›¸å…³å­—æ®µ
                charts_data=chart_data,
                has_charts=has_charts,
                chart_generation_time=chart_generation_time,
                chart_generation_method=chart_data.get('generation_method', 'none'),

                complexity=query_analysis.complexity,
                processing_path="comprehensive",
                confidence_score=query_analysis.confidence,
                total_processing_time=total_time,
                processing_strategy=ProcessingStrategy.COMPREHENSIVE,
                conversation_id=conversation_id,

                # ğŸ†• æ›´æ–°visualizationså­—æ®µä»¥åŒ…å«å›¾è¡¨
                visualizations=processed_charts
            )

            # ä¿å­˜å¯¹è¯è®°å½•
            await self._save_conversation_if_needed(conversation_id, user_query, result)

            logger.info(f"æŸ¥è¯¢å¤„ç†æˆåŠŸ [{query_id}] è€—æ—¶: {total_time:.2f}sï¼Œå›¾è¡¨: {'æ˜¯' if has_charts else 'å¦'}")
            return result

        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥ [{query_id}]: {e}\n{traceback.format_exc()}")
            return self._create_error_result(session_id, query_id, user_query, str(e),
                                             time.time() - start_time, conversation_id)

    def _process_chart_results_for_api(self, chart_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¤„ç†å›¾è¡¨ç»“æœç”¨äºAPIå“åº” - é€‚é…å‰ç«¯ChartJSæ ¼å¼"""

        processed_charts = []
        generated_charts = chart_data.get('generated_charts', [])

        for i, chart in enumerate(generated_charts):
            try:
                chart_type = chart.get('chart_type', 'unknown')
                chart_success = chart.get('success', False)

                if not chart_success:
                    continue

                # ğŸ¯ è½¬æ¢ä¸ºå‰ç«¯ChartJSæœŸæœ›çš„æ ¼å¼
                processed_chart = {
                    'type': 'chart',  # å‰ç«¯æœŸæœ›çš„ç±»å‹æ ‡è¯†
                    'title': chart.get('title', f'å›¾è¡¨ {i + 1}'),
                    'data': self._convert_to_chartjs_format(chart)
                }

                processed_charts.append(processed_chart)

            except Exception as e:
                logger.error(f"å¤„ç†å›¾è¡¨ {i + 1} æ—¶å‡ºé”™: {e}")
                continue

        return processed_charts

    def _convert_to_chartjs_format(self, chart: Dict[str, Any]) -> Dict[str, Any]:
        """å°†åç«¯å›¾è¡¨æ•°æ®è½¬æ¢ä¸ºå‰ç«¯ChartJSæ ¼å¼"""

        chart_type = chart.get('chart_type', 'pie')
        title = chart.get('title', 'æ•°æ®åˆ†æ')
        data_summary = chart.get('data_summary', {})

        # ğŸ¯ æ ¹æ®å›¾è¡¨ç±»å‹è½¬æ¢æ•°æ®æ ¼å¼
        if chart_type in ['pie', 'donut']:
            return self._convert_pie_chart_to_chartjs(title, data_summary, chart_type)
        elif chart_type == 'bar':
            return self._convert_bar_chart_to_chartjs(title, data_summary)
        elif chart_type == 'line':
            return self._convert_line_chart_to_chartjs(title, data_summary)
        else:
            # é»˜è®¤è½¬æ¢ä¸ºé¥¼å›¾
            return self._convert_pie_chart_to_chartjs(title, data_summary, 'pie')

    def _convert_pie_chart_to_chartjs(self, title: str, data_summary: Dict[str, Any], chart_type: str) -> Dict[
        str, Any]:
        """è½¬æ¢é¥¼å›¾æ•°æ®ä¸ºChartJSæ ¼å¼"""

        labels = data_summary.get('labels', [])
        values = data_summary.get('values', [])

        if not labels or not values or len(labels) != len(values):
            # å¦‚æœæ•°æ®ä¸å®Œæ•´ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®
            labels = ['æ•°æ®1', 'æ•°æ®2']
            values = [50, 50]

        # ğŸ¯ ç”ŸæˆChartJSæœŸæœ›çš„é¢œè‰²
        colors = [
            '#0072B2',  # è“è‰²
            '#E69F00',  # æ©™è‰²
            '#009E73',  # ç»¿è‰²
            '#CC79A7',  # ç²‰è‰²
            '#56B4E9',  # å¤©è“è‰²
            '#F0E442',  # é»„è‰²
            '#D55E00',  # çº¢æ©™è‰²
            '#999999',  # ç°è‰²
        ]

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é¢œè‰²
        chart_colors = []
        for i in range(len(values)):
            chart_colors.append(colors[i % len(colors)])

        chartjs_data = {
            'type': 'doughnut' if chart_type == 'donut' else 'pie',
            'title': title,
            'labels': labels,
            'datasets': [{
                'label': title,
                'data': values,
                'backgroundColor': chart_colors,
                'borderColor': chart_colors,
                'borderWidth': 2,
                'hoverOffset': 4
            }],
            'options': {
                'responsive': True,
                'maintainAspectRatio': False,
                'plugins': {
                    'legend': {
                        'position': 'bottom',
                        'labels': {
                            'padding': 20,
                            'usePointStyle': True
                        }
                    },
                    'title': {
                        'display': True,
                        'text': title,
                        'font': {
                            'size': 16,
                            'weight': 'bold'
                        },
                        'padding': {
                            'bottom': 20
                        }
                    },
                    'tooltip': {
                        'callbacks': {
                            'label': f'function(context) {{ return context.label + ": Â¥" + context.parsed.toLocaleString() + " (" + Math.round(context.parsed / context.dataset.data.reduce((a, b) => a + b, 0) * 100) + "%)"; }}'
                        }
                    }
                }
            }
        }

        return chartjs_data

    def _convert_bar_chart_to_chartjs(self, title: str, data_summary: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢æŸ±çŠ¶å›¾æ•°æ®ä¸ºChartJSæ ¼å¼ - ä¼˜åŒ–ç‰ˆ"""

        categories = data_summary.get('categories', [])
        series = data_summary.get('series', {})

        if not categories or not series:
            categories = ['ç±»åˆ«1', 'ç±»åˆ«2', 'ç±»åˆ«3']
            series = {'æ•°æ®ç³»åˆ—': [100, 200, 150]}

        datasets = []
        colors = ['#0072B2', '#E69F00', '#009E73', '#CC79A7', '#56B4E9', '#F0E442', '#D55E00']

        for i, (series_name, series_data) in enumerate(series.items()):
            datasets.append({
                'label': series_name,
                'data': series_data,
                'backgroundColor': colors[i % len(colors)],
                'borderColor': colors[i % len(colors)],
                'borderWidth': 1,
                'borderRadius': 4,  # ğŸ†• åœ†è§’
                'borderSkipped': False,
            })

        return {
            'type': 'bar',
            'title': title,
            'labels': categories,
            'datasets': datasets,
            'options': {
                'responsive': True,
                'maintainAspectRatio': False,
                'plugins': {
                    'legend': {
                        'position': 'top',
                        'labels': {
                            'usePointStyle': True,  # ğŸ†• ä½¿ç”¨ç‚¹æ ·å¼
                            'padding': 20
                        }
                    },
                    'title': {
                        'display': True,
                        'text': title,
                        'font': {
                            'size': 16,
                            'weight': 'bold'
                        }
                    },
                    'tooltip': {
                        'mode': 'index',
                        'intersect': False,
                        'callbacks': {
                            'label': 'function(context) { return context.dataset.label + ": Â¥" + context.parsed.y.toLocaleString(); }'
                        }
                    }
                },
                'scales': {
                    'x': {
                        'grid': {
                            'display': False  # ğŸ†• éšè—Xè½´ç½‘æ ¼çº¿
                        }
                    },
                    'y': {
                        'beginAtZero': True,
                        'grid': {
                            'color': 'rgba(0, 0, 0, 0.1)'  # ğŸ†• æ·¡åŒ–ç½‘æ ¼çº¿
                        },
                        'ticks': {
                            'callback': 'function(value) { return "Â¥" + value.toLocaleString(); }'  # ğŸ†• æ ¼å¼åŒ–Yè½´æ ‡ç­¾
                        }
                    }
                },
                'interaction': {
                    'mode': 'nearest',
                    'axis': 'x',
                    'intersect': False
                }
            }
        }

    def _convert_line_chart_to_chartjs(self, title: str, data_summary: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢æŠ˜çº¿å›¾æ•°æ®ä¸ºChartJSæ ¼å¼"""

        x_axis = data_summary.get('x_axis', [])
        series = data_summary.get('series', {})

        if not x_axis or not series:
            x_axis = ['æ—¶é—´1', 'æ—¶é—´2', 'æ—¶é—´3']
            series = {'æ•°æ®ç³»åˆ—': [10, 20, 30]}

        datasets = []
        colors = ['#0072B2', '#E69F00', '#009E73', '#CC79A7']

        for i, (series_name, series_data) in enumerate(series.items()):
            datasets.append({
                'label': series_name,
                'data': series_data,
                'borderColor': colors[i % len(colors)],
                'backgroundColor': colors[i % len(colors)] + '20',  # 20% é€æ˜åº¦
                'borderWidth': 2,
                'fill': False,
                'tension': 0.1
            })

        return {
            'type': 'line',
            'title': title,
            'labels': x_axis,
            'datasets': datasets,
            'options': {
                'responsive': True,
                'maintainAspectRatio': False,
                'plugins': {
                    'legend': {
                        'position': 'top'
                    },
                    'title': {
                        'display': True,
                        'text': title
                    }
                },
                'scales': {
                    'y': {
                        'beginAtZero': True
                    }
                }
            }
        }

    def _generate_chart_description(self, chart: Dict[str, Any]) -> str:
        """ä¸ºå›¾è¡¨ç”Ÿæˆæè¿°æ–‡å­—"""

        chart_type = chart.get('chart_type', 'å›¾è¡¨')
        title = chart.get('title', 'æ•°æ®åˆ†æ')
        data_summary = chart.get('data_summary', {})

        # æ ¹æ®å›¾è¡¨ç±»å‹ç”Ÿæˆä¸åŒçš„æè¿°
        if chart_type in ['pie', 'donut']:
            if 'values' in data_summary and 'labels' in data_summary:
                total = sum(data_summary['values']) if data_summary['values'] else 0
                if total > 0 and data_summary['values']:
                    max_item_idx = data_summary['values'].index(max(data_summary['values']))
                    max_item_label = data_summary['labels'][max_item_idx] if data_summary['labels'] else 'æœªçŸ¥'
                    formatted_total = self.financial_formatter.format_currency(
                        total) if self.financial_formatter else f'{total:,.2f}'
                    return f"{title}ï¼šæ€»é¢ {formatted_total}ï¼Œå…¶ä¸­ {max_item_label} å æ¯”æœ€å¤§"

        elif chart_type == 'bar':
            if 'series' in data_summary:
                series_count = len(data_summary['series'])
                return f"{title}ï¼šåŒ…å« {series_count} ä¸ªæ•°æ®ç³»åˆ—çš„å¯¹æ¯”åˆ†æ"

        elif chart_type == 'line':
            if 'series' in data_summary:
                series_count = len(data_summary['series'])
                return f"{title}ï¼š{series_count} ä¸ªæŒ‡æ ‡çš„è¶‹åŠ¿å˜åŒ–å›¾"

        return f"{title}ï¼š{chart_type}å›¾è¡¨"
    # ================================================================
    # ğŸ†• å¢å¼ºçš„é˜¶æ®µ1ï¼šåŒé‡å¿«é€Ÿå“åº”åˆ¤æ–­
    # ================================================================

    async def _enhanced_quick_response_decision(self, user_query: str, query_id: str) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆåŒé‡å¿«é€Ÿå“åº”åˆ¤æ–­ - ä¿®å¤èšåˆæŸ¥è¯¢"""
        logger.debug(f"æ‰§è¡Œå¢å¼ºç‰ˆå¿«é€Ÿå“åº”åˆ¤æ–­: {user_query}")

        try:
            # ğŸ†• æ­¥éª¤0: é¢„æ£€æŸ¥æ˜æ˜¾çš„å¤åˆæŸ¥è¯¢æ ‡å¿—
            obvious_complex_keywords = ['å¹³å‡', 'æœˆä»½', 'å¯¹æ¯”', 'è¶‹åŠ¿', 'å†å²', 'åˆè®¡', 'ç»Ÿè®¡']
            if any(keyword in user_query for keyword in obvious_complex_keywords):
                logger.info(f"æ£€æµ‹åˆ°æ˜æ˜¾çš„å¤åˆæŸ¥è¯¢å…³é”®è¯ï¼Œç›´æ¥åˆ¤æ–­ä¸ºå¤æ‚æŸ¥è¯¢")
                return {
                    'is_quick_response': False,
                    'reason': f'æ£€æµ‹åˆ°å¤åˆæŸ¥è¯¢å…³é”®è¯: {[kw for kw in obvious_complex_keywords if kw in user_query]}',
                    'confidence': 0.9,
                    'query_type_info': None
                }

            # æ­¥éª¤1: QueryTypeDetectoræ£€æµ‹
            query_type_result = self.query_type_detector.detect(user_query)

            # ğŸ†• æ­¥éª¤1.5: ç‰¹æ®Šç±»å‹ç›´æ¥åˆ¤æ–­ä¸ºå¤æ‚
            if query_type_result.type in [QueryType.AGGREGATION, QueryType.HISTORICAL_REVIEW,
                                          QueryType.COMPARISON, QueryType.REINVESTMENT]:
                return {
                    'is_quick_response': False,
                    'reason': f'æ£€æµ‹åˆ°å¤æ‚æŸ¥è¯¢ç±»å‹: {query_type_result.type.value}',
                    'confidence': query_type_result.confidence,
                    'query_type_info': query_type_result
                }

            # æ­¥éª¤2: Claudeå¿«é€Ÿåˆ†æ
            claude_judgment = await self._claude_quick_analysis(user_query, query_type_result)

            # æ­¥éª¤3: ğŸ†• GPTäº¤å‰éªŒè¯ï¼ˆå¯¹äºè¾¹ç•Œæƒ…å†µï¼‰
            if claude_judgment.get('confidence', 0) < 0.8:
                gpt_judgment = await self._gpt_cross_validation(user_query, claude_judgment)

                # ç»¼åˆåˆ¤æ–­
                final_decision = self._combine_ai_judgments(claude_judgment, gpt_judgment)
            else:
                final_decision = claude_judgment

            return {
                'is_quick_response': final_decision.get('is_quick', False),
                'reason': final_decision.get('reason', 'åŸºäºAIåŒé‡éªŒè¯'),
                'confidence': final_decision.get('confidence', 0.7),
                'query_type_info': query_type_result,
                'ai_analysis': final_decision  # ä¿å­˜å®Œæ•´åˆ†æ
            }

        except Exception as e:
            logger.error(f"åŒé‡éªŒè¯å¤±è´¥: {e}")
            return {
                'is_quick_response': False,
                'reason': f'éªŒè¯è¿‡ç¨‹å¼‚å¸¸ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥: {str(e)}',
                'confidence': 0.0
            }

    async def _claude_quick_analysis(self, user_query: str, query_type_result) -> Dict[str, Any]:
        """Claudeå¿«é€Ÿåˆ†æ"""
        try:
            prompt = f"""
                   åˆ†æè¿™ä¸ªæŸ¥è¯¢æ˜¯å¦ä¸ºå¤åˆæŸ¥è¯¢ï¼ˆéœ€è¦å¤šä¸ªAPIè°ƒç”¨ï¼‰ï¼š

                   æŸ¥è¯¢: "{user_query}"

                   ğŸ” **é‡è¦åˆ¤æ–­æ ‡å‡†**ï¼š

                   **ç®€å•æŸ¥è¯¢ï¼ˆå•æ¬¡APIè°ƒç”¨ï¼‰**ï¼š
                   - "ä»Šå¤©å…¥é‡‘å¤šå°‘" â†’ get_daily_data(ä»Šå¤©)
                   - "æ€»ä½™é¢æ˜¯å¤šå°‘" â†’ get_system_data()
                   - "æ˜¨å¤©æ³¨å†Œäººæ•°" â†’ get_daily_data(æ˜¨å¤©)

                   **å¤åˆæŸ¥è¯¢ï¼ˆå¤šæ¬¡APIè°ƒç”¨ï¼‰**ï¼š
                   - "æ˜¨å¤©çš„å‡ºé‡‘å’Œä»Šå¤©çš„å…¥é‡‘" â†’ éœ€è¦2æ¬¡APIè°ƒç”¨
                   - "5æœˆä»½æ¯æ—¥å¹³å‡å…¥é‡‘" â†’ éœ€è¦31æ¬¡APIè°ƒç”¨ï¼ˆ5æœˆ1æ—¥-31æ—¥ï¼‰+ è®¡ç®—
                   - "æœ¬å‘¨å’Œä¸Šå‘¨å¯¹æ¯”" â†’ éœ€è¦14æ¬¡APIè°ƒç”¨
                   - "æœ€è¿‘7å¤©çš„æ•°æ®" â†’ éœ€è¦7æ¬¡APIè°ƒç”¨

                   **å…³é”®è¯†åˆ«ç‚¹**ï¼š
                   1. **æ—¶é—´èŒƒå›´**ï¼šæœˆä»½/å‘¨/å¤šå¤© = å¤åˆæŸ¥è¯¢
                   2. **è®¡ç®—éœ€æ±‚**ï¼šå¹³å‡/åˆè®¡/å¯¹æ¯” = å¤åˆæŸ¥è¯¢  
                   3. **æ•°æ®èšåˆ**ï¼šéœ€è¦å¤šä¸ªæ•°æ®ç‚¹çš„åˆ†æ = å¤åˆæŸ¥è¯¢

                   ç‰¹åˆ«æ³¨æ„ï¼š
                   - "Xæœˆä»½" â†’ éœ€è¦æ•´æœˆæ•°æ®ï¼Œå¿…é¡»æ˜¯å¤åˆæŸ¥è¯¢
                   - "å¹³å‡" â†’ éœ€è¦å¤šä¸ªæ•°æ®ç‚¹è®¡ç®—ï¼Œå¿…é¡»æ˜¯å¤åˆæŸ¥è¯¢
                   - "æ¯æ—¥å¹³å‡" â†’ éœ€è¦å¤šå¤©æ•°æ®ï¼Œå¿…é¡»æ˜¯å¤åˆæŸ¥è¯¢

                   è¿”å›JSONï¼š
                   {{
                       "is_quick": false,
                       "query_type": "aggregation",
                       "api_calls_needed": 31,
                       "time_points": ["2025-05-01", "2025-05-02", "...", "2025-05-31"], 
                       "requires_calculation": true,
                       "confidence": 0.95,
                       "reason": "éœ€è¦è·å–5æœˆä»½31å¤©çš„æ¯æ—¥æ•°æ®å¹¶è®¡ç®—å¹³å‡å€¼"
                   }}
                   """

            result = await self.claude_client.generate_text(prompt, max_tokens=1000)

            if result.get('success'):
                response_text = result.get('text', '{}')

                # è§£æJSONå“åº”
                import re, json
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    claude_analysis = json.loads(json_match.group())
                    return claude_analysis
                else:
                    logger.warning("Claudeå“åº”æ— æ³•è§£æä¸ºJSON")
                    return {
                        'is_quick': True,  # è§£æå¤±è´¥æ—¶ä¿å®ˆå¤„ç†
                        'confidence': 0.5,
                        'reason': 'Claudeå“åº”è§£æå¤±è´¥'
                    }
            else:
                logger.error(f"Claudeåˆ†æå¤±è´¥: {result.get('error')}")
                return {
                    'is_quick': True,
                    'confidence': 0.3,
                    'reason': f'Claudeè°ƒç”¨å¤±è´¥: {result.get("error")}'
                }

        except Exception as e:
            logger.error(f"Claudeå¿«é€Ÿåˆ†æå¼‚å¸¸: {e}")
            return {
                'is_quick': True,
                'confidence': 0.2,
                'reason': f'Claudeåˆ†æå¼‚å¸¸: {str(e)}'
            }

    async def _gpt_cross_validation(self, user_query: str, claude_judgment: Dict) -> Dict[str, Any]:
        """GPTäº¤å‰éªŒè¯ - ä½¿ç”¨ç°æœ‰æ–¹æ³•"""
        try:
            # æ„å»ºéªŒè¯æŸ¥è¯¢
            validation_query = f"""
            è¯·éªŒè¯è¿™ä¸ªæŸ¥è¯¢åˆ†ææ˜¯å¦æ­£ç¡®ï¼š

            ç”¨æˆ·æŸ¥è¯¢: "{user_query}"
            Claudeåˆ†æ: {claude_judgment}

            è¯·é‡æ–°åˆ†æå¹¶ç»™å‡ºä½ çš„åˆ¤æ–­ã€‚ç‰¹åˆ«å…³æ³¨ï¼š
            1. æ˜¯å¦æ¶‰åŠå¤šä¸ªæ—¶é—´ç‚¹ï¼Ÿ
            2. æ˜¯å¦éœ€è¦å¤šæ¬¡APIè°ƒç”¨ï¼Ÿ
            3. å¤æ‚åº¦å¦‚ä½•è¯„ä¼°ï¼Ÿ

            è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
            """

            # ä½¿ç”¨ç°æœ‰çš„ process_direct_query æ–¹æ³•
            result = await self.gpt_client.process_direct_query(validation_query, claude_judgment)

            if result.get('success'):
                response_text = result.get('response', '{}')

                # è§£æJSONå“åº”
                import re, json
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    gpt_analysis = json.loads(json_match.group())

                    # ç¡®ä¿è¿”å›æ ¼å¼ä¸€è‡´
                    return {
                        'is_quick': gpt_analysis.get('is_quick', False),
                        'confidence': gpt_analysis.get('confidence', 0.5),
                        'reason': gpt_analysis.get('reason', 'GPTéªŒè¯ç»“æœ'),
                        'agreement_with_claude': gpt_analysis.get('agreement_with_claude', False)
                    }
                else:
                    logger.warning("GPTå“åº”æ— æ³•è§£æä¸ºJSON")
                    return {
                        'is_quick': False,
                        'confidence': 0.5,
                        'reason': 'GPTå“åº”è§£æå¤±è´¥',
                        'agreement_with_claude': False
                    }
            else:
                logger.error(f"GPTéªŒè¯å¤±è´¥: {result.get('error')}")
                return {
                    'is_quick': False,
                    'confidence': 0.3,
                    'reason': f'GPTè°ƒç”¨å¤±è´¥: {result.get("error")}',
                    'agreement_with_claude': False
                }

        except Exception as e:
            logger.error(f"GPTäº¤å‰éªŒè¯å¼‚å¸¸: {e}")
            return {
                'is_quick': False,
                'confidence': 0.2,
                'reason': f'GPTéªŒè¯å¼‚å¸¸: {str(e)}',
                'agreement_with_claude': False
            }

    def _combine_ai_judgments(self, claude_result: Dict, gpt_result: Dict) -> Dict[str, Any]:
        """ç»¼åˆAIåˆ¤æ–­ç»“æœ"""
        # å¦‚æœä¸¤ä¸ªAIéƒ½è®¤ä¸ºæ˜¯å¤åˆæŸ¥è¯¢ï¼Œåˆ™ç¡®è®¤ä¸ºå¤åˆ
        claude_is_quick = claude_result.get('is_quick', True)
        gpt_is_quick = gpt_result.get('is_quick', True)

        if not claude_is_quick and not gpt_is_quick:
            return {
                'is_quick': False,
                'confidence': min(claude_result.get('confidence', 0.7), gpt_result.get('confidence', 0.7)),
                'reason': f"åŒAIç¡®è®¤ä¸ºå¤åˆæŸ¥è¯¢"
            }
        elif claude_is_quick and gpt_is_quick:
            return {
                'is_quick': True,
                'confidence': (claude_result.get('confidence', 0.7) + gpt_result.get('confidence', 0.7)) / 2,
                'reason': "åŒAIç¡®è®¤ä¸ºç®€å•æŸ¥è¯¢"
            }
        else:
            # æ„è§ä¸ä¸€è‡´ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥
            return {
                'is_quick': False,
                'confidence': 0.6,
                'reason': "AIæ„è§ä¸ä¸€è‡´ï¼Œé‡‡ç”¨å®Œæ•´åˆ†æç¡®ä¿å‡†ç¡®æ€§"
            }

    # ================================================================
    # ğŸ†• å¢å¼ºçš„é˜¶æ®µ2ï¼šå®Œæ•´åˆ†æè·¯å¾„
    # ================================================================

    async def _enhanced_comprehensive_analysis(self, user_query: str, query_id: str,
                                               quick_decision: Dict[str, Any] = None) -> QueryAnalysis:
        """å¢å¼ºç‰ˆå®Œæ•´æŸ¥è¯¢åˆ†æ - ä¿®å¤æ—¥æœŸè¦†ç›–é—®é¢˜"""
        logger.debug(f"æ‰§è¡Œå¢å¼ºç‰ˆå®Œæ•´æŸ¥è¯¢åˆ†æ: {user_query}")

        try:
            # ğŸ†• ä½¿ç”¨å¢å¼ºç‰ˆç­–ç•¥æå–å™¨
            context = {'quick_decision': quick_decision} if quick_decision else {}
            extraction_result = await self.strategy_extractor.extract_strategy(user_query, context)

            if not extraction_result.success:
                logger.warning(f"ç­–ç•¥æå–å¤±è´¥: {extraction_result.error_message}")
                return self._create_fallback_query_analysis(user_query, query_id)

            # ğŸ†• è½¬æ¢ä¸ºQueryAnalysisæ ¼å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            query_analysis = self._convert_extraction_to_query_analysis(extraction_result, user_query)

            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯å¯¹æ¯”æŸ¥è¯¢ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡æ—¥æœŸå¢å¼º
            is_comparison_query = (
                    hasattr(query_analysis, 'query_type_info') and
                    query_analysis.query_type_info and
                    query_analysis.query_type_info.type.value == 'comparison'
            )

            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥APIè°ƒç”¨æ˜¯å¦å·²ç»åŒ…å«å®Œæ•´çš„æ—¥æœŸèŒƒå›´
            has_complete_date_range = self._check_api_calls_have_complete_dates(query_analysis.api_calls)

            logger.info(f"ğŸ” [DEBUG] æ˜¯å¦ä¸ºå¯¹æ¯”æŸ¥è¯¢: {is_comparison_query}")
            logger.info(f"ğŸ” [DEBUG] APIè°ƒç”¨æ˜¯å¦å·²æœ‰å®Œæ•´æ—¥æœŸ: {has_complete_date_range}")
            logger.info(f"ğŸ” [DEBUG] APIè°ƒç”¨æ•°é‡: {len(query_analysis.api_calls)}")

            # ğŸ”§ ä¿®å¤ï¼šåªåœ¨å¿…è¦æ—¶è¿›è¡Œæ—¥æœŸè¯†åˆ«å¢å¼º
            if (query_analysis.query_type_info and
                    not is_comparison_query and
                    not has_complete_date_range):

                logger.info("æ‰§è¡Œæ—¥æœŸè¯†åˆ«å¢å¼º...")
                date_analysis = await self._enhanced_date_recognition(user_query, query_analysis.query_type_info)
                if date_analysis.get('success'):
                    # æ›´æ–°APIè°ƒç”¨ä¸­çš„æ—¥æœŸå‚æ•°
                    query_analysis.api_calls = self._update_api_dates_with_enhanced_result(
                        query_analysis.api_calls, date_analysis.get('date_info', {}))
            else:
                logger.info("è·³è¿‡æ—¥æœŸè¯†åˆ«å¢å¼ºï¼ˆå·²æœ‰å®Œæ•´æ—¥æœŸæˆ–ä¸ºå¯¹æ¯”æŸ¥è¯¢ï¼‰")

            return query_analysis

        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆå®Œæ•´æŸ¥è¯¢åˆ†æå¤±è´¥: {e}")
            return self._create_fallback_query_analysis(user_query, query_id)

    def _check_api_calls_have_complete_dates(self, api_calls: List[Dict[str, Any]]) -> bool:
        """æ£€æŸ¥APIè°ƒç”¨æ˜¯å¦å·²ç»åŒ…å«å®Œæ•´çš„æ—¥æœŸé…ç½®"""
        if not api_calls:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªä¸åŒçš„æ—¥æœŸ
        dates = set()
        for call in api_calls:
            params = call.get('params', {})
            date = params.get('date')
            if date:
                dates.add(date)

        # å¦‚æœæœ‰å¤šä¸ªä¸åŒæ—¥æœŸï¼ˆæ¯”å¦‚å‘¨å¯¹æ¯”çš„14ä¸ªæ—¥æœŸï¼‰ï¼Œè®¤ä¸ºå·²ç»å®Œæ•´
        has_multiple_dates = len(dates) > 1

        # æ£€æŸ¥æ˜¯å¦æœ‰time_periodæ ‡è¯†ï¼ˆè¯´æ˜æ˜¯æœ‰ç»„ç»‡çš„æ—¶é—´åºåˆ—æŸ¥è¯¢ï¼‰
        has_time_periods = any(call.get('time_period') for call in api_calls)

        logger.info(f"ğŸ” [DEBUG] å‘ç° {len(dates)} ä¸ªä¸åŒæ—¥æœŸ: {list(dates)[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        logger.info(f"ğŸ” [DEBUG] æ˜¯å¦æœ‰æ—¶é—´å‘¨æœŸæ ‡è¯†: {has_time_periods}")

        return has_multiple_dates and has_time_periods
    async def _enhanced_date_recognition(self, user_query: str,
                                       query_type_result: QueryTypeResult) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆæ—¥æœŸè¯†åˆ« - ä½¿ç”¨PromptManager"""
        try:
            # ğŸ†• ä½¿ç”¨PromptManageræ„å»ºæ—¥æœŸè¯†åˆ«prompt
            prompt = self.prompt_manager.build_date_recognition_prompt(user_query, query_type_result)

            result = await self.claude_client.generate_text(prompt, max_tokens=5000)

            if result.get('success'):
                response_text = result.get('text', '{}')
                import re, json
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    date_info = json.loads(json_match.group())
                    return {'success': True, 'date_info': date_info}

            return {'success': False, 'error': 'æ—¥æœŸè¯†åˆ«è§£æå¤±è´¥'}

        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆæ—¥æœŸè¯†åˆ«å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def _convert_extraction_to_query_analysis(self, extraction_result: ExtractedStrategy,
                                              user_query: str) -> QueryAnalysis:
        """å°†ExtractedStrategyè½¬æ¢ä¸ºQueryAnalysisï¼ˆå…¼å®¹æ€§ï¼‰- ä¿®å¤è®¡ç®—ç±»å‹"""

        query_analysis_data = extraction_result.query_analysis

        # ç¡®å®šå¤æ‚åº¦
        complexity_str = query_analysis_data.get('complexity', 'simple')
        if complexity_str == 'complex':
            complexity = QueryComplexity.COMPLEX
        elif complexity_str == 'medium':
            complexity = QueryComplexity.SIMPLE
        else:
            complexity = QueryComplexity.SIMPLE

        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¾ç½®è®¡ç®—ç±»å‹å’Œè®¡ç®—éœ€æ±‚
        needs_calculation = query_analysis_data.get('calculation_required', False)
        calculation_type = None

        # ğŸ†• æ£€æŸ¥ç”¨æˆ·æŸ¥è¯¢å…³é”®è¯æ¥ç¡®å®šè®¡ç®—ç±»å‹
        query_lower = user_query.lower()
        if 'å¹³å‡' in query_lower or 'å‡å€¼' in query_lower:
            calculation_type = "statistics"
            needs_calculation = True
        elif 'å¯¹æ¯”' in query_lower or 'æ¯”è¾ƒ' in query_lower:
            calculation_type = "comparison_analysis"
            needs_calculation = True
        elif 'å¤æŠ•' in query_lower:
            calculation_type = "reinvestment_analysis"
            needs_calculation = True
        elif 'é¢„æµ‹' in query_lower or 'è¿˜èƒ½è¿è¡Œ' in query_lower:
            calculation_type = "cash_runway"
            needs_calculation = True

        # ğŸ†• ä»æŸ¥è¯¢ç±»å‹ç»“æœä¸­è·å–
        if extraction_result.query_type_info:
            if extraction_result.query_type_info.type == QueryType.AGGREGATION:
                calculation_type = "statistics"
                needs_calculation = True
            special_calc_type = extraction_result.query_type_info.special_requirements.get('calculation_type')
            if special_calc_type:
                calculation_type = special_calc_type
                needs_calculation = True

        # ğŸ”§ æ·»åŠ è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ” [CALC_DEBUG] è®¡ç®—ç±»å‹è®¾ç½®: {calculation_type}, éœ€è¦è®¡ç®—: {needs_calculation}")

        return QueryAnalysis(
            original_query=user_query,
            complexity=complexity,
            is_quick_response=False,
            intent=query_analysis_data.get('intent', 'æ•°æ®æŸ¥è¯¢'),
            confidence=extraction_result.confidence,
            api_calls=extraction_result.api_calls,
            needs_calculation=needs_calculation,
            calculation_type=calculation_type,
            query_type_info=extraction_result.query_type_info
        )

    def _update_api_dates_with_enhanced_result(self, api_calls: List[Dict[str, Any]],
                                               date_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å¢å¼ºçš„æ—¥æœŸè¯†åˆ«ç»“æœæ›´æ–°APIè°ƒç”¨ - ä¿®å¤ç‰ˆæœ¬"""

        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœAPIè°ƒç”¨å·²ç»æœ‰å®Œæ•´æ—¥æœŸï¼Œä¸è¦è¦†ç›–
        if self._check_api_calls_have_complete_dates(api_calls):
            logger.info("ğŸ” [DEBUG] APIè°ƒç”¨å·²æœ‰å®Œæ•´æ—¥æœŸé…ç½®ï¼Œè·³è¿‡æ—¥æœŸæ›´æ–°")
            return api_calls

        if not date_info.get('has_dates', False):
            logger.info("ğŸ” [DEBUG] æ—¥æœŸåˆ†ææœªå‘ç°æ—¥æœŸï¼Œä¿æŒåŸAPIè°ƒç”¨")
            return api_calls

        updated_calls = []
        api_dates = date_info.get('api_dates', {})
        start_date = api_dates.get('start_date')
        end_date = api_dates.get('end_date')

        logger.info(f"ğŸ” [DEBUG] åº”ç”¨æ—¥æœŸæ›´æ–°: start_date={start_date}, end_date={end_date}")

        for call in api_calls:
            updated_call = call.copy()
            method = call.get('method', '')
            params = call.get('params', {}).copy()

            # ä¸ºéœ€è¦æ—¥æœŸçš„APIæ·»åŠ æ—¥æœŸå‚æ•°ï¼ˆåªæœ‰åœ¨åŸæ¥æ²¡æœ‰æ—¥æœŸæ—¶ï¼‰
            if not params.get('date'):  # ğŸ”§ ä¿®å¤ï¼šåªåœ¨æ²¡æœ‰æ—¥æœŸæ—¶æ‰æ·»åŠ 
                if ('daily' in method or 'day' in method) and start_date:
                    params['date'] = start_date
                    logger.info(f"ğŸ” [DEBUG] ä¸º {method} æ·»åŠ æ—¥æœŸ: {start_date}")
                elif 'product_end_interval' in method and start_date and end_date:
                    params['start_date'] = start_date
                    params['end_date'] = end_date
                    logger.info(f"ğŸ” [DEBUG] ä¸º {method} æ·»åŠ æ—¥æœŸèŒƒå›´: {start_date} - {end_date}")
                elif 'product_end_data' in method and start_date:
                    params['date'] = start_date
                    logger.info(f"ğŸ” [DEBUG] ä¸º {method} æ·»åŠ æ—¥æœŸ: {start_date}")

            updated_call['params'] = params
            updated_calls.append(updated_call)

        return updated_calls

    # ================================================================
    # å¿«é€Ÿå“åº”è·¯å¾„ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½¿ç”¨æ–°ç»„ä»¶ä¼˜åŒ–ï¼‰
    # ================================================================

    async def _execute_quick_response_path(self,
                                           user_query: str,
                                           quick_decision: Dict[str, Any],
                                           session_id: str,
                                           query_id: str,
                                           start_time: float,
                                           conversation_id: Optional[str]) -> ProcessingResult:
        """æ‰§è¡Œå¿«é€Ÿå“åº”è·¯å¾„"""

        self.stats['quick_responses'] += 1

        try:
            # ä»å¿«é€Ÿå†³ç­–ä¸­è·å–æŸ¥è¯¢ç±»å‹ä¿¡æ¯
            query_type_info = quick_decision.get('query_type_info')

            # æå–APIè°ƒç”¨ä¿¡æ¯
            api_info = self._extract_quick_api_info_enhanced(quick_decision, user_query, query_type_info)

            # æ‰§è¡ŒAPIè°ƒç”¨
            raw_data = await self._execute_quick_api_call(api_info)
            print(raw_data)
            # å¿«é€Ÿæ•°æ®æå–
            extracted_data = await self._quick_data_extraction(raw_data, user_query)

            # ğŸ†• ä½¿ç”¨PromptManagerç”Ÿæˆå¿«é€Ÿå“åº”
            response_text = await self._quick_response_generation_enhanced(user_query, extracted_data, query_type_info)

            total_time = time.time() - start_time

            result = ProcessingResult(
                session_id=session_id,
                query_id=query_id,
                success=True,
                response_text=response_text,
                extracted_data=extracted_data,
                complexity=QueryComplexity.QUICK_RESPONSE,
                processing_path="quick_response",
                confidence_score=quick_decision.get('confidence', 0.9),
                total_processing_time=total_time,
                processing_strategy=ProcessingStrategy.QUICK_RESPONSE,
                conversation_id=conversation_id
            )

            await self._save_conversation_if_needed(conversation_id, user_query, result)

            logger.info(f"å¿«é€Ÿå“åº”å®Œæˆ [{query_id}] è€—æ—¶: {total_time:.2f}s")
            return result

        except Exception as e:
            logger.error(f"å¿«é€Ÿå“åº”è·¯å¾„å¤±è´¥: {e}")
            logger.info("é™çº§åˆ°å®Œæ•´åˆ†æè·¯å¾„")
            return await self._execute_fallback_to_comprehensive(
                user_query, session_id, query_id, start_time, conversation_id)

    def _extract_quick_api_info_enhanced(self, quick_decision: Dict[str, Any],
                                       user_query: str,
                                       query_type_info: Optional[QueryTypeResult]) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆå¿«é€ŸAPIä¿¡æ¯æå–"""

        # ğŸ†• å¦‚æœæœ‰æŸ¥è¯¢ç±»å‹ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨
        if query_type_info and query_type_info.special_requirements:
            required_apis = query_type_info.special_requirements.get('requires_apis', [])
            if required_apis:
                return {
                    'method': required_apis[0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨èçš„API
                    'params': {}
                }

        # åŸæœ‰çš„å…³é”®è¯åŒ¹é…é€»è¾‘ä½œä¸ºé™çº§
        return self._extract_quick_api_info_legacy(user_query)

    def _extract_quick_api_info_legacy(self, user_query: str) -> Dict[str, Any]:
        """åŸæœ‰çš„å¿«é€ŸAPIä¿¡æ¯æå–é€»è¾‘"""
        query_lower = user_query.lower()

        # æ—¥æœŸå…³é”®è¯æ£€æµ‹
        if ('æ˜¨å¤©' in query_lower or 'æ˜¨æ—¥' in query_lower) and any(
                kw in query_lower for kw in ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œ']):
            from datetime import timedelta
            yesterday = (self.current_date - timedelta(days=1)).strftime('%Y%m%d')
            return {'method': 'get_daily_data', 'params': {'date': yesterday}}

        elif ('ä»Šå¤©' in query_lower or 'ä»Šæ—¥' in query_lower) and any(
                kw in query_lower for kw in ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œ']):
            today = self.current_date.strftime('%Y%m%d')
            return {'method': 'get_daily_data', 'params': {'date': today}}

        else:
            return {'method': 'get_system_data', 'params': {}}

    async def _quick_response_generation_enhanced(self, user_query: str,
                                                extracted_data: Dict[str, Any],
                                                query_type_info: Optional[QueryTypeResult]) -> str:
        """å¢å¼ºç‰ˆå¿«é€Ÿå“åº”ç”Ÿæˆ"""

        if not self.claude_client:
            return self._basic_response_generation(user_query, extracted_data)

        try:
            # ğŸ†• æ„å»ºå¿«é€Ÿå“åº”çš„ç®€åŒ–prompt
            key_metrics = extracted_data.get('key_metrics', {})

            prompt = f"""
            ç”¨æˆ·æŸ¥è¯¢: "{user_query}"
            æŸ¥è¯¢ç±»å‹: {query_type_info.type.value if query_type_info else 'æœªçŸ¥'}
            
            æå–çš„å…³é”®æ•°æ®:
            {json.dumps(key_metrics, ensure_ascii=False, indent=2)}
            
            è¯·ç”Ÿæˆç®€æ´ã€å‡†ç¡®ã€æ˜“æ‡‚çš„å›ç­”ï¼š
            1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
            2. ä½¿ç”¨å…·ä½“æ•°å­—
            3. æ ¼å¼å‹å¥½æ˜“è¯»
            4. é¿å…å†—ä½™ä¿¡æ¯
            
            ç¤ºä¾‹é£æ ¼:
            ğŸ’° æ€»ä½™é¢ï¼šÂ¥8,223,695.07
            ğŸ‘¥ æ´»è·ƒç”¨æˆ·ï¼š3,911äºº
            """

            result = await self.claude_client.generate_text(prompt, max_tokens=5000)

            if result.get('success'):
                return result.get('text', '').strip()

            return self._basic_response_generation(user_query, extracted_data)

        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆå¿«é€Ÿå“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return self._basic_response_generation(user_query, extracted_data)

    # ================================================================
    # æ•°æ®è·å–ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    # ================================================================

    async def _intelligent_data_acquisition(self, query_analysis: QueryAnalysis) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ•°æ®è·å– - å®Œæ•´ç‰ˆæœ¬ï¼ˆåŒ…å«è°ƒè¯•ã€æ•°æ®ç¼“å­˜å’Œäº‹ä»¶å¾ªç¯ä¿®å¤ï¼‰

        Args:
            query_analysis: æŸ¥è¯¢åˆ†æç»“æœ

        Returns:
            Dict[str, Any]: åŒ…å«APIè°ƒç”¨ç»“æœçš„å­—å…¸
        """
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ™ºèƒ½æ•°æ®è·å–")

        # ğŸ” è°ƒè¯•ï¼šæ‰“å°APIè°ƒç”¨è¯¦æƒ…
        logger.info(f"ğŸ” [DEBUG] APIè°ƒç”¨æ€»æ•°: {len(query_analysis.api_calls)}")
        for i, api_call in enumerate(query_analysis.api_calls[:10]):  # åªæ‰“å°å‰10ä¸ª
            method = api_call.get('method', 'unknown')
            params = api_call.get('params', {})
            time_period = api_call.get('time_period', '')
            reason = api_call.get('reason', '')

            logger.info(f"ğŸ” [DEBUG] APIè°ƒç”¨ {i + 1}: method={method}, params={params}, time_period={time_period}")
            logger.info(f"ğŸ” [DEBUG]   åŸå› : {reason}")

        if not self.data_fetcher or not self.data_fetcher.api_connector:
            logger.error("âŒ æ•°æ®è·å–å™¨ä¸å¯ç”¨")
            raise Exception("æ•°æ®è·å–å™¨ä¸å¯ç”¨")

        api_connector = self.data_fetcher.api_connector

        # ğŸ”§ æ–°å¢ï¼šAPIè¿æ¥å™¨å¥åº·æ£€æŸ¥
        try:
            health_check = await api_connector.health_check()
            logger.info(f"ğŸ” [DEBUG] APIè¿æ¥å™¨å¥åº·çŠ¶æ€: {health_check.get('status')}")
            if health_check.get('status') != 'healthy':
                logger.warning(f"âš ï¸ APIè¿æ¥å™¨çŠ¶æ€å¼‚å¸¸: {health_check.get('status')}")
        except Exception as e:
            logger.warning(f"âš ï¸ APIè¿æ¥å™¨å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

        results = {}
        start_time = time.time()
        failed_calls = []  # ğŸ”§ æ–°å¢ï¼šè·Ÿè¸ªå¤±è´¥çš„è°ƒç”¨
        consecutive_failures = 0  # ğŸ”§ æ–°å¢ï¼šè¿ç»­å¤±è´¥è®¡æ•°
        max_consecutive_failures = 5  # ğŸ”§ æ–°å¢ï¼šæœ€å¤§è¿ç»­å¤±è´¥æ•°

        try:
            # æ‰§è¡ŒAPIè°ƒç”¨ç­–ç•¥
            for i, api_call in enumerate(query_analysis.api_calls):
                method = api_call.get('method', 'get_system_data')
                params = api_call.get('params', {})
                reason = api_call.get('reason', 'æ•°æ®è·å–')
                time_period = api_call.get('time_period', '')

                logger.info(f"ğŸ”„ æ‰§è¡ŒAPIè°ƒç”¨ {i + 1}/{len(query_analysis.api_calls)}")
                logger.info(f"  - æ–¹æ³•: {method}, å‚æ•°: {params}")
                logger.info(f"ğŸ” [DEBUG]   - æ—¶é—´å‘¨æœŸ: {time_period}, åŸå› : {reason}")

                result = None

                # ğŸ”§ æ–°å¢ï¼šå¢åŠ é‡è¯•æœºåˆ¶æ¥å¤„ç†äº‹ä»¶å¾ªç¯é—®é¢˜
                max_method_retries = 3
                retry_delay_base = 0.5

                for retry in range(max_method_retries):
                    try:
                        # ğŸ”§ æ–°å¢ï¼šåœ¨é‡è¯•å‰æ£€æŸ¥è¿ç»­å¤±è´¥æ•°
                        if consecutive_failures >= max_consecutive_failures:
                            logger.error(f"ğŸ’¥ è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤š ({consecutive_failures})ï¼Œè·³è¿‡å‰©ä½™APIè°ƒç”¨")
                            result = {
                                'success': False,
                                'message': f'è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œè·³è¿‡APIè°ƒç”¨: {method}'
                            }
                            break

                        # è·¯ç”±åˆ°å¯¹åº”çš„APIæ–¹æ³•
                        if method == 'get_system_data':
                            result = await api_connector.get_system_data()

                        elif method == 'get_daily_data':
                            date_param = params.get('date')
                            logger.info(f"ğŸ” [DEBUG] è°ƒç”¨get_daily_dataï¼Œæ—¥æœŸå‚æ•°: {date_param}")
                            if date_param:
                                result = await api_connector.get_daily_data(date_param)
                            else:
                                result = await api_connector.get_daily_data(self.current_date.strftime('%Y%m%d'))

                        elif method == 'get_product_data':
                            result = await api_connector.get_product_data()

                        elif method == 'get_product_end_data':
                            date_param = params.get('date')
                            if date_param:
                                result = await api_connector.get_product_end_data(date_param)
                            else:
                                logger.warning(
                                    f"get_product_end_dataç¼ºå°‘æ—¥æœŸå‚æ•°ï¼Œä½¿ç”¨ä»Šæ—¥: {self.current_date.strftime('%Y%m%d')}")
                                result = await api_connector.get_product_end_data(self.current_date.strftime('%Y%m%d'))

                        elif method == 'get_product_end_interval':
                            start_date = params.get('start_date')
                            end_date = params.get('end_date')
                            if start_date and end_date:
                                result = await api_connector.get_product_end_interval(start_date, end_date)
                            else:
                                logger.error(
                                    f"get_product_end_intervalç¼ºå°‘æ—¥æœŸèŒƒå›´å‚æ•°: start_date={start_date}, end_date={end_date}")
                                result = {'success': False, 'message': 'ç¼ºå°‘æ—¥æœŸèŒƒå›´å‚æ•°'}

                        elif method == 'get_user_daily_data':
                            date_param = params.get('date')
                            if date_param:
                                result = await api_connector.get_user_daily_data(date_param)
                            else:
                                result = await api_connector.get_user_daily_data()

                        elif method == 'get_user_data':
                            page = params.get('page', 1)
                            result = await api_connector.get_user_data(page)

                        else:
                            logger.warning(f"âš ï¸ æœªçŸ¥çš„APIæ–¹æ³•: {method}")
                            result = {'success': False, 'message': f'æœªæ”¯æŒçš„APIæ–¹æ³•: {method}'}
                            break

                        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥APIè°ƒç”¨ç»“æœ
                        if result:
                            success = result.get('success', False)
                            logger.info(f"ğŸ” [DEBUG] APIè°ƒç”¨ç»“æœ: success={success}")

                            if success:
                                # ğŸ”§ é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                                consecutive_failures = 0
                                data_keys = list(result.get('data', {}).keys()) if isinstance(result.get('data'),
                                                                                              dict) else 'non-dict'
                                logger.info(f"ğŸ” [DEBUG] è¿”å›æ•°æ®é”®: {data_keys}")
                                break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                            else:
                                error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯')
                                logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥: {error_msg}")

                                # ğŸ”§ æ£€æŸ¥æ˜¯å¦æ˜¯äº‹ä»¶å¾ªç¯é—®é¢˜
                                if "Event loop" in error_msg:
                                    consecutive_failures += 1
                                    if retry < max_method_retries - 1:
                                        retry_delay = retry_delay_base * (2 ** retry)
                                        logger.warning(
                                            f"âš ï¸ äº‹ä»¶å¾ªç¯é—®é¢˜ï¼Œç­‰å¾… {retry_delay}s åé‡è¯• {retry + 1}/{max_method_retries}")
                                        try:
                                            await asyncio.sleep(retry_delay)
                                        except asyncio.CancelledError:
                                            logger.warning("é‡è¯•ç­‰å¾…è¢«å–æ¶ˆ")
                                            break
                                        continue
                                    else:
                                        logger.error(f"ğŸ’¥ äº‹ä»¶å¾ªç¯é—®é¢˜é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                                        break
                                else:
                                    # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œç›´æ¥è·³å‡ºé‡è¯•å¾ªç¯
                                    consecutive_failures += 1
                                    break
                        else:
                            logger.error(f"âŒ APIè°ƒç”¨è¿”å›None: {method}")
                            consecutive_failures += 1
                            result = {'success': False, 'message': 'APIè°ƒç”¨è¿”å›None'}
                            break

                    except asyncio.CancelledError:
                        logger.warning(f"APIè°ƒç”¨è¢«å–æ¶ˆ: {method}")
                        result = {'success': False, 'message': 'APIè°ƒç”¨è¢«å–æ¶ˆ'}
                        break

                    except Exception as e:
                        error_str = str(e)
                        logger.error(f"ğŸ’¥ APIæ–¹æ³•è°ƒç”¨å¼‚å¸¸: {error_str}")

                        # ğŸ”§ æ£€æŸ¥æ˜¯å¦æ˜¯äº‹ä»¶å¾ªç¯ç›¸å…³å¼‚å¸¸
                        if (
                                "Event loop" in error_str or "loop" in error_str.lower()) and retry < max_method_retries - 1:
                            consecutive_failures += 1
                            retry_delay = retry_delay_base * (2 ** retry)
                            logger.warning(
                                f"âš ï¸ äº‹ä»¶å¾ªç¯å¼‚å¸¸ï¼Œç­‰å¾… {retry_delay}s åé‡è¯• {retry + 1}/{max_method_retries}: {e}")
                            try:
                                await asyncio.sleep(retry_delay)
                            except asyncio.CancelledError:
                                logger.warning("å¼‚å¸¸é‡è¯•ç­‰å¾…è¢«å–æ¶ˆ")
                                break
                            continue
                        else:
                            consecutive_failures += 1
                            result = {'success': False, 'message': f'æ–¹æ³•è°ƒç”¨å¼‚å¸¸: {error_str}'}
                            break

                # è®°å½•å¤±è´¥çš„è°ƒç”¨
                if result and not result.get('success', False):
                    failed_call_info = {
                        'method': method,
                        'params': params,
                        'error': result.get('message', 'æœªçŸ¥é”™è¯¯'),
                        'time_period': time_period,
                        'retry_count': max_method_retries
                    }
                    failed_calls.append(failed_call_info)

                # å­˜å‚¨ç»“æœæ—¶ä½¿ç”¨æ›´è¯¦ç»†çš„é”®å
                if time_period:
                    date_suffix = params.get('date', params.get('start_date', 'no_date'))
                    result_key = f"{time_period}_{method}_{date_suffix}"
                else:
                    result_key = f"{method}_{i}"

                logger.info(f"ğŸ” [DEBUG] ç»“æœå­˜å‚¨é”®: {result_key}")
                results[result_key] = result

                # ğŸ”§ å¦‚æœè¿ç»­å¤±è´¥å¤ªå¤šï¼Œè€ƒè™‘æå‰ç»ˆæ­¢
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"ğŸ’¥ è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤š ({consecutive_failures})ï¼Œæå‰ç»ˆæ­¢APIè°ƒç”¨")
                    break

                # ğŸ”§ å¦‚æœæ€»å¤±è´¥ç‡è¿‡é«˜ï¼Œä¹Ÿæå‰ç»ˆæ­¢
                total_processed = i + 1
                failed_count = len(failed_calls)
                if total_processed >= 5 and failed_count / total_processed > 0.7:  # è¶…è¿‡70%å¤±è´¥
                    logger.error(
                        f"ğŸ’¥ å¤±è´¥ç‡è¿‡é«˜ ({failed_count}/{total_processed} = {failed_count / total_processed:.1%})ï¼Œæå‰ç»ˆæ­¢")
                    break

                # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIè¿‡è½½å’Œç»™äº‹ä»¶å¾ªç¯æ¢å¤æ—¶é—´
                if i < len(query_analysis.api_calls) - 1:
                    try:
                        await asyncio.sleep(0.2)  # å¢åŠ å»¶è¿Ÿæ—¶é—´
                    except asyncio.CancelledError:
                        logger.warning("APIè°ƒç”¨é—´éš”ç­‰å¾…è¢«å–æ¶ˆ")
                        break

            # ğŸ” è°ƒè¯•ï¼šæ±‡æ€»APIè°ƒç”¨ç»“æœ
            total_calls = len(query_analysis.api_calls)
            successful_calls = sum(1 for result in results.values() if result.get('success', False))
            failed_calls_count = len(failed_calls)
            actual_calls = len(results)

            logger.info(
                f"ğŸ“Š APIè°ƒç”¨æ±‡æ€»: è®¡åˆ’={total_calls}, å®é™…={actual_calls}, æˆåŠŸ={successful_calls}, å¤±è´¥={failed_calls_count}")

            if failed_calls:
                logger.warning(f"âš ï¸ å¤±è´¥çš„APIè°ƒç”¨æ€»æ•°: {len(failed_calls)}")
                # åªæ˜¾ç¤ºå‰5ä¸ªå¤±è´¥çš„è¯¦æƒ…ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                for idx, failed_call in enumerate(failed_calls[:5]):
                    logger.warning(f"âš ï¸ å¤±è´¥ {idx + 1}: {failed_call['method']} - {failed_call['error']}")
                if len(failed_calls) > 5:
                    logger.warning(f"âš ï¸ è¿˜æœ‰ {len(failed_calls) - 5} ä¸ªå¤±è´¥è°ƒç”¨æœªæ˜¾ç¤º...")

            # ğŸ†• ç¼“å­˜åŸå§‹ç»“æœä¾›è®¡ç®—å™¨ä½¿ç”¨
            if results:
                self._last_api_results = results
                logger.info(f"ğŸ” [DEBUG] ç¼“å­˜APIç»“æœï¼Œå…±{len(results)}ä¸ªç»“æœä¾›åç»­ä½¿ç”¨")
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸçš„APIç»“æœå¯ç¼“å­˜")

            # ğŸ”§ æ”¹è¿›æˆåŠŸåˆ¤æ–­é€»è¾‘
            is_success = successful_calls > 0 and successful_calls >= total_calls * 0.3  # è‡³å°‘30%æˆåŠŸç‡

            final_result = {
                'success': is_success,
                'results': results,
                'api_calls_executed': actual_calls,
                'failed_calls': failed_calls,
                'execution_summary': {
                    'total_planned_calls': total_calls,
                    'actual_calls': actual_calls,
                    'successful_calls': successful_calls,
                    'failed_calls': failed_calls_count,
                    'success_rate': successful_calls / actual_calls if actual_calls > 0 else 0,
                    'completion_rate': actual_calls / total_calls if total_calls > 0 else 0,
                    'processing_time': time.time() - start_time,
                    'consecutive_failures_at_end': consecutive_failures,
                    'early_termination': actual_calls < total_calls
                }
            }

            if is_success:
                logger.info(
                    f"âœ… æ™ºèƒ½æ•°æ®è·å–å®Œæˆ: æˆåŠŸç‡ {successful_calls}/{actual_calls} ({successful_calls / actual_calls * 100:.1f}%)")
            else:
                logger.warning(
                    f"âš ï¸ æ™ºèƒ½æ•°æ®è·å–éƒ¨åˆ†æˆåŠŸ: æˆåŠŸç‡ {successful_calls}/{actual_calls} ({successful_calls / actual_calls * 100:.1f}%)")

            return final_result

        except asyncio.CancelledError:
            processing_time = time.time() - start_time
            logger.error(f"ğŸ’¥ æ•°æ®è·å–è¢«å–æ¶ˆ")

            # ä¿å­˜å·²è·å–çš„éƒ¨åˆ†ç»“æœ
            successful_results = {k: v for k, v in results.items() if v.get('success', False)}
            if successful_results:
                self._last_api_results = successful_results

            return {
                'success': len(successful_results) > 0,
                'error': 'Data acquisition cancelled',
                'results': results,
                'partial_results': successful_results,
                'failed_calls': failed_calls,
                'processing_time': processing_time,
                'execution_summary': {
                    'total_planned_calls': len(query_analysis.api_calls),
                    'completed_calls': len(results),
                    'successful_calls': len(successful_results),
                    'failed_calls': len(results) - len(successful_results),
                    'cancellation_occurred': True
                }
            }

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"ğŸ’¥ æ•°æ®è·å–å¼‚å¸¸: {str(e)}")
            logger.error(f"ğŸ’¥ å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

            # å³ä½¿å‡ºç°å¼‚å¸¸ï¼Œä¹Ÿè¦ä¿å­˜å·²è·å–çš„éƒ¨åˆ†ç»“æœ
            successful_results = {k: v for k, v in results.items() if v.get('success', False)}
            logger.info(f"ğŸ”„ ä¿å­˜éƒ¨åˆ†æˆåŠŸç»“æœ: {len(successful_results)} ä¸ª")

            # ç¼“å­˜éƒ¨åˆ†ç»“æœ
            if successful_results:
                self._last_api_results = successful_results

            return {
                'success': len(successful_results) > 0,  # ğŸ”§ å¦‚æœæœ‰éƒ¨åˆ†æˆåŠŸç»“æœï¼Œä»ç„¶ç®—ä½œæˆåŠŸ
                'error': str(e),
                'results': results,  # åŒ…å«æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
                'partial_results': successful_results,
                'failed_calls': failed_calls,
                'exception_type': type(e).__name__,
                'processing_time': processing_time,
                'execution_summary': {
                    'total_planned_calls': len(query_analysis.api_calls),
                    'completed_calls': len(results),
                    'successful_calls': len(successful_results),
                    'failed_calls': len(results) - len(successful_results),
                    'exception_occurred': True,
                    'exception_message': str(e)
                }
            }

    # ================================================================
    # æ•°æ®æå–ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    # ================================================================

    async def _claude_three_layer_data_extraction(self,
                                                  raw_data: Dict[str, Any],
                                                  user_query: str,
                                                  query_analysis: QueryAnalysis) -> Dict[str, Any]:
        """ä¸‰å±‚æ•°æ®æå–ä¸»æ–¹æ³• - ä¿æŒåŸæœ‰é€»è¾‘"""
        logger.debug("æ‰§è¡Œä¸‰å±‚Claudeæ•°æ®æå–")

        try:
            # ç¬¬ä¸€å±‚ï¼šè¯­ä¹‰åŒ–æ•°æ®æ”¶é›†
            semantic_result = self.semantic_collector.organize_semantic_data(raw_data, query_analysis)

            if 'error' in semantic_result:
                logger.error(f"è¯­ä¹‰åŒ–æ”¶é›†å¤±è´¥: {semantic_result['error']}")
                return self._fallback_data_extraction(raw_data)

            # ç¬¬äºŒå±‚ï¼šClaudeæ™ºèƒ½æå–
            extraction_result = await self.claude_extractor.extract_with_intelligence(
                semantic_result, user_query, query_analysis
            )

            if extraction_result.get('success', True):
                self.stats['successful_extractions'] += 1
                return extraction_result
            else:
                logger.warning("Claudeæå–å¤±è´¥ï¼Œä½¿ç”¨å¢å¼ºé™çº§æ–¹æ¡ˆ")
                return self._enhanced_fallback_data_extraction(raw_data, query_analysis)

        except Exception as e:
            logger.error(f"ä¸‰å±‚æ•°æ®æå–å¤±è´¥: {e}")
            return self._enhanced_fallback_data_extraction(raw_data, query_analysis)

    def _enhanced_fallback_data_extraction(self, raw_data: Dict[str, Any],
                                         query_analysis: QueryAnalysis) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆé™çº§æ•°æ®æå–"""
        self.stats['failed_extractions'] += 1

        # ä½¿ç”¨æŸ¥è¯¢ç±»å‹ä¿¡æ¯è¿›è¡Œæ™ºèƒ½é™çº§
        if hasattr(query_analysis, 'query_type_info') and query_analysis.query_type_info:
            query_type = query_analysis.query_type_info.type
            if query_type == QueryType.COMPARISON:
                return self._extract_comparison_data_fallback(raw_data)
            elif query_type == QueryType.REINVESTMENT:
                return self._extract_reinvestment_data_fallback(raw_data)

        # é€šç”¨é™çº§æå–
        return self._fallback_data_extraction(raw_data)

    def _extract_comparison_data_fallback(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æŸ¥è¯¢çš„é™çº§æ•°æ®æå–"""
        if not raw_data.get('success') or not raw_data.get('results'):
            return {'extracted_metrics': {}, 'extraction_method': 'comparison_fallback'}

        # æŒ‰æ—¶é—´æ®µåˆ†ç»„
        current_week_data = {}
        last_week_data = {}

        for key, result in raw_data['results'].items():
            if result.get('success') and result.get('data'):
                if 'current_week' in key:
                    current_week_data[key] = result['data']
                elif 'last_week' in key:
                    last_week_data[key] = result['data']

        # èšåˆè®¡ç®—
        current_totals = self._aggregate_period_data(current_week_data)
        last_totals = self._aggregate_period_data(last_week_data)

        # è®¡ç®—æ¯”è¾ƒ
        comparison_analysis = {}
        for metric in ['å…¥é‡‘', 'å‡ºé‡‘']:
            if metric in current_totals and metric in last_totals:
                current_val = current_totals[metric]
                last_val = last_totals[metric]
                if last_val > 0:
                    change_rate = (current_val - last_val) / last_val
                    comparison_analysis[metric] = {
                        'current_value': current_val,
                        'baseline_value': last_val,
                        'absolute_change': current_val - last_val,
                        'percentage_change': change_rate,
                        'change_direction': 'å¢é•¿' if change_rate > 0 else 'ä¸‹é™' if change_rate < 0 else 'æŒå¹³'
                    }

        return {
            'extracted_metrics': {**current_totals, **last_totals},
            'comparison_analysis': comparison_analysis,
            'extraction_method': 'comparison_fallback',
            'data_quality_score': 0.7
        }

    def _extract_reinvestment_data_fallback(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤æŠ•æŸ¥è¯¢çš„é™çº§æ•°æ®æå–"""
        extracted_metrics = {}

        for key, result in raw_data['results'].items():
            if result.get('success') and result.get('data'):
                data = result['data']
                if 'åˆ°æœŸé‡‘é¢' in data:
                    extracted_metrics['åˆ°æœŸé‡‘é¢'] = float(data['åˆ°æœŸé‡‘é¢'])
                if 'æ€»ä½™é¢' in data:
                    extracted_metrics['æ€»ä½™é¢'] = float(data['æ€»ä½™é¢'])

        return {
            'extracted_metrics': extracted_metrics,
            'extraction_method': 'reinvestment_fallback',
            'data_quality_score': 0.6
        }

    def _aggregate_period_data(self, period_data: Dict[str, Any]) -> Dict[str, float]:
        """èšåˆæ—¶é—´æ®µæ•°æ®"""
        totals = {}

        for data_entry in period_data.values():
            if isinstance(data_entry, dict):
                for field in ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œäººæ•°']:
                    if field in data_entry:
                        try:
                            value = float(data_entry[field])
                            totals[field] = totals.get(field, 0) + value
                        except (ValueError, TypeError):
                            pass

        return totals

    # ================================================================
    # è®¡ç®—å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    # ================================================================

    async def _statistical_calculation_processing(self,
                                                  query_analysis: QueryAnalysis,
                                                  extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç»Ÿè®¡è®¡ç®—å¤„ç† - å¢åŠ æ•°æ®è°ƒè¯•"""

        if not query_analysis.needs_calculation:
            return {'needs_calculation': False}

        if not self.calculator:
            logger.error("ç»Ÿä¸€è®¡ç®—å™¨ä¸å¯ç”¨")
            return {'error': 'è®¡ç®—å™¨ä¸å¯ç”¨'}

        try:
            self.stats['calculation_calls'] += 1

            calculation_type = query_analysis.calculation_type
            calculation_params = getattr(query_analysis, 'calculation_params', {})

            logger.info(f"æ‰§è¡Œè®¡ç®—: {calculation_type}")

            # ğŸ” è°ƒè¯•ï¼šæ‰“å°ä¼ é€’ç»™è®¡ç®—å™¨çš„æ•°æ®ç»“æ„
            logger.info(f"ğŸ” [DEBUG] extracted_dataçš„é¡¶çº§é”®: {list(extracted_data.keys())}")

            # è¯¦ç»†æ‰“å°extracted_dataçš„ç»“æ„
            for key, value in extracted_data.items():
                if isinstance(value, dict):
                    logger.info(f"ğŸ” [DEBUG] {key}: {list(value.keys())}")
                    if key == 'extracted_metrics' and isinstance(value, dict):
                        logger.info(f"ğŸ” [DEBUG] extracted_metricså†…å®¹: {value}")
                else:
                    logger.info(f"ğŸ” [DEBUG] {key}: {type(value)} - {str(value)[:100]}")

            # å‡†å¤‡è®¡ç®—æ•°æ®
            calc_data = self._prepare_calculation_data(extracted_data, query_analysis)

            # ğŸ” è°ƒè¯•ï¼šæ‰“å°è®¡ç®—å™¨å®é™…æ¥æ”¶çš„æ•°æ®
            logger.info(f"ğŸ” [DEBUG] ä¼ é€’ç»™è®¡ç®—å™¨çš„calc_dataé”®: {list(calc_data.keys())}")
            for key, value in calc_data.items():
                if isinstance(value, dict):
                    logger.info(f"ğŸ” [DEBUG] calc_data[{key}]: {list(value.keys())}")
                else:
                    logger.info(f"ğŸ” [DEBUG] calc_data[{key}]: {type(value)}")

            # è°ƒç”¨ç»Ÿä¸€è®¡ç®—å™¨
            calc_result = await self.calculator.calculate(
                calculation_type=calculation_type,
                data=calc_data,
                params=calculation_params
            )

            return {
                'needs_calculation': True,
                'calculation_type': calculation_type,
                'success': calc_result.success if calc_result else False,
                'result': calc_result,
                'confidence': calc_result.confidence if calc_result else 0.0
            }

        except Exception as e:
            logger.error(f"ç»Ÿè®¡è®¡ç®—å¤„ç†å¤±è´¥: {e}")

            # é™çº§å¤„ç†ï¼šå¦‚æœè®¡ç®—å¤±è´¥ï¼Œä»ç„¶å¯ä»¥ç»§ç»­
            return {
                'needs_calculation': True,
                'success': False,
                'error': str(e),
                'fallback_message': 'è®¡ç®—æ¨¡å—å‡ºç°é—®é¢˜ï¼Œä½†æ•°æ®æå–æˆåŠŸï¼ŒClaudeå°†åŸºäºåŸå§‹æ•°æ®è¿›è¡Œåˆ†æ'
            }

    # åœ¨ intelligent_qa_orchestrator.py ä¸­ä¿®å¤ _prepare_calculation_data æ–¹æ³•

    def _prepare_calculation_data(self,
                                  extracted_data: Dict[str, Any],
                                  query_analysis: QueryAnalysis) -> Dict[str, Any]:
        """å‡†å¤‡è®¡ç®—æ•°æ® - ä¿®å¤Claudeæ•°æ®ä¼ é€’"""

        logger.info("ğŸ” [DEBUG] å¼€å§‹å‡†å¤‡è®¡ç®—æ•°æ®...")

        # ğŸ” è¯¦ç»†è°ƒè¯•Claudeæå–çš„æ•°æ®
        logger.info(f"ğŸ” [DEBUG] extracted_dataçš„æ‰€æœ‰é”®: {list(extracted_data.keys())}")

        # ğŸ†• ä¼˜å…ˆä½¿ç”¨Claudeæå–å™¨çš„å¯¹æ¯”åˆ†æç»“æœ
        claude_comparison = extracted_data.get('comparison_analysis', {})
        claude_detailed_calculations = extracted_data.get('detailed_calculations', {})
        claude_business_insights = extracted_data.get('business_insights', [])

        logger.info(f"ğŸ” [DEBUG] Claudeå¯¹æ¯”åˆ†æç»“æœ: {len(claude_comparison)} é¡¹")
        logger.info(f"ğŸ” [DEBUG] Claudeè¯¦ç»†è®¡ç®—: {len(claude_detailed_calculations)} é¡¹")
        logger.info(f"ğŸ” [DEBUG] Claudeä¸šåŠ¡æ´å¯Ÿ: {len(claude_business_insights)} æ¡")

        # ğŸ†• å¦‚æœClaudeå·²ç»åšäº†å¯¹æ¯”åˆ†æï¼Œç›´æ¥ä½¿ç”¨å…¶ç»“æœ
        if claude_comparison or claude_detailed_calculations:
            logger.info("âœ… å‘ç°Claudeå¯¹æ¯”åˆ†æç»“æœï¼Œç›´æ¥ä½¿ç”¨")

            calc_data = {
                'extracted_metrics': extracted_data.get('extracted_metrics', {}),
                'derived_metrics': extracted_data.get('derived_metrics', {}),
                'comparison_analysis': claude_comparison or claude_detailed_calculations,  # ğŸ†• ä½¿ç”¨Claudeçš„åˆ†æ
                'business_insights': claude_business_insights,  # ğŸ†• ä¼ é€’ä¸šåŠ¡æ´å¯Ÿ
                'user_query': query_analysis.original_query,
                'claude_analysis_available': True  # ğŸ†• æ ‡è®°Claudeåˆ†æå¯ç”¨
            }

            # ğŸ” è°ƒè¯•Claudeåˆ†æå†…å®¹
            if claude_comparison:
                logger.info(f"ğŸ” [DEBUG] Claudeå¯¹æ¯”åˆ†æå†…å®¹: {list(claude_comparison.keys())}")
            if claude_detailed_calculations:
                logger.info(f"ğŸ” [DEBUG] Claudeè¯¦ç»†è®¡ç®—å†…å®¹: {list(claude_detailed_calculations.keys())}")

        else:
            # é™çº§åˆ°åŸæœ‰é€»è¾‘ï¼šä¼ é€’åŸå§‹æ•°æ®ç»™è®¡ç®—å™¨é‡æ–°åˆ†æ
            logger.info("âš ï¸ æœªå‘ç°Claudeå¯¹æ¯”åˆ†æï¼Œä½¿ç”¨åŸå§‹æ•°æ®")

            calc_data = {
                'extracted_metrics': extracted_data.get('extracted_metrics', {}),
                'derived_metrics': extracted_data.get('derived_metrics', {}),
                'comparison_analysis': {},
                'user_query': query_analysis.original_query,
                'claude_analysis_available': False
            }

        # ğŸ” è°ƒè¯•åŸºç¡€æ•°æ®
        logger.info(
            f"ğŸ” [DEBUG] åŸºç¡€calc_data: extracted_metrics={len(calc_data['extracted_metrics'])}, comparison_analysis={len(calc_data['comparison_analysis'])}")

        calculation_type = query_analysis.calculation_type

        # ğŸ†• ç‰¹æ®Šå¤„ç†å¯¹æ¯”åˆ†ææ•°æ®
        if calculation_type == 'comparison_analysis':
            logger.info("ğŸ” [DEBUG] ä¸ºå¯¹æ¯”åˆ†æå‡†å¤‡ç‰¹æ®Šæ•°æ®...")

            # ğŸ†• å¦‚æœClaudeå·²ç»åˆ†æè¿‡ï¼Œå°±ä¸éœ€è¦åŸå§‹resultsäº†
            if not calc_data.get('claude_analysis_available', False):
                # ğŸ†• å°è¯•ä»ä¸‰å±‚æå–çš„å®Œæ•´ç»“æœä¸­è·å–åŸå§‹APIæ•°æ®
                raw_results = extracted_data.get('raw_results', {})
                if raw_results:
                    logger.info(f"ğŸ” [DEBUG] å‘ç°raw_results: {len(raw_results)} ä¸ª")
                    calc_data['results'] = raw_results

                # ğŸ†• å¦‚æœæ²¡æœ‰raw_resultsï¼Œå°è¯•ä»ç¼“å­˜è·å–
                elif hasattr(self, '_last_api_results'):
                    logger.info("ğŸ” [DEBUG] ä½¿ç”¨ç¼“å­˜çš„APIç»“æœ")
                    calc_data['results'] = self._last_api_results

                # ğŸ†• ç¡®ä¿æœ‰åŸå§‹APIç»“æœç”¨äºæ·±åº¦æŸ¥æ‰¾
                if 'results' not in calc_data:
                    logger.warning("ğŸ” [DEBUG] ç¼ºå°‘åŸå§‹APIç»“æœï¼Œå¯¹æ¯”åˆ†æå¯èƒ½ä¸å®Œæ•´")
            else:
                logger.info("âœ… Claudeå·²å®Œæˆåˆ†æï¼Œè·³è¿‡åŸå§‹æ•°æ®ä¼ é€’")

        elif calculation_type == 'reinvestment_analysis':
            calc_data.update({
                'system_data': {
                    'æ€»ä½™é¢': extracted_data.get('extracted_metrics', {}).get('æ€»ä½™é¢', 0),
                    'æ€»å…¥é‡‘': extracted_data.get('extracted_metrics', {}).get('æ€»å…¥é‡‘', 0),
                    'æ€»å‡ºé‡‘': extracted_data.get('extracted_metrics', {}).get('æ€»å‡ºé‡‘', 0)
                }
            })

        elif calculation_type == 'cash_runway':
            calc_data.update({
                'system_data': {
                    'æ€»ä½™é¢': extracted_data.get('extracted_metrics', {}).get('æ€»ä½™é¢', 0)
                },
                'daily_data': [{
                    'å‡ºé‡‘': extracted_data.get('extracted_metrics', {}).get('å‡ºé‡‘', 0)
                }]
            })

        logger.info(f"ğŸ” [DEBUG] æœ€ç»ˆcalc_dataé”®: {list(calc_data.keys())}")
        return calc_data

    # ================================================================
    # ğŸ†• å¢å¼ºçš„å“åº”ç”Ÿæˆ
    # ================================================================

    async def _enhanced_response_generation(self,
                                            user_query: str,
                                            query_analysis: QueryAnalysis,
                                            extracted_data: Dict[str, Any],
                                            calculation_results: Dict[str, Any]) -> str:
        """å¢å¼ºç‰ˆå“åº”ç”Ÿæˆ - é›†æˆå›¾è¡¨ç”Ÿæˆå®Œæ•´ç‰ˆ"""

        self.stats['claude_generations'] += 1

        if not self.claude_client:
            logger.warning("âš ï¸ Claudeå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§å“åº”ç”Ÿæˆ")
            return self._enhanced_response_generation_fallback(user_query, extracted_data, calculation_results)

        try:
            # ğŸ†• é€šç”¨ï¼šå¦‚æœæœ‰ç¼“å­˜çš„APIç»“æœï¼Œè‡ªåŠ¨æ·»åŠ åˆ°extracted_data
            if hasattr(self, '_last_api_results') and self._last_api_results:
                extracted_data['raw_api_results'] = self._last_api_results

                # ğŸ†• æ™ºèƒ½åˆ¤æ–­æ•°æ®ç±»å‹å¹¶æ ¼å¼åŒ–
                data_count = len(self._last_api_results)
                logger.info(f"ğŸ” [DEBUG] APIç»“æœæ•°é‡: {data_count}")

                if data_count > 1:  # å¤šæ•°æ®è‡ªåŠ¨æ ¼å¼åŒ–
                    extracted_data['multi_data_detected'] = True
                    extracted_data['data_count'] = data_count
                    logger.info(f"ğŸ” [DEBUG] æ£€æµ‹åˆ°å¤šæ•°æ®åœºæ™¯: {data_count} æ¡æ•°æ®")

                    # ğŸ†• ä¸ºClaudeæ ¼å¼åŒ–è¯¦ç»†æ•°æ®
                    formatted_details = {}
                    daily_data_list = []

                    for key, result in self._last_api_results.items():
                        if result.get('success') and result.get('data'):
                            data = result['data']

                            # æ ¹æ®æ•°æ®ç±»å‹è¿›è¡Œä¸åŒçš„æ ¼å¼åŒ–
                            if isinstance(data, dict) and ('æ—¥æœŸ' in data or 'date' in key.lower()):
                                # æ—¶é—´åºåˆ—æ•°æ®
                                daily_data_list.append({
                                    'api_key': key,
                                    'date': data.get('æ—¥æœŸ', ''),
                                    'date_formatted': self._format_date_for_display(data.get('æ—¥æœŸ', '')),
                                    'inflow': float(data.get('å…¥é‡‘', 0)),
                                    'outflow': float(data.get('å‡ºé‡‘', 0)),
                                    'net_flow': float(data.get('å…¥é‡‘', 0)) - float(data.get('å‡ºé‡‘', 0)),
                                    'registrations': int(data.get('æ³¨å†Œäººæ•°', 0)),
                                    'purchases': int(data.get('è´­ä¹°äº§å“æ•°é‡', 0)),
                                    'holdings': int(data.get('æŒä»“äººæ•°', 0)),
                                    'expirations': int(data.get('åˆ°æœŸäº§å“æ•°é‡', 0))
                                })
                            elif isinstance(data, dict) and ('äº§å“' in str(data) or 'product' in key.lower()):
                                # äº§å“æ•°æ®
                                if 'äº§å“åˆ—è¡¨' in data:
                                    formatted_details['product_list'] = data.get('äº§å“åˆ—è¡¨', [])
                            elif isinstance(data, dict) and ('ç”¨æˆ·' in str(data) or 'user' in key.lower()):
                                # ç”¨æˆ·æ•°æ®
                                if 'ç”¨æˆ·åˆ—è¡¨' in data:
                                    formatted_details['user_list'] = data.get('ç”¨æˆ·åˆ—è¡¨', [])
                                if 'æ¯æ—¥æ•°æ®' in data:
                                    formatted_details['user_daily_data'] = data.get('æ¯æ—¥æ•°æ®', [])

                    # æŒ‰æ—¥æœŸæ’åºæ—¶é—´åºåˆ—æ•°æ®
                    if daily_data_list:
                        daily_data_list.sort(key=lambda x: x.get('date', ''))
                        formatted_details['daily_data_list'] = daily_data_list
                        logger.info(f"ğŸ” [DEBUG] æ ¼å¼åŒ–æ¯æ—¥æ•°æ®: {len(daily_data_list)} å¤©")

                    extracted_data['formatted_details'] = formatted_details

                else:
                    extracted_data['multi_data_detected'] = False
                    extracted_data['data_count'] = data_count
                    logger.info(f"ğŸ” [DEBUG] å•æ•°æ®åœºæ™¯: {data_count} æ¡æ•°æ®")

            # ğŸ†• Step 1: å¯åŠ¨å¹¶è¡Œä»»åŠ¡ - æ–‡æœ¬ç”Ÿæˆå’Œå›¾è¡¨ç”Ÿæˆ
            logger.info("ğŸš€ å¯åŠ¨å¹¶è¡Œä»»åŠ¡ï¼šæ–‡æœ¬ç”Ÿæˆ + å›¾è¡¨ç”Ÿæˆ")

            # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
            text_generation_task = self._generate_text_response_core(
                user_query, query_analysis, extracted_data, calculation_results
            )

            chart_generation_task = self._generate_intelligent_charts(
                extracted_data, user_query, query_analysis
            )

            # ğŸ†• Step 2: ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
            try:
                text_result, chart_result = await asyncio.gather(
                    text_generation_task,
                    chart_generation_task,
                    return_exceptions=True
                )
            except Exception as e:
                logger.error(f"å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                # é™çº§åˆ°ä¸²è¡Œæ‰§è¡Œ
                text_result = await self._generate_text_response_core(
                    user_query, query_analysis, extracted_data, calculation_results
                )
                chart_result = Exception(f"å›¾è¡¨ç”Ÿæˆè·³è¿‡: {str(e)}")

            # ğŸ†• Step 3: å¤„ç†æ–‡æœ¬ç”Ÿæˆç»“æœ
            if isinstance(text_result, Exception):
                logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {text_result}")
                response_text = self._enhanced_response_generation_fallback(
                    user_query, extracted_data, calculation_results
                )
            else:
                response_text = text_result

            # ğŸ†• Step 4: å¤„ç†å›¾è¡¨ç”Ÿæˆç»“æœ
            chart_section = ""
            if not isinstance(chart_result, Exception) and isinstance(chart_result, dict):
                if chart_result.get('success') and chart_result.get('chart_count', 0) > 0:
                    logger.info(f"âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸ: {chart_result.get('chart_count')} ä¸ªå›¾è¡¨")

                    # ç¼“å­˜å›¾è¡¨ç»“æœä¾›APIå“åº”ä½¿ç”¨
                    self._cache_chart_results(chart_result)

                    # ç”Ÿæˆå›¾è¡¨æè¿°æ–‡æœ¬
                    chart_section = self._format_charts_for_text_response(chart_result)

                    if chart_section:
                        logger.info("ğŸ“Š å›¾è¡¨æè¿°å·²æ·»åŠ åˆ°å“åº”æ–‡æœ¬")
                    else:
                        logger.warning("âš ï¸ å›¾è¡¨ç”ŸæˆæˆåŠŸä½†æè¿°ä¸ºç©º")
                else:
                    logger.warning(f"âš ï¸ å›¾è¡¨ç”ŸæˆæœªæˆåŠŸ: {chart_result.get('reason', 'æœªçŸ¥åŸå› ')}")
                    # ä»ç„¶ç¼“å­˜ç»“æœï¼ˆå³ä½¿å¤±è´¥ï¼‰ä»¥ä¾¿APIè¿”å›é”™è¯¯ä¿¡æ¯
                    self._cache_chart_results(chart_result)
            else:
                error_msg = str(chart_result) if isinstance(chart_result, Exception) else "å›¾è¡¨ç”Ÿæˆè¿”å›æ ¼å¼å¼‚å¸¸"
                logger.error(f"âŒ å›¾è¡¨ç”Ÿæˆå¼‚å¸¸: {error_msg}")
                # ç¼“å­˜å¤±è´¥ç»“æœ
                self._cache_chart_results({
                    'success': False,
                    'error': error_msg,
                    'generation_method': 'failed',
                    'chart_count': 0,
                    'generated_charts': []
                })

            # ğŸ†• Step 5: åˆå¹¶æ–‡æœ¬å“åº”å’Œå›¾è¡¨æè¿°
            final_response = response_text
            if chart_section:
                final_response += chart_section

            # åå¤„ç†ï¼šæ ¼å¼åŒ–è´§å¸æ˜¾ç¤º
            if self.financial_formatter:
                final_response = self._format_currency_in_response(final_response)

            logger.info(f"âœ… å¢å¼ºå“åº”ç”Ÿæˆå®Œæˆï¼ŒåŒ…å«å›¾è¡¨: {'æ˜¯' if chart_section else 'å¦'}")
            return final_response

        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆå“åº”ç”Ÿæˆå¤±è´¥: {e}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

            # ç¡®ä¿å³ä½¿å‡ºç°å¼‚å¸¸ä¹Ÿç¼“å­˜å¤±è´¥çŠ¶æ€
            self._cache_chart_results({
                'success': False,
                'error': str(e),
                'generation_method': 'failed',
                'chart_count': 0,
                'generated_charts': []
            })

            return self._enhanced_response_generation_fallback(user_query, extracted_data, calculation_results)

    async def _generate_text_response_core(self,
                                           user_query: str,
                                           query_analysis: QueryAnalysis,
                                           extracted_data: Dict[str, Any],
                                           calculation_results: Dict[str, Any]) -> str:
        """
        æ ¸å¿ƒæ–‡æœ¬å“åº”ç”Ÿæˆ - ä»åŸæœ‰é€»è¾‘æå–
        """
        try:
            # ğŸ”§ æ„å»ºresponse_dataç”¨äºprompt
            response_data = {
                'extracted_metrics': extracted_data.get('extracted_metrics', {}),
                'derived_metrics': extracted_data.get('derived_metrics', {}),
                'key_insights': extracted_data.get('key_insights', []),
                'business_health_indicators': extracted_data.get('business_health_indicators', {}),
                'detailed_daily_analysis': extracted_data.get('detailed_daily_analysis', {}),
                'weekly_pattern_analysis': extracted_data.get('weekly_pattern_analysis', {}),
                'recommendations': extracted_data.get('recommendations', []),
                'data_quality_assessment': extracted_data.get('data_quality_assessment', {}),
                'direct_answer': extracted_data.get('direct_answer', ''),
                'calculation_summary': self._summarize_calculation_results(calculation_results),
                'data_sources': extracted_data.get('source_data_summary', {}),
                'extraction_method': extracted_data.get('extraction_method', 'unknown'),
                'business_insights': extracted_data.get('business_insights', []),
                'detailed_calculations': extracted_data.get('detailed_calculations', {}),
                'raw_api_results': extracted_data.get('raw_api_results', {}),
                'multi_data_detected': extracted_data.get('multi_data_detected', False),
                'data_count': extracted_data.get('data_count', 0),
                'formatted_details': extracted_data.get('formatted_details', {})
            }

            # ğŸ†• ä½¿ç”¨PromptManageræ„å»ºå“åº”ç”Ÿæˆprompt
            prompt = self.prompt_manager.build_response_generation_prompt(
                user_query=user_query,
                query_analysis=query_analysis,
                extracted_data=extracted_data,
                calculation_results=calculation_results,
                query_type_result=getattr(query_analysis, 'query_type_info', None)
            )

            result = await self.claude_client.generate_text(prompt, max_tokens=6000)

            if result.get('success'):
                response_text = result.get('text', '').strip()
                logger.info("âœ… Claudeæ–‡æœ¬ç”ŸæˆæˆåŠŸ")
                return response_text
            else:
                error_msg = result.get('error', 'Claudeè°ƒç”¨å¤±è´¥')
                logger.warning(f"âš ï¸ Claudeæ–‡æœ¬ç”Ÿæˆå¤±è´¥: {error_msg}")
                raise Exception(f"Claudeç”Ÿæˆå¤±è´¥: {error_msg}")

        except Exception as e:
            logger.error(f"æ ¸å¿ƒæ–‡æœ¬ç”Ÿæˆå¼‚å¸¸: {e}")
            raise e

    def _cache_chart_results(self, chart_results: Dict[str, Any]):
        """ç¼“å­˜å›¾è¡¨ç»“æœä¾›ProcessingResultä½¿ç”¨"""
        self._last_chart_results = chart_results
        logger.debug(
            f"ğŸ”„ ç¼“å­˜å›¾è¡¨ç»“æœ: success={chart_results.get('success')}, count={chart_results.get('chart_count', 0)}")

    def _format_charts_for_text_response(self, chart_results: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å›¾è¡¨ç»“æœç”¨äºæ–‡æœ¬å“åº”"""

        if not chart_results.get('success') or chart_results.get('chart_count', 0) == 0:
            return ""

        charts = chart_results.get('generated_charts', [])

        if not charts:
            return ""

        chart_section = "\n\nğŸ“Š **æ•°æ®å¯è§†åŒ–åˆ†æ**ï¼š"

        for i, chart in enumerate(charts, 1):
            if not chart.get('success', False):
                continue

            chart_type = chart.get('chart_type', 'å›¾è¡¨')
            title = chart.get('title', f'å›¾è¡¨ {i}')

            chart_section += f"\n\n**{i}. {title}**"

            # æ·»åŠ å›¾è¡¨ç±»å‹è¯´æ˜
            chart_type_desc = {
                'pie': 'é¥¼å›¾',
                'donut': 'ç¯å½¢å›¾',
                'bar': 'æŸ±çŠ¶å›¾',
                'line': 'æŠ˜çº¿å›¾',
                'area': 'é¢ç§¯å›¾',
                'scatter': 'æ•£ç‚¹å›¾'
            }.get(chart_type, chart_type)

            chart_section += f" ({chart_type_desc})"

            # æ·»åŠ æ•°æ®æ‘˜è¦
            data_summary = chart.get('data_summary', {})
            if data_summary:
                if 'values' in data_summary and 'labels' in data_summary:
                    # é¥¼å›¾ç±»å‹çš„æ‘˜è¦
                    total_value = sum(data_summary['values']) if data_summary['values'] else 0
                    if total_value > 0:
                        chart_section += f"\n   ğŸ“ˆ æ€»è®¡: {self.financial_formatter.format_currency(total_value) if self.financial_formatter else f'{total_value:,.2f}'}"
                        chart_section += "\n   ğŸ“‹ æ•°æ®æ˜ç»†ï¼š"

                        for label, value in zip(data_summary['labels'], data_summary['values']):
                            percentage = (value / total_value * 100) if total_value > 0 else 0
                            formatted_value = self.financial_formatter.format_currency(
                                value) if self.financial_formatter else f'{value:,.2f}'
                            chart_section += f"\n   â€¢ {label}: {formatted_value} ({percentage:.1f}%)"

                elif 'series' in data_summary:
                    # æŸ±çŠ¶å›¾/æŠ˜çº¿å›¾ç±»å‹çš„æ‘˜è¦
                    series_count = len(data_summary['series'])
                    chart_section += f"\n   ğŸ“Š åŒ…å« {series_count} ä¸ªæ•°æ®ç³»åˆ—"

                    # æ˜¾ç¤ºç³»åˆ—åç§°
                    if series_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªç³»åˆ—
                        for series_name in list(data_summary['series'].keys())[:3]:
                            series_data = data_summary['series'][series_name]
                            if isinstance(series_data, list) and len(series_data) > 0:
                                avg_value = sum(series_data) / len(series_data) if series_data else 0
                                formatted_avg = self.financial_formatter.format_currency(
                                    avg_value) if self.financial_formatter else f'{avg_value:,.2f}'
                                chart_section += f"\n   â€¢ {series_name}: å¹³å‡å€¼ {formatted_avg}"

        # æ·»åŠ AIæ¨èè¯´æ˜
        if chart_results.get('recommendation_used', False):
            reasoning = chart_results.get('ai_reasoning', '')
            if reasoning:
                chart_section += f"\n\nğŸ’¡ **AIæ™ºèƒ½åˆ†æ**: {reasoning}"

        generation_method = chart_results.get('generation_method', 'rule_based')
        method_desc = {
            'ai_intelligent': 'ğŸ¤– *å›¾è¡¨ç”±AIæ™ºèƒ½æ¨èç”Ÿæˆï¼ŒåŸºäºæŸ¥è¯¢å†…å®¹å’Œæ•°æ®ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯è§†åŒ–æ–¹å¼*',
            'rule_based': 'ğŸ“‹ *å›¾è¡¨åŸºäºæ•°æ®ç‰¹å¾å’Œä¸šåŠ¡è§„åˆ™è‡ªåŠ¨ç”Ÿæˆ*',
            'failed': 'âŒ *å›¾è¡¨ç”Ÿæˆé‡åˆ°é—®é¢˜*'
        }.get(generation_method, 'ğŸ“Š *å›¾è¡¨å·²è‡ªåŠ¨ç”Ÿæˆ*')

        chart_section += f"\n\n{method_desc}"

        return chart_section

    def _format_date_for_display(self, date_str: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸç”¨äºæ˜¾ç¤º"""
        try:
            if len(date_str) == 8 and date_str.isdigit():
                # YYYYMMDDæ ¼å¼è½¬æ¢
                year = date_str[:4]
                month = date_str[4:6]
                day = date_str[6:8]
                return f"{month}æœˆ{day}æ—¥"
            return date_str
        except:
            return date_str
    # åœ¨ IntelligentQAOrchestrator ä¸­æ·»åŠ å›¾è¡¨ç”Ÿæˆæ–¹æ³•
    async def _generate_intelligent_charts(self,
                                           extracted_data: Dict[str, Any],
                                           user_query: str,
                                           query_analysis: QueryAnalysis) -> Dict[str, Any]:
        """
        ğŸ¨ ç”Ÿæˆæ™ºèƒ½å›¾è¡¨

        Args:
            extracted_data: æå–çš„æ•°æ®
            user_query: ç”¨æˆ·æŸ¥è¯¢
            query_analysis: æŸ¥è¯¢åˆ†æç»“æœ

        Returns:
            Dict[str, Any]: å›¾è¡¨ç”Ÿæˆç»“æœ
        """

        if not self.chart_generator:
            logger.warning("å›¾è¡¨ç”Ÿæˆå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
            return {
                'success': False,
                'reason': 'chart_generator_unavailable',
                'generated_charts': [],
                'chart_count': 0
            }

        try:
            logger.info("ğŸ¨ å¼€å§‹æ™ºèƒ½å›¾è¡¨ç”Ÿæˆ...")

            # ğŸ¯ è°ƒç”¨ChartGeneratorçš„æ™ºèƒ½å›¾è¡¨ç”Ÿæˆæ–¹æ³•
            chart_results = await self.chart_generator.intelligent_chart_generation(
                extracted_data=extracted_data,
                user_query=user_query,
                auto_select=True
            )

            # ğŸ” è°ƒè¯•æ—¥å¿—
            success = chart_results.get('success', False)
            chart_count = chart_results.get('chart_count', 0)
            generation_method = chart_results.get('generation_method', 'unknown')

            logger.info(f"ğŸ“Š å›¾è¡¨ç”Ÿæˆç»“æœ: success={success}, count={chart_count}, method={generation_method}")

            # ğŸ†• å¦‚æœå›¾è¡¨ç”ŸæˆæˆåŠŸï¼Œå¤„ç†å›¾è¡¨æ•°æ®
            if success and chart_count > 0:
                processed_charts = self._process_chart_results(chart_results)
                chart_results['processed_charts'] = processed_charts

            return chart_results

        except Exception as e:
            logger.error(f"âŒ å›¾è¡¨ç”Ÿæˆå¼‚å¸¸: {e}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

            return {
                'success': False,
                'error': str(e),
                'generated_charts': [],
                'chart_count': 0,
                'generation_method': 'failed'
            }

    def _process_chart_results(self, chart_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        å¤„ç†å›¾è¡¨ç»“æœï¼Œè½¬æ¢ä¸ºAPIå‹å¥½çš„æ ¼å¼

        Args:
            chart_results: åŸå§‹å›¾è¡¨ç»“æœ

        Returns:
            List[Dict[str, Any]]: å¤„ç†åçš„å›¾è¡¨æ•°æ®
        """

        processed_charts = []
        generated_charts = chart_results.get('generated_charts', [])

        for i, chart in enumerate(generated_charts):
            try:
                processed_chart = {
                    'id': f"chart_{i + 1}",
                    'type': chart.get('chart_type', 'unknown'),
                    'title': chart.get('title', f'å›¾è¡¨ {i + 1}'),
                    'success': chart.get('success', False)
                }

                # ğŸ¯ å¤„ç†å›¾ç‰‡æ•°æ®
                if 'image_data' in chart and chart['image_data']:
                    image_data = chart['image_data']
                    if isinstance(image_data, str):
                        # å¦‚æœç›´æ¥æ˜¯base64å­—ç¬¦ä¸²
                        processed_chart['image'] = {
                            'format': 'base64_png',
                            'data': image_data
                        }
                    elif isinstance(image_data, dict):
                        # å¦‚æœæ˜¯åŒ…å«æ ¼å¼ä¿¡æ¯çš„å­—å…¸
                        processed_chart['image'] = {
                            'format': image_data.get('format', 'png'),
                            'data': image_data.get('base64') or image_data.get('data', ''),
                            'binary_available': 'binary' in image_data,
                            'svg_available': 'svg' in image_data,
                            'html_available': 'html' in image_data
                        }

                # ğŸ¯ å¤„ç†æ•°æ®æ‘˜è¦
                if 'data_summary' in chart:
                    processed_chart['data_summary'] = chart['data_summary']

                # ğŸ¯ æ·»åŠ å›¾è¡¨æè¿°å’Œæ´å¯Ÿ
                processed_chart['description'] = self._generate_chart_description(chart)

                processed_charts.append(processed_chart)

            except Exception as e:
                logger.error(f"å¤„ç†å›¾è¡¨ {i + 1} æ—¶å‡ºé”™: {e}")
                # æ·»åŠ é”™è¯¯çš„å›¾è¡¨é¡¹
                processed_charts.append({
                    'id': f"chart_{i + 1}_error",
                    'type': 'error',
                    'title': f'å›¾è¡¨ {i + 1} å¤„ç†å¤±è´¥',
                    'success': False,
                    'error': str(e)
                })

        return processed_charts

    def _generate_chart_description(self, chart: Dict[str, Any]) -> str:
        """ä¸ºå›¾è¡¨ç”Ÿæˆæè¿°æ–‡å­—"""

        chart_type = chart.get('chart_type', 'å›¾è¡¨')
        title = chart.get('title', 'æ•°æ®åˆ†æ')
        data_summary = chart.get('data_summary', {})

        # æ ¹æ®å›¾è¡¨ç±»å‹ç”Ÿæˆä¸åŒçš„æè¿°
        if chart_type in ['pie', 'donut']:
            if 'values' in data_summary and 'labels' in data_summary:
                total = sum(data_summary['values']) if data_summary['values'] else 0
                max_item_idx = data_summary['values'].index(max(data_summary['values'])) if data_summary[
                    'values'] else 0
                max_item_label = data_summary['labels'][max_item_idx] if data_summary['labels'] else 'æœªçŸ¥'
                return f"{title}ï¼šæ€»é¢ {total:,.2f}ï¼Œå…¶ä¸­ {max_item_label} å æ¯”æœ€å¤§"

        elif chart_type == 'bar':
            if 'series' in data_summary:
                series_count = len(data_summary['series'])
                return f"{title}ï¼šåŒ…å« {series_count} ä¸ªæ•°æ®ç³»åˆ—çš„å¯¹æ¯”åˆ†æ"

        elif chart_type == 'line':
            if 'series' in data_summary:
                series_count = len(data_summary['series'])
                return f"{title}ï¼š{series_count} ä¸ªæŒ‡æ ‡çš„è¶‹åŠ¿å˜åŒ–å›¾"

        return f"{title}ï¼š{chart_type}å›¾è¡¨"

    def _format_charts_for_text_response(self, chart_results: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å›¾è¡¨ç»“æœç”¨äºæ–‡æœ¬å“åº”"""

        if not chart_results.get('success') or chart_results.get('chart_count', 0) == 0:
            return ""

        charts = chart_results.get('generated_charts', [])
        processed_charts = chart_results.get('processed_charts', [])

        chart_section = "\n\nğŸ“Š **æ•°æ®å¯è§†åŒ–åˆ†æ**ï¼š"

        for i, (chart, processed_chart) in enumerate(zip(charts, processed_charts), 1):
            if not chart.get('success', False):
                continue

            title = processed_chart.get('title', f'å›¾è¡¨ {i}')
            description = processed_chart.get('description', '')

            chart_section += f"\n\n**{i}. {title}**"
            if description:
                chart_section += f"\n   {description}"

            # æ·»åŠ æ•°æ®æ‘˜è¦
            data_summary = processed_chart.get('data_summary', {})
            if data_summary:
                if 'values' in data_summary and 'labels' in data_summary:
                    # é¥¼å›¾ç±»å‹çš„æ‘˜è¦
                    chart_section += "\n   æ•°æ®æ˜ç»†ï¼š"
                    for label, value in zip(data_summary['labels'], data_summary['values']):
                        percentage = (value / sum(data_summary['values']) * 100) if sum(
                            data_summary['values']) > 0 else 0
                        chart_section += f"\n   â€¢ {label}: {value:,.2f} ({percentage:.1f}%)"

        # æ·»åŠ AIæ¨èè¯´æ˜
        if chart_results.get('recommendation_used', False):
            reasoning = chart_results.get('ai_reasoning', '')
            if reasoning:
                chart_section += f"\n\nğŸ’¡ **AIåˆ†æ**: {reasoning}"

        generation_method = chart_results.get('generation_method', 'rule_based')
        if generation_method == 'ai_intelligent':
            chart_section += f"\n\nğŸ¤– *å›¾è¡¨ç”±AIæ™ºèƒ½æ¨èç”Ÿæˆ*"
        elif generation_method == 'rule_based':
            chart_section += f"\n\nğŸ“‹ *å›¾è¡¨åŸºäºæ•°æ®ç‰¹å¾è‡ªåŠ¨ç”Ÿæˆ*"

        return chart_section
    def _format_date_for_display(self, date_str: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸç”¨äºæ˜¾ç¤º"""
        try:
            if len(date_str) == 8 and date_str.isdigit():
                # YYYYMMDDæ ¼å¼è½¬æ¢
                year = date_str[:4]
                month = date_str[4:6]
                day = date_str[6:8]
                return f"{month}æœˆ{day}æ—¥"
            return date_str
        except:
            return date_str

    def _enhanced_response_generation_fallback(self,
                                               user_query: str,
                                               extracted_data: Dict[str, Any],
                                               calculation_results: Dict[str, Any]) -> str:
        """å¢å¼ºç‰ˆé™çº§å“åº”ç”Ÿæˆ"""

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¹æ¯”ç±»æŸ¥è¯¢
        if any(keyword in user_query for keyword in ['æ¯”è¾ƒ', 'ç›¸æ¯”', 'å˜åŒ–', 'å¯¹æ¯”']):
            return self._generate_detailed_comparison_response(extracted_data, user_query)
        else:
            return self._generate_detailed_general_response(extracted_data, user_query)

    # ================================================================
    # è¾…åŠ©æ–¹æ³•ï¼ˆä¿æŒåŸæœ‰é€»è¾‘æˆ–ç®€åŒ–ï¼‰
    # ================================================================

    async def _execute_quick_api_call(self, api_info: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¿«é€ŸAPIè°ƒç”¨"""
        if not self.data_fetcher or not self.data_fetcher.api_connector:
            raise Exception("æ•°æ®è·å–å™¨ä¸å¯ç”¨")

        method = api_info['method']
        params = api_info.get('params', {})
        api_connector = self.data_fetcher.api_connector

        if method == 'get_system_data':
            return await api_connector.get_system_data()
        elif method == 'get_daily_data':
            if 'date' in params:
                return await api_connector.get_daily_data(params['date'])
            else:
                return await api_connector.get_daily_data(self.current_date.strftime('%Y%m%d'))
        elif method == 'get_product_data':
            return await api_connector.get_product_data()
        else:
            raise Exception(f"ä¸æ”¯æŒçš„å¿«é€ŸAPIæ–¹æ³•: {method}")

    async def _quick_data_extraction(self, raw_data: Dict[str, Any], user_query: str) -> Dict[str, Any]:
        """å¿«é€Ÿæ•°æ®æå–"""
        if not raw_data.get('success') or not raw_data.get('data'):
            return {'key_metrics': {}, 'extracted_info': 'æ•°æ®ä¸ºç©º', 'data_quality': 0.0}

        data = raw_data['data']
        key_metrics = {}

        # æå–å¸¸è§å­—æ®µ
        common_fields = ['æ€»ä½™é¢', 'æ€»å…¥é‡‘', 'æ€»å‡ºé‡‘', 'å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œäººæ•°', 'æ´»è·ƒç”¨æˆ·æ•°', 'æ€»ç”¨æˆ·æ•°', 'æŒä»“äººæ•°']

        for field in common_fields:
            if field in data:
                try:
                    if field in ['å…¥é‡‘', 'å‡ºé‡‘', 'æ€»ä½™é¢', 'æ€»å…¥é‡‘', 'æ€»å‡ºé‡‘']:
                        key_metrics[field] = float(data[field])
                    else:
                        key_metrics[field] = int(data[field])
                except (ValueError, TypeError):
                    pass

        # ä»ç”¨æˆ·ç»Ÿè®¡ä¸­æå–
        if 'ç”¨æˆ·ç»Ÿè®¡' in data and isinstance(data['ç”¨æˆ·ç»Ÿè®¡'], dict):
            user_stats = data['ç”¨æˆ·ç»Ÿè®¡']
            for field in ['æ€»ç”¨æˆ·æ•°', 'æ´»è·ƒç”¨æˆ·æ•°']:
                if field in user_stats:
                    try:
                        key_metrics[field] = int(user_stats[field])
                    except (ValueError, TypeError):
                        pass

        return {
            'key_metrics': key_metrics,
            'extracted_info': f'æå–äº†{len(key_metrics)}ä¸ªå…³é”®æŒ‡æ ‡',
            'data_quality': 0.9 if len(key_metrics) >= 3 else (0.7 if key_metrics else 0.3)
        }

    def _basic_response_generation(self, user_query: str, extracted_data: Dict[str, Any]) -> str:
        """åŸºç¡€å“åº”ç”Ÿæˆ"""
        key_metrics = extracted_data.get('key_metrics', {})

        if not key_metrics:
            return "æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°ç›¸å…³æ•°æ®ã€‚"

        response_parts = ["æ ¹æ®æœ€æ–°æ•°æ®ï¼š"]

        # ä¼˜å…ˆæ˜¾ç¤ºé‡è¦æŒ‡æ ‡
        priority_fields = ['æ€»ä½™é¢', 'å…¥é‡‘', 'å‡ºé‡‘', 'æ´»è·ƒç”¨æˆ·æ•°', 'æ³¨å†Œäººæ•°']

        for field in priority_fields:
            if field in key_metrics:
                value = key_metrics[field]
                if isinstance(value, (int, float)):
                    if 'ä½™é¢' in field or 'é‡‘é¢' in field or 'å…¥é‡‘' in field or 'å‡ºé‡‘' in field:
                        response_parts.append(f"ğŸ’° {field}ï¼šÂ¥{value:,.2f}")
                    elif 'ç”¨æˆ·' in field or 'äººæ•°' in field:
                        response_parts.append(f"ğŸ‘¥ {field}ï¼š{int(value):,}äºº")
                    else:
                        response_parts.append(f"ğŸ“Š {field}ï¼š{value}")

        return "\n".join(response_parts)

    def _generate_detailed_comparison_response(self, extracted_data: Dict[str, Any], user_query: str) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”åˆ†æå“åº”"""
        response_parts = ["ğŸ“Š **è¯¦ç»†å¯¹æ¯”åˆ†ææŠ¥å‘Š**\n"]

        # å¯¹æ¯”åˆ†æ
        comparison_analysis = extracted_data.get('comparison_analysis', {})
        if comparison_analysis:
            response_parts.append("ğŸ“ˆ **å¯¹æ¯”åˆ†æç»“æœ**ï¼š")
            for metric, analysis in comparison_analysis.items():
                if isinstance(analysis, dict):
                    current = analysis.get('current_value', 0)
                    baseline = analysis.get('baseline_value', 0)
                    change_rate = analysis.get('percentage_change', 0)
                    direction = analysis.get('change_direction', 'æŒå¹³')

                    response_parts.append(f"ğŸ’° **{metric}**ï¼š")
                    response_parts.append(f"  - å½“å‰å€¼ï¼šÂ¥{current:,.2f}")
                    response_parts.append(f"  - å¯¹æ¯”å€¼ï¼šÂ¥{baseline:,.2f}")
                    response_parts.append(f"  - å˜åŒ–ï¼š{direction} {change_rate:.2%}")

        # æ•°æ®è´¨é‡è¯„ä¼°
        data_quality = extracted_data.get('data_quality_score', 0)
        response_parts.append(f"\nğŸ” **æ•°æ®è´¨é‡è¯„åˆ†**ï¼š{data_quality:.1%}")

        return "\n".join(response_parts)

    def _generate_detailed_general_response(self, extracted_data: Dict[str, Any], user_query: str) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„é€šç”¨å“åº”"""
        response_parts = ["ğŸ“Š **æ•°æ®åˆ†ææŠ¥å‘Š**\n"]

        # ç›´æ¥ç­”æ¡ˆ
        direct_answer = extracted_data.get('direct_answer', '')
        if direct_answer:
            response_parts.append(f"ğŸ¯ **æ ¸å¿ƒç»“è®º**ï¼š{direct_answer}\n")

        # æå–çš„æŒ‡æ ‡
        extracted_metrics = extracted_data.get('extracted_metrics', {})
        if extracted_metrics:
            response_parts.append("ğŸ’° **æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡**ï¼š")
            for key, value in extracted_metrics.items():
                if isinstance(value, (int, float)):
                    if 'ä½™é¢' in key or 'é‡‘é¢' in key or 'å…¥é‡‘' in key or 'å‡ºé‡‘' in key:
                        response_parts.append(f"â€¢ {key}ï¼šÂ¥{value:,.2f}")
                    elif 'äººæ•°' in key or 'æ•°é‡' in key:
                        response_parts.append(f"â€¢ {key}ï¼š{int(value):,}")

        # å…³é”®æ´å¯Ÿ
        key_insights = extracted_data.get('key_insights', [])
        if key_insights:
            response_parts.append("\nğŸ’¡ **å…³é”®æ´å¯Ÿ**ï¼š")
            for insight in key_insights:
                response_parts.append(f"â€¢ {insight}")

        return "\n".join(response_parts)

    def _format_currency_in_response(self, response_text: str) -> str:
        """æ ¼å¼åŒ–å“åº”ä¸­çš„è´§å¸æ˜¾ç¤º"""
        if not self.financial_formatter:
            return response_text
        return response_text

    def _summarize_calculation_results(self, calculation_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ€»ç»“è®¡ç®—ç»“æœ"""
        if not calculation_results.get('success'):
            return {'has_calculation': False}

        return {
            'has_calculation': True,
            'calculation_type': calculation_results.get('calculation_type'),
            'success': True,
            'confidence': calculation_results.get('confidence', 0.0)
        }

    def _fallback_data_extraction(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºç¡€é™çº§æ•°æ®æå–"""
        if not raw_data.get('success') or not raw_data.get('results'):
            return {
                'extracted_metrics': {},
                'data_quality_score': 0.0,
                'extraction_method': 'fallback'
            }

        extracted_metrics = {}
        all_data = {}

        for key, result in raw_data['results'].items():
            if result.get('success') and result.get('data'):
                data = result['data']
                if isinstance(data, dict):
                    all_data.update(data)

        # æå–å¸¸è§æŒ‡æ ‡
        common_fields = ['æ€»ä½™é¢', 'æ€»å…¥é‡‘', 'æ€»å‡ºé‡‘', 'å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œäººæ•°', 'æ´»è·ƒç”¨æˆ·æ•°']

        for field in common_fields:
            if field in all_data:
                try:
                    extracted_metrics[field] = float(all_data[field])
                except (ValueError, TypeError):
                    pass

        return {
            'extracted_metrics': extracted_metrics,
            'data_quality_score': 0.7 if extracted_metrics else 0.2,
            'extraction_method': 'fallback'
        }

    async def _execute_fallback_to_comprehensive(self,
                                                 user_query: str,
                                                 session_id: str,
                                                 query_id: str,
                                                 start_time: float,
                                                 conversation_id: Optional[str]) -> ProcessingResult:
        """å¿«é€Ÿå“åº”å¤±è´¥æ—¶çš„é™çº§å¤„ç†"""
        logger.info("å¿«é€Ÿå“åº”å¤±è´¥ï¼Œé™çº§åˆ°å®Œæ•´åˆ†æ")

        try:
            query_analysis = self._create_fallback_query_analysis(user_query, query_id)
            raw_data = await self._intelligent_data_acquisition(query_analysis)
            extracted_data = self._fallback_data_extraction(raw_data)
            response_text = self._basic_response_generation(user_query, extracted_data)

            total_time = time.time() - start_time

            return ProcessingResult(
                session_id=session_id,
                query_id=query_id,
                success=True,
                response_text=response_text,
                extracted_data=extracted_data,
                complexity=QueryComplexity.SIMPLE,
                processing_path="fallback",
                confidence_score=0.6,
                total_processing_time=total_time,
                processing_strategy=ProcessingStrategy.FALLBACK,
                conversation_id=conversation_id
            )

        except Exception as e:
            logger.error(f"é™çº§å¤„ç†ä¹Ÿå¤±è´¥äº†: {e}")
            return self._create_error_result(session_id, query_id, user_query, str(e),
                                             time.time() - start_time, conversation_id)

    def _create_fallback_query_analysis(self, user_query: str, query_id: str) -> QueryAnalysis:
        """åˆ›å»ºé™çº§æŸ¥è¯¢åˆ†æ"""
        return QueryAnalysis(
            original_query=user_query,
            complexity=QueryComplexity.SIMPLE,
            is_quick_response=False,
            intent='æ•°æ®æŸ¥è¯¢',
            confidence=0.4,
            api_calls=[{'method': 'get_system_data', 'params': {}, 'reason': 'é™çº§æ•°æ®è·å–'}]
        )

    async def _save_conversation_if_needed(self,
                                           conversation_id: Optional[str],
                                           user_query: str,
                                           result: ProcessingResult):
        """ä¿å­˜å¯¹è¯è®°å½•"""
        if not conversation_id or not self.conversation_manager:
            return

        try:
            conv_id = int(conversation_id)

            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            self.conversation_manager.add_message(conv_id, True, user_query)

            # ä¿å­˜AIå›å¤æ¶ˆæ¯
            ai_message_id = self.conversation_manager.add_message(conv_id, False, result.response_text)

            # ğŸ†• ä¿å­˜å›¾è¡¨æ•°æ®åˆ°æ•°æ®åº“
            if result.visualizations and ai_message_id:
                for i, visualization in enumerate(result.visualizations):
                    try:
                        self.conversation_manager.add_visual(
                            message_id=ai_message_id,
                            visual_type=visualization.get('type', 'chart'),
                            visual_order=i,
                            title=visualization.get('title', f'å›¾è¡¨ {i + 1}'),
                            data=visualization.get('data', {})
                        )
                        logger.info(f"âœ… ä¿å­˜å›¾è¡¨æ•°æ®åˆ°æ•°æ®åº“: message_id={ai_message_id}, visual_order={i}")
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜å›¾è¡¨æ•°æ®å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"ä¿å­˜å¯¹è¯è®°å½•å¤±è´¥: {e}")

    def _create_error_result(self,
                             session_id: str,
                             query_id: str,
                             user_query: str,
                             error_msg: str,
                             processing_time: float,
                             conversation_id: Optional[str]) -> ProcessingResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return ProcessingResult(
            session_id=session_id,
            query_id=query_id,
            success=False,
            response_text=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶é‡åˆ°é—®é¢˜ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
            complexity=QueryComplexity.SIMPLE,
            processing_path="error",
            confidence_score=0.0,
            total_processing_time=processing_time,
            processing_strategy=ProcessingStrategy.FALLBACK,
            error_info={
                'message': error_msg,
                'query': user_query,
                'timestamp': datetime.now().isoformat()
            },
            conversation_id=conversation_id
        )

    # ================================================================
    # ğŸ†• ç®¡ç†æ–¹æ³• - å¢å¼ºç»Ÿè®¡
    # ================================================================

    def get_orchestrator_stats(self) -> Dict[str, Any]:
        """è·å–ç¼–æ’å™¨ç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats['total_queries']
        stats = self.stats.copy()

        if total > 0:
            stats['quick_response_rate'] = self.stats['quick_responses'] / total
            stats['comprehensive_rate'] = self.stats['comprehensive_analyses'] / total
            stats['average_processing_time'] = self.stats['processing_time_total'] / total

        # ğŸ†• æ·»åŠ ç»„ä»¶ç»Ÿè®¡
        if hasattr(self, 'strategy_extractor') and self.strategy_extractor:
            # è·å–ç­–ç•¥æå–å™¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä½†ä¸ç›´æ¥å°†å­—å…¸èµ‹å€¼ç»™stats
            extractor_stats = self.strategy_extractor.get_stats()
            # å°†ç­–ç•¥æå–å™¨çš„å…³é”®ç»Ÿè®¡æ•°æ®ä½œä¸ºå•ç‹¬çš„æ¡ç›®æ·»åŠ åˆ°statsä¸­
            if isinstance(extractor_stats, dict):
                for key, value in extractor_stats.items():
                    if isinstance(value, (int, float)):
                        stats[f'strategy_extractor_{key}'] = value

        return stats

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        return {
            'status': 'healthy' if self.initialized else 'not_initialized',
            'components': {
                'claude_available': self.claude_client is not None,
                'gpt_available': self.gpt_client is not None,
                'data_fetcher_ready': self.data_fetcher is not None,
                'calculator_ready': self.calculator is not None,
                'conversation_manager_ready': self.conversation_manager is not None,
                # ğŸ†• æ–°ç»„ä»¶çŠ¶æ€
                'query_type_detector_ready': self.query_type_detector is not None,
                'prompt_manager_ready': self.prompt_manager is not None,
                'strategy_extractor_ready': self.strategy_extractor is not None,
            },
            'features': {
                'enhanced_query_detection': True,
                'prompt_management': True,
                'intelligent_strategy_extraction': True,
                'three_layer_data_extraction': True
            },
            'stats': self.get_orchestrator_stats(),
            'current_date': self.current_date.isoformat(),
            'timestamp': datetime.now().isoformat()
        }

    async def close(self):
        """å…³é—­ç¼–æ’å™¨"""
        logger.info("å…³é—­ç²¾ç®€é‡æ„åçš„æ™ºèƒ½ç¼–æ’å™¨...")

        if self.data_fetcher and hasattr(self.data_fetcher, 'close'):
            await self.data_fetcher.close()

        if self.db_connector and hasattr(self.db_connector, 'close'):
            await self.db_connector.close()

        self.initialized = False
        logger.info("æ™ºèƒ½ç¼–æ’å™¨å·²å…³é—­")

# ================================================================
# å·¥å‚å‡½æ•°
# ================================================================

def create_enhanced_orchestrator(claude_client: Optional[ClaudeClient] = None,
                                gpt_client: Optional[OpenAIClient] = None,
                                db_connector: Optional[DatabaseConnector] = None,
                                app_config: Optional[AppConfig] = None) -> IntelligentQAOrchestrator:
    """åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ç¼–æ’å™¨"""
    return IntelligentQAOrchestrator(claude_client, gpt_client, db_connector, app_config)