"""
Claudeæ™ºèƒ½æå–å™¨ - ç¬¬äºŒå±‚ (å®Œæ•´ç‰ˆ)
è´Ÿè´£ä½¿ç”¨Claudeç†è§£è¯­ä¹‰åŒ–æ•°æ®å¹¶æå–ä¸šåŠ¡ä¿¡æ¯
"""

import logging
import json
import re
import time
from typing import Dict, Any, Optional, List
from datetime import datetime

logger = logging.getLogger(__name__)


class ClaudeIntelligentExtractor:
    """Claudeæ™ºèƒ½æå–å™¨"""

    def __init__(self, claude_client):
        self.claude_client = claude_client
        self.extraction_templates = self._load_extraction_templates()

    async def extract_with_intelligence(self,
                                      semantic_data_result: Dict[str, Any],
                                      user_query: str,
                                      query_analysis) -> Dict[str, Any]:
        """
        Claudeæ™ºèƒ½æå–ä¸»æ–¹æ³•

        Args:
            semantic_data_result: è¯­ä¹‰åŒ–æ”¶é›†å™¨çš„ç»“æœ
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            query_analysis: æŸ¥è¯¢åˆ†æç»“æœ

        Returns:
            Claudeæå–çš„æ™ºèƒ½ç»“æœ
        """
        try:
            if 'error' in semantic_data_result:
                logger.error(f"è¯­ä¹‰æ•°æ®æ”¶é›†å¤±è´¥: {semantic_data_result['error']}")
                return self._create_error_result(semantic_data_result['error'])

            semantic_data = semantic_data_result.get('semantic_data', {})
            if not semantic_data:
                return self._create_error_result("æ— æœ‰æ•ˆçš„è¯­ä¹‰åŒ–æ•°æ®")

            # ğŸ¯ æ ¹æ®æŸ¥è¯¢æ„å›¾é€‰æ‹©æå–ç­–ç•¥
            intent = getattr(query_analysis, 'intent', 'æ•°æ®æŸ¥è¯¢')
            extraction_strategy = self._determine_extraction_strategy(intent, semantic_data)

            # ğŸ¯ æ„å»ºé’ˆå¯¹æ€§çš„æå–æç¤ºè¯
            extraction_prompt = self._build_extraction_prompt(
                semantic_data, user_query, query_analysis, extraction_strategy
            )

            # ğŸ¯ è°ƒç”¨Claudeè¿›è¡Œæ™ºèƒ½æå–
            claude_result = await self._call_claude_extraction(extraction_prompt)

            if claude_result.get('success'):
                # ğŸ¯ éªŒè¯å’Œå¢å¼ºClaudeçš„æå–ç»“æœ
                validated_result = self._validate_and_enhance_result(
                    claude_result['extracted_data'], semantic_data, query_analysis
                )
                return validated_result
            else:
                # Claudeæå–å¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½é™çº§
                logger.warning("Claudeæå–å¤±è´¥ï¼Œå¯ç”¨æ™ºèƒ½é™çº§")
                return self._intelligent_fallback_extraction(semantic_data, query_analysis)

        except Exception as e:
            logger.error(f"Claudeæ™ºèƒ½æå–å¼‚å¸¸: {e}")
            return self._intelligent_fallback_extraction(semantic_data, query_analysis)

    def _determine_extraction_strategy(self, intent: str, semantic_data: Dict[str, Any]) -> str:
        """ç¡®å®šæå–ç­–ç•¥"""

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¹æ¯”æŸ¥è¯¢
        if any(keyword in intent for keyword in ['æ¯”è¾ƒ', 'å˜åŒ–', 'å¯¹æ¯”', 'ç›¸æ¯”']):
            return 'comparison_analysis'

        # æ£€æŸ¥æ˜¯å¦æ˜¯è¶‹åŠ¿æŸ¥è¯¢
        if any(keyword in intent for keyword in ['è¶‹åŠ¿', 'å¢é•¿', 'å˜åŒ–ç‡']):
            return 'trend_analysis'

        # æ£€æŸ¥æ˜¯å¦æ˜¯è®¡ç®—æŸ¥è¯¢
        if any(keyword in intent for keyword in ['è®¡ç®—', 'å¤æŠ•', 'é¢„æµ‹']):
            return 'calculation_analysis'

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ±‡æ€»æŸ¥è¯¢
        if any(keyword in intent for keyword in ['æ€»è®¡', 'æ±‡æ€»', 'ç»Ÿè®¡']):
            return 'summary_analysis'

        # æ£€æŸ¥è¯­ä¹‰æ•°æ®çš„ç‰¹å¾
        semantic_keys = list(semantic_data.keys())

        # å¦‚æœæœ‰å¤šä¸ªæ—¶é—´ç»´åº¦çš„æ•°æ®ï¼Œå¾ˆå¯èƒ½æ˜¯å¯¹æ¯”
        time_dimensions = sum(1 for key in semantic_keys
                            if any(time_word in key for time_word in
                                 ['current_week', 'last_week', 'today', 'yesterday']))

        if time_dimensions >= 2:
            return 'comparison_analysis'
        elif time_dimensions == 1:
            return 'single_period_analysis'
        else:
            return 'general_analysis'

    def _build_extraction_prompt(self,
                               semantic_data: Dict[str, Any],
                               user_query: str,
                               query_analysis,
                               strategy: str) -> str:
        """æ„å»ºæå–æç¤ºè¯"""

        base_info = f"""
        ä½œä¸ºé‡‘èæ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·ä»è¯­ä¹‰åŒ–æ•°æ®ä¸­æ™ºèƒ½æå–ä¿¡æ¯ï¼š
        
        ç”¨æˆ·æŸ¥è¯¢: "{user_query}"
        æŸ¥è¯¢æ„å›¾: {getattr(query_analysis, 'intent', 'æ•°æ®æŸ¥è¯¢')}
        å½“å‰æ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}
        æå–ç­–ç•¥: {strategy}
        
        è¯­ä¹‰åŒ–æ•°æ®:
        {json.dumps(semantic_data, ensure_ascii=False, indent=2)}
        """

        # ğŸ¯ æ ¹æ®ç­–ç•¥é€‰æ‹©ä¸“é—¨çš„æç¤ºè¯æ¨¡æ¿
        if strategy == 'comparison_analysis':
            return self._build_comparison_prompt(base_info, semantic_data)
        elif strategy == 'trend_analysis':
            return self._build_trend_prompt(base_info, semantic_data)
        elif strategy == 'calculation_analysis':
            return self._build_calculation_prompt(base_info, semantic_data)
        else:
            return self._build_general_prompt(base_info, semantic_data)

    def _build_calculation_prompt(self, base_info: str, semantic_data: Dict[str, Any]) -> str:
        """æ„å»ºè®¡ç®—åˆ†ææç¤ºè¯"""
        return f"""
        {base_info}

        ğŸ¯ **è®¡ç®—åˆ†æä»»åŠ¡**ï¼š
        1. è¯†åˆ«è®¡ç®—ç±»å‹ï¼ˆå¤æŠ•ã€é¢„æµ‹ã€æ”¶ç›Šè®¡ç®—ç­‰ï¼‰
        2. æå–è®¡ç®—æ‰€éœ€çš„åŸºç¡€æ•°æ®
        3. æ‰§è¡Œç²¾ç¡®çš„æ•°å€¼è®¡ç®—
        4. æä¾›è®¡ç®—è¿‡ç¨‹çš„é€æ˜å±•ç¤º

        **ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›**ï¼š
        {{
            "extraction_type": "calculation_analysis",
            "calculation_type": "reinvestment_analysis|yield_calculation|prediction",
            "base_data": {{
                "åˆ°æœŸé‡‘é¢": åˆ°æœŸæ€»é‡‘é¢,
                "å½“å‰ä½™é¢": ç³»ç»Ÿä½™é¢,
                "è®¡ç®—å‚æ•°": {{
                    "æç°æ¯”ä¾‹": 0.5,
                    "å¤æŠ•æ¯”ä¾‹": 0.5
                }}
            }},
            "detailed_calculations": {{
                "åˆ°æœŸé‡‘é¢": {{
                    "calculation_formula": "ä»APIæ•°æ®ä¸­æå–",
                    "step_by_step": [
                        "æ­¥éª¤1: è·å–5æœˆ11æ—¥è‡³5æœˆ31æ—¥åˆ°æœŸäº§å“æ•°æ®",
                        "æ­¥éª¤2: æ±‡æ€»æ‰€æœ‰åˆ°æœŸé‡‘é¢",
                        "æ­¥éª¤3: è®¡ç®—åˆ©æ¯æ”¶ç›Š"
                    ],
                    "total_expiry_amount": å…·ä½“é‡‘é¢,
                    "interest_earned": åˆ©æ¯é‡‘é¢
                }},
                "å¤æŠ•åˆ†é…": {{
                    "calculation_formula": "åˆ°æœŸæ€»é‡‘é¢ Ã— å„æ¯”ä¾‹",
                    "step_by_step": [
                        "æ­¥éª¤1: æç°é‡‘é¢ = åˆ°æœŸæ€»é‡‘é¢ Ã— 50%",
                        "æ­¥éª¤2: å¤æŠ•é‡‘é¢ = åˆ°æœŸæ€»é‡‘é¢ Ã— 50%"
                    ],
                    "withdrawal_amount": æç°é‡‘é¢,
                    "reinvestment_amount": å¤æŠ•é‡‘é¢
                }}
            }},
            "key_insights": [
                "åŸºäºå®é™…æ•°æ®çš„è®¡ç®—ç»“æœæ‘˜è¦",
                "é£é™©æç¤ºå’Œå»ºè®®"
            ],
            "extraction_confidence": 0.92
        }}

        **é‡è¦æé†’**ï¼š
        - æ‰€æœ‰è®¡ç®—å¿…é¡»åŸºäºå®é™…APIæ•°æ®
        - æä¾›å®Œæ•´çš„è®¡ç®—æ­¥éª¤
        - ç¡®ä¿æ•°å€¼ç²¾ç¡®æ€§
        - æ ‡æ˜æ•°æ®æ¥æºå’Œè®¡ç®—å‡è®¾
        """
    def _build_comparison_prompt(self, base_info: str, semantic_data: Dict[str, Any]) -> str:
        """æ„å»ºå¯¹æ¯”åˆ†ææç¤ºè¯"""
        return f"""
        {base_info}
        
        ğŸ¯ **å¯¹æ¯”åˆ†æä»»åŠ¡**ï¼š
        1. è‡ªåŠ¨è¯†åˆ«å¯¹æ¯”çš„æ—¶é—´æ®µï¼ˆå¦‚æœ¬å‘¨vsä¸Šå‘¨ã€ä»Šå¤©vsæ˜¨å¤©ï¼‰
        2. æå–æ¯ä¸ªæ—¶é—´æ®µçš„å…³é”®ä¸šåŠ¡æŒ‡æ ‡
        3. è®¡ç®—å˜åŒ–é‡ã€å˜åŒ–ç‡å’Œå˜åŒ–æ–¹å‘
        4. è¯„ä¼°å˜åŒ–çš„ä¸šåŠ¡æ„ä¹‰å’Œé‡è¦ç¨‹åº¦
        
        **ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›**ï¼š
        {{
            "extraction_type": "comparison_analysis",
            "comparison_periods": {{
                "current_period": {{
                    "period_name": "æœ¬å‘¨ï¼ˆ5æœˆ26æ—¥-6æœˆ1æ—¥ï¼‰",
                    "semantic_key": "current_week_get_daily_data",
                    "key_metrics": {{
                        "å…¥é‡‘": 229469.35,
                        "å‡ºé‡‘": 185886.25,
                        "å‡€æµå…¥": 43583.10,
                        "æ³¨å†Œäººæ•°": 156
                    }},
                    "period_summary": "æ•°æ®å®Œæ•´åº¦è¾ƒå¥½ï¼Œæ¶µç›–7å¤©"
                }},
                "baseline_period": {{
                    "period_name": "ä¸Šå‘¨ï¼ˆ5æœˆ19æ—¥-5æœˆ25æ—¥ï¼‰",
                    "semantic_key": "last_week_get_daily_data", 
                    "key_metrics": {{
                        "å…¥é‡‘": 198234.50,
                        "å‡ºé‡‘": 167234.80,
                        "å‡€æµå…¥": 31000.70,
                        "æ³¨å†Œäººæ•°": 134
                    }},
                    "period_summary": "æ•°æ®å®Œæ•´åº¦è¾ƒå¥½ï¼Œæ¶µç›–7å¤©"
                }}
            }},
            "comparison_analysis": {{
                "å…¥é‡‘": {{
                    "current_value": 229469.35,
                    "baseline_value": 198234.50,
                    "absolute_change": 31234.85,
                    "percentage_change": 0.157,
                    "change_direction": "å¢é•¿",
                    "significance_level": "æ˜¾è‘—",
                    "business_impact": "æ­£é¢"
                }},
                "å‡ºé‡‘": {{
                    "current_value": 185886.25,
                    "baseline_value": 167234.80,
                    "absolute_change": 18651.45,
                    "percentage_change": 0.111,
                    "change_direction": "å¢é•¿",
                    "significance_level": "ä¸­ç­‰",
                    "business_impact": "éœ€å…³æ³¨"
                }},
                "å‡€æµå…¥": {{
                    "current_value": 43583.10,
                    "baseline_value": 31000.70,
                    "absolute_change": 12582.40,
                    "percentage_change": 0.406,
                    "change_direction": "å¤§å¹…å¢é•¿",
                    "significance_level": "éå¸¸æ˜¾è‘—",
                    "business_impact": "éå¸¸æ­£é¢"
                }}
            }},
            "key_insights": [
                "æœ¬å‘¨å…¥é‡‘è¾ƒä¸Šå‘¨å¢é•¿15.7%ï¼Œæ˜¾ç¤ºä¸šåŠ¡å¢é•¿åŠ¿å¤´è‰¯å¥½",
                "å‡ºé‡‘è™½ç„¶ä¹Ÿæœ‰å¢é•¿ï¼Œä½†å‡€æµå…¥å®ç°40.6%çš„å¤§å¹…å¢é•¿",
                "æ³¨å†Œç”¨æˆ·æ•°å¢é•¿16.4%ï¼Œç”¨æˆ·å¢é•¿ä¸èµ„é‡‘å¢é•¿ä¿æŒåŒæ­¥"
            ],
            "data_quality_assessment": {{
                "overall_quality": 0.95,
                "completeness": 0.98,
                "consistency": 0.92,
                "issues": []
            }},
            "extraction_confidence": 0.94
        }}
        
        **é‡è¦æé†’**ï¼š
        - æ‰€æœ‰ç™¾åˆ†æ¯” = (æ–°å€¼ - æ—§å€¼) / æ—§å€¼
        - é‡‘é¢ä¿ç•™2ä½å°æ•°ï¼Œç™¾åˆ†æ¯”ä¿ç•™3ä½å°æ•°
        - å¿…é¡»åŸºäºå®é™…æ•°æ®è®¡ç®—ï¼Œä¸èƒ½ç¼–é€ æ•°å­—
        - è¯†åˆ«æ•°æ®å¼‚å¸¸å¹¶åœ¨issuesä¸­è¯´æ˜
        """

    def _build_trend_prompt(self, base_info: str, semantic_data: Dict[str, Any]) -> str:
        """æ„å»ºè¶‹åŠ¿åˆ†ææç¤ºè¯"""
        return f"""
        {base_info}
        
        ğŸ¯ **è¶‹åŠ¿åˆ†æä»»åŠ¡**ï¼š
        1. è¯†åˆ«æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„è¶‹åŠ¿æ¨¡å¼
        2. è®¡ç®—è¶‹åŠ¿å¼ºåº¦å’Œæ–¹å‘
        3. åˆ†æå‘¨æœŸæ€§å’Œå­£èŠ‚æ€§ç‰¹å¾
        4. é¢„æµ‹çŸ­æœŸå‘å±•æ–¹å‘
        
        **ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›**ï¼š
        {{
            "extraction_type": "trend_analysis",
            "time_series_data": {{
                "data_points": [
                    {{
                        "period": "5æœˆ26æ—¥",
                        "key_metrics": {{
                            "å…¥é‡‘": 32781.34,
                            "å‡ºé‡‘": 26555.18,
                            "å‡€æµå…¥": 6226.16
                        }}
                    }},
                    {{
                        "period": "5æœˆ27æ—¥",
                        "key_metrics": {{
                            "å…¥é‡‘": 33156.89,
                            "å‡ºé‡‘": 26890.44,
                            "å‡€æµå…¥": 6266.45
                        }}
                    }}
                ],
                "period_type": "daily",
                "total_periods": 7
            }},
            "trend_analysis": {{
                "å…¥é‡‘": {{
                    "trend_direction": "ä¸Šå‡",
                    "trend_strength": "ä¸­ç­‰",
                    "growth_rate": 0.023,
                    "volatility": "ä½",
                    "trend_score": 0.75
                }},
                "å‡ºé‡‘": {{
                    "trend_direction": "ä¸Šå‡",
                    "trend_strength": "è½»å¾®",
                    "growth_rate": 0.015,
                    "volatility": "ä¸­ç­‰",
                    "trend_score": 0.65
                }},
                "å‡€æµå…¥": {{
                    "trend_direction": "ç¨³å®šä¸Šå‡",
                    "trend_strength": "å¼º",
                    "growth_rate": 0.045,
                    "volatility": "ä½",
                    "trend_score": 0.88
                }}
            }},
            "pattern_recognition": {{
                "weekly_pattern": "å·¥ä½œæ—¥è¾ƒé«˜ï¼Œå‘¨æœ«è¾ƒä½",
                "daily_peak": "ä¸Šåˆ10-12ç‚¹",
                "seasonality": "æœˆåˆè¾ƒæ´»è·ƒ",
                "anomalies": []
            }},
            "trend_predictions": {{
                "short_term_outlook": "ç»§ç»­ä¸Šå‡è¶‹åŠ¿",
                "confidence_level": 0.82,
                "risk_factors": ["å¸‚åœºæ³¢åŠ¨", "å­£èŠ‚æ€§å½±å“"],
                "recommended_actions": ["ç»´æŒå½“å‰ç­–ç•¥", "å…³æ³¨ç”¨æˆ·åé¦ˆ"]
            }},
            "key_insights": [
                "å…¥é‡‘å‘ˆç°ç¨³å®šä¸Šå‡è¶‹åŠ¿ï¼Œå¢é•¿ç‡2.3%",
                "å‡€æµå…¥è¶‹åŠ¿å¼ºåŠ²ï¼Œæ˜¾ç¤ºå¥åº·çš„èµ„é‡‘æµåŠ¨",
                "å»ºè®®å…³æ³¨å‡ºé‡‘å¢é•¿ï¼Œç¡®ä¿æµåŠ¨æ€§å……è¶³"
            ],
            "extraction_confidence": 0.87
        }}
        
        **åˆ†æè¦ç‚¹**ï¼š
        - è¶‹åŠ¿å¼ºåº¦ï¼šå¼º/ä¸­ç­‰/è½»å¾®/æ— æ˜æ˜¾è¶‹åŠ¿
        - å¢é•¿ç‡ï¼šæ—¥å¢é•¿ç‡æˆ–å‘¨å¢é•¿ç‡
        - æ³¢åŠ¨æ€§ï¼šé«˜/ä¸­ç­‰/ä½
        - é¢„æµ‹ç½®ä¿¡åº¦ï¼šåŸºäºæ•°æ®è´¨é‡å’Œè¶‹åŠ¿ç¨³å®šæ€§
        """

    def _build_comparison_prompt(self, base_info: str, semantic_data: Dict[str, Any]) -> str:
        """æ„å»ºå¯¹æ¯”åˆ†ææç¤ºè¯ - å¢å¼ºç‰ˆ"""
        return f"""
        {base_info}

        ğŸ¯ **å¯¹æ¯”åˆ†æä»»åŠ¡ - è¯¦ç»†ç‰ˆ**ï¼š
        1. è‡ªåŠ¨è¯†åˆ«å¯¹æ¯”çš„æ—¶é—´æ®µï¼ˆå¦‚æœ¬å‘¨vsä¸Šå‘¨ã€ä»Šå¤©vsæ˜¨å¤©ï¼‰
        2. æå–æ¯ä¸ªæ—¶é—´æ®µçš„**æ¯æ—¥æ˜ç»†æ•°æ®**
        3. è®¡ç®—å˜åŒ–é‡ã€å˜åŒ–ç‡å’Œå˜åŒ–æ–¹å‘
        4. ä¿ç•™**å®Œæ•´çš„æ•°æ®è½¨è¿¹**ç”¨äºé€æ˜å±•ç¤º
        5. åŸºäºç”¨æˆ·çš„é—®é¢˜è¿›è¡Œè®¡ç®—

        **ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›**ï¼š
        {{
            "extraction_type": "comparison_analysis",
            "raw_data_details": {{
                "current_period_daily": [
                    {{
                        "date": "2025-05-26",
                        "å…¥é‡‘": 32781.34,
                        "å‡ºé‡‘": 26555.18,
                        "å‡€æµå…¥": 6226.16,
                        "æ³¨å†Œäººæ•°": 23
                    }},
                    {{
                        "date": "2025-05-27", 
                        "å…¥é‡‘": 33156.89,
                        "å‡ºé‡‘": 26890.44,
                        "å‡€æµå…¥": 6266.45,
                        "æ³¨å†Œäººæ•°": 25
                    }}
                    // ... ç»§ç»­æ¯ä¸€å¤©
                ],
                "baseline_period_daily": [
                    {{
                        "date": "2025-05-19",
                        "å…¥é‡‘": 28345.67,
                        "å‡ºé‡‘": 23456.78,
                        "å‡€æµå…¥": 4888.89,
                        "æ³¨å†Œäººæ•°": 19
                    }}
                    // ... ç»§ç»­æ¯ä¸€å¤©
                ]
            }},
            "aggregated_totals": {{
                "current_period": {{
                    "period_name": "æœ¬å‘¨ï¼ˆ5æœˆ26æ—¥-6æœˆ1æ—¥ï¼‰",
                    "total_days": 7,
                    "æ€»å…¥é‡‘": 229469.35,
                    "æ€»å‡ºé‡‘": 185886.25,
                    "æ€»å‡€æµå…¥": 43583.10,
                    "æ€»æ³¨å†Œäººæ•°": 156,
                    "æ—¥å‡å…¥é‡‘": 32781.34,
                    "æ—¥å‡å‡ºé‡‘": 26555.18
                }},
                "baseline_period": {{
                    "period_name": "ä¸Šå‘¨ï¼ˆ5æœˆ19æ—¥-5æœˆ25æ—¥ï¼‰",
                    "total_days": 7,
                    "æ€»å…¥é‡‘": 198234.50,
                    "æ€»å‡ºé‡‘": 167234.80,
                    "æ€»å‡€æµå…¥": 31000.70,
                    "æ€»æ³¨å†Œäººæ•°": 134,
                    "æ—¥å‡å…¥é‡‘": 28319.21,
                    "æ—¥å‡å‡ºé‡‘": 23890.69
                }}
            }},
            "detailed_calculations": {{
                "å…¥é‡‘": {{
                    "calculation_formula": "(229469.35 - 198234.50) / 198234.50 * 100",
                    "step_by_step": [
                        "æ­¥éª¤1: æœ¬å‘¨æ€»å…¥é‡‘ = 32781.34 + 33156.89 + 31945.67 + 34567.23 + 35890.45 + 29123.56 + 32004.21 = 229,469.35å…ƒ",
                        "æ­¥éª¤2: ä¸Šå‘¨æ€»å…¥é‡‘ = 28345.67 + 29876.54 + 27892.33 + 30456.78 + 32123.45 + 26789.12 + 22750.61 = 198,234.50å…ƒ",
                        "æ­¥éª¤3: å˜åŒ–é‡‘é¢ = 229,469.35 - 198,234.50 = 31,234.85å…ƒ",
                        "æ­¥éª¤4: å˜åŒ–ç‡ = 31,234.85 Ã· 198,234.50 Ã— 100% = 15.75%"
                    ],
                    "current_value": 229469.35,
                    "baseline_value": 198234.50,
                    "absolute_change": 31234.85,
                    "percentage_change": 0.1575,
                    "change_direction": "å¢é•¿",
                    "significance_level": "æ˜¾è‘—"
                }},
                "å‡ºé‡‘": {{
                    "calculation_formula": "(185886.25 - 167234.80) / 167234.80 * 100",
                    "step_by_step": [
                        "æ­¥éª¤1: æœ¬å‘¨æ€»å‡ºé‡‘ = 26555.18 + 26890.44 + 25123.78 + 27890.12 + 28456.90 + 24567.89 + 26401.94 = 185,886.25å…ƒ",
                        "æ­¥éª¤2: ä¸Šå‘¨æ€»å‡ºé‡‘ = 23456.78 + 24567.89 + 22890.45 + 25678.90 + 26789.12 + 22345.67 + 21505.99 = 167,234.80å…ƒ",
                        "æ­¥éª¤3: å˜åŒ–é‡‘é¢ = 185,886.25 - 167,234.80 = 18,651.45å…ƒ",
                        "æ­¥éª¤4: å˜åŒ–ç‡ = 18,651.45 Ã· 167,234.80 Ã— 100% = 11.15%"
                    ],
                    "current_value": 185886.25,
                    "baseline_value": 167234.80,
                    "absolute_change": 18651.45,
                    "percentage_change": 0.1115,
                    "change_direction": "å¢é•¿",
                    "significance_level": "ä¸­ç­‰"
                }}
            }},
            "data_validation": {{
                "data_anomalies": [],
                "quality_checks": {{
                    "duplicate_values": false,
                    "missing_dates": false,
                    "unrealistic_values": false,
                    "zero_variance_detected": false
                }},
                "confidence_assessment": {{
                    "data_completeness": 1.0,
                    "calculation_accuracy": 0.98,
                    "business_logic_consistency": 0.95
                }}
            }},
            "business_insights": [
                "æœ¬å‘¨å·¥ä½œæ—¥å¹³å‡å…¥é‡‘33,892å…ƒï¼Œè¾ƒä¸Šå‘¨çš„29,539å…ƒå¢é•¿14.7%",
                "å‘¨æœ«è¡¨ç°ï¼šæœ¬å‘¨å‘¨æœ«æ—¥å‡30,564å…ƒï¼Œä¸Šå‘¨24,770å…ƒï¼Œæå‡23.4%",
                "å‡ºé‡‘æ§åˆ¶è‰¯å¥½ï¼šå…¥é‡‘å¢é•¿15.75%çš„åŒæ—¶ï¼Œå‡ºé‡‘ä»…å¢é•¿11.15%",
                "å‡€æµå…¥å¤§å¹…æ”¹å–„ï¼šä»ä¸Šå‘¨31,000å…ƒå¢è‡³43,583å…ƒï¼Œå¢å¹…40.6%"
            ],
            "extraction_confidence": 0.94
        }}

        **å…³é”®è¦æ±‚**ï¼š
        - å¿…é¡»æä¾›æ¯æ—¥æ˜ç»†æ•°æ®ï¼Œä¸èƒ½åªæœ‰æ±‡æ€»
        - è®¡ç®—æ­¥éª¤è¦å®Œæ•´ï¼ŒåŒ…å«å…·ä½“çš„åŠ æ³•è¿‡ç¨‹
        - æ‰€æœ‰æ•°å€¼å¿…é¡»åŸºäºå®é™…æ•°æ®ï¼Œä¸èƒ½ç¼–é€ 
        - å¦‚æœå‘ç°æ•°æ®å¼‚å¸¸ï¼ˆå¦‚å®Œå…¨ç›¸åŒçš„å€¼ï¼‰ï¼Œå¿…é¡»åœ¨data_anomaliesä¸­æ ‡æ³¨
        - ç™¾åˆ†æ¯”è®¡ç®—ç²¾ç¡®åˆ°å°æ•°ç‚¹å2ä½
        """

    def _build_general_prompt(self, base_info: str, semantic_data: Dict[str, Any]) -> str:
        """æ„å»ºé€šç”¨åˆ†ææç¤ºè¯"""
        return f"""
        {base_info}
        
        ğŸ¯ **é€šç”¨æ•°æ®æå–ä»»åŠ¡**ï¼š
        1. è¯†åˆ«å¹¶æå–æ‰€æœ‰å¯ç”¨çš„ä¸šåŠ¡æŒ‡æ ‡
        2. è®¡ç®—è¡ç”ŸæŒ‡æ ‡ï¼ˆå¦‚å‡€æµå…¥ã€æ´»è·ƒç‡ç­‰ï¼‰
        3. è¯„ä¼°æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§
        4. æä¾›æ•°æ®æ‘˜è¦å’Œä¸šåŠ¡æ´å¯Ÿ
        
        **è¿”å›JSONæ ¼å¼**ï¼š
        {{
            "extraction_type": "general_analysis",
            "extracted_metrics": {{
                "primary_metrics": {{
                    "å…¥é‡‘": 171403.26,
                    "å‡ºé‡‘": 161710.52,
                    "æ€»ä½™é¢": 8223695.07,
                    "æ´»è·ƒç”¨æˆ·æ•°": 3911,
                    "æ³¨å†Œäººæ•°": 87
                }},
                "derived_metrics": {{
                    "å‡€æµå…¥": 9692.74,
                    "æ´»è·ƒç‡": 0.285,
                    "èµ„é‡‘æµå…¥æ¯”": 1.06,
                    "äººå‡ä½™é¢": 2102.47
                }}
            }},
            "business_health_indicators": {{
                "liquidity_status": "è‰¯å¥½",
                "growth_momentum": "ç§¯æ",
                "user_engagement": "ä¸­ç­‰",
                "risk_level": "ä½"
            }},
            "data_summary": {{
                "data_sources": ["system_data", "daily_data"],
                "time_coverage": "å½“å‰æ—¶ç‚¹",
                "metrics_count": 9,
                "completeness": 0.95
            }},
            "key_insights": [
                "èµ„é‡‘å‡€æµå…¥ä¸ºæ­£ï¼Œæ˜¾ç¤ºå¥åº·çš„èµ„é‡‘æµ",
                "ç”¨æˆ·æ´»è·ƒç‡28.5%ï¼Œå¤„äºåˆç†æ°´å¹³",
                "äººå‡ä½™é¢2,102å…ƒï¼Œç”¨æˆ·ä»·å€¼è¾ƒé«˜"
            ],
            "recommendations": [
                "ç»´æŒå½“å‰çš„è¿è¥ç­–ç•¥",
                "ç»§ç»­ç›‘æ§èµ„é‡‘æµåŠ¨æ€§",
                "è€ƒè™‘æå‡ç”¨æˆ·æ´»è·ƒåº¦"
            ],
            "data_quality_assessment": {{
                "overall_quality": 0.88,
                "completeness": 0.90,
                "consistency": 0.86,
                "reliability": 0.89
            }},
            "extraction_confidence": 0.87
        }}
        
        **åˆ†æè¦ç‚¹**ï¼š
        - æ¶µç›–æ‰€æœ‰å¯ç”¨çš„ä¸šåŠ¡æŒ‡æ ‡
        - è®¡ç®—æœ‰æ„ä¹‰çš„è¡ç”ŸæŒ‡æ ‡
        - æä¾›ä¸šåŠ¡å¥åº·åº¦è¯„ä¼°
        - ç»™å‡ºå¯æ“ä½œçš„å»ºè®®
        """

    async def _call_claude_extraction(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Claudeè¿›è¡Œæå–"""
        try:
            result = await self.claude_client.generate_text(prompt, max_tokens=5000)

            if result.get('success'):
                response_text = result.get('text', '{}')

                # ğŸ¯ è§£æClaudeè¿”å›çš„JSON
                extracted_data = self._parse_claude_json_response(response_text)

                if extracted_data:
                    return {
                        'success': True,
                        'extracted_data': extracted_data,
                        'raw_response': response_text[:500]  # ä¿ç•™éƒ¨åˆ†åŸå§‹å“åº”ç”¨äºè°ƒè¯•
                    }
                else:
                    return {
                        'success': False,
                        'error': 'JSONè§£æå¤±è´¥',
                        'raw_response': response_text[:200]
                    }
            else:
                return {
                    'success': False,
                    'error': result.get('error', 'Claudeè°ƒç”¨å¤±è´¥')
                }

        except Exception as e:
            logger.error(f"Claudeæå–è°ƒç”¨å¼‚å¸¸: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def _parse_claude_json_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """è§£æClaudeçš„JSONå“åº”"""
        try:
            # ç›´æ¥è§£æ
            return json.loads(response_text.strip())
        except json.JSONDecodeError:
            try:
                # æå–ä»£ç å—ä¸­çš„JSON
                json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group(1))

                # æå–å¤§æ‹¬å·ä¸­çš„å†…å®¹
                brace_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if brace_match:
                    return json.loads(brace_match.group())

            except json.JSONDecodeError:
                pass

        logger.error(f"æ— æ³•è§£æClaudeå“åº”ä¸ºJSON: {response_text[:300]}")
        return None

    def _validate_and_enhance_result(self,
                                   claude_result: Dict[str, Any],
                                   semantic_data: Dict[str, Any],
                                   query_analysis) -> Dict[str, Any]:
        """éªŒè¯å’Œå¢å¼ºClaudeçš„æå–ç»“æœ"""

        enhanced_result = claude_result.copy()

        # ğŸ¯ æ·»åŠ æå–æ–¹æ³•æ ‡è¯†
        enhanced_result['extraction_method'] = 'claude_intelligent'
        enhanced_result['extraction_timestamp'] = datetime.now().isoformat()

        # ğŸ¯ éªŒè¯å…³é”®å­—æ®µå­˜åœ¨æ€§
        validation_result = self._validate_extraction_completeness(claude_result, semantic_data)
        enhanced_result['validation_result'] = validation_result

        # ğŸ¯ æ·»åŠ åŸå§‹æ•°æ®å¼•ç”¨
        enhanced_result['source_data_summary'] = {
            'semantic_keys': list(semantic_data.keys()),
            'data_sources': len(semantic_data)
        }

        return enhanced_result

    def _validate_extraction_completeness(self,
                                        extraction: Dict[str, Any],
                                        semantic_data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯æå–ç»“æœçš„å®Œæ•´æ€§"""

        validation = {
            'is_complete': True,
            'missing_elements': [],
            'quality_score': 1.0
        }

        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['extraction_type', 'extraction_confidence']
        for field in required_fields:
            if field not in extraction:
                validation['missing_elements'].append(field)
                validation['quality_score'] -= 0.2

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if 'extraction_type' in extraction:
            extraction_type = extraction['extraction_type']

            if extraction_type == 'comparison_analysis':
                if 'comparison_periods' not in extraction:
                    validation['missing_elements'].append('comparison_periods')
                    validation['quality_score'] -= 0.3
                if 'comparison_analysis' not in extraction:
                    validation['missing_elements'].append('comparison_analysis')
                    validation['quality_score'] -= 0.3

        validation['is_complete'] = len(validation['missing_elements']) == 0
        validation['quality_score'] = max(0.0, validation['quality_score'])

        return validation

    def _intelligent_fallback_extraction(self,
                                       semantic_data: Dict[str, Any],
                                       query_analysis) -> Dict[str, Any]:
        """æ™ºèƒ½é™çº§æå–"""
        logger.info("æ‰§è¡Œæ™ºèƒ½é™çº§æå–")

        try:
            # ğŸ¯ åˆ†æè¯­ä¹‰æ•°æ®çš„ç‰¹å¾
            analysis = self._analyze_semantic_data_features(semantic_data)

            # ğŸ¯ æ ¹æ®ç‰¹å¾é€‰æ‹©é™çº§ç­–ç•¥
            if analysis['has_comparison_data']:
                return self._fallback_comparison_extraction(semantic_data, analysis)
            elif analysis['has_time_series']:
                return self._fallback_time_series_extraction(semantic_data, analysis)
            else:
                return self._fallback_general_extraction(semantic_data, analysis)

        except Exception as e:
            logger.error(f"æ™ºèƒ½é™çº§æå–å¤±è´¥: {e}")
            return self._create_error_result(f"æ™ºèƒ½é™çº§æå–å¤±è´¥: {str(e)}")

    def _analyze_semantic_data_features(self, semantic_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯­ä¹‰æ•°æ®çš„ç‰¹å¾"""

        semantic_keys = list(semantic_data.keys())

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹æ¯”æ•°æ®
        has_current_week = any('current_week' in key for key in semantic_keys)
        has_last_week = any('last_week' in key for key in semantic_keys)
        has_today = any('today' in key for key in semantic_keys)
        has_yesterday = any('yesterday' in key for key in semantic_keys)

        has_comparison_data = (has_current_week and has_last_week) or (has_today and has_yesterday)

        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´åºåˆ—æ•°æ®
        time_related_keys = [key for key in semantic_keys
                           if any(time_word in key for time_word in
                                ['week', 'day', 'date_', 'today', 'yesterday'])]

        has_time_series = len(time_related_keys) > 1

        return {
            'semantic_keys': semantic_keys,
            'has_comparison_data': has_comparison_data,
            'has_time_series': has_time_series,
            'time_related_keys': time_related_keys,
            'data_count': len(semantic_data)
        }

    def _fallback_comparison_extraction(self,
                                      semantic_data: Dict[str, Any],
                                      analysis: Dict[str, Any]) -> Dict[str, Any]:
        """é™çº§å¯¹æ¯”æå–"""

        # æ‰¾åˆ°å¯¹æ¯”çš„ä¸¤ä¸ªæ•°æ®é›†
        current_data = None
        baseline_data = None
        current_key = ""
        baseline_key = ""

        for key, data_entry in semantic_data.items():
            if 'current_week' in key or 'today' in key:
                current_data = data_entry
                current_key = key
            elif 'last_week' in key or 'yesterday' in key:
                baseline_data = data_entry
                baseline_key = key

        if not current_data or not baseline_data:
            return self._create_error_result("æ— æ³•æ‰¾åˆ°å¯¹æ¯”æ•°æ®é›†")

        # æå–å¯¹æ¯”æŒ‡æ ‡
        current_metrics = self._extract_key_metrics(current_data['data'])
        baseline_metrics = self._extract_key_metrics(baseline_data['data'])

        # è®¡ç®—å˜åŒ–
        comparison_analysis = {}
        for metric in current_metrics:
            if metric in baseline_metrics:
                current_val = current_metrics[metric]
                baseline_val = baseline_metrics[metric]

                if baseline_val != 0:
                    change_rate = (current_val - baseline_val) / baseline_val
                    comparison_analysis[metric] = {
                        'current_value': current_val,
                        'baseline_value': baseline_val,
                        'absolute_change': current_val - baseline_val,
                        'percentage_change': change_rate,
                        'change_direction': 'å¢é•¿' if change_rate > 0 else 'ä¸‹é™' if change_rate < 0 else 'æŒå¹³',
                        'significance_level': self._assess_change_significance(abs(change_rate)),
                        'business_impact': self._assess_business_impact(metric, change_rate)
                    }

        # ç”Ÿæˆæ´å¯Ÿ
        insights = self._generate_fallback_insights(comparison_analysis)

        return {
            'extraction_type': 'fallback_comparison',
            'comparison_periods': {
                'current_period': {
                    'semantic_key': current_key,
                    'key_metrics': current_metrics,
                    'period_name': self._generate_period_name(current_key)
                },
                'baseline_period': {
                    'semantic_key': baseline_key,
                    'key_metrics': baseline_metrics,
                    'period_name': self._generate_period_name(baseline_key)
                }
            },
            'comparison_analysis': comparison_analysis,
            'key_insights': insights,
            'extraction_method': 'intelligent_fallback',
            'extraction_confidence': 0.75,
            'data_quality_assessment': {
                'overall_quality': 0.7,
                'method': 'fallback',
                'completeness': len(comparison_analysis) / max(len(current_metrics), len(baseline_metrics))
            }
        }

    def _fallback_time_series_extraction(self,
                                       semantic_data: Dict[str, Any],
                                       analysis: Dict[str, Any]) -> Dict[str, Any]:
        """é™çº§æ—¶é—´åºåˆ—æå–"""

        time_series_data = []

        # æŒ‰æ—¶é—´é¡ºåºæ•´ç†æ•°æ®
        for key in sorted(analysis['time_related_keys']):
            if key in semantic_data:
                data_entry = semantic_data[key]
                metrics = self._extract_key_metrics(data_entry['data'])

                time_series_data.append({
                    'period': self._generate_period_name(key),
                    'semantic_key': key,
                    'key_metrics': metrics
                })

        # åŸºç¡€è¶‹åŠ¿åˆ†æ
        trend_analysis = {}
        if len(time_series_data) >= 2:
            first_metrics = time_series_data[0]['key_metrics']
            last_metrics = time_series_data[-1]['key_metrics']

            for metric in first_metrics:
                if metric in last_metrics:
                    first_val = first_metrics[metric]
                    last_val = last_metrics[metric]

                    if first_val != 0:
                        growth_rate = (last_val - first_val) / first_val
                        trend_analysis[metric] = {
                            'trend_direction': 'ä¸Šå‡' if growth_rate > 0 else 'ä¸‹é™' if growth_rate < 0 else 'æŒå¹³',
                            'growth_rate': growth_rate,
                            'trend_strength': self._assess_trend_strength(growth_rate),
                            'volatility': 'N/A'  # éœ€è¦æ›´å¤šæ•°æ®ç‚¹æ‰èƒ½è®¡ç®—
                        }

        insights = [
            f"æ—¶é—´åºåˆ—åŒ…å«{len(time_series_data)}ä¸ªæ•°æ®ç‚¹",
            "åŸºäºé¦–æœ«æ•°æ®ç‚¹è¿›è¡Œè¶‹åŠ¿åˆ†æ"
        ]

        for metric, trend in trend_analysis.items():
            insights.append(f"{metric}{trend['trend_direction']}è¶‹åŠ¿ï¼Œå¢é•¿ç‡{trend['growth_rate']:.1%}")

        return {
            'extraction_type': 'fallback_time_series',
            'time_series_data': {
                'data_points': time_series_data,
                'period_type': 'mixed',
                'total_periods': len(time_series_data)
            },
            'trend_analysis': trend_analysis,
            'key_insights': insights,
            'extraction_method': 'intelligent_fallback',
            'extraction_confidence': 0.65,
            'data_quality_assessment': {
                'overall_quality': 0.6,
                'method': 'fallback_time_series',
                'data_points': len(time_series_data)
            }
        }

    def _fallback_general_extraction(self,
                                   semantic_data: Dict[str, Any],
                                   analysis: Dict[str, Any]) -> Dict[str, Any]:
        """é™çº§é€šç”¨æå–"""

        # åˆå¹¶æ‰€æœ‰å¯ç”¨æ•°æ®
        all_metrics = {}
        data_sources = []

        for key, data_entry in semantic_data.items():
            metrics = self._extract_key_metrics(data_entry['data'])

            # é¿å…é‡å¤æŒ‡æ ‡ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°/æœ€ç›¸å…³çš„æ•°æ®
            for metric, value in metrics.items():
                if metric not in all_metrics:
                    all_metrics[metric] = value

            data_sources.append(key)

        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        derived_metrics = {}
        if 'å…¥é‡‘' in all_metrics and 'å‡ºé‡‘' in all_metrics:
            derived_metrics['å‡€æµå…¥'] = all_metrics['å…¥é‡‘'] - all_metrics['å‡ºé‡‘']
            derived_metrics['èµ„é‡‘æµå…¥æ¯”'] = all_metrics['å…¥é‡‘'] / all_metrics['å‡ºé‡‘'] if all_metrics['å‡ºé‡‘'] > 0 else 0

        if 'æ´»è·ƒç”¨æˆ·æ•°' in all_metrics and 'æ€»ç”¨æˆ·æ•°' in all_metrics:
            derived_metrics['æ´»è·ƒç‡'] = all_metrics['æ´»è·ƒç”¨æˆ·æ•°'] / all_metrics['æ€»ç”¨æˆ·æ•°'] if all_metrics['æ€»ç”¨æˆ·æ•°'] > 0 else 0

        # ç”ŸæˆåŸºç¡€æ´å¯Ÿ
        insights = []
        if 'å‡€æµå…¥' in derived_metrics:
            net_flow = derived_metrics['å‡€æµå…¥']
            if net_flow > 0:
                insights.append(f"å‡€æµå…¥ä¸ºæ­£({net_flow:,.2f})ï¼Œèµ„é‡‘çŠ¶å†µå¥åº·")
            else:
                insights.append(f"å‡€æµå…¥ä¸ºè´Ÿ({net_flow:,.2f})ï¼Œéœ€å…³æ³¨èµ„é‡‘æµåŠ¨")

        if 'æ´»è·ƒç‡' in derived_metrics:
            activity_rate = derived_metrics['æ´»è·ƒç‡']
            insights.append(f"ç”¨æˆ·æ´»è·ƒç‡{activity_rate:.1%}ï¼Œ{'å¤„äºè‰¯å¥½æ°´å¹³' if activity_rate > 0.3 else 'æœ‰æå‡ç©ºé—´'}")

        insights.append(f"æ•°æ®æ¥æºåŒ…æ‹¬{len(data_sources)}ä¸ªæ•°æ®é›†")

        return {
            'extraction_type': 'fallback_general',
            'extracted_metrics': {
                'primary_metrics': all_metrics,
                'derived_metrics': derived_metrics
            },
            'data_summary': {
                'data_sources': data_sources,
                'metrics_count': len(all_metrics) + len(derived_metrics),
                'time_coverage': 'æ··åˆæ—¶é—´ç‚¹'
            },
            'key_insights': insights,
            'extraction_method': 'intelligent_fallback',
            'extraction_confidence': 0.70,
            'data_quality_assessment': {
                'overall_quality': 0.65,
                'method': 'fallback_general',
                'completeness': min(1.0, len(all_metrics) / 8)  # å‡è®¾8ä¸ªæ˜¯ç†æƒ³æŒ‡æ ‡æ•°é‡
            }
        }

    def _extract_key_metrics(self, data: Dict[str, Any]) -> Dict[str, float]:
        """æå–å…³é”®æŒ‡æ ‡"""
        metrics = {}

        # èšåˆå¤šå¤©æ•°æ®ï¼ˆå¦‚æœæ˜¯åˆ—è¡¨ï¼‰
        if isinstance(data, list):
            # ç´¯åŠ æ‰€æœ‰å¤©çš„æ•°æ®
            aggregated = {}
            for day_data in data:
                if isinstance(day_data, dict):
                    for key, value in day_data.items():
                        if isinstance(value, (int, float)):
                            aggregated[key] = aggregated.get(key, 0) + value
            data = aggregated

        # æå–æ•°å€¼å­—æ®µ
        key_fields = ['å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œäººæ•°', 'è´­ä¹°äº§å“æ•°é‡', 'åˆ°æœŸäº§å“æ•°é‡', 'æŒä»“äººæ•°', 'æ€»ä½™é¢', 'æ€»å…¥é‡‘', 'æ€»å‡ºé‡‘', 'æ´»è·ƒç”¨æˆ·æ•°', 'æ€»ç”¨æˆ·æ•°']

        for key, value in data.items():
            if key in key_fields and isinstance(value, (int, float)):
                metrics[key] = float(value)

        # å¤„ç†åµŒå¥—çš„ç”¨æˆ·ç»Ÿè®¡æ•°æ®
        if 'ç”¨æˆ·ç»Ÿè®¡' in data and isinstance(data['ç”¨æˆ·ç»Ÿè®¡'], dict):
            user_stats = data['ç”¨æˆ·ç»Ÿè®¡']
            for field in ['æ€»ç”¨æˆ·æ•°', 'æ´»è·ƒç”¨æˆ·æ•°']:
                if field in user_stats:
                    try:
                        metrics[field] = float(user_stats[field])
                    except (ValueError, TypeError):
                        pass

        # è®¡ç®—åŸºç¡€è¡ç”ŸæŒ‡æ ‡
        if 'å…¥é‡‘' in metrics and 'å‡ºé‡‘' in metrics:
            metrics['å‡€æµå…¥'] = metrics['å…¥é‡‘'] - metrics['å‡ºé‡‘']

        return metrics

    def _assess_change_significance(self, change_rate: float) -> str:
        """è¯„ä¼°å˜åŒ–æ˜¾è‘—æ€§"""
        if change_rate > 0.5:
            return "ææ˜¾è‘—"
        elif change_rate > 0.2:
            return "éå¸¸æ˜¾è‘—"
        elif change_rate > 0.1:
            return "æ˜¾è‘—"
        elif change_rate > 0.05:
            return "ä¸­ç­‰"
        elif change_rate > 0.01:
            return "è½»å¾®"
        else:
            return "å¾®å¼±"

    def _assess_business_impact(self, metric: str, change_rate: float) -> str:
        """è¯„ä¼°ä¸šåŠ¡å½±å“"""
        impact_direction = "æ­£é¢" if change_rate > 0 else "è´Ÿé¢"

        if metric in ['å…¥é‡‘', 'æ³¨å†Œäººæ•°', 'è´­ä¹°äº§å“æ•°é‡']:
            return "æ­£é¢" if change_rate > 0 else "éœ€å…³æ³¨"
        elif metric in ['å‡ºé‡‘']:
            return "éœ€å…³æ³¨" if change_rate > 0 else "æ­£é¢"
        else:
            return impact_direction

    def _assess_trend_strength(self, growth_rate: float) -> str:
        """è¯„ä¼°è¶‹åŠ¿å¼ºåº¦"""
        abs_rate = abs(growth_rate)
        if abs_rate > 0.3:
            return "å¼º"
        elif abs_rate > 0.1:
            return "ä¸­ç­‰"
        elif abs_rate > 0.02:
            return "è½»å¾®"
        else:
            return "å¾®å¼±"

    def _generate_period_name(self, semantic_key: str) -> str:
        """æ ¹æ®è¯­ä¹‰é”®ç”ŸæˆæœŸé—´åç§°"""
        if 'current_week' in semantic_key:
            return "æœ¬å‘¨"
        elif 'last_week' in semantic_key:
            return "ä¸Šå‘¨"
        elif 'today' in semantic_key:
            return "ä»Šå¤©"
        elif 'yesterday' in semantic_key:
            return "æ˜¨å¤©"
        elif 'date_' in semantic_key:
            # æå–æ—¥æœŸ
            parts = semantic_key.split('_')
            for part in parts:
                if len(part) == 8 and part.isdigit():
                    try:
                        date_obj = datetime.strptime(part, '%Y%m%d')
                        return date_obj.strftime('%mæœˆ%dæ—¥')
                    except:
                        pass
            return f"ç‰¹å®šæ—¥æœŸ({semantic_key})"
        else:
            return semantic_key.replace('_', ' ')

    def _generate_fallback_insights(self, comparison_analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆé™çº§åˆ†æçš„æ´å¯Ÿ"""
        insights = []

        for metric, analysis in comparison_analysis.items():
            change_rate = analysis.get('percentage_change', 0)
            direction = analysis.get('change_direction', 'æŒå¹³')

            if abs(change_rate) > 0.01:  # å˜åŒ–è¶…è¿‡1%æ‰æŠ¥å‘Š
                insights.append(f"{metric}{direction}{abs(change_rate):.1%}ï¼Œ{analysis.get('significance_level', 'ä¸­ç­‰')}å˜åŒ–")

        if not insights:
            insights.append("å„æŒ‡æ ‡å˜åŒ–è¾ƒå°ï¼Œæ€»ä½“ä¿æŒç¨³å®š")

        return insights

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'extraction_type': 'error',
            'extraction_method': 'error_fallback',
            'success': False,
            'error': error_message,
            'extraction_confidence': 0.0,
            'extraction_timestamp': datetime.now().isoformat()
        }

    def _load_extraction_templates(self) -> Dict[str, str]:
        """åŠ è½½æå–æ¨¡æ¿ï¼ˆæœªæ¥å¯ä»¥ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰"""
        return {
            'comparison': 'å¯¹æ¯”åˆ†ææ¨¡æ¿',
            'trend': 'è¶‹åŠ¿åˆ†ææ¨¡æ¿',
            'calculation': 'è®¡ç®—åˆ†ææ¨¡æ¿',
            'general': 'é€šç”¨åˆ†ææ¨¡æ¿'
        }