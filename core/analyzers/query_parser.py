"""
ğŸ§  Claudeé©±åŠ¨çš„æ™ºèƒ½æŸ¥è¯¢è§£æå™¨ (é‡æ„ç‰ˆ)
ä¸“æ³¨äºè®©Claudeä¸€æ­¥åˆ°ä½å®ŒæˆæŸ¥è¯¢ç†è§£å’Œæ‰§è¡Œå†³ç­–

æ ¸å¿ƒæ”¹è¿›:
- Claudeç›´æ¥å†³å®šAPIè°ƒç”¨ç­–ç•¥
- åˆ é™¤å†—ä½™çš„GPTæ•°æ®éœ€æ±‚åˆ†æ
- ç®€åŒ–æ•°æ®ç»“æ„å’Œæµç¨‹
- ä¸“æ³¨äºå†³ç­–è€Œéå¤æ‚åˆ†æ
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import json
import re
import asyncio
import traceback
from dataclasses import dataclass, field
from enum import Enum

# å¯¼å…¥AIå®¢æˆ·ç«¯å’Œå·¥å…·
from core.models.claude_client import ClaudeClient
from core.models.openai_client import OpenAIClient
from utils.helpers.date_utils import DateUtils, create_date_utils

logger = logging.getLogger(__name__)


# ============= æšä¸¾å®šä¹‰ =============

class QueryComplexity(Enum):
    """æŸ¥è¯¢å¤æ‚åº¦ç­‰çº§"""
    SIMPLE = "simple"  # ç®€å•æŸ¥è¯¢ (ç›´æ¥æ•°æ®è·å–)
    MEDIUM = "medium"  # ä¸­ç­‰å¤æ‚ (åŸºç¡€åˆ†æè®¡ç®—)
    COMPLEX = "complex"  # å¤æ‚æŸ¥è¯¢ (å¤šæ­¥éª¤åˆ†æ)
    EXPERT = "expert"  # ä¸“å®¶çº§ (æ·±åº¦é¢„æµ‹åˆ†æ)


class QueryType(Enum):
    """æŸ¥è¯¢æ„å›¾ç±»å‹"""
    DATA_RETRIEVAL = "data_retrieval"  # æ•°æ®è·å–
    CALCULATION = "calculation"  # è®¡ç®—è¯·æ±‚
    TREND_ANALYSIS = "trend_analysis"  # è¶‹åŠ¿åˆ†æ
    COMPARISON = "comparison"  # å¯¹æ¯”åˆ†æ
    PREDICTION = "prediction"  # é¢„æµ‹è¯·æ±‚
    SCENARIO_SIMULATION = "scenario_simulation"  # åœºæ™¯æ¨¡æ‹Ÿ
    RISK_ASSESSMENT = "risk_assessment"  # é£é™©è¯„ä¼°
    GENERAL_KNOWLEDGE = "general_knowledge"  # ä¸€èˆ¬çŸ¥è¯†
    UNKNOWN = "unknown"  # æœªçŸ¥ç±»å‹


class BusinessScenario(Enum):
    """ä¸šåŠ¡åœºæ™¯ç±»å‹"""
    FINANCIAL_OVERVIEW = "financial_overview"  # è´¢åŠ¡æ¦‚è§ˆ
    DAILY_OPERATIONS = "daily_operations"  # æ—¥å¸¸è¿è¥
    USER_ANALYSIS = "user_analysis"  # ç”¨æˆ·è¡Œä¸ºåˆ†æ
    PRODUCT_ANALYSIS = "product_analysis"  # äº§å“è¡¨ç°åˆ†æ
    HISTORICAL_PERFORMANCE = "historical_performance"  # å†å²ä¸šç»©
    FUTURE_PROJECTION = "future_projection"  # æœªæ¥é¢„æµ‹
    RISK_MANAGEMENT = "risk_management"  # é£é™©ç®¡ç†
    UNKNOWN_SCENARIO = "unknown_scenario"  # æœªçŸ¥åœºæ™¯


# ============= æ•°æ®ç±»å®šä¹‰ =============

@dataclass
class QueryAnalysisResult:
    """
    ğŸ¯ ç®€åŒ–ç‰ˆæŸ¥è¯¢åˆ†æç»“æœ
    ä¸“æ³¨äºClaudeçš„ç†è§£å’Œå†³ç­–ç»“æœ
    """
    # åŸºç¡€ç†è§£ç»“æœ
    original_query: str
    complexity: QueryComplexity
    query_type: QueryType
    business_scenario: BusinessScenario
    confidence_score: float

    # ğŸ¯ æ ¸å¿ƒï¼šClaudeç›´æ¥å†³å®šçš„æ‰§è¡Œç­–ç•¥
    api_calls_needed: List[Dict[str, Any]] = field(default_factory=list)
    needs_calculation: bool = False
    calculation_type: Optional[str] = None

    # ç®€åŒ–çš„æ—¶é—´ä¿¡æ¯
    time_range: Optional[Dict[str, str]] = None

    # å…ƒæ•°æ®
    analysis_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    processing_metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'original_query': self.original_query,
            'complexity': self.complexity.value,
            'query_type': self.query_type.value,
            'business_scenario': self.business_scenario.value,
            'confidence_score': self.confidence_score,
            'api_calls_needed': self.api_calls_needed,
            'needs_calculation': self.needs_calculation,
            'calculation_type': self.calculation_type,
            'time_range': self.time_range,
            'analysis_timestamp': self.analysis_timestamp,
            'processing_metadata': self.processing_metadata
        }


# ============= ä¸»è¦ç±»å®šä¹‰ =============

class SmartQueryParser:
    """
    ğŸ§  Claudeé©±åŠ¨çš„æ™ºèƒ½æŸ¥è¯¢è§£æå™¨ (é‡æ„ç‰ˆ)

    æ ¸å¿ƒåŠŸèƒ½:
    1. Claudeä¸€æ­¥åˆ°ä½ç†è§£æŸ¥è¯¢
    2. ç›´æ¥å†³å®šAPIè°ƒç”¨ç­–ç•¥
    3. åˆ¤æ–­æ˜¯å¦éœ€è¦GPTè®¡ç®—
    4. è¾“å‡ºç®€åŒ–çš„æ‰§è¡Œè®¡åˆ’
    """

    def __init__(self, claude_client: Optional[ClaudeClient] = None, gpt_client=None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æŸ¥è¯¢è§£æå™¨

        Args:
            claude_client: Claudeå®¢æˆ·ç«¯ï¼Œè´Ÿè´£æŸ¥è¯¢ç†è§£å’Œå†³ç­–
            gpt_client: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ï¼Œä½†ä¸å†ä½¿ç”¨
        """
        self.claude_client = claude_client

        # ğŸ†• æ·»åŠ æ—¶é—´å¤„ç†èƒ½åŠ›
        self.date_utils = create_date_utils(claude_client) if claude_client else None

        # å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            'total_queries': 0,
            'successful_parses': 0,
            'fallback_parses': 0,
            'claude_failures': 0,
            'average_confidence': 0.0,
            'complexity_distribution': {
                'simple': 0, 'medium': 0, 'complex': 0, 'expert': 0
            },
            'query_type_distribution': {}
        }

        logger.info("SmartQueryParser (Refactored) initialized with Claude-driven architecture")

    # ============= æ ¸å¿ƒæŸ¥è¯¢è§£ææ–¹æ³• =============

    async def parse_complex_query(self, query: str, context: Dict[str, Any] = None) -> QueryAnalysisResult:
        """
        ğŸ¯ ç®€åŒ–ç‰ˆæŸ¥è¯¢è§£æ - Claudeä¸€æ­¥åˆ°ä½

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡ (å¯¹è¯å†å²ç­‰)

        Returns:
            QueryAnalysisResult: ç®€åŒ–çš„æŸ¥è¯¢åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ§  Claudeè§£ææŸ¥è¯¢: {query[:50]}...")
            self.processing_stats['total_queries'] += 1

            # åŸºç¡€é¢„å¤„ç†
            clean_query = self._preprocess_query(query)

            # ğŸ¯ æ ¸å¿ƒï¼šClaudeä¸€æ­¥åˆ°ä½ç†è§£æŸ¥è¯¢å¹¶åˆ¶å®šæ‰§è¡Œè®¡åˆ’
            claude_plan = await self._claude_understand_and_plan(clean_query, context)

            if not claude_plan.get("success"):
                logger.warning(f"Claudeç†è§£å¤±è´¥: {claude_plan.get('error')}, ä½¿ç”¨é™çº§è§£æ")
                self.processing_stats['claude_failures'] += 1
                return await self._fallback_analysis(query, claude_plan.get('error', ''))

            # æ„å»ºåˆ†æç»“æœ
            result = self._build_analysis_result(query, claude_plan)

            # æ›´æ–°ç»Ÿè®¡
            self._update_processing_stats(result)

            logger.info(
                f"âœ… Claudeè§£æå®Œæˆ: {result.query_type.value} | APIs: {len(result.api_calls_needed)} | "
                f"éœ€è¦è®¡ç®—: {result.needs_calculation} | ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
            return result

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢è§£æå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            self.processing_stats['claude_failures'] += 1
            return await self._fallback_analysis(query, str(e))

    async def _claude_understand_and_plan(self, query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ğŸ¯ Claudeä¸€æ­¥åˆ°ä½ï¼šç†è§£æŸ¥è¯¢ + å†³å®šæ‰§è¡Œç­–ç•¥
        """
        if not self.claude_client:
            logger.warning("Claudeå®¢æˆ·ç«¯ä¸å¯ç”¨")
            return {"success": False, "error": "Claudeå®¢æˆ·ç«¯æœªé…ç½®"}

        try:
            current_date = datetime.now().strftime("%Y-%m-%d")
            current_date_api = datetime.now().strftime("%Y%m%d")

            # ğŸ¯ è®©Claudeç›´æ¥å†³å®šAPIè°ƒç”¨å’Œè®¡ç®—éœ€æ±‚
            planning_prompt = f"""
ä½ æ˜¯é‡‘èAIç³»ç»Ÿçš„å†³ç­–å¤§è„‘ã€‚è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶ç›´æ¥åˆ¶å®šæ‰§è¡Œè®¡åˆ’ã€‚

ç”¨æˆ·æŸ¥è¯¢: "{query}"
å½“å‰æ—¥æœŸ: {current_date} (APIæ ¼å¼: {current_date_api})
å¯¹è¯å†å²: {json.dumps(context or {}, ensure_ascii=False)[:200]}

è¯·åˆ†æå¹¶è¿”å›JSONæ ¼å¼çš„æ‰§è¡Œè®¡åˆ’ï¼š

{{
    "query_understanding": {{
        "complexity": "simple|medium|complex|expert",
        "query_type": "data_retrieval|trend_analysis|prediction|calculation|comparison|risk_assessment|general_knowledge",
        "business_scenario": "financial_overview|daily_operations|user_analysis|product_analysis|historical_performance|future_projection|risk_management",
        "user_intent": "ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ",
        "confidence": 0.8
    }},
    "execution_plan": {{
        "api_calls": [
            {{
                "api_method": "get_system_data",
                "params": {{}},
                "reason": "è·å–ç³»ç»Ÿæ¦‚è§ˆæ•°æ®"
            }}
        ],
        "needs_calculation": false,
        "calculation_type": "statistics|trend_analysis|growth_calculation|prediction|comparison|none",
        "calculation_description": "éœ€è¦GPTåšä»€ä¹ˆè®¡ç®—"
    }},
    "time_analysis": {{
        "has_time_requirement": false,
        "start_date": "20240501", 
        "end_date": "20240531",
        "time_description": "æ—¶é—´èŒƒå›´æè¿°"
    }}
}}

å¯ç”¨çš„APIæ–¹æ³•ï¼š
- get_system_data(): å½“å‰ç³»ç»Ÿæ¦‚è§ˆ (æ€»ä½™é¢ã€ç”¨æˆ·ç»Ÿè®¡ã€ä»Šæ—¥æ•°æ®)
- get_daily_data(date): ç‰¹å®šæ—¥æœŸçš„ä¸šåŠ¡æ•°æ®ï¼Œdateæ ¼å¼ä¸ºYYYYMMDD
- get_product_data(): äº§å“ä¿¡æ¯å’ŒæŒæœ‰æƒ…å†µ
- get_product_end_data(date): ç‰¹å®šæ—¥æœŸåˆ°æœŸäº§å“ï¼Œdateæ ¼å¼ä¸ºYYYYMMDD
- get_product_end_interval(start_date, end_date): åŒºé—´åˆ°æœŸæ•°æ®ï¼Œæ—¥æœŸæ ¼å¼ä¸ºYYYYMMDD
- get_user_daily_data(date): ç”¨æˆ·æ¯æ—¥ç»Ÿè®¡ï¼Œdateæ ¼å¼ä¸ºYYYYMMDD
- get_user_data(page): è¯¦ç»†ç”¨æˆ·æ•°æ®ï¼Œpageä¸ºé¡µç 

è®¡ç®—ç±»å‹è¯´æ˜ï¼š
- statistics: åŸºç¡€ç»Ÿè®¡ï¼ˆå‡å€¼ã€æ€»å’Œã€å¢é•¿ç‡ç­‰ï¼‰
- trend_analysis: è¶‹åŠ¿è®¡ç®—å’Œæ¨¡å¼è¯†åˆ«
- growth_calculation: å¢é•¿ç‡å’Œå˜åŒ–è®¡ç®—
- prediction: åŸºäºå†å²æ•°æ®çš„é¢„æµ‹
- comparison: å¯¹æ¯”åˆ†æ
- none: ä¸éœ€è¦è®¡ç®—ï¼Œç›´æ¥å±•ç¤ºæ•°æ®

åˆ†æè§„åˆ™ï¼š
- å¦‚æœæ˜¯"ä»Šå¤©/ä»Šæ—¥"æŸ¥è¯¢ â†’ get_system_data + get_daily_data(ä»Šæ—¥æ—¥æœŸ)
- å¦‚æœæåˆ°"å†å²/è¿‡å»Nå¤©"æ—¶é—´ â†’ éœ€è¦æ—¶é—´èŒƒå›´çš„API + å¯èƒ½éœ€è¦è®¡ç®—
- å¦‚æœæ˜¯ä½™é¢/æ¦‚è§ˆæŸ¥è¯¢ â†’ get_system_data
- å¦‚æœæ˜¯äº§å“ç›¸å…³ â†’ get_product_data
- å¦‚æœæåˆ°"åˆ°æœŸ" â†’ get_product_end_* ç›¸å…³API
- å¦‚æœæåˆ°ç”¨æˆ·/æ³¨å†Œ â†’ get_user_daily_data æˆ– get_user_data
- å¦‚æœéœ€è¦"åˆ†æ/è®¡ç®—/è¶‹åŠ¿/é¢„æµ‹" â†’ needs_calculation = true


ğŸ” **æ™ºèƒ½APIé€‰æ‹©è§„åˆ™ï¼š**

**æ—¶é—´ç›¸å…³æŸ¥è¯¢ï¼š**
- å…·ä½“æ—¥æœŸï¼ˆå¦‚"6æœˆ1æ—¥"ï¼‰ â†’ get_product_end_data(date) / get_daily_data(date)
- æ—¥æœŸåŒºé—´ï¼ˆå¦‚"6æœˆ1æ—¥è‡³6æœˆ30æ—¥"ï¼‰ â†’ get_product_end_interval(start, end)
- "ä»Šå¤©/ä»Šæ—¥" â†’ get_system_data() + get_expiring_products_today()
- "æœ¬å‘¨" â†’ get_expiring_products_week()
- "å†å²è¶‹åŠ¿" â†’ get_date_range_data() å¤šä¸ªæ—¥æœŸ

**æ•°æ®ç±»å‹è¯†åˆ«ï¼š**
- æåˆ°"åˆ°æœŸ/è¿‡æœŸ" â†’ ä¼˜å…ˆä½¿ç”¨ get_product_end_* ç³»åˆ—API
- æåˆ°"æ€»èµ„é‡‘/ä½™é¢/æ¦‚è§ˆ" â†’ å¿…é¡»åŒ…å« get_system_data()
- æåˆ°"å…¥é‡‘/å‡ºé‡‘/æ³¨å†Œ" â†’ ä½¿ç”¨ get_daily_data()
- æåˆ°"ç”¨æˆ·/VIP/æ´»è·ƒ" â†’ ä½¿ç”¨ get_user_* ç³»åˆ—API
- æåˆ°"äº§å“/æŒä»“" â†’ ä½¿ç”¨ get_product_data()

**è®¡ç®—éœ€æ±‚è¯†åˆ«ï¼š**
- æåˆ°"å¤æŠ•/å†æŠ•èµ„" â†’ calculation_type: "reinvestment_analysis"
- æåˆ°"è¶‹åŠ¿/å¢é•¿/å˜åŒ–" â†’ calculation_type: "trend_analysis"  
- æåˆ°"é¢„æµ‹/é¢„è®¡/è¿˜èƒ½è¿è¡Œ" â†’ calculation_type: "cash_runway"
- æåˆ°"å¯¹æ¯”/æ¯”è¾ƒ" â†’ calculation_type: "comparison"

**æ™ºèƒ½ç»„åˆç¤ºä¾‹ï¼š**

æŸ¥è¯¢ï¼š"6æœˆ1æ—¥çš„æœ‰å¤šå°‘äº§å“åˆ°æœŸï¼Œæ€»èµ„é‡‘å¤šå°‘"
â†’ APIç»„åˆï¼šget_product_end_data(20240601) + get_system_data()

æŸ¥è¯¢ï¼š"6æœˆ1æ—¥è‡³6æœˆ30æ—¥äº§å“åˆ°æœŸé‡‘é¢ï¼Œ25%å¤æŠ•ï¼Œ7æœˆ1æ—¥å‰©ä½™èµ„é‡‘"  
â†’ APIç»„åˆï¼šget_product_end_interval(20240601,20240630) + get_system_data()
â†’ è®¡ç®—ï¼šreinvestment_analysis

æŸ¥è¯¢ï¼š"æœ¬å‘¨åˆ°æœŸé‡‘é¢å’Œä»Šæ—¥åˆ°æœŸé‡‘é¢å˜åŒ–è¶‹åŠ¿"
â†’ APIç»„åˆï¼šget_expiring_products_today() + get_expiring_products_week() 
â†’ è®¡ç®—ï¼štrend_analysis

æŸ¥è¯¢ï¼š"5æœˆ28æ—¥å…¥é‡‘"
â†’ APIç»„åˆï¼šget_daily_data(20240528)

æŸ¥è¯¢ï¼š"ç›®å‰å…¬å¸æ´»è·ƒä¼šå‘˜æœ‰å¤šå°‘"  
â†’ APIç»„åˆï¼šget_system_data()

æŸ¥è¯¢ï¼š"å‡è®¾ç°åœ¨æ²¡å…¥é‡‘å…¬å¸è¿˜èƒ½è¿è¡Œå¤šä¹…"
â†’ APIç»„åˆï¼šget_system_data() + get_date_range_data(æœ€è¿‘30å¤©)
â†’ è®¡ç®—ï¼šcash_runway

**æ—¥æœŸè§£æå¢å¼ºï¼š**
- è‡ªåŠ¨å°†"6æœˆ1æ—¥"è½¬æ¢ä¸º"20240601"æ ¼å¼
- è¯†åˆ«"ä¸Šä¸ªæ˜ŸæœŸ"ä¸ºå…·ä½“æ—¥æœŸèŒƒå›´
- å¤„ç†"5æœˆ11æ—¥è‡³5æœˆ31æ—¥"ä¸ºåŒºé—´æŸ¥è¯¢

è¯·æ ¹æ®ä»¥ä¸Šè§„åˆ™åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œé€‰æ‹©æœ€ä¼˜çš„APIç»„åˆç­–ç•¥ã€‚


æ—¥æœŸæ ¼å¼è¦æ±‚ï¼š
- æ‰€æœ‰APIçš„æ—¥æœŸå‚æ•°å¿…é¡»ä½¿ç”¨YYYYMMDDæ ¼å¼ï¼ˆå¦‚ï¼š20240501ï¼‰
- ä»Šæ—¥æ—¥æœŸæ˜¯ï¼š{current_date_api}

è¯·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢é€‰æ‹©æœ€åˆé€‚çš„APIå’Œè®¡ç®—ç±»å‹ï¼Œç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®ã€‚
"""
            # è°ƒç”¨Claude
            result = await asyncio.wait_for(
                self.claude_client.analyze_complex_query(planning_prompt, {
                    "query": query,
                    "context": context,
                    "current_date": current_date
                }),
                timeout=30.0  # 30ç§’è¶…æ—¶
            )

            if result.get("success"):
                # è§£æClaudeçš„åˆ†æç»“æœ
                analysis_text = result.get("analysis", "{}")

                # å°è¯•æå–JSON
                analysis = self._extract_json_from_response(analysis_text)

                if analysis:
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if self._validate_claude_response(analysis):
                        return {
                            "success": True,
                            "claude_plan": analysis,
                            "processing_method": "claude_integrated_planning"
                        }
                    else:
                        logger.error("Claudeå“åº”ç¼ºå°‘å¿…è¦å­—æ®µ")
                        return {"success": False, "error": "Claudeå“åº”æ ¼å¼ä¸å®Œæ•´"}
                else:
                    logger.error(f"æ— æ³•è§£æClaudeçš„JSONå“åº”: {analysis_text[:200]}")
                    return {"success": False, "error": "Claudeå“åº”JSONè§£æå¤±è´¥"}
            else:
                logger.error(f"Claudeåˆ†æå¤±è´¥: {result.get('error')}")
                return {"success": False, "error": result.get('error', 'Claudeè°ƒç”¨å¤±è´¥')}

        except asyncio.TimeoutError:
            logger.error("Claudeè°ƒç”¨è¶…æ—¶")
            return {"success": False, "error": "Claudeå“åº”è¶…æ—¶"}
        except Exception as e:
            logger.error(f"Claudeç†è§£å’Œè§„åˆ’å¼‚å¸¸: {str(e)}\n{traceback.format_exc()}")
            return {"success": False, "error": str(e)}

    def _validate_claude_response(self, analysis: Dict[str, Any]) -> bool:
        """éªŒè¯Claudeå“åº”çš„å®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥å¿…è¦çš„é¡¶çº§å­—æ®µ
            required_top_fields = ["query_understanding", "execution_plan"]
            for field in required_top_fields:
                if field not in analysis:
                    logger.error(f"Claudeå“åº”ç¼ºå°‘å­—æ®µ: {field}")
                    return False

            # æ£€æŸ¥query_understandingå­—æ®µ
            understanding = analysis["query_understanding"]
            required_understanding_fields = ["complexity", "query_type", "confidence"]
            for field in required_understanding_fields:
                if field not in understanding:
                    logger.error(f"query_understandingç¼ºå°‘å­—æ®µ: {field}")
                    return False

            # æ£€æŸ¥execution_planå­—æ®µ
            execution = analysis["execution_plan"]
            required_execution_fields = ["api_calls", "needs_calculation"]
            for field in required_execution_fields:
                if field not in execution:
                    logger.error(f"execution_planç¼ºå°‘å­—æ®µ: {field}")
                    return False

            # æ£€æŸ¥api_callsæ˜¯å¦ä¸ºåˆ—è¡¨
            if not isinstance(execution["api_calls"], list):
                logger.error("api_callså¿…é¡»æ˜¯åˆ—è¡¨")
                return False

            return True
        except Exception as e:
            logger.error(f"éªŒè¯Claudeå“åº”æ—¶å‡ºé”™: {e}")
            return False

    def _extract_json_from_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """ä»Claudeå“åº”ä¸­æå–JSON"""
        try:
            # ç›´æ¥å°è¯•è§£ææ•´ä¸ªå“åº”
            return json.loads(response_text.strip())
        except json.JSONDecodeError:
            try:
                # å°è¯•æå–ä»£ç å—ä¸­çš„JSON
                json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group(1))

                # å°è¯•æå–å¤§æ‹¬å·ä¸­çš„å†…å®¹
                brace_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if brace_match:
                    return json.loads(brace_match.group())

            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")

        logger.error(f"æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆJSON: {response_text[:300]}")
        return None

    def _build_analysis_result(self, original_query: str, claude_plan: Dict[str, Any]) -> QueryAnalysisResult:
        """æ„å»ºåˆ†æç»“æœ"""
        try:
            plan_data = claude_plan["claude_plan"]
            understanding = plan_data["query_understanding"]
            execution = plan_data["execution_plan"]
            time_info = plan_data.get("time_analysis", {})

            # ğŸ†• å¢å¼ºAPIè°ƒç”¨å‚æ•°å¤„ç†
            api_calls = []
            for api_call in execution.get("api_calls", []):
                method = api_call.get("api_method", "get_system_data")
                params = api_call.get("params", {})

                # ğŸ¯ å¤„ç†æ—¥æœŸå‚æ•°æ ¼å¼è½¬æ¢
                params = self._process_api_params(params)

                api_calls.append({
                    "method": method,
                    "params": params,
                    "reason": api_call.get("reason", "æ•°æ®è·å–")
                })

            # æ„å»ºæ—¶é—´èŒƒå›´
            time_range = None
            if time_info.get("has_time_requirement"):
                time_range = {
                    "start_date": time_info.get("start_date"),
                    "end_date": time_info.get("end_date"),
                    "description": time_info.get("time_description", "")
                }

            # å®‰å…¨åœ°è·å–æšä¸¾å€¼
            complexity = self._safe_get_enum(QueryComplexity, understanding.get("complexity", "medium"))
            query_type = self._safe_get_enum(QueryType, understanding.get("query_type", "data_retrieval"))
            business_scenario = self._safe_get_enum(BusinessScenario,
                                                    understanding.get("business_scenario", "daily_operations"))

            return QueryAnalysisResult(
                original_query=original_query,
                complexity=complexity,
                query_type=query_type,
                business_scenario=business_scenario,
                confidence_score=float(understanding.get("confidence", 0.75)),

                # æ ¸å¿ƒæ‰§è¡Œä¿¡æ¯
                api_calls_needed=api_calls,
                needs_calculation=execution.get("needs_calculation", False),
                calculation_type=execution.get("calculation_type") if execution.get("needs_calculation") else None,

                # æ—¶é—´ä¿¡æ¯
                time_range=time_range,

                processing_metadata={
                    "user_intent": understanding.get("user_intent", ""),
                    "api_count": len(api_calls),
                    "processing_method": "claude_integrated",
                    "calculation_description": execution.get("calculation_description", "")
                }
            )
        except Exception as e:
            logger.error(f"æ„å»ºåˆ†æç»“æœå¤±è´¥: {e}\n{traceback.format_exc()}")
            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç»“æœ
            return QueryAnalysisResult(
                original_query=original_query,
                complexity=QueryComplexity.SIMPLE,
                query_type=QueryType.DATA_RETRIEVAL,
                business_scenario=BusinessScenario.DAILY_OPERATIONS,
                confidence_score=0.5,
                api_calls_needed=[{"method": "get_system_data", "params": {}, "reason": "é™çº§æ•°æ®è·å–"}],
                processing_metadata={"error": str(e), "fallback": True}
            )

    def _safe_get_enum(self, enum_class, value: str):
        """å®‰å…¨åœ°è·å–æšä¸¾å€¼"""
        try:
            return enum_class(value)
        except ValueError:
            logger.warning(f"æ— æ•ˆçš„æšä¸¾å€¼ {value} for {enum_class.__name__}, ä½¿ç”¨é»˜è®¤å€¼")
            return list(enum_class)[0]  # è¿”å›ç¬¬ä¸€ä¸ªæšä¸¾å€¼ä½œä¸ºé»˜è®¤

    def _process_api_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†APIå‚æ•°ï¼Œç‰¹åˆ«æ˜¯æ—¥æœŸæ ¼å¼"""
        processed_params = params.copy()

        # å¤„ç†æ—¥æœŸå‚æ•°
        date_fields = ["date", "start_date", "end_date"]
        for field in date_fields:
            if field in processed_params and processed_params[field]:
                processed_params[field] = self._convert_date_for_api(processed_params[field])

        return processed_params

    def _convert_date_for_api(self, date_str: str) -> str:
        """å°†æ—¥æœŸè½¬æ¢ä¸ºAPIéœ€è¦çš„YYYYMMDDæ ¼å¼"""
        if not date_str:
            return datetime.now().strftime('%Y%m%d')

        try:
            # å¦‚æœå·²ç»æ˜¯YYYYMMDDæ ¼å¼
            if len(date_str) == 8 and date_str.isdigit():
                return date_str

            # å¦‚æœæ˜¯YYYY-MM-DDæ ¼å¼
            if len(date_str) == 10 and '-' in date_str:
                return date_str.replace('-', '')

            # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•è§£æ
            if self.date_utils:
                # ä½¿ç”¨date_utilsè¿›è¡Œæ™ºèƒ½è½¬æ¢
                try:
                    parsed_date = self.date_utils.api_format_to_date(date_str)
                    return parsed_date.strftime('%Y%m%d')
                except:
                    pass

            # é»˜è®¤è¿”å›ä»Šå¤©
            logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}, ä½¿ç”¨ä»Šå¤©æ—¥æœŸ")
            return datetime.now().strftime('%Y%m%d')

        except Exception as e:
            logger.error(f"æ—¥æœŸè½¬æ¢å¤±è´¥: {date_str}, é”™è¯¯: {e}")
            return datetime.now().strftime('%Y%m%d')

    # ============= é™çº§å’Œå·¥å…·æ–¹æ³• =============

    async def _fallback_analysis(self, query: str, error: str = "") -> QueryAnalysisResult:
        """é™çº§åˆ†æï¼šå¢å¼ºç‰ˆåŸºäºè§„åˆ™çš„è§£æ"""
        logger.info(f"æ‰§è¡Œé™çº§è§£æ: {query[:50]}...")
        self.processing_stats['fallback_parses'] += 1

        query_lower = query.lower()

        # ğŸ†• æ›´è¯¦ç»†çš„å…³é”®è¯åˆ¤æ–­
        if any(kw in query_lower for kw in ["ä»Šå¤©", "ä»Šæ—¥", "å½“å‰", "ç°åœ¨"]):
            query_type = QueryType.DATA_RETRIEVAL
            api_calls = [
                {"method": "get_system_data", "params": {}, "reason": "è·å–å½“å‰ç³»ç»Ÿæ•°æ®"},
                {"method": "get_daily_data", "params": {"date": datetime.now().strftime('%Y%m%d')},
                 "reason": "è·å–ä»Šæ—¥æ•°æ®"}
            ]
            complexity = QueryComplexity.SIMPLE
            scenario = BusinessScenario.DAILY_OPERATIONS

        elif any(kw in query_lower for kw in ["ä½™é¢", "æ€»èµ„äº§", "æ€»é‡‘é¢", "èµ„é‡‘"]):
            query_type = QueryType.DATA_RETRIEVAL
            api_calls = [{"method": "get_system_data", "params": {}, "reason": "è·å–èµ„äº§æ¦‚è§ˆ"}]
            complexity = QueryComplexity.SIMPLE
            scenario = BusinessScenario.FINANCIAL_OVERVIEW

        elif any(kw in query_lower for kw in ["äº§å“", "åˆ°æœŸ", "æŒæœ‰"]):
            query_type = QueryType.DATA_RETRIEVAL
            if "åˆ°æœŸ" in query_lower:
                api_calls = [
                    {"method": "get_product_data", "params": {}, "reason": "è·å–äº§å“ä¿¡æ¯"},
                    {"method": "get_product_end_data", "params": {"date": datetime.now().strftime('%Y%m%d')},
                     "reason": "è·å–ä»Šæ—¥åˆ°æœŸäº§å“"}
                ]
            else:
                api_calls = [{"method": "get_product_data", "params": {}, "reason": "è·å–äº§å“ä¿¡æ¯"}]
            complexity = QueryComplexity.SIMPLE
            scenario = BusinessScenario.PRODUCT_ANALYSIS

        elif any(kw in query_lower for kw in ["ç”¨æˆ·", "æ³¨å†Œ", "æ´»è·ƒ"]):
            query_type = QueryType.DATA_RETRIEVAL
            api_calls = [
                {"method": "get_user_daily_data", "params": {"date": datetime.now().strftime('%Y%m%d')},
                 "reason": "è·å–ç”¨æˆ·æ•°æ®"}
            ]
            complexity = QueryComplexity.SIMPLE
            scenario = BusinessScenario.USER_ANALYSIS

        elif any(kw in query_lower for kw in ["è¶‹åŠ¿", "å¢é•¿", "å˜åŒ–", "å†å²"]):
            query_type = QueryType.TREND_ANALYSIS
            api_calls = [
                {"method": "get_system_data", "params": {}, "reason": "è·å–åŸºç¡€æ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æ"},
                {"method": "get_daily_data", "params": {"date": datetime.now().strftime('%Y%m%d')},
                 "reason": "è·å–ä»Šæ—¥æ•°æ®"}
            ]
            complexity = QueryComplexity.MEDIUM
            scenario = BusinessScenario.HISTORICAL_PERFORMANCE

        elif any(kw in query_lower for kw in ["é¢„æµ‹", "é¢„è®¡", "æœªæ¥", "é¢„æœŸ"]):
            query_type = QueryType.PREDICTION
            api_calls = [{"method": "get_system_data", "params": {}, "reason": "è·å–æ•°æ®è¿›è¡Œé¢„æµ‹"}]
            complexity = QueryComplexity.COMPLEX
            scenario = BusinessScenario.FUTURE_PROJECTION

        elif any(kw in query_lower for kw in ["é£é™©", "å®‰å…¨", "å±é™©"]):
            query_type = QueryType.RISK_ASSESSMENT
            api_calls = [{"method": "get_system_data", "params": {}, "reason": "è·å–æ•°æ®è¿›è¡Œé£é™©è¯„ä¼°"}]
            complexity = QueryComplexity.COMPLEX
            scenario = BusinessScenario.RISK_MANAGEMENT

        else:
            # é»˜è®¤æƒ…å†µ
            query_type = QueryType.DATA_RETRIEVAL
            api_calls = [{"method": "get_system_data", "params": {}, "reason": "è·å–ç³»ç»Ÿæ¦‚è§ˆ"}]
            complexity = QueryComplexity.SIMPLE
            scenario = BusinessScenario.DAILY_OPERATIONS

        return QueryAnalysisResult(
            original_query=query,
            complexity=complexity,
            query_type=query_type,
            business_scenario=scenario,
            confidence_score=0.6,  # é™çº§è§£æç½®ä¿¡åº¦è¾ƒä½
            api_calls_needed=api_calls,
            needs_calculation=query_type in [QueryType.TREND_ANALYSIS, QueryType.PREDICTION, QueryType.CALCULATION],
            calculation_type="statistics" if query_type == QueryType.TREND_ANALYSIS else
            "prediction" if query_type == QueryType.PREDICTION else
            "calculation" if query_type == QueryType.CALCULATION else None,
            processing_metadata={
                "parsing_method": "fallback_rule_based",
                "fallback_reason": error or "Claudeåˆ†æä¸å¯ç”¨",
                "keywords_matched": [kw for kw in ["ä»Šå¤©", "ä½™é¢", "äº§å“", "ç”¨æˆ·", "è¶‹åŠ¿", "é¢„æµ‹"] if kw in query_lower]
            }
        )

    def _preprocess_query(self, query: str) -> str:
        """é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬"""
        # åŸºç¡€æ¸…ç†
        cleaned = query.strip()
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        cleaned = re.sub(r'\s+', ' ', cleaned)
        # æ ‡å‡†åŒ–å¸¸è§æœ¯è¯­
        replacements = {
            'è¤‡æŠ•': 'å¤æŠ•', 'ç¾é‡‘': 'ç°é‡‘', 'è³‡é‡‘': 'èµ„é‡‘',
            'é æ¸¬': 'é¢„æµ‹', 'é è¨ˆ': 'é¢„è®¡', 'é¤˜é¡': 'ä½™é¢'
        }
        for old, new in replacements.items():
            cleaned = cleaned.replace(old, new)
        return cleaned

    def _update_processing_stats(self, result: QueryAnalysisResult):
        """æ›´æ–°å¤„ç†ç»Ÿè®¡"""
        self.processing_stats['successful_parses'] += 1

        # æ›´æ–°å¤æ‚åº¦åˆ†å¸ƒ
        complexity_key = result.complexity.value
        self.processing_stats['complexity_distribution'][complexity_key] += 1

        # æ›´æ–°æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ
        query_type_key = result.query_type.value
        if query_type_key not in self.processing_stats['query_type_distribution']:
            self.processing_stats['query_type_distribution'][query_type_key] = 0
        self.processing_stats['query_type_distribution'][query_type_key] += 1

        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        total = self.processing_stats['total_queries']
        current_avg = self.processing_stats['average_confidence']
        self.processing_stats['average_confidence'] = (
                (current_avg * (total - 1) + result.confidence_score) / total
        )

    # ============= å¤–éƒ¨æ¥å£ =============

    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.processing_stats.copy()
        total = stats['total_queries']
        if total > 0:
            stats['success_rate'] = stats['successful_parses'] / total
            stats['fallback_rate'] = stats['fallback_parses'] / total
            stats['claude_failure_rate'] = stats['claude_failures'] / total
        else:
            stats['success_rate'] = 0.0
            stats['fallback_rate'] = 0.0
            stats['claude_failure_rate'] = 0.0
        return stats

    async def validate_query(self, query: str) -> Dict[str, Any]:
        """éªŒè¯æŸ¥è¯¢æœ‰æ•ˆæ€§"""
        if not query or len(query.strip()) == 0:
            return {"valid": False, "error": "æŸ¥è¯¢ä¸ºç©º"}

        if len(query) > 1000:
            return {"valid": False, "error": "æŸ¥è¯¢è¿‡é•¿ï¼ˆè¶…è¿‡1000å­—ç¬¦ï¼‰"}

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹
        if len(query.strip()) < 2:
            return {"valid": False, "error": "æŸ¥è¯¢å†…å®¹è¿‡çŸ­"}

        return {"valid": True}

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        status = "healthy"
        issues = []

        if not self.claude_client:
            status = "degraded"
            issues.append("Claudeå®¢æˆ·ç«¯æœªé…ç½®")

        # æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯æ˜¯å¦å¥åº·
        stats = self.get_processing_stats()
        if stats['total_queries'] > 10:  # æœ‰è¶³å¤Ÿæ ·æœ¬æ—¶æ£€æŸ¥
            if stats['claude_failure_rate'] > 0.5:  # Claudeå¤±è´¥ç‡è¶…è¿‡50%
                status = "degraded"
                issues.append("Claudeå¤±è´¥ç‡è¿‡é«˜")

            if stats['fallback_rate'] > 0.8:  # é™çº§ç‡è¶…è¿‡80%
                status = "degraded"
                issues.append("é™çº§è§£æç‡è¿‡é«˜")

        return {
            "status": status,
            "claude_available": self.claude_client is not None,
            "date_utils_available": self.date_utils is not None,
            "processing_stats": stats,
            "issues": issues,
            "timestamp": datetime.now().isoformat()
        }


# ============= å·¥å‚å‡½æ•° =============

def create_smart_query_parser(claude_client: Optional[ClaudeClient] = None,
                              gpt_client=None) -> SmartQueryParser:
    """
    åˆ›å»ºæ™ºèƒ½æŸ¥è¯¢è§£æå™¨å®ä¾‹

    Args:
        claude_client: Claudeå®¢æˆ·ç«¯å®ä¾‹
        gpt_client: ä¿ç•™å…¼å®¹æ€§ï¼Œä½†ä¸å†ä½¿ç”¨

    Returns:
        SmartQueryParser: é‡æ„åçš„æŸ¥è¯¢è§£æå™¨å®ä¾‹
    """
    return SmartQueryParser(claude_client, gpt_client)

