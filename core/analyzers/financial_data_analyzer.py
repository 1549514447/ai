# core/analyzers/financial_data_analyzer.py
"""
ğŸ¯ AIé©±åŠ¨çš„é‡‘èæ•°æ®æ·±åº¦åˆ†æå™¨
é‡‘èAIåˆ†æç³»ç»Ÿçš„æ ¸å¿ƒåˆ†æå¼•æ“ï¼Œè´Ÿè´£å¯¹è·å–çš„æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æ

æ ¸å¿ƒç‰¹ç‚¹:
- åŒAIåä½œçš„æ·±åº¦æ•°æ®åˆ†æ (Claude + GPT-4o)
- å¤šç»´åº¦é‡‘èæŒ‡æ ‡è®¡ç®—å’Œåˆ†æ
- æ™ºèƒ½è¶‹åŠ¿è¯†åˆ«å’Œæ¨¡å¼å‘ç°
- å¼‚å¸¸æ£€æµ‹å’Œé£é™©é¢„è­¦
- ä¸šåŠ¡æ´å¯Ÿå’Œå†³ç­–æ”¯æŒ
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import asyncio
from dataclasses import dataclass
from enum import Enum
import json
import statistics
from decimal import Decimal

from utils.calculators.statistical_calculator import create_financial_calculator
# å¯¼å…¥å·²å®Œæˆçš„å·¥å…·ç±»
from utils.helpers.date_utils import DateUtils, create_date_utils
from utils.helpers.validation_utils import ValidationUtils, create_validation_utils, ValidationLevel
from utils.data_transformers.time_series_builder import TimeSeriesBuilder, create_time_series_builder, DataQuality

logger = logging.getLogger(__name__)


class AnalysisType(Enum):
    """åˆ†æç±»å‹"""
    TREND_ANALYSIS = "trend_analysis"  # è¶‹åŠ¿åˆ†æ
    PERFORMANCE_ANALYSIS = "performance"  # ä¸šç»©åˆ†æ
    RISK_ASSESSMENT = "risk_assessment"  # é£é™©è¯„ä¼°
    ANOMALY_DETECTION = "anomaly_detection"  # å¼‚å¸¸æ£€æµ‹
    CORRELATION_ANALYSIS = "correlation"  # ç›¸å…³æ€§åˆ†æ
    SEASONAL_ANALYSIS = "seasonal"  # å­£èŠ‚æ€§åˆ†æ
    COMPARATIVE_ANALYSIS = "comparative"  # å¯¹æ¯”åˆ†æ
    PREDICTIVE_ANALYSIS = "predictive"  # é¢„æµ‹æ€§åˆ†æ


class AnalysisScope(Enum):
    """åˆ†æèŒƒå›´"""
    SYSTEM_LEVEL = "system"  # ç³»ç»Ÿçº§åˆ†æ
    USER_LEVEL = "user"  # ç”¨æˆ·çº§åˆ†æ
    PRODUCT_LEVEL = "product"  # äº§å“çº§åˆ†æ
    FINANCIAL_LEVEL = "financial"  # è´¢åŠ¡çº§åˆ†æ
    OPERATIONAL_LEVEL = "operational"  # è¿è¥çº§åˆ†æ


class ConfidenceLevel(Enum):
    """ç½®ä¿¡åº¦ç­‰çº§"""
    VERY_HIGH = "very_high"  # éå¸¸é«˜ (>0.9)
    HIGH = "high"  # é«˜ (0.8-0.9)
    MEDIUM = "medium"  # ä¸­ç­‰ (0.6-0.8)
    LOW = "low"  # ä½ (0.4-0.6)
    VERY_LOW = "very_low"  # å¾ˆä½ (<0.4)


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœæ•°æ®ç±»"""
    analysis_id: str  # åˆ†æID
    analysis_type: AnalysisType  # åˆ†æç±»å‹
    analysis_scope: AnalysisScope  # åˆ†æèŒƒå›´
    confidence_score: float  # ç½®ä¿¡åº¦è¯„åˆ† (0-1)

    # æ ¸å¿ƒç»“æœ
    key_findings: List[str]  # å…³é”®å‘ç°
    trends: List[Dict[str, Any]]  # è¶‹åŠ¿ä¿¡æ¯
    anomalies: List[Dict[str, Any]]  # å¼‚å¸¸ä¿¡æ¯
    metrics: Dict[str, float]  # å…³é”®æŒ‡æ ‡

    # ä¸šåŠ¡æ´å¯Ÿ
    business_insights: List[str]  # ä¸šåŠ¡æ´å¯Ÿ
    risk_factors: List[str]  # é£é™©å› ç´ 
    opportunities: List[str]  # æœºä¼šç‚¹
    recommendations: List[str]  # å»ºè®®

    # æŠ€æœ¯ä¿¡æ¯
    data_quality: DataQuality  # æ•°æ®è´¨é‡
    analysis_metadata: Dict[str, Any]  # åˆ†æå…ƒæ•°æ®
    processing_time: float  # å¤„ç†æ—¶é—´
    timestamp: str  # åˆ†ææ—¶é—´æˆ³


@dataclass
class TrendAnalysis:
    """è¶‹åŠ¿åˆ†æç»“æœ"""
    metric_name: str  # æŒ‡æ ‡åç§°
    trend_direction: str  # è¶‹åŠ¿æ–¹å‘ (increasing/decreasing/stable)
    trend_strength: float  # è¶‹åŠ¿å¼ºåº¦ (0-1)
    growth_rate: float  # å¢é•¿ç‡
    volatility: float  # æ³¢åŠ¨æ€§
    trend_confidence: float  # è¶‹åŠ¿ç½®ä¿¡åº¦
    inflection_points: List[str]  # æ‹ç‚¹æ—¥æœŸ
    seasonal_patterns: Dict[str, Any]  # å­£èŠ‚æ€§æ¨¡å¼


@dataclass
class AnomalyDetection:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    date: str  # å¼‚å¸¸æ—¥æœŸ
    metric: str  # å¼‚å¸¸æŒ‡æ ‡
    actual_value: float  # å®é™…å€¼
    expected_value: float  # æœŸæœ›å€¼
    deviation_score: float  # åç¦»ç¨‹åº¦
    anomaly_type: str  # å¼‚å¸¸ç±»å‹
    severity: str  # ä¸¥é‡ç¨‹åº¦
    possible_causes: List[str]  # å¯èƒ½åŸå› 
    impact_assessment: str  # å½±å“è¯„ä¼°


class FinancialDataAnalyzer:
    """
    ğŸ¯ AIé©±åŠ¨çš„é‡‘èæ•°æ®æ·±åº¦åˆ†æå™¨

    åŠŸèƒ½æ¶æ„:
    1. å¤šç»´åº¦æ•°æ®åˆ†æèƒ½åŠ›
    2. åŒAIåä½œçš„æ·±åº¦æ´å¯Ÿ
    3. æ™ºèƒ½æ¨¡å¼è¯†åˆ«
    4. é£é™©é¢„è­¦å’Œæœºä¼šå‘ç°
    """

    def __init__(self, claude_client=None, gpt_client=None):
        """
        åˆå§‹åŒ–é‡‘èæ•°æ®åˆ†æå™¨

        Args:
            claude_client: Claudeå®¢æˆ·ç«¯ï¼Œè´Ÿè´£ä¸šåŠ¡æ´å¯Ÿåˆ†æ
            gpt_client: GPTå®¢æˆ·ç«¯ï¼Œè´Ÿè´£æ•°å€¼è®¡ç®—å’Œç»Ÿè®¡åˆ†æ
        """
        self.claude_client = claude_client
        self.gpt_client = gpt_client

        # åˆå§‹åŒ–å·¥å…·ç»„ä»¶
        self.date_utils = create_date_utils(claude_client)
        self.validator = create_validation_utils(claude_client, gpt_client)
        self.time_series_builder = create_time_series_builder(claude_client, gpt_client)
        self.financial_calculator = create_financial_calculator(gpt_client, 4)  # ä¿®æ­£å‚æ•°é¡ºåºï¼Œæ˜ç¡®æŒ‡å®šprecisionä¸º4

        # åˆ†æé…ç½®
        self.analysis_config = self._load_analysis_config()

        # åˆ†æç»Ÿè®¡
        self.analysis_stats = {
            'total_analyses': 0,
            'analysis_by_type': {},
            'avg_confidence_score': 0.0,
            'anomalies_detected': 0,
            'insights_generated': 0
        }

        logger.info("FinancialDataAnalyzer initialized with dual-AI capabilities")

    def _load_analysis_config(self) -> Dict[str, Any]:
        """åŠ è½½åˆ†æé…ç½®"""
        return {
            # è¶‹åŠ¿åˆ†æé…ç½®
            'trend_analysis': {
                'min_data_points': 7,  # æœ€å°‘æ•°æ®ç‚¹
                'volatility_threshold': 0.15,  # æ³¢åŠ¨æ€§é˜ˆå€¼
                'trend_strength_threshold': 0.7,  # è¶‹åŠ¿å¼ºåº¦é˜ˆå€¼
                'growth_rate_bounds': (-0.5, 2.0)  # å¢é•¿ç‡åˆç†èŒƒå›´
            },

            # å¼‚å¸¸æ£€æµ‹é…ç½®
            'anomaly_detection': {
                'sensitivity': 2.0,  # æ•æ„Ÿåº¦ (æ ‡å‡†å·®å€æ•°)
                'min_history_days': 14,  # æœ€å°‘å†å²å¤©æ•°
                'outlier_threshold': 0.05,  # å¼‚å¸¸å€¼é˜ˆå€¼
                'severe_anomaly_threshold': 5.0  # ä¸¥é‡å¼‚å¸¸é˜ˆå€¼
            },

            # é£é™©è¯„ä¼°é…ç½®
            'risk_assessment': {
                'high_risk_threshold': 0.8,  # é«˜é£é™©é˜ˆå€¼
                'volatility_risk_multiplier': 2.0,  # æ³¢åŠ¨æ€§é£é™©ä¹˜æ•°
                'liquidity_risk_threshold': 0.1  # æµåŠ¨æ€§é£é™©é˜ˆå€¼
            },

            # AIåˆ†æé…ç½®
            'ai_analysis': {
                'use_claude_for_insights': True,  # ä½¿ç”¨Claudeç”Ÿæˆæ´å¯Ÿ
                'use_gpt_for_calculations': True,  # ä½¿ç”¨GPTè¿›è¡Œè®¡ç®—
                'confidence_threshold': 0.6,  # æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
                'max_ai_retries': 3  # AIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
            }
        }

    # ============= æ ¸å¿ƒåˆ†ææ–¹æ³• =============

    async def analyze_trend(self, data_source: str, metric: str, time_range: int) -> AnalysisResult:
        """
        ğŸ¯ è¶‹åŠ¿åˆ†æ - åˆ†ææŒ‡å®šæŒ‡æ ‡çš„è¶‹åŠ¿å˜åŒ–

        Args:
            data_source: æ•°æ®æºç±»å‹ (system/daily/productç­‰)
            metric: åˆ†ææŒ‡æ ‡ (total_balance/daily_inflowç­‰)
            time_range: æ—¶é—´èŒƒå›´ (å¤©æ•°)

        Returns:
            AnalysisResult: å®Œæ•´çš„è¶‹åŠ¿åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹è¶‹åŠ¿åˆ†æ: {data_source}.{metric}, æ—¶é—´èŒƒå›´: {time_range}å¤©")

            analysis_start_time = datetime.now()
            self.analysis_stats['total_analyses'] += 1

            # Step 1: æ•°æ®å‡†å¤‡å’ŒéªŒè¯
            analysis_data = await self._prepare_trend_analysis_data(data_source, metric, time_range)

            if not analysis_data['is_valid']:
                return self._create_error_analysis_result("trend_analysis", analysis_data['error'])

            # Step 2: æ„å»ºæ—¶é—´åºåˆ—
            time_series_result = await self._build_trend_time_series(analysis_data['raw_data'], metric)

            # Step 3: è®¡ç®—è¶‹åŠ¿ç»Ÿè®¡æŒ‡æ ‡
            trend_statistics = await self._calculate_trend_statistics(time_series_result, metric)

            # Step 4: AIè¶‹åŠ¿æ¨¡å¼è¯†åˆ«
            trend_patterns = await self._ai_identify_trend_patterns(time_series_result, trend_statistics)

            # Step 5: å¼‚å¸¸æ£€æµ‹
            anomalies = await self._detect_trend_anomalies(time_series_result, trend_statistics)

            # Step 6: ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ
            business_insights = await self._generate_trend_insights(
                trend_statistics, trend_patterns, anomalies, data_source, metric
            )

            # Step 7: è®¡ç®—ç½®ä¿¡åº¦
            confidence_score = self._calculate_analysis_confidence(
                analysis_data, trend_statistics, anomalies
            )

            # Step 8: æ„å»ºåˆ†æç»“æœ
            processing_time = (datetime.now() - analysis_start_time).total_seconds()

            analysis_result = AnalysisResult(
                analysis_id=f"trend_{data_source}_{metric}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                analysis_type=AnalysisType.TREND_ANALYSIS,
                analysis_scope=self._determine_analysis_scope(data_source),
                confidence_score=confidence_score,

                key_findings=trend_patterns.get('key_findings', []),
                trends=[{
                    'metric': metric,
                    'direction': trend_statistics['trend_direction'],
                    'strength': trend_statistics['trend_strength'],
                    'growth_rate': trend_statistics['growth_rate'],
                    'volatility': trend_statistics['volatility']
                }],
                anomalies=anomalies,
                metrics=trend_statistics,

                business_insights=business_insights.get('insights', []),
                risk_factors=business_insights.get('risks', []),
                opportunities=business_insights.get('opportunities', []),
                recommendations=business_insights.get('recommendations', []),

                data_quality=analysis_data['data_quality'],
                analysis_metadata={
                    'data_source': data_source,
                    'metric': metric,
                    'time_range_days': time_range,
                    'data_points': len(time_series_result.get('series_data', [])),
                    'ai_models_used': ['claude', 'gpt'] if self.claude_client and self.gpt_client else []
                },
                processing_time=processing_time,
                timestamp=datetime.now().isoformat()
            )

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_analysis_stats('trend_analysis', confidence_score)

            logger.info(f"âœ… è¶‹åŠ¿åˆ†æå®Œæˆ: ç½®ä¿¡åº¦={confidence_score:.2f}, è€—æ—¶={processing_time:.2f}ç§’")
            return analysis_result

        except Exception as e:
            logger.error(f"âŒ è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
            return self._create_error_analysis_result("trend_analysis", str(e))

    async def analyze_business_performance(self, scope: str, time_range: int) -> AnalysisResult:
        """
        ğŸ“Š ä¸šåŠ¡è¡¨ç°åˆ†æ - ç»¼åˆåˆ†æä¸šåŠ¡å„æ–¹é¢è¡¨ç°

        Args:
            scope: åˆ†æèŒƒå›´ (financial/operational/user/product)
            time_range: æ—¶é—´èŒƒå›´ (å¤©æ•°)

        Returns:
            AnalysisResult: ä¸šåŠ¡è¡¨ç°åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ“Š å¼€å§‹ä¸šåŠ¡è¡¨ç°åˆ†æ: {scope}, æ—¶é—´èŒƒå›´: {time_range}å¤©")

            analysis_start_time = datetime.now()

            # Step 1: æ ¹æ®scopeç¡®å®šåˆ†ææŒ‡æ ‡
            performance_metrics = self._get_performance_metrics(scope)

            # Step 2: è·å–å’Œå‡†å¤‡æ•°æ®
            performance_data = await self._prepare_performance_analysis_data(scope, performance_metrics, time_range)

            # Step 3: è®¡ç®—å…³é”®ç»©æ•ˆæŒ‡æ ‡
            kpi_results = await self._calculate_business_kpis(performance_data, scope)

            # Step 4: å¯¹æ¯”åˆ†æ (ä¸å†å²åŒæœŸå¯¹æ¯”)
            comparative_analysis = await self._perform_comparative_analysis(performance_data, scope)

            # Step 5: AIä¸šåŠ¡æ´å¯Ÿç”Ÿæˆ
            business_analysis = await self._ai_analyze_business_performance(
                kpi_results, comparative_analysis, scope
            )

            # Step 6: é£é™©å’Œæœºä¼šè¯†åˆ«
            risk_opportunity_analysis = await self._assess_performance_risks_opportunities(
                kpi_results, comparative_analysis
            )

            processing_time = (datetime.now() - analysis_start_time).total_seconds()

            analysis_result = AnalysisResult(
                analysis_id=f"performance_{scope}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                analysis_type=AnalysisType.PERFORMANCE_ANALYSIS,
                analysis_scope=AnalysisScope(scope),
                confidence_score=business_analysis.get('confidence', 0.8),

                key_findings=business_analysis.get('key_findings', []),
                trends=comparative_analysis.get('trends', []),
                anomalies=kpi_results.get('anomalies', []),
                metrics=kpi_results.get('kpis', {}),

                business_insights=business_analysis.get('insights', []),
                risk_factors=risk_opportunity_analysis.get('risks', []),
                opportunities=risk_opportunity_analysis.get('opportunities', []),
                recommendations=business_analysis.get('recommendations', []),

                data_quality=performance_data.get('data_quality', DataQuality.GOOD),
                analysis_metadata={
                    'scope': scope,
                    'metrics_analyzed': len(performance_metrics),
                    'time_range_days': time_range,
                    'comparative_analysis': True
                },
                processing_time=processing_time,
                timestamp=datetime.now().isoformat()
            )

            self._update_analysis_stats('performance_analysis', business_analysis.get('confidence', 0.8))

            logger.info(f"âœ… ä¸šåŠ¡è¡¨ç°åˆ†æå®Œæˆ: {scope}")
            return analysis_result

        except Exception as e:
            logger.error(f"âŒ ä¸šåŠ¡è¡¨ç°åˆ†æå¤±è´¥: {str(e)}")
            return self._create_error_analysis_result("performance_analysis", str(e))

    # async def detect_anomalies(self, data_source: str, metrics: List[str],
    #                            sensitivity: float = 2.0) -> AnalysisResult:
    #     """
    #     ğŸš¨ å¼‚å¸¸æ£€æµ‹ - æ£€æµ‹æ•°æ®ä¸­çš„å¼‚å¸¸æ¨¡å¼
    #
    #     Args:
    #         data_source: æ•°æ®æºç±»å‹
    #         metrics: æ£€æµ‹çš„æŒ‡æ ‡åˆ—è¡¨
    #         sensitivity: æ•æ„Ÿåº¦ (æ ‡å‡†å·®å€æ•°)
    #
    #     Returns:
    #         AnalysisResult: å¼‚å¸¸æ£€æµ‹ç»“æœ
    #     """
    #     try:
    #         logger.info(f"ğŸš¨ å¼€å§‹å¼‚å¸¸æ£€æµ‹: {data_source}, æŒ‡æ ‡: {metrics}")
    #
    #         analysis_start_time = datetime.now()
    #
    #         # Step 1: å‡†å¤‡å¼‚å¸¸æ£€æµ‹æ•°æ®
    #         anomaly_data = await self._prepare_anomaly_detection_data(data_source, metrics)
    #
    #         # Step 2: ç»Ÿè®¡å­¦å¼‚å¸¸æ£€æµ‹
    #         statistical_anomalies = await self._statistical_anomaly_detection(
    #             anomaly_data, metrics, sensitivity
    #         )
    #
    #         # Step 3: AIæ¨¡å¼å¼‚å¸¸æ£€æµ‹
    #         pattern_anomalies = await self._ai_pattern_anomaly_detection(
    #             anomaly_data, statistical_anomalies
    #         )
    #
    #         # Step 4: ä¸šåŠ¡é€»è¾‘å¼‚å¸¸æ£€æµ‹
    #         business_anomalies = await self._business_logic_anomaly_detection(
    #             anomaly_data, metrics
    #         )
    #
    #         # Step 5: å¼‚å¸¸ç»¼åˆè¯„ä¼°å’Œåˆ†ç±»
    #         consolidated_anomalies = self._consolidate_anomalies(
    #             statistical_anomalies, pattern_anomalies, business_anomalies
    #         )
    #
    #         # Step 6: å¼‚å¸¸å½±å“è¯„ä¼°
    #         impact_assessment = await self._assess_anomaly_impact(consolidated_anomalies, data_source)
    #
    #         # Step 7: AIå¼‚å¸¸è§£é‡Šå’Œå»ºè®®
    #         anomaly_insights = await self._ai_explain_anomalies(consolidated_anomalies, impact_assessment)
    #
    #         processing_time = (datetime.now() - analysis_start_time).total_seconds()
    #
    #         analysis_result = AnalysisResult(
    #             analysis_id=f"anomaly_{data_source}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    #             analysis_type=AnalysisType.ANOMALY_DETECTION,
    #             analysis_scope=self._determine_analysis_scope(data_source),
    #             confidence_score=anomaly_insights.get('confidence', 0.8),
    #
    #             key_findings=anomaly_insights.get('key_findings', []),
    #             trends=[],
    #             anomalies=consolidated_anomalies,
    #             metrics={'total_anomalies': len(consolidated_anomalies), 'sensitivity': sensitivity},
    #
    #             business_insights=anomaly_insights.get('insights', []),
    #             risk_factors=impact_assessment.get('risks', []),
    #             opportunities=impact_assessment.get('opportunities', []),
    #             recommendations=anomaly_insights.get('recommendations', []),
    #
    #             data_quality=anomaly_data.get('data_quality', DataQuality.GOOD),
    #             analysis_metadata={
    #                 'data_source': data_source,
    #                 'metrics_checked': metrics,
    #                 'sensitivity_level': sensitivity,
    #                 'detection_methods': ['statistical', 'pattern', 'business_logic']
    #             },
    #             processing_time=processing_time,
    #             timestamp=datetime.now().isoformat()
    #         )
    #
    #         # æ›´æ–°å¼‚å¸¸ç»Ÿè®¡
    #         self.analysis_stats['anomalies_detected'] += len(consolidated_anomalies)
    #         self._update_analysis_stats('anomaly_detection', anomaly_insights.get('confidence', 0.8))
    #
    #         logger.info(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç°{len(consolidated_anomalies)}ä¸ªå¼‚å¸¸")
    #         return analysis_result
    #
    #     except Exception as e:
    #         logger.error(f"âŒ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
    #         return self._create_error_analysis_result("anomaly_detection", str(e))

    # ============= æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç† =============

    async def _prepare_trend_analysis_data(self, data_source: str, metric: str,
                                           time_range: int) -> Dict[str, Any]:
        """å‡†å¤‡è¶‹åŠ¿åˆ†ææ•°æ®"""
        try:
            # è¿™é‡Œéœ€è¦è°ƒç”¨smart_data_fetcherè·å–æ•°æ®
            # ç”±äºè¿™æ˜¯åˆ†æå™¨ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®å·²ç»é€šè¿‡fetcherè·å–
            # å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•ä¼šæ¥æ”¶fetcherä¼ é€’çš„æ•°æ®

            # æ¨¡æ‹Ÿæ•°æ®éªŒè¯é€»è¾‘
            if self.validator:
                validation_result = await self.validator.validate_data(
                    {'data_source': data_source, 'metric': metric, 'time_range': time_range},
                    'trend_analysis_data'
                )

                if not validation_result.is_valid:
                    return {
                        'is_valid': False,
                        'error': f"æ•°æ®éªŒè¯å¤±è´¥: {validation_result.issues}",
                        'data_quality': DataQuality.POOR
                    }

            return {
                'is_valid': True,
                'raw_data': {},  # å®é™…æ•°æ®å°†åœ¨è¿™é‡Œ
                'data_quality': DataQuality.GOOD,
                'validation_passed': True
            }

        except Exception as e:
            logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
            return {
                'is_valid': False,
                'error': str(e),
                'data_quality': DataQuality.INSUFFICIENT
            }

    async def _build_trend_time_series(self, raw_data: Dict[str, Any], metric: str) -> Dict[str, Any]:
        """æ„å»ºè¶‹åŠ¿åˆ†ææ—¶é—´åºåˆ—"""
        try:
            if not self.time_series_builder:
                # åŸºç¡€æ—¶é—´åºåˆ—æ„å»ºé€»è¾‘
                return {'series_data': [], 'metadata': {'method': 'basic'}}

            # ä½¿ç”¨time_series_builderæ„å»ºæ—¶é—´åºåˆ—
            # è¿™é‡Œå‡è®¾raw_dataåŒ…å«äº†æ—¥æœŸå’Œæ•°å€¼ä¿¡æ¯
            mock_daily_data = []  # å®é™…ä½¿ç”¨æ—¶ä¼šæœ‰çœŸå®æ•°æ®

            time_series_result = await self.time_series_builder.build_daily_time_series(
                mock_daily_data, metric, 'date'
            )

            return time_series_result

        except Exception as e:
            logger.error(f"æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {str(e)}")
            return {'series_data': [], 'metadata': {'error': str(e)}}

    # ============= ç»Ÿè®¡è®¡ç®—å’ŒAIåˆ†æ =============

    async def _calculate_trend_statistics(self, time_series_result: Dict[str, Any],
                                          metric: str) -> Dict[str, Any]:
        """è®¡ç®—è¶‹åŠ¿ç»Ÿè®¡æŒ‡æ ‡"""
        try:
            series_data = time_series_result.get('series_data', [])

            if len(series_data) < 2:
                return {
                    'trend_direction': 'insufficient_data',
                    'trend_strength': 0.0,
                    'growth_rate': 0.0,
                    'volatility': 0.0,
                    'data_points': len(series_data)
                }

            # æå–æ•°å€¼åºåˆ—
            values = [point.value for point in series_data if hasattr(point, 'value')]

            if not values:
                values = [float(point.get('value', 0)) for point in series_data if isinstance(point, dict)]

            if len(values) < 2:
                return {'trend_direction': 'no_valid_data', 'data_points': len(series_data)}

            # è®¡ç®—åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
            if self.financial_calculator:
                # ä½¿ç”¨financial_calculatorè®¡ç®—å¢é•¿ç‡
                growth_rate = await self.financial_calculator.calculate_growth_rate(values, 'simple')
                growth_rate_value = growth_rate.result_value if hasattr(growth_rate, 'result_value') else growth_rate
            else:
                # åŸºç¡€å¢é•¿ç‡è®¡ç®—
                growth_rate_value = (values[-1] - values[0]) / abs(values[0]) if values[0] != 0 else 0.0

            # è®¡ç®—è¶‹åŠ¿æ–¹å‘å’Œå¼ºåº¦
            trend_direction = self._determine_trend_direction(values)
            trend_strength = self._calculate_trend_strength(values)
            volatility = self._calculate_volatility(values)

            # ä½¿ç”¨GPTè¿›è¡Œæ›´ç²¾ç¡®çš„ç»Ÿè®¡åˆ†æ
            if self.gpt_client:
                enhanced_stats = await self._gpt_enhanced_statistics(values, metric)
                if enhanced_stats:
                    return {**enhanced_stats, 'ai_enhanced': True}

            return {
                'trend_direction': trend_direction,
                'trend_strength': trend_strength,
                'growth_rate': growth_rate_value,
                'volatility': volatility,
                'data_points': len(values),
                'mean_value': statistics.mean(values),
                'median_value': statistics.median(values),
                'std_deviation': statistics.stdev(values) if len(values) > 1 else 0.0,
                'min_value': min(values),
                'max_value': max(values)
            }

        except Exception as e:
            logger.error(f"è¶‹åŠ¿ç»Ÿè®¡è®¡ç®—å¤±è´¥: {str(e)}")
            return {'error': str(e), 'trend_direction': 'calculation_error'}

    async def _ai_identify_trend_patterns(self, time_series_result: Dict[str, Any],
                                          trend_statistics: Dict[str, Any]) -> Dict[str, Any]:
        """AIè¯†åˆ«è¶‹åŠ¿æ¨¡å¼"""
        try:
            if not self.claude_client:
                return {
                    'key_findings': ['åŸºç¡€è¶‹åŠ¿åˆ†æå®Œæˆ'],
                    'patterns': [],
                    'method': 'basic'
                }

            # ä½¿ç”¨Claudeè¿›è¡Œæ¨¡å¼è¯†åˆ«
            pattern_analysis_prompt = f"""
            ä½œä¸ºé‡‘èæ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹è¶‹åŠ¿æ•°æ®çš„æ¨¡å¼ï¼š

            è¶‹åŠ¿ç»Ÿè®¡ä¿¡æ¯ï¼š
            {json.dumps(trend_statistics, ensure_ascii=False, indent=2)}

            æ—¶é—´åºåˆ—å…ƒæ•°æ®ï¼š
            {json.dumps(time_series_result.get('metadata', {}), ensure_ascii=False, indent=2)}

            è¯·è¯†åˆ«ï¼š
            1. ä¸»è¦è¶‹åŠ¿æ¨¡å¼ (çº¿æ€§ã€æŒ‡æ•°ã€å‘¨æœŸæ€§ç­‰)
            2. å…³é”®ç‰¹å¾å’Œè½¬æŠ˜ç‚¹
            3. æ½œåœ¨çš„å­£èŠ‚æ€§æˆ–å‘¨æœŸæ€§
            4. å¼‚å¸¸æ¨¡å¼æˆ–çªå‘å˜åŒ–
            5. è¶‹åŠ¿çš„å¯æŒç»­æ€§è¯„ä¼°

            è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…æ‹¬ï¼š
            {{
                "key_findings": ["å…³é”®å‘ç°1", "å…³é”®å‘ç°2"],
                "pattern_type": "pattern_description",
                "seasonality": "seasonal_analysis",
                "inflection_points": ["date1", "date2"],
                "sustainability_assessment": "assessment_text",
                "confidence": 0.0-1.0
            }}
            """

            claude_result = await self.claude_client.analyze_complex_query(
                pattern_analysis_prompt,
                {
                    "trend_stats": trend_statistics,
                    "time_series_metadata": time_series_result.get('metadata', {})
                }
            )

            if claude_result.get('success'):
                analysis = claude_result.get('analysis', {})
                return {
                    **analysis,
                    'ai_analysis': True,
                    'model_used': 'claude'
                }

        except Exception as e:
            logger.error(f"AIæ¨¡å¼è¯†åˆ«å¤±è´¥: {str(e)}")

        # é™çº§åˆ°åŸºç¡€åˆ†æ
        return {
            'key_findings': [
                f"è¶‹åŠ¿æ–¹å‘: {trend_statistics.get('trend_direction', 'unknown')}",
                f"å¢é•¿ç‡: {trend_statistics.get('growth_rate', 0):.2%}",
                f"æ³¢åŠ¨æ€§: {trend_statistics.get('volatility', 0):.2f}"
            ],
            'pattern_type': 'basic_trend',
            'confidence': 0.6
        }

    async def _detect_trend_anomalies(self, time_series_result: Dict[str, Any],
                                      trend_statistics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹è¶‹åŠ¿ä¸­çš„å¼‚å¸¸"""
        try:
            series_data = time_series_result.get('series_data', [])

            if len(series_data) < self.analysis_config['anomaly_detection']['min_history_days']:
                return []

            anomalies = []
            sensitivity = self.analysis_config['anomaly_detection']['sensitivity']

            # æå–æ•°å€¼å’Œæ—¥æœŸ
            values = []
            dates = []

            for point in series_data:
                if hasattr(point, 'value') and hasattr(point, 'date'):
                    values.append(point.value)
                    dates.append(point.date)
                elif isinstance(point, dict):
                    values.append(float(point.get('value', 0)))
                    dates.append(point.get('date', ''))

            if len(values) < 3:
                return anomalies

            # è®¡ç®—ç»Ÿè®¡é˜ˆå€¼
            mean_val = statistics.mean(values)
            std_val = statistics.stdev(values) if len(values) > 1 else 0
            upper_threshold = mean_val + (sensitivity * std_val)
            lower_threshold = mean_val - (sensitivity * std_val)

            # æ£€æµ‹ç»Ÿè®¡å¼‚å¸¸
            for i, (value, date) in enumerate(zip(values, dates)):
                if value > upper_threshold or value < lower_threshold:
                    deviation_score = abs(value - mean_val) / std_val if std_val > 0 else 0

                    anomaly = {
                        'date': date,
                        'metric': 'trend_value',
                        'actual_value': value,
                        'expected_value': mean_val,
                        'deviation_score': deviation_score,
                        'anomaly_type': 'statistical_outlier',
                        'severity': 'high' if deviation_score > 3 else 'medium',
                        'detection_method': 'statistical'
                    }
                    anomalies.append(anomaly)

            # æ£€æµ‹è¶‹åŠ¿çªå˜å¼‚å¸¸
            if len(values) >= 5:
                for i in range(2, len(values) - 2):
                    # è®¡ç®—å±€éƒ¨è¶‹åŠ¿å˜åŒ–
                    before_trend = (values[i] - values[i - 2]) / 2 if values[i - 2] != 0 else 0
                    after_trend = (values[i + 2] - values[i]) / 2 if values[i] != 0 else 0

                    # æ£€æµ‹è¶‹åŠ¿åè½¬
                    if abs(before_trend - after_trend) > std_val:
                        trend_change_anomaly = {
                            'date': dates[i],
                            'metric': 'trend_change',
                            'actual_value': values[i],
                            'expected_value': values[i - 1],
                            'deviation_score': abs(before_trend - after_trend),
                            'anomaly_type': 'trend_reversal',
                            'severity': 'medium',
                            'detection_method': 'trend_analysis',
                            'before_trend': before_trend,
                            'after_trend': after_trend
                        }
                        anomalies.append(trend_change_anomaly)

            # AIå¢å¼ºå¼‚å¸¸æ£€æµ‹
            if self.claude_client and len(anomalies) > 0:
                ai_enhanced_anomalies = await self._ai_enhance_anomaly_detection(
                    anomalies, values, dates, trend_statistics
                )
                anomalies = ai_enhanced_anomalies

            return anomalies

        except Exception as e:
            logger.error(f"è¶‹åŠ¿å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return []

    async def _ai_enhance_anomaly_detection(self, anomalies: List[Dict[str, Any]],
                                            values: List[float], dates: List[str],
                                            trend_statistics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """AIå¢å¼ºå¼‚å¸¸æ£€æµ‹"""
        try:
            if not self.claude_client or not anomalies:
                return anomalies

            enhancement_prompt = f"""
            ä½œä¸ºé‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æ£€æµ‹åˆ°çš„æ•°æ®å¼‚å¸¸ï¼š

            æ£€æµ‹åˆ°çš„å¼‚å¸¸ï¼š
            {json.dumps(anomalies, ensure_ascii=False, indent=2)}

            è¶‹åŠ¿ç»Ÿè®¡èƒŒæ™¯ï¼š
            {json.dumps(trend_statistics, ensure_ascii=False, indent=2)}

            è¯·ä¸ºæ¯ä¸ªå¼‚å¸¸æä¾›ï¼š
            1. å¯èƒ½çš„ä¸šåŠ¡åŸå› 
            2. é£é™©çº§åˆ«è¯„ä¼°
            3. å½±å“èŒƒå›´é¢„ä¼°
            4. å»ºè®®çš„å¤„ç†æªæ–½

            è¿”å›å¢å¼ºåçš„å¼‚å¸¸ä¿¡æ¯JSONæ•°ç»„ã€‚
            """

            claude_result = await self.claude_client.analyze_complex_query(
                enhancement_prompt,
                {
                    "anomalies": anomalies,
                    "trend_context": trend_statistics
                }
            )

            if claude_result.get('success'):
                enhanced_analysis = claude_result.get('analysis', {})

                # åˆå¹¶AIåˆ†æç»“æœåˆ°åŸå¼‚å¸¸æ•°æ®
                enhanced_anomalies = []
                for i, anomaly in enumerate(anomalies):
                    enhanced_anomaly = anomaly.copy()

                    if isinstance(enhanced_analysis, list) and i < len(enhanced_analysis):
                        ai_insight = enhanced_analysis[i]
                        enhanced_anomaly.update({
                            'possible_causes': ai_insight.get('possible_causes', []),
                            'impact_assessment': ai_insight.get('impact_assessment', 'unknown'),
                            'risk_level': ai_insight.get('risk_level', 'medium'),
                            'recommended_actions': ai_insight.get('recommended_actions', []),
                            'ai_enhanced': True
                        })

                    enhanced_anomalies.append(enhanced_anomaly)

                return enhanced_anomalies

        except Exception as e:
            logger.error(f"AIå¼‚å¸¸å¢å¼ºåˆ†æå¤±è´¥: {str(e)}")

        # AIå¤±è´¥æ—¶è¿”å›åŸå§‹å¼‚å¸¸
        return anomalies

    async def _generate_trend_insights(self, trend_statistics: Dict[str, Any],
                                       trend_patterns: Dict[str, Any],
                                       anomalies: List[Dict[str, Any]],
                                       data_source: str, metric: str) -> Dict[str, Any]:
        """ç”Ÿæˆè¶‹åŠ¿ä¸šåŠ¡æ´å¯Ÿ"""
        try:
            if not self.claude_client:
                # åŸºç¡€æ´å¯Ÿç”Ÿæˆ
                return {
                    'insights': [
                        f"{metric}è¶‹åŠ¿æ–¹å‘: {trend_statistics.get('trend_direction', 'unknown')}",
                        f"å¢é•¿ç‡: {trend_statistics.get('growth_rate', 0):.2%}",
                        f"æ£€æµ‹åˆ°{len(anomalies)}ä¸ªå¼‚å¸¸ç‚¹"
                    ],
                    'risks': ['æ•°æ®æ³¢åŠ¨æ€§è¾ƒé«˜'] if trend_statistics.get('volatility', 0) > 0.2 else [],
                    'opportunities': ['å¢é•¿è¶‹åŠ¿è‰¯å¥½'] if trend_statistics.get('growth_rate', 0) > 0 else [],
                    'recommendations': ['ç»§ç»­ç›‘æ§è¶‹åŠ¿å˜åŒ–']
                }

            insight_prompt = f"""
            ä½œä¸ºèµ„æ·±é‡‘èä¸šåŠ¡åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹æ•°æ®æä¾›æ·±åº¦ä¸šåŠ¡æ´å¯Ÿï¼š

            æ•°æ®æº: {data_source}
            åˆ†ææŒ‡æ ‡: {metric}

            è¶‹åŠ¿ç»Ÿè®¡ï¼š
            {json.dumps(trend_statistics, ensure_ascii=False, indent=2)}

            è¶‹åŠ¿æ¨¡å¼ï¼š
            {json.dumps(trend_patterns, ensure_ascii=False, indent=2)}

            å¼‚å¸¸æƒ…å†µï¼š
            æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸ç‚¹
            {json.dumps(anomalies[:3], ensure_ascii=False, indent=2) if anomalies else "æ— å¼‚å¸¸"}

            è¯·æä¾›ï¼š
            1. æ ¸å¿ƒä¸šåŠ¡æ´å¯Ÿ (3-5ä¸ªå…³é”®å‘ç°)
            2. æ½œåœ¨é£é™©å› ç´ 
            3. ä¸šåŠ¡æœºä¼šè¯†åˆ«
            4. å…·ä½“è¡ŒåŠ¨å»ºè®®

            è¿”å›JSONæ ¼å¼ï¼š
            {{
                "insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", ...],
                "risks": ["é£é™©1", "é£é™©2", ...],
                "opportunities": ["æœºä¼š1", "æœºä¼š2", ...],
                "recommendations": ["å»ºè®®1", "å»ºè®®2", ...],
                "confidence": 0.0-1.0
            }}
            """

            claude_result = await self.claude_client.analyze_complex_query(
                insight_prompt,
                {
                    "trend_statistics": trend_statistics,
                    "patterns": trend_patterns,
                    "anomalies": anomalies
                }
            )

            if claude_result.get('success'):
                insights = claude_result.get('analysis', {})
                return {
                    **insights,
                    'ai_generated': True,
                    'model_used': 'claude'
                }

        except Exception as e:
            logger.error(f"ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")

        # é™çº§ç”ŸæˆåŸºç¡€æ´å¯Ÿ
        return {
            'insights': [f"{metric}çš„è¶‹åŠ¿åˆ†æå·²å®Œæˆ"],
            'risks': [],
            'opportunities': [],
            'recommendations': ['å»ºè®®å®šæœŸç›‘æ§è¯¥æŒ‡æ ‡çš„å˜åŒ–'],
            'confidence': 0.5
        }

    # ============= ä¸šåŠ¡è¡¨ç°åˆ†æç›¸å…³æ–¹æ³• =============

    def _get_performance_metrics(self, scope: str) -> List[str]:
        """æ ¹æ®åˆ†æèŒƒå›´è·å–æ€§èƒ½æŒ‡æ ‡"""
        metrics_map = {
            'financial': [
                'total_balance', 'total_inflow', 'total_outflow', 'net_cash_flow',
                'liquidity_ratio', 'growth_rate', 'return_on_investment'
            ],
            'operational': [
                'daily_registrations', 'active_users', 'product_purchases',
                'product_maturities', 'user_activity_rate', 'conversion_rate'
            ],
            'user': [
                'new_users', 'active_users', 'user_retention', 'vip_distribution',
                'average_investment_per_user', 'user_lifetime_value'
            ],
            'product': [
                'product_sales', 'product_performance', 'maturity_distribution',
                'product_roi', 'popular_products', 'product_risk_assessment'
            ]
        }

        return metrics_map.get(scope, ['total_balance', 'total_inflow', 'total_outflow'])

    async def _prepare_performance_analysis_data(self, scope: str, metrics: List[str],
                                                 time_range: int) -> Dict[str, Any]:
        """å‡†å¤‡ä¸šåŠ¡è¡¨ç°åˆ†ææ•°æ®"""
        try:
            # æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡è¿‡ç¨‹
            # å®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨smart_data_fetcherè·å–æ•°æ®

            performance_data = {
                'scope': scope,
                'metrics': metrics,
                'time_range': time_range,
                'data_quality': DataQuality.GOOD,
                'raw_data': {},  # å®é™…æ•°æ®
                'metadata': {
                    'data_collection_time': datetime.now().isoformat(),
                    'data_sources': ['system', 'daily', 'product', 'user'],
                    'completeness': 0.95
                }
            }

            return performance_data

        except Exception as e:
            logger.error(f"ä¸šåŠ¡è¡¨ç°æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
            return {
                'scope': scope,
                'data_quality': DataQuality.POOR,
                'error': str(e)
            }

    async def _calculate_business_kpis(self, performance_data: Dict[str, Any],
                                       scope: str) -> Dict[str, Any]:
        """è®¡ç®—ä¸šåŠ¡å…³é”®ç»©æ•ˆæŒ‡æ ‡"""
        try:
            kpis = {}
            metrics = performance_data.get('metrics', [])

            # åŸºç¡€KPIè®¡ç®—
            if 'total_balance' in metrics:
                kpis['current_balance'] = 85000000.0  # æ¨¡æ‹Ÿæ•°æ®
                kpis['balance_growth_rate'] = 0.08

            if 'total_inflow' in metrics:
                kpis['total_inflow'] = 45000000.0
                kpis['inflow_growth_rate'] = 0.12

            if 'active_users' in metrics:
                kpis['active_users'] = 12500
                kpis['user_growth_rate'] = 0.15

            # ä½¿ç”¨financial_calculatorè¿›è¡Œå¤æ‚è®¡ç®—
            if self.financial_calculator:
                # è®¡ç®—æŠ•èµ„å›æŠ¥ç‡
                roi_calculation = await self.financial_calculator.calculate_return_on_investment(
                    initial_investment=80000000.0,
                    current_value=85000000.0,
                    time_period=30
                )
                kpis['roi'] = roi_calculation.result_value if hasattr(roi_calculation, 'result_value') else 0.06

            # GPTå¢å¼ºè®¡ç®—
            if self.gpt_client:
                enhanced_kpis = await self._gpt_enhanced_kpi_calculation(performance_data, kpis)
                if enhanced_kpis:
                    kpis.update(enhanced_kpis)

            return {
                'kpis': kpis,
                'calculation_time': datetime.now().isoformat(),
                'scope': scope,
                'anomalies': []  # åœ¨KPIè®¡ç®—ä¸­å‘ç°çš„å¼‚å¸¸
            }

        except Exception as e:
            logger.error(f"KPIè®¡ç®—å¤±è´¥: {str(e)}")
            return {'kpis': {}, 'error': str(e)}

    async def _perform_comparative_analysis(self, performance_data: Dict[str, Any],
                                            scope: str) -> Dict[str, Any]:
        """æ‰§è¡Œå¯¹æ¯”åˆ†æ"""
        try:
            # æ¨¡æ‹Ÿå¯¹æ¯”åˆ†æ
            # å®é™…å®ç°ä¼šå¯¹æ¯”å†å²åŒæœŸæ•°æ®

            comparative_results = {
                'period_comparison': {
                    'current_period': 'last_30_days',
                    'comparison_period': 'previous_30_days',
                    'improvements': [
                        'ç”¨æˆ·å¢é•¿ç‡æå‡15%',
                        'èµ„é‡‘æµå…¥å¢åŠ 12%'
                    ],
                    'deteriorations': [
                        'ç”¨æˆ·æ´»è·ƒåº¦ä¸‹é™3%'
                    ]
                },
                'trend_analysis': {
                    'overall_trend': 'positive',
                    'trend_strength': 0.75,
                    'sustainability': 'likely'
                },
                'benchmarks': {
                    'industry_comparison': 'above_average',
                    'historical_performance': 'improving'
                }
            }

            return comparative_results

        except Exception as e:
            logger.error(f"å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}")
            return {'error': str(e)}

    async def _ai_analyze_business_performance(self, kpi_results: Dict[str, Any],
                                               comparative_analysis: Dict[str, Any],
                                               scope: str) -> Dict[str, Any]:
        """AIåˆ†æä¸šåŠ¡è¡¨ç°"""
        try:
            if not self.claude_client:
                return {
                    'key_findings': ['ä¸šåŠ¡è¡¨ç°åˆ†æå®Œæˆ'],
                    'insights': ['åŸºäºKPIçš„åŸºç¡€åˆ†æ'],
                    'recommendations': ['ç»§ç»­ç›‘æ§å…³é”®æŒ‡æ ‡'],
                    'confidence': 0.6
                }

            analysis_prompt = f"""
            ä½œä¸ºä¸šåŠ¡åˆ†æä¸“å®¶ï¼Œè¯·æ·±åº¦åˆ†æä»¥ä¸‹ä¸šåŠ¡è¡¨ç°æ•°æ®ï¼š

            åˆ†æèŒƒå›´: {scope}

            å…³é”®ç»©æ•ˆæŒ‡æ ‡ï¼š
            {json.dumps(kpi_results.get('kpis', {}), ensure_ascii=False, indent=2)}

            å¯¹æ¯”åˆ†æç»“æœï¼š
            {json.dumps(comparative_analysis, ensure_ascii=False, indent=2)}

            è¯·æä¾›ï¼š
            1. å…³é”®ä¸šåŠ¡å‘ç° (5ä¸ªæœ€é‡è¦çš„å‘ç°)
            2. æ·±åº¦ä¸šåŠ¡æ´å¯Ÿ
            3. æˆ˜ç•¥å»ºè®®
            4. é£é™©é¢„è­¦
            5. æœºä¼šè¯†åˆ«

            è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
            """

            claude_result = await self.claude_client.analyze_complex_query(
                analysis_prompt,
                {
                    "scope": scope,
                    "kpis": kpi_results,
                    "comparative": comparative_analysis
                }
            )

            if claude_result.get('success'):
                analysis = claude_result.get('analysis', {})
                return {
                    **analysis,
                    'ai_analysis': True,
                    'model_used': 'claude'
                }

        except Exception as e:
            logger.error(f"AIä¸šåŠ¡è¡¨ç°åˆ†æå¤±è´¥: {str(e)}")

        return {
            'key_findings': ['ä¸šåŠ¡è¡¨ç°è¯„ä¼°å®Œæˆ'],
            'insights': ['éœ€è¦è¿›ä¸€æ­¥æ•°æ®åˆ†æ'],
            'recommendations': ['å®šæœŸç›‘æ§ä¸šåŠ¡æŒ‡æ ‡'],
            'confidence': 0.5
        }

    # ============= è¾…åŠ©è®¡ç®—æ–¹æ³• =============

    def _determine_trend_direction(self, values: List[float]) -> str:
        """ç¡®å®šè¶‹åŠ¿æ–¹å‘"""
        if len(values) < 2:
            return 'insufficient_data'

        # è®¡ç®—çº¿æ€§å›å½’æ–œç‡
        n = len(values)
        x = list(range(n))

        # ç®€å•çº¿æ€§å›å½’
        x_mean = sum(x) / n
        y_mean = sum(values) / n

        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))

        if denominator == 0:
            return 'stable'

        slope = numerator / denominator

        if slope > 0.01:
            return 'increasing'
        elif slope < -0.01:
            return 'decreasing'
        else:
            return 'stable'

    def _calculate_trend_strength(self, values: List[float]) -> float:
        """è®¡ç®—è¶‹åŠ¿å¼ºåº¦"""
        if len(values) < 3:
            return 0.0

        try:
            # è®¡ç®—ç›¸å…³ç³»æ•°ä½œä¸ºè¶‹åŠ¿å¼ºåº¦
            n = len(values)
            x = list(range(n))

            x_mean = sum(x) / n
            y_mean = sum(values) / n

            numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
            x_var = sum((x[i] - x_mean) ** 2 for i in range(n))
            y_var = sum((values[i] - y_mean) ** 2 for i in range(n))

            if x_var == 0 or y_var == 0:
                return 0.0

            correlation = numerator / (x_var * y_var) ** 0.5
            return abs(correlation)

        except Exception:
            return 0.0

    def _calculate_volatility(self, values: List[float]) -> float:
        """è®¡ç®—æ³¢åŠ¨æ€§"""
        if len(values) < 2:
            return 0.0

        try:
            # è®¡ç®—ç›¸å¯¹æ ‡å‡†å·®ä½œä¸ºæ³¢åŠ¨æ€§æŒ‡æ ‡
            mean_val = statistics.mean(values)
            if mean_val == 0:
                return 0.0

            std_val = statistics.stdev(values)
            return std_val / abs(mean_val)  # å˜å¼‚ç³»æ•°

        except Exception:
            return 0.0

    async def _gpt_enhanced_statistics(self, values: List[float], metric: str) -> Optional[Dict[str, Any]]:
        """GPTå¢å¼ºç»Ÿè®¡åˆ†æ"""
        try:
            if not self.gpt_client:
                return None

            stats_prompt = f"""
            å¯¹ä»¥ä¸‹æ•°æ®åºåˆ—è¿›è¡Œé«˜çº§ç»Ÿè®¡åˆ†æï¼š

            æŒ‡æ ‡: {metric}
            æ•°æ®ç‚¹: {len(values)}
            æ•°æ®åºåˆ—: {values}

            è¯·è®¡ç®—ï¼š
            1. åŸºç¡€ç»Ÿè®¡é‡ (å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ç­‰)
            2. è¶‹åŠ¿åˆ†æ (çº¿æ€§è¶‹åŠ¿ã€å¢é•¿ç‡ç­‰)
            3. å¼‚å¸¸å€¼æ£€æµ‹
            4. å‘¨æœŸæ€§æ£€æµ‹
            5. é¢„æµ‹æ€§æŒ‡æ ‡

            è¿”å›JSONæ ¼å¼çš„è¯¦ç»†ç»Ÿè®¡ç»“æœã€‚
            """

            gpt_result = await self.gpt_client.process_direct_query(
                stats_prompt,
                {"values": values, "metric": metric}
            )

            if gpt_result.get('success'):
                # è§£æGPTè¿”å›çš„ç»Ÿè®¡ç»“æœ
                response_text = gpt_result.get('response', '')

                # ç®€åŒ–è§£æï¼Œå®é™…ä¼šæœ‰æ›´å¤æ‚çš„JSONè§£æé€»è¾‘
                enhanced_stats = {
                    'ai_enhanced': True,
                    'model_used': 'gpt',
                    'detailed_analysis': response_text
                }

                return enhanced_stats

        except Exception as e:
            logger.error(f"GPTç»Ÿè®¡å¢å¼ºå¤±è´¥: {str(e)}")
            return None

    # ============= å·¥å…·æ–¹æ³• =============

    def _determine_analysis_scope(self, data_source: str) -> AnalysisScope:
        """æ ¹æ®æ•°æ®æºç¡®å®šåˆ†æèŒƒå›´"""
        scope_mapping = {
            'system': AnalysisScope.SYSTEM_LEVEL,
            'daily': AnalysisScope.OPERATIONAL_LEVEL,
            'product': AnalysisScope.PRODUCT_LEVEL,
            'user': AnalysisScope.USER_LEVEL,
            'financial': AnalysisScope.FINANCIAL_LEVEL
        }
        return scope_mapping.get(data_source, AnalysisScope.SYSTEM_LEVEL)

    def _calculate_analysis_confidence(self, analysis_data: Dict[str, Any],
                                       trend_statistics: Dict[str, Any],
                                       anomalies: List[Dict[str, Any]]) -> float:
        """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
        confidence_factors = []

        # æ•°æ®è´¨é‡å› å­
        data_quality = analysis_data.get('data_quality', DataQuality.GOOD)
        if data_quality == DataQuality.EXCELLENT:
            confidence_factors.append(0.95)
        elif data_quality == DataQuality.GOOD:
            confidence_factors.append(0.85)
        elif data_quality == DataQuality.ACCEPTABLE:
            confidence_factors.append(0.7)
        else:
            confidence_factors.append(0.5)

        # æ•°æ®ç‚¹æ•°é‡å› å­
        data_points = trend_statistics.get('data_points', 0)
        if data_points >= 30:
            confidence_factors.append(0.9)
        elif data_points >= 14:
            confidence_factors.append(0.8)
        elif data_points >= 7:
            confidence_factors.append(0.7)
        else:
            confidence_factors.append(0.5)

        # è¶‹åŠ¿å¼ºåº¦å› å­
        trend_strength = trend_statistics.get('trend_strength', 0)
        confidence_factors.append(max(0.5, min(0.95, trend_strength)))

        # å¼‚å¸¸å½±å“å› å­
        if len(anomalies) == 0:
            confidence_factors.append(0.9)
        elif len(anomalies) <= 2:
            confidence_factors.append(0.8)
        else:
            confidence_factors.append(0.6)

        return sum(confidence_factors) / len(confidence_factors)

    def _update_analysis_stats(self, analysis_type: str, confidence: float):
        """æ›´æ–°åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        if analysis_type not in self.analysis_stats['analysis_by_type']:
            self.analysis_stats['analysis_by_type'][analysis_type] = 0

        self.analysis_stats['analysis_by_type'][analysis_type] += 1

        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        total_analyses = self.analysis_stats['total_analyses']
        current_avg = self.analysis_stats['avg_confidence_score']
        new_avg = (current_avg * (total_analyses - 1) + confidence) / total_analyses
        self.analysis_stats['avg_confidence_score'] = new_avg

    def _create_error_analysis_result(self, analysis_type: str, error: str) -> AnalysisResult:
        """åˆ›å»ºé”™è¯¯åˆ†æç»“æœ"""
        return AnalysisResult(
            analysis_id=f"error_{analysis_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analysis_type=AnalysisType(analysis_type),
            analysis_scope=AnalysisScope.SYSTEM_LEVEL,
            confidence_score=0.0,

            key_findings=[f"åˆ†æå¤±è´¥: {error}"],
            trends=[],
            anomalies=[],
            metrics={},

            business_insights=[],
            risk_factors=[f"åˆ†æé”™è¯¯: {error}"],
            opportunities=[],
            recommendations=["è¯·æ£€æŸ¥æ•°æ®æºå’Œåˆ†æå‚æ•°"],

            data_quality=DataQuality.INSUFFICIENT,
            analysis_metadata={"error": error},
            processing_time=0.0,
            timestamp=datetime.now().isoformat()
        )

    # ============= å¤–éƒ¨æ¥å£æ–¹æ³• =============

    def get_analysis_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        return self.analysis_stats.copy()

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        return {
            "status": "healthy",
            "ai_clients": {
                "claude_available": self.claude_client is not None,
                "gpt_available": self.gpt_client is not None
            },
            "tools": {
                "date_utils": self.date_utils is not None,
                "validator": self.validator is not None,
                "time_series_builder": self.time_series_builder is not None,
                "financial_calculator": self.financial_calculator is not None
            },
            "analysis_stats": self.analysis_stats,
            "timestamp": datetime.now().isoformat()
        }


# ============= å·¥å‚å‡½æ•° =============

def create_financial_data_analyzer(claude_client=None, gpt_client=None) -> FinancialDataAnalyzer:
    """
    åˆ›å»ºé‡‘èæ•°æ®åˆ†æå™¨å®ä¾‹

    Args:
        claude_client: Claudeå®¢æˆ·ç«¯å®ä¾‹
        gpt_client: GPTå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        FinancialDataAnalyzer: é‡‘èæ•°æ®åˆ†æå™¨å®ä¾‹
    """
    return FinancialDataAnalyzer(claude_client, gpt_client)


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""

    # åˆ›å»ºåˆ†æå™¨
    analyzer = create_financial_data_analyzer()

    print("=== é‡‘èæ•°æ®åˆ†æå™¨æµ‹è¯• ===")

    # 1. è¶‹åŠ¿åˆ†æ
    trend_result = await analyzer.analyze_trend(
        data_source="system",
        metric="total_balance",
        time_range=30
    )

    print(f"è¶‹åŠ¿åˆ†æç»“æœ:")
    print(f"- åˆ†æID: {trend_result.analysis_id}")
    print(f"- ç½®ä¿¡åº¦: {trend_result.confidence_score:.2f}")
    print(f"- å…³é”®å‘ç°: {trend_result.key_findings}")
    print(f"- å¤„ç†æ—¶é—´: {trend_result.processing_time:.2f}ç§’")

    # 2. ä¸šåŠ¡è¡¨ç°åˆ†æ
    performance_result = await analyzer.analyze_business_performance(
        scope="financial",
        time_range=30
    )

    print(f"\nä¸šåŠ¡è¡¨ç°åˆ†æç»“æœ:")
    print(f"- åˆ†æèŒƒå›´: {performance_result.analysis_scope.value}")
    print(f"- ä¸šåŠ¡æ´å¯Ÿ: {performance_result.business_insights}")
    print(f"- é£é™©å› ç´ : {performance_result.risk_factors}")

    # 3. å¼‚å¸¸æ£€æµ‹
    anomaly_result = await analyzer.detect_anomalies(
        data_source="daily",
        metrics=["daily_inflow", "daily_outflow"],
        sensitivity=2.0
    )

    print(f"\nå¼‚å¸¸æ£€æµ‹ç»“æœ:")
    print(f"- å‘ç°å¼‚å¸¸: {len(anomaly_result.anomalies)}ä¸ª")
    print(f"- å»ºè®®æªæ–½: {anomaly_result.recommendations}")

    # 4. å¥åº·æ£€æŸ¥
    health_status = await analyzer.health_check()
    print(f"\nç³»ç»Ÿå¥åº·çŠ¶æ€: {health_status['status']}")

    # 5. ç»Ÿè®¡ä¿¡æ¯
    stats = analyzer.get_analysis_stats()
    print(f"æ€»åˆ†ææ¬¡æ•°: {stats['total_analyses']}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence_score']:.2f}")


if __name__ == "__main__":
    asyncio.run(main())