# core/analyzers/data_requirements_analyzer.py
"""
ğŸ” æ™ºèƒ½æ•°æ®éœ€æ±‚åˆ†æå™¨
åŸºäºæŸ¥è¯¢è§£æç»“æœï¼Œç²¾ç¡®åˆ†æå’Œè§„åˆ’æ•°æ®è·å–ç­–ç•¥

æ ¸å¿ƒç‰¹ç‚¹:
- ç²¾ç»†åŒ–APIè°ƒç”¨ç­–ç•¥è§„åˆ’
- æ™ºèƒ½æ•°æ®ä¾èµ–å…³ç³»åˆ†æ
- åŠ¨æ€æ•°æ®è´¨é‡è¦æ±‚è¯„ä¼°
- ä¼˜åŒ–çš„æ•°æ®è·å–æ—¶åºå®‰æ’
- æ•°æ®æˆæœ¬æ•ˆç›Šåˆ†æ
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union, Set
from datetime import datetime, timedelta
import asyncio
from dataclasses import dataclass
from enum import Enum
import json

# å¯¼å…¥æˆ‘ä»¬çš„å·¥å…·ç±»å’Œç±»å‹
from utils.helpers.date_utils import DateUtils, create_date_utils
from utils.helpers.validation_utils import ValidationUtils, create_validation_utils

logger = logging.getLogger(__name__)


class DataPriority(Enum):
    """æ•°æ®ä¼˜å…ˆçº§"""
    CRITICAL = "critical"  # å…³é”®æ•°æ®ï¼Œå¿…é¡»è·å–
    HIGH = "high"  # é«˜ä¼˜å…ˆçº§ï¼Œå¼ºçƒˆå»ºè®®è·å–
    MEDIUM = "medium"  # ä¸­ç­‰ä¼˜å…ˆçº§ï¼Œå»ºè®®è·å–
    LOW = "low"  # ä½ä¼˜å…ˆçº§ï¼Œå¯é€‰è·å–
    OPTIONAL = "optional"  # å¯é€‰æ•°æ®ï¼Œèµ„æºå……è¶³æ—¶è·å–


class DataFreshness(Enum):
    """æ•°æ®æ–°é²œåº¦è¦æ±‚"""
    REALTIME = "realtime"  # å®æ—¶æ•°æ® (<1åˆ†é’Ÿ)
    FRESH = "fresh"  # æ–°é²œæ•°æ® (<5åˆ†é’Ÿ)
    CURRENT = "current"  # å½“å‰æ•°æ® (<30åˆ†é’Ÿ)
    RECENT = "recent"  # æœ€è¿‘æ•°æ® (<2å°æ—¶)
    DAILY = "daily"  # æ—¥åº¦æ•°æ® (<24å°æ—¶)


class DataScope(Enum):
    """æ•°æ®èŒƒå›´ç±»å‹"""
    POINT = "point"  # å•ç‚¹æ•°æ® (ç‰¹å®šæ—¥æœŸ/æ—¶åˆ»)
    RANGE = "range"  # èŒƒå›´æ•°æ® (æ—¶é—´æ®µ)
    COMPREHENSIVE = "comprehensive"  # ç»¼åˆæ•°æ® (å¤šç»´åº¦)
    COMPARATIVE = "comparative"  # å¯¹æ¯”æ•°æ® (å¤šæ—¶æœŸ)


@dataclass
class APICallPlan:
    """APIè°ƒç”¨è®¡åˆ’"""
    api_endpoint: str  # APIç«¯ç‚¹
    call_method: str  # è°ƒç”¨æ–¹æ³•å
    parameters: Dict[str, Any]  # è°ƒç”¨å‚æ•°
    priority: DataPriority  # ä¼˜å…ˆçº§
    estimated_time: float  # é¢„ä¼°è€—æ—¶(ç§’)
    estimated_cost: float  # é¢„ä¼°æˆæœ¬(ç›¸å¯¹å€¼)
    data_volume_expected: str  # é¢„æœŸæ•°æ®é‡ (small/medium/large)
    depends_on: List[str]  # ä¾èµ–çš„å…¶ä»–è°ƒç”¨
    cache_strategy: str  # ç¼“å­˜ç­–ç•¥
    retry_strategy: str  # é‡è¯•ç­–ç•¥
    validation_rules: List[str]  # æ•°æ®éªŒè¯è§„åˆ™


@dataclass
class DataRequirement:
    """æ•°æ®éœ€æ±‚å®šä¹‰"""
    requirement_id: str  # éœ€æ±‚ID
    requirement_type: str  # éœ€æ±‚ç±»å‹
    data_source: str  # æ•°æ®æº
    data_scope: DataScope  # æ•°æ®èŒƒå›´
    freshness_requirement: DataFreshness  # æ–°é²œåº¦è¦æ±‚
    priority: DataPriority  # ä¼˜å…ˆçº§
    quality_threshold: float  # è´¨é‡é˜ˆå€¼ (0-1)
    fallback_options: List[str]  # é™çº§é€‰é¡¹
    processing_requirements: Dict[str, Any]  # å¤„ç†è¦æ±‚
    business_justification: str  # ä¸šåŠ¡ç†ç”±


@dataclass
class DataAcquisitionPlan:
    """æ•°æ®è·å–è®¡åˆ’"""
    plan_id: str  # è®¡åˆ’ID
    query_analysis_summary: Dict[str, Any]  # æŸ¥è¯¢åˆ†ææ‘˜è¦
    data_requirements: List[DataRequirement]  # æ•°æ®éœ€æ±‚åˆ—è¡¨
    api_call_plans: List[APICallPlan]  # APIè°ƒç”¨è®¡åˆ’
    execution_sequence: List[str]  # æ‰§è¡Œåºåˆ—
    parallel_groups: List[List[str]]  # å¹¶è¡Œæ‰§è¡Œç»„
    total_estimated_time: float  # æ€»é¢„ä¼°æ—¶é—´
    total_estimated_cost: float  # æ€»é¢„ä¼°æˆæœ¬
    success_criteria: Dict[str, Any]  # æˆåŠŸæ ‡å‡†
    fallback_strategies: Dict[str, Any]  # é™çº§ç­–ç•¥
    optimization_notes: List[str]  # ä¼˜åŒ–å»ºè®®
    plan_confidence: float  # è®¡åˆ’ç½®ä¿¡åº¦


class DataRequirementsAnalyzer:
    """
    ğŸ” æ™ºèƒ½æ•°æ®éœ€æ±‚åˆ†æå™¨

    åŠŸèƒ½æ¶æ„:
    1. åŸºäºæŸ¥è¯¢è§£æç»“æœçš„ç²¾ç»†åŒ–éœ€æ±‚åˆ†æ
    2. å¤šç»´åº¦æ•°æ®ä¾èµ–å…³ç³»åˆ†æ
    3. ä¼˜åŒ–çš„APIè°ƒç”¨ç­–ç•¥è§„åˆ’
    4. åŠ¨æ€æ•°æ®è´¨é‡å’Œæˆæœ¬æ§åˆ¶
    """

    def __init__(self, claude_client=None, gpt_client=None):
        """
        åˆå§‹åŒ–æ•°æ®éœ€æ±‚åˆ†æå™¨

        Args:
            claude_client: Claudeå®¢æˆ·ç«¯ï¼Œç”¨äºä¸šåŠ¡é€»è¾‘åˆ†æ
            gpt_client: GPTå®¢æˆ·ç«¯ï¼Œç”¨äºæ•°æ®ç­–ç•¥ä¼˜åŒ–
        """
        self.claude_client = claude_client
        self.gpt_client = gpt_client
        self.date_utils = create_date_utils(claude_client)
        self.validator = create_validation_utils(claude_client, gpt_client)

        # APIé…ç½®ä¿¡æ¯
        self.api_catalog = self._build_api_catalog()

        # æ•°æ®æˆæœ¬æ¨¡å‹
        self.cost_model = self._build_cost_model()

        # åˆ†æç»Ÿè®¡
        self.analysis_stats = {
            'total_analyses': 0,
            'optimization_suggestions': 0,
            'cost_savings_achieved': 0.0,
            'avg_plan_confidence': 0.0
        }

        logger.info("DataRequirementsAnalyzer initialized")

    def _build_api_catalog(self) -> Dict[str, Any]:
        """æ„å»ºAPIç›®å½•ä¿¡æ¯"""
        return {
            "system": {
                "endpoint": "/api/sta/system",
                "method": "get_system_data",
                "data_type": "realtime_overview",
                "typical_response_time": 2.0,
                "data_volume": "small",
                "cache_recommended": True,
                "cache_ttl": 60,
                "provides": [
                    "æ€»ä½™é¢", "æ€»å…¥é‡‘", "æ€»å‡ºé‡‘", "æ€»æŠ•èµ„é‡‘é¢", "æ€»å¥–åŠ±å‘æ”¾",
                    "ç”¨æˆ·ç»Ÿè®¡", "äº§å“ç»Ÿè®¡", "åˆ°æœŸæ¦‚è§ˆ"
                ],
                "best_for": ["current_status", "overview", "realtime_queries"]
            },
            "daily": {
                "endpoint": "/api/sta/day",
                "method": "get_daily_data",
                "data_type": "historical_daily",
                "typical_response_time": 1.5,
                "data_volume": "small",
                "cache_recommended": True,
                "cache_ttl": 300,
                "provides": [
                    "æ—¥æœŸ", "æ³¨å†Œäººæ•°", "æŒä»“äººæ•°", "è´­ä¹°äº§å“æ•°é‡",
                    "åˆ°æœŸäº§å“æ•°é‡", "å…¥é‡‘", "å‡ºé‡‘"
                ],
                "best_for": ["trend_analysis", "historical_data", "daily_metrics"]
            },
            "product": {
                "endpoint": "/api/sta/product",
                "method": "get_product_data",
                "data_type": "product_catalog",
                "typical_response_time": 3.0,
                "data_volume": "medium",
                "cache_recommended": True,
                "cache_ttl": 600,
                "provides": [
                    "äº§å“åˆ—è¡¨", "äº§å“ä»·æ ¼", "æœŸé™å¤©æ•°", "æ¯æ—¥åˆ©ç‡",
                    "æ€»è´­ä¹°æ¬¡æ•°", "æŒæœ‰æƒ…å†µ", "å³å°†åˆ°æœŸæ•°"
                ],
                "best_for": ["product_analysis", "expiry_prediction", "portfolio_analysis"]
            },
            "user_daily": {
                "endpoint": "/api/sta/user_daily",
                "method": "get_user_daily_data",
                "data_type": "user_behavior_daily",
                "typical_response_time": 2.0,
                "data_volume": "small",
                "cache_recommended": True,
                "cache_ttl": 300,
                "provides": [
                    "æ¯æ—¥æ–°å¢ç”¨æˆ·", "VIPç­‰çº§åˆ†å¸ƒ", "ç”¨æˆ·è¡Œä¸ºæ•°æ®"
                ],
                "best_for": ["user_analysis", "behavior_trends", "segmentation"]
            },
            "user": {
                "endpoint": "/api/sta/user",
                "method": "get_user_data",
                "data_type": "user_details",
                "typical_response_time": 4.0,
                "data_volume": "large",
                "cache_recommended": False,
                "cache_ttl": 0,
                "provides": [
                    "ç”¨æˆ·è¯¦æƒ…", "æŠ•èµ„é‡‘é¢", "ç´¯è®¡å¥–åŠ±", "æŠ•æŠ¥æ¯”"
                ],
                "best_for": ["user_profiling", "detailed_analysis", "compliance"]
            },
            "product_end_single": {
                "endpoint": "/api/sta/product_end",
                "method": "get_product_end_data",
                "data_type": "single_day_expiry",
                "typical_response_time": 1.5,
                "data_volume": "small",
                "cache_recommended": True,
                "cache_ttl": 180,
                "provides": [
                    "æŒ‡å®šæ—¥æœŸåˆ°æœŸäº§å“", "åˆ°æœŸé‡‘é¢", "äº§å“åˆ†å¸ƒ"
                ],
                "best_for": ["specific_date_expiry", "cash_flow_planning"]
            },
            "product_end_interval": {
                "endpoint": "/api/sta/product_end_interval",
                "method": "get_product_end_interval",
                "data_type": "range_expiry",
                "typical_response_time": 2.5,
                "data_volume": "medium",
                "cache_recommended": True,
                "cache_ttl": 240,
                "provides": [
                    "æ—¶é—´æ®µåˆ°æœŸäº§å“", "åŒºé—´åˆ°æœŸé‡‘é¢", "åˆ°æœŸåˆ†å¸ƒç»Ÿè®¡"
                ],
                "best_for": ["period_expiry_analysis", "cash_flow_forecasting"]
            }
        }

    def _build_cost_model(self) -> Dict[str, Any]:
        """æ„å»ºæ•°æ®æˆæœ¬æ¨¡å‹"""
        return {
            "api_call_cost": {
                "system": 1.0,  # åŸºå‡†æˆæœ¬
                "daily": 1.2,  # ç¨é«˜ï¼Œå› ä¸ºå¯èƒ½éœ€è¦æ‰¹é‡è°ƒç”¨
                "product": 1.5,  # ä¸­ç­‰ï¼Œæ•°æ®é‡è¾ƒå¤§
                "user_daily": 1.1,  # è¾ƒä½
                "user": 3.0,  # æœ€é«˜ï¼Œå¤§æ•°æ®é‡ä¸”åˆ†é¡µ
                "product_end_single": 1.0,
                "product_end_interval": 1.3
            },
            "batch_cost_multiplier": {
                "1-5": 1.0,  # 1-5ä¸ªè°ƒç”¨ï¼Œæ­£å¸¸æˆæœ¬
                "6-20": 0.9,  # 6-20ä¸ªè°ƒç”¨ï¼Œå°å¹…ä¼˜åŒ–
                "21-50": 0.8,  # 21-50ä¸ªè°ƒç”¨ï¼Œä¸­ç­‰ä¼˜åŒ–
                "50+": 0.7  # 50+ä¸ªè°ƒç”¨ï¼Œæœ€å¤§ä¼˜åŒ–
            },
            "cache_cost_benefit": 0.3,  # ç¼“å­˜å¯èŠ‚çœ30%æˆæœ¬
            "parallel_efficiency": 0.15  # å¹¶è¡Œæ‰§è¡Œå¯æå‡15%æ•ˆç‡
        }

    # ============= æ ¸å¿ƒéœ€æ±‚åˆ†ææ–¹æ³• =============

    async def analyze_data_requirements(self, query_analysis_result: Any) -> DataAcquisitionPlan:
        """
        ğŸ¯ åˆ†ææ•°æ®éœ€æ±‚ - æ ¸å¿ƒå…¥å£æ–¹æ³•

        Args:
            query_analysis_result: æŸ¥è¯¢è§£æç»“æœ (æ¥è‡ªSmartQueryParser)

        Returns:
            DataAcquisitionPlan: å®Œæ•´çš„æ•°æ®è·å–è®¡åˆ’
        """
        try:
            logger.info("ğŸ” å¼€å§‹åˆ†ææ•°æ®éœ€æ±‚")
            self.analysis_stats['total_analyses'] += 1

            # ç¬¬1æ­¥: æå–æŸ¥è¯¢åˆ†æå…³é”®ä¿¡æ¯
            query_summary = self._extract_query_summary(query_analysis_result)

            # ç¬¬2æ­¥: åŸºäºå¤æ‚åº¦å’Œç±»å‹ç¡®å®šæ•°æ®éœ€æ±‚
            base_requirements = await self._determine_base_requirements(query_analysis_result)

            # ç¬¬3æ­¥: åˆ†ææ—¶é—´ç»´åº¦çš„æ•°æ®éœ€æ±‚
            temporal_requirements = await self._analyze_temporal_requirements(query_analysis_result)

            # ç¬¬4æ­¥: åˆ†æä¸šåŠ¡å‚æ•°ç›¸å…³çš„æ•°æ®éœ€æ±‚
            business_requirements = await self._analyze_business_requirements(query_analysis_result)

            # ç¬¬5æ­¥: åˆå¹¶å’Œå»é‡æ•°æ®éœ€æ±‚
            consolidated_requirements = self._consolidate_requirements(
                base_requirements, temporal_requirements, business_requirements
            )

            # ç¬¬6æ­¥: ç”ŸæˆAPIè°ƒç”¨è®¡åˆ’
            api_call_plans = await self._generate_api_call_plans(consolidated_requirements)

            # ç¬¬7æ­¥: ä¼˜åŒ–æ‰§è¡Œåºåˆ—å’Œå¹¶è¡Œç­–ç•¥
            execution_strategy = await self._optimize_execution_strategy(api_call_plans)

            # ç¬¬8æ­¥: æˆæœ¬æ•ˆç›Šåˆ†æå’Œä¼˜åŒ–å»ºè®®
            cost_analysis = self._analyze_cost_and_optimize(api_call_plans, execution_strategy)

            # ç¬¬9æ­¥: æ„å»ºæœ€ç»ˆæ•°æ®è·å–è®¡åˆ’
            acquisition_plan = self._build_acquisition_plan(
                query_summary, consolidated_requirements, api_call_plans,
                execution_strategy, cost_analysis
            )

            logger.info(f"âœ… æ•°æ®éœ€æ±‚åˆ†æå®Œæˆ: {len(consolidated_requirements)}ä¸ªéœ€æ±‚, {len(api_call_plans)}ä¸ªAPIè°ƒç”¨")

            return acquisition_plan

        except Exception as e:
            logger.error(f"âŒ æ•°æ®éœ€æ±‚åˆ†æå¤±è´¥: {str(e)}")
            return self._create_fallback_plan(query_analysis_result, str(e))

    # ============= éœ€æ±‚åˆ†æå±‚ =============

    def _extract_query_summary(self, query_analysis_result: Any) -> Dict[str, Any]:
        """æå–æŸ¥è¯¢åˆ†æå…³é”®ä¿¡æ¯"""

        return {
            "original_query": query_analysis_result.original_query,
            "complexity": query_analysis_result.complexity.value,
            "query_type": query_analysis_result.query_type.value,
            "business_scenario": query_analysis_result.business_scenario.value,
            "confidence_score": query_analysis_result.confidence_score,
            "estimated_time": query_analysis_result.estimated_total_time,
            "execution_steps": len(query_analysis_result.execution_plan),
            "ai_collaboration_level": query_analysis_result.ai_collaboration_plan.get("collaboration_level", "minimal")
        }

    async def _determine_base_requirements(self, query_analysis_result: Any) -> List[DataRequirement]:
        """ç¡®å®šåŸºç¡€æ•°æ®éœ€æ±‚"""

        base_requirements = []
        complexity = query_analysis_result.complexity
        query_type = query_analysis_result.query_type

        # ğŸ¯ æ ¹æ®å¤æ‚åº¦ç¡®å®šåŸºç¡€éœ€æ±‚
        if complexity.value == "simple":
            # ç®€å•æŸ¥è¯¢ï¼šé€šå¸¸åªéœ€è¦å½“å‰çŠ¶æ€æ•°æ®
            base_requirements.append(DataRequirement(
                requirement_id="simple_current_state",
                requirement_type="current_status",
                data_source="system",
                data_scope=DataScope.POINT,
                freshness_requirement=DataFreshness.CURRENT,
                priority=DataPriority.CRITICAL,
                quality_threshold=0.9,
                fallback_options=["cached_system_data"],
                processing_requirements={"format": "summary"},
                business_justification="è·å–å½“å‰ç³»ç»ŸçŠ¶æ€ç”¨äºç›´æ¥å›ç­”"
            ))

        elif complexity.value == "medium":
            # ä¸­ç­‰å¤æ‚åº¦ï¼šéœ€è¦å½“å‰æ•°æ® + ä¸€äº›å†å²æ•°æ®
            base_requirements.extend([
                DataRequirement(
                    requirement_id="medium_current_state",
                    requirement_type="current_status",
                    data_source="system",
                    data_scope=DataScope.POINT,
                    freshness_requirement=DataFreshness.FRESH,
                    priority=DataPriority.CRITICAL,
                    quality_threshold=0.85,
                    fallback_options=[],
                    processing_requirements={"format": "detailed"},
                    business_justification="å½“å‰çŠ¶æ€ä½œä¸ºåˆ†æåŸºçº¿"
                ),
                DataRequirement(
                    requirement_id="medium_recent_history",
                    requirement_type="historical_trend",
                    data_source="daily",
                    data_scope=DataScope.RANGE,
                    freshness_requirement=DataFreshness.DAILY,
                    priority=DataPriority.HIGH,
                    quality_threshold=0.8,
                    fallback_options=["shorter_history"],
                    processing_requirements={"time_range": "recent_30_days"},
                    business_justification="æœ€è¿‘è¶‹åŠ¿åˆ†ææ‰€éœ€å†å²æ•°æ®"
                )
            ])

        elif complexity.value in ["complex", "expert"]:
            # å¤æ‚æŸ¥è¯¢ï¼šéœ€è¦ç»¼åˆæ•°æ®
            base_requirements.extend([
                DataRequirement(
                    requirement_id="complex_comprehensive_current",
                    requirement_type="comprehensive_current",
                    data_source="system",
                    data_scope=DataScope.COMPREHENSIVE,
                    freshness_requirement=DataFreshness.FRESH,
                    priority=DataPriority.CRITICAL,
                    quality_threshold=0.9,
                    fallback_options=[],
                    processing_requirements={"include_metadata": True},
                    business_justification="å¤æ‚åˆ†æçš„å½“å‰çŠ¶æ€åŸºç¡€"
                ),
                DataRequirement(
                    requirement_id="complex_extended_history",
                    requirement_type="extended_historical",
                    data_source="daily",
                    data_scope=DataScope.RANGE,
                    freshness_requirement=DataFreshness.DAILY,
                    priority=DataPriority.HIGH,
                    quality_threshold=0.85,
                    fallback_options=["medium_history"],
                    processing_requirements={"time_range": "extended_90_days"},
                    business_justification="æ·±åº¦åˆ†ææ‰€éœ€æ‰©å±•å†å²æ•°æ®"
                ),
                DataRequirement(
                    requirement_id="complex_product_analysis",
                    requirement_type="product_portfolio",
                    data_source="product",
                    data_scope=DataScope.COMPREHENSIVE,
                    freshness_requirement=DataFreshness.CURRENT,
                    priority=DataPriority.MEDIUM,
                    quality_threshold=0.8,
                    fallback_options=[],
                    processing_requirements={"include_predictions": True},
                    business_justification="äº§å“ç»„åˆåˆ†æå’Œåˆ°æœŸé¢„æµ‹"
                )
            ])

            # ä¸“å®¶çº§è¿˜éœ€è¦ç”¨æˆ·æ•°æ®
            if complexity.value == "expert":
                base_requirements.append(DataRequirement(
                    requirement_id="expert_user_behavior",
                    requirement_type="user_behavior_analysis",
                    data_source="user_daily",
                    data_scope=DataScope.RANGE,
                    freshness_requirement=DataFreshness.DAILY,
                    priority=DataPriority.MEDIUM,
                    quality_threshold=0.75,
                    fallback_options=["aggregated_user_stats"],
                    processing_requirements={"time_range": "user_behavior_60_days"},
                    business_justification="ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ"
                ))

        # ğŸ¯ æ ¹æ®æŸ¥è¯¢ç±»å‹æ·»åŠ ç‰¹å®šéœ€æ±‚
        if query_type.value == "prediction":
            base_requirements.append(DataRequirement(
                requirement_id="prediction_baseline",
                requirement_type="prediction_foundation",
                data_source="daily",
                data_scope=DataScope.RANGE,
                freshness_requirement=DataFreshness.DAILY,
                priority=DataPriority.CRITICAL,
                quality_threshold=0.9,
                fallback_options=[],
                processing_requirements={"min_data_points": 60},
                business_justification="é¢„æµ‹æ¨¡å‹éœ€è¦å……è¶³çš„å†å²æ•°æ®"
            ))

        elif query_type.value == "risk_assessment":
            base_requirements.append(DataRequirement(
                requirement_id="risk_comprehensive_data",
                requirement_type="risk_analysis_data",
                data_source="comprehensive",
                data_scope=DataScope.COMPREHENSIVE,
                freshness_requirement=DataFreshness.FRESH,
                priority=DataPriority.CRITICAL,
                quality_threshold=0.95,
                fallback_options=[],
                processing_requirements={"include_stress_scenarios": True},
                business_justification="é£é™©è¯„ä¼°éœ€è¦å…¨é¢å‡†ç¡®çš„æ•°æ®"
            ))

        return base_requirements

    async def _analyze_temporal_requirements(self, query_analysis_result: Any) -> List[DataRequirement]:
        """åˆ†ææ—¶é—´ç»´åº¦æ•°æ®éœ€æ±‚"""

        temporal_requirements = []
        time_requirements = query_analysis_result.time_requirements
        date_parse_result = query_analysis_result.date_parse_result

        # å¦‚æœæŸ¥è¯¢ä¸­åŒ…å«æ˜ç¡®çš„æ—¶é—´ä¿¡æ¯
        if date_parse_result and date_parse_result.has_time_info:

            # å¤„ç†å…·ä½“æ—¥æœŸéœ€æ±‚
            for date in date_parse_result.dates:
                temporal_requirements.append(DataRequirement(
                    requirement_id=f"specific_date_{date.replace('-', '_')}",
                    requirement_type="specific_date_data",
                    data_source="daily",
                    data_scope=DataScope.POINT,
                    freshness_requirement=DataFreshness.DAILY,
                    priority=DataPriority.HIGH,
                    quality_threshold=0.85,
                    fallback_options=["interpolated_data"],
                    processing_requirements={"target_date": date},
                    business_justification=f"ç”¨æˆ·æ˜ç¡®è¦æ±‚{date}çš„æ•°æ®"
                ))

            # å¤„ç†æ—¥æœŸèŒƒå›´éœ€æ±‚
            for date_range in date_parse_result.ranges:
                range_id = f"range_{date_range.start_date.replace('-', '_')}_to_{date_range.end_date.replace('-', '_')}"
                temporal_requirements.append(DataRequirement(
                    requirement_id=range_id,
                    requirement_type="date_range_data",
                    data_source="daily",
                    data_scope=DataScope.RANGE,
                    freshness_requirement=DataFreshness.DAILY,
                    priority=DataPriority.HIGH,
                    quality_threshold=0.8,
                    fallback_options=["partial_range"],
                    processing_requirements={
                        "start_date": date_range.start_date,
                        "end_date": date_range.end_date,
                        "range_type": date_range.range_type
                    },
                    business_justification=f"ç”¨æˆ·è¦æ±‚æ—¶é—´èŒƒå›´: {date_range.description}"
                ))

        # åˆ†ææ˜¯å¦éœ€è¦åˆ°æœŸæ•°æ®
        original_query = query_analysis_result.original_query.lower()
        if any(keyword in original_query for keyword in ["åˆ°æœŸ", "expiry", "mature", "åˆ°æœŸ"]):

            # ç¡®å®šåˆ°æœŸæ•°æ®çš„æ—¶é—´èŒƒå›´
            if date_parse_result and date_parse_result.ranges:
                # ä½¿ç”¨æŸ¥è¯¢ä¸­çš„æ—¶é—´èŒƒå›´
                for date_range in date_parse_result.ranges:
                    temporal_requirements.append(DataRequirement(
                        requirement_id=f"expiry_range_{date_range.start_date.replace('-', '_')}",
                        requirement_type="expiry_range_data",
                        data_source="product_end_interval",
                        data_scope=DataScope.RANGE,
                        freshness_requirement=DataFreshness.CURRENT,
                        priority=DataPriority.CRITICAL,
                        quality_threshold=0.9,
                        fallback_options=["daily_expiry_aggregation"],
                        processing_requirements={
                            "start_date": date_range.start_date,
                            "end_date": date_range.end_date
                        },
                        business_justification="ç”¨æˆ·æŸ¥è¯¢æ¶‰åŠäº§å“åˆ°æœŸåˆ†æ"
                    ))
            else:
                # é»˜è®¤è·å–æœªæ¥ä¸€å‘¨çš„åˆ°æœŸæ•°æ®
                today = datetime.now()
                week_end = today + timedelta(days=7)
                temporal_requirements.append(DataRequirement(
                    requirement_id="default_expiry_week",
                    requirement_type="default_expiry_data",
                    data_source="product_end_interval",
                    data_scope=DataScope.RANGE,
                    freshness_requirement=DataFreshness.CURRENT,
                    priority=DataPriority.HIGH,
                    quality_threshold=0.85,
                    fallback_options=["single_day_expiry"],
                    processing_requirements={
                        "start_date": today.strftime("%Y-%m-%d"),
                        "end_date": week_end.strftime("%Y-%m-%d")
                    },
                    business_justification="åˆ°æœŸç›¸å…³æŸ¥è¯¢çš„é»˜è®¤æ—¶é—´çª—å£"
                ))

        return temporal_requirements

    async def _analyze_business_requirements(self, query_analysis_result: Any) -> List[DataRequirement]:
        """åˆ†æä¸šåŠ¡å‚æ•°ç›¸å…³æ•°æ®éœ€æ±‚"""

        business_requirements = []
        business_parameters = query_analysis_result.business_parameters
        calculation_requirements = query_analysis_result.calculation_requirements
        query_type = query_analysis_result.query_type

        # åˆ†ææŸ¥è¯¢ä¸­çš„å…³é”®ä¸šåŠ¡æ¦‚å¿µ
        original_query = query_analysis_result.original_query.lower()

        # ğŸ” å¤æŠ•/æç°ç›¸å…³æŸ¥è¯¢
        if any(keyword in original_query for keyword in ["å¤æŠ•", "æç°", "reinvest", "cashout"]):
            business_requirements.append(DataRequirement(
                requirement_id="reinvestment_analysis_data",
                requirement_type="reinvestment_calculation_base",
                data_source="product",
                data_scope=DataScope.COMPREHENSIVE,
                freshness_requirement=DataFreshness.CURRENT,
                priority=DataPriority.CRITICAL,
                quality_threshold=0.9,
                fallback_options=[],
                processing_requirements={
                    "include_product_rates": True,
                    "include_expiry_schedule": True
                },
                business_justification="å¤æŠ•/æç°è®¡ç®—éœ€è¦å®Œæ•´çš„äº§å“å’Œåˆ°æœŸä¿¡æ¯"
            ))

        # ğŸ” ç”¨æˆ·ç›¸å…³åˆ†æ
        if any(keyword in original_query for keyword in ["ç”¨æˆ·", "ä¼šå‘˜", "user", "member"]):
            if query_type.value in ["user_behavior", "growth_analysis"]:
                business_requirements.append(DataRequirement(
                    requirement_id="user_comprehensive_analysis",
                    requirement_type="user_analysis_base",
                    data_source="user_daily",
                    data_scope=DataScope.RANGE,
                    freshness_requirement=DataFreshness.DAILY,
                    priority=DataPriority.HIGH,
                    quality_threshold=0.8,
                    fallback_options=["aggregated_user_metrics"],
                    processing_requirements={"include_vip_breakdown": True},
                    business_justification="ç”¨æˆ·è¡Œä¸ºå’Œå¢é•¿åˆ†æéœ€è¦ç”¨æˆ·è¯¦ç»†æ•°æ®"
                ))

        # ğŸ” äº§å“è¡¨ç°åˆ†æ
        if any(keyword in original_query for keyword in ["äº§å“", "product", "portfolio"]):
            business_requirements.append(DataRequirement(
                requirement_id="product_performance_data",
                requirement_type="product_analysis_base",
                data_source="product",
                data_scope=DataScope.COMPREHENSIVE,
                freshness_requirement=DataFreshness.CURRENT,
                priority=DataPriority.HIGH,
                quality_threshold=0.85,
                fallback_options=[],
                processing_requirements={
                    "include_performance_metrics": True,
                    "include_holding_analysis": True
                },
                business_justification="äº§å“è¡¨ç°åˆ†æéœ€è¦å®Œæ•´çš„äº§å“æ•°æ®"
            ))

        # ğŸ” è´¢åŠ¡è§„åˆ’ç›¸å…³
        if query_analysis_result.business_scenario.value == "financial_planning":
            business_requirements.append(DataRequirement(
                requirement_id="financial_planning_comprehensive",
                requirement_type="financial_planning_base",
                data_source="comprehensive",
                data_scope=DataScope.COMPREHENSIVE,
                freshness_requirement=DataFreshness.FRESH,
                priority=DataPriority.CRITICAL,
                quality_threshold=0.9,
                fallback_options=[],
                processing_requirements={"include_all_metrics": True},
                business_justification="è´¢åŠ¡è§„åˆ’éœ€è¦å…¨é¢çš„è´¢åŠ¡æ•°æ®"
            ))

        return business_requirements

    # ============= éœ€æ±‚æ•´åˆå’ŒAPIè§„åˆ’å±‚ =============

    def _consolidate_requirements(self, *requirement_lists: List[DataRequirement]) -> List[DataRequirement]:
        """åˆå¹¶å’Œå»é‡æ•°æ®éœ€æ±‚"""

        all_requirements = []
        for req_list in requirement_lists:
            all_requirements.extend(req_list)

        # å»é‡å’Œåˆå¹¶ç›¸ä¼¼éœ€æ±‚
        consolidated = {}

        for req in all_requirements:
            key = f"{req.data_source}_{req.requirement_type}"

            if key in consolidated:
                # åˆå¹¶ç›¸ä¼¼éœ€æ±‚ï¼Œå–ä¼˜å…ˆçº§æ›´é«˜çš„
                existing = consolidated[key]
                if req.priority.value == "critical" or existing.priority.value != "critical":
                    consolidated[key] = req
            else:
                consolidated[key] = req

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3, "optional": 4}
        sorted_requirements = sorted(
            consolidated.values(),
            key=lambda x: priority_order[x.priority.value]
        )

        logger.info(f"ğŸ“‹ åˆå¹¶éœ€æ±‚: {len(all_requirements)} -> {len(sorted_requirements)}")
        return sorted_requirements

    async def _generate_api_call_plans(self, requirements: List[DataRequirement]) -> List[APICallPlan]:
        """ç”ŸæˆAPIè°ƒç”¨è®¡åˆ’"""

        api_call_plans = []

        for req in requirements:
            # ğŸ¯ æ ¹æ®æ•°æ®æºå’Œéœ€æ±‚ç±»å‹ç¡®å®šAPIè°ƒç”¨
            if req.data_source == "system":
                api_call_plans.append(self._create_system_api_plan(req))

            elif req.data_source == "daily":
                if req.data_scope == DataScope.POINT:
                    api_call_plans.append(self._create_daily_point_api_plan(req))
                elif req.data_scope == DataScope.RANGE:
                    api_call_plans.extend(self._create_daily_range_api_plans(req))

            elif req.data_source == "product":
                api_call_plans.append(self._create_product_api_plan(req))

            elif req.data_source == "user_daily":
                if req.data_scope == DataScope.RANGE:
                    api_call_plans.extend(self._create_user_daily_range_api_plans(req))

            elif req.data_source == "product_end_interval":
                api_call_plans.append(self._create_product_end_interval_api_plan(req))

            elif req.data_source == "comprehensive":
                # ç»¼åˆæ•°æ®éœ€æ±‚ï¼Œéœ€è¦å¤šä¸ªAPIè°ƒç”¨
                api_call_plans.extend(self._create_comprehensive_api_plans(req))

        # æ·»åŠ ä¾èµ–å…³ç³»åˆ†æ
        self._analyze_api_dependencies(api_call_plans)

        logger.info(f"ğŸ”— ç”ŸæˆAPIè°ƒç”¨è®¡åˆ’: {len(api_call_plans)}ä¸ª")
        return api_call_plans

    def _create_system_api_plan(self, req: DataRequirement) -> APICallPlan:
        """åˆ›å»ºç³»ç»ŸAPIè°ƒç”¨è®¡åˆ’"""

        api_info = self.api_catalog["system"]

        return APICallPlan(
            api_endpoint=api_info["endpoint"],
            call_method=api_info["method"],
            parameters={},
            priority=req.priority,
            estimated_time=api_info["typical_response_time"],
            estimated_cost=self.cost_model["api_call_cost"]["system"],
            data_volume_expected=api_info["data_volume"],
            depends_on=[],
            cache_strategy="aggressive" if api_info["cache_recommended"] else "none",
            retry_strategy="standard",
            validation_rules=["non_empty_response", "positive_balance"]
        )

    def _create_daily_point_api_plan(self, req: DataRequirement) -> APICallPlan:
        """åˆ›å»ºå•æ—¥æ•°æ®APIè°ƒç”¨è®¡åˆ’"""

        api_info = self.api_catalog["daily"]
        target_date = req.processing_requirements.get("target_date", "")

        # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºAPIéœ€è¦çš„YYYYMMDD
        if self.date_utils and target_date:
            api_date = self.date_utils.date_to_api_format(target_date)
        else:
            api_date = datetime.now().strftime("%Y%m%d")

        return APICallPlan(
            api_endpoint=api_info["endpoint"],
            call_method=api_info["method"],
            parameters={"date": api_date},
            priority=req.priority,
            estimated_time=api_info["typical_response_time"],
            estimated_cost=self.cost_model["api_call_cost"]["daily"],
            data_volume_expected=api_info["data_volume"],
            depends_on=[],
            cache_strategy="standard",
            retry_strategy="standard",
            validation_rules=["valid_date_format", "non_negative_amounts"]
        )

    def _create_daily_range_api_plans(self, req: DataRequirement) -> List[APICallPlan]:
        """åˆ›å»ºæ—¥æœŸèŒƒå›´æ•°æ®APIè°ƒç”¨è®¡åˆ’"""

        api_info = self.api_catalog["daily"]

        # è·å–æ—¶é—´èŒƒå›´
        start_date = req.processing_requirements.get("start_date", "")
        end_date = req.processing_requirements.get("end_date", "")

        if not start_date or not end_date:
            # ä½¿ç”¨é»˜è®¤èŒƒå›´
            end_dt = datetime.now()
            start_dt = end_dt - timedelta(days=30)
            start_date = start_dt.strftime("%Y-%m-%d")
            end_date = end_dt.strftime("%Y-%m-%d")

        # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
        if self.date_utils:
            dates = self.date_utils.generate_date_range(start_date, end_date, "api")
        else:
            # é™çº§æ–¹æ¡ˆ
            dates = [datetime.now().strftime("%Y%m%d")]

        # ä¸ºæ¯ä¸ªæ—¥æœŸåˆ›å»ºAPIè°ƒç”¨è®¡åˆ’
        api_plans = []
        for date in dates:
            api_plans.append(APICallPlan(
                api_endpoint=api_info["endpoint"],
                call_method=api_info["method"],
                parameters={"date": date},
                priority=req.priority,
                estimated_time=api_info["typical_response_time"],
                estimated_cost=self.cost_model["api_call_cost"]["daily"],
                data_volume_expected=api_info["data_volume"],
                depends_on=[],
                cache_strategy="standard",
                retry_strategy="batch_retry",
                validation_rules=["valid_date_format", "non_negative_amounts"]
            ))

        return api_plans

    def _create_product_api_plan(self, req: DataRequirement) -> APICallPlan:
        """åˆ›å»ºäº§å“APIè°ƒç”¨è®¡åˆ’"""

        api_info = self.api_catalog["product"]

        return APICallPlan(
            api_endpoint=api_info["endpoint"],
            call_method=api_info["method"],
            parameters={},
            priority=req.priority,
            estimated_time=api_info["typical_response_time"],
            estimated_cost=self.cost_model["api_call_cost"]["product"],
            data_volume_expected=api_info["data_volume"],
            depends_on=[],
            cache_strategy="extended" if api_info["cache_recommended"] else "none",
            retry_strategy="standard",
            validation_rules=["product_list_not_empty", "valid_product_data"]
        )

    def _create_user_daily_range_api_plans(self, req: DataRequirement) -> List[APICallPlan]:
        """åˆ›å»ºç”¨æˆ·æ¯æ—¥æ•°æ®èŒƒå›´APIè°ƒç”¨è®¡åˆ’"""

        api_info = self.api_catalog["user_daily"]

        # ç®€åŒ–å¤„ç†ï¼Œåªè·å–æœ€è¿‘30å¤©çš„ç”¨æˆ·æ•°æ®
        api_plans = []
        for i in range(30):
            date_dt = datetime.now() - timedelta(days=i)
            api_date = date_dt.strftime("%Y%m%d")

            api_plans.append(APICallPlan(
                api_endpoint=api_info["endpoint"],
                call_method=api_info["method"],
                parameters={"date": api_date},
                priority=req.priority,
                estimated_time=api_info["typical_response_time"],
                estimated_cost=self.cost_model["api_call_cost"]["user_daily"],
                data_volume_expected=api_info["data_volume"],
                depends_on=[],
                cache_strategy="standard",
                retry_strategy="batch_retry",
                validation_rules=["valid_user_data"]
            ))

        return api_plans[:10]  # é™åˆ¶ä¸ºæœ€è¿‘10å¤©ä»¥æ§åˆ¶æˆæœ¬

    def _create_product_end_interval_api_plan(self, req: DataRequirement) -> APICallPlan:
        """åˆ›å»ºäº§å“åŒºé—´åˆ°æœŸAPIè°ƒç”¨è®¡åˆ’"""

        api_info = self.api_catalog["product_end_interval"]

        start_date = req.processing_requirements.get("start_date", "")
        end_date = req.processing_requirements.get("end_date", "")

        # è½¬æ¢ä¸ºAPIæ ¼å¼
        if self.date_utils and start_date and end_date:
            api_start = self.date_utils.date_to_api_format(start_date)
            api_end = self.date_utils.date_to_api_format(end_date)
        else:
            # é»˜è®¤æœªæ¥ä¸€å‘¨
            today = datetime.now()
            week_end = today + timedelta(days=7)
            api_start = today.strftime("%Y%m%d")
            api_end = week_end.strftime("%Y%m%d")

        return APICallPlan(
            api_endpoint=api_info["endpoint"],
            call_method=api_info["method"],
            parameters={"start_date": api_start, "end_date": api_end},
            priority=req.priority,
            estimated_time=api_info["typical_response_time"],
            estimated_cost=self.cost_model["api_call_cost"]["product_end_interval"],
            data_volume_expected=api_info["data_volume"],
            depends_on=[],
            cache_strategy="extended",
            retry_strategy="standard",
            validation_rules=["valid_date_range", "non_negative_expiry_amounts"]
        )

    def _create_comprehensive_api_plans(self, req: DataRequirement) -> List[APICallPlan]:
        """åˆ›å»ºç»¼åˆæ•°æ®APIè°ƒç”¨è®¡åˆ’"""

        # ç»¼åˆæ•°æ®éœ€æ±‚åŒ…æ‹¬å¤šä¸ªåŸºç¡€APIè°ƒç”¨
        comprehensive_plans = []

        # æ·»åŠ ç³»ç»Ÿæ•°æ®
        comprehensive_plans.append(self._create_system_api_plan(req))

        # æ·»åŠ äº§å“æ•°æ®
        comprehensive_plans.append(self._create_product_api_plan(req))

        # æ·»åŠ æœ€è¿‘çš„æ¯æ—¥æ•°æ®
        recent_req = DataRequirement(
            requirement_id="comprehensive_recent_daily",
            requirement_type="recent_daily_for_comprehensive",
            data_source="daily",
            data_scope=DataScope.RANGE,
            freshness_requirement=req.freshness_requirement,
            priority=req.priority,
            quality_threshold=req.quality_threshold,
            fallback_options=req.fallback_options,
            processing_requirements={
                "start_date": (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d"),
                "end_date": datetime.now().strftime("%Y-%m-%d")
            },
            business_justification="ç»¼åˆåˆ†æçš„æœ€è¿‘æ•°æ®ç»„ä»¶"
        )
        comprehensive_plans.extend(self._create_daily_range_api_plans(recent_req))

        return comprehensive_plans

    def _analyze_api_dependencies(self, api_call_plans: List[APICallPlan]):
        """åˆ†æAPIè°ƒç”¨ä¾èµ–å…³ç³»"""

        # ç®€å•çš„ä¾èµ–å…³ç³»åˆ†æï¼š
        # 1. ç³»ç»Ÿæ•°æ®é€šå¸¸æ˜¯å…¶ä»–åˆ†æçš„åŸºç¡€
        # 2. äº§å“æ•°æ®åœ¨åˆ°æœŸåˆ†æä¹‹å‰éœ€è¦è·å–

        system_calls = [plan for plan in api_call_plans if "system" in plan.call_method]
        product_calls = [plan for plan in api_call_plans if
                         "product" in plan.call_method and "end" not in plan.call_method]
        expiry_calls = [plan for plan in api_call_plans if "product_end" in plan.call_method]

        # è®¾ç½®ä¾èµ–å…³ç³»
        for expiry_call in expiry_calls:
            if product_calls:
                expiry_call.depends_on = [product_calls[0].call_method]

        # å…¶ä»–è°ƒç”¨å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
        logger.info("ğŸ”— APIä¾èµ–å…³ç³»åˆ†æå®Œæˆ")

    # ============= æ‰§è¡Œç­–ç•¥ä¼˜åŒ–å±‚ =============

    async def _optimize_execution_strategy(self, api_call_plans: List[APICallPlan]) -> Dict[str, Any]:
        """ä¼˜åŒ–æ‰§è¡Œç­–ç•¥"""

        try:
            # ğŸ¯ åˆ†æå¹¶è¡Œæ‰§è¡Œå¯èƒ½æ€§
            parallel_groups = self._identify_parallel_groups(api_call_plans)

            # ğŸ¯ ç”Ÿæˆæ‰§è¡Œåºåˆ—
            execution_sequence = self._generate_execution_sequence(api_call_plans, parallel_groups)

            # ğŸ¯ ä¼˜åŒ–æ‰¹é‡è°ƒç”¨
            batch_optimizations = self._identify_batch_opportunities(api_call_plans)

            # ğŸ¯ ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
            cache_strategy = self._optimize_cache_strategy(api_call_plans)

            return {
                "execution_sequence": execution_sequence,
                "parallel_groups": parallel_groups,
                "batch_optimizations": batch_optimizations,
                "cache_strategy": cache_strategy,
                "estimated_total_time": self._calculate_optimized_time(api_call_plans, parallel_groups),
                "optimization_notes": self._generate_optimization_notes(parallel_groups, batch_optimizations)
            }

        except Exception as e:
            logger.error(f"æ‰§è¡Œç­–ç•¥ä¼˜åŒ–å¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€ç­–ç•¥
            return {
                "execution_sequence": [plan.call_method for plan in api_call_plans],
                "parallel_groups": [],
                "batch_optimizations": {},
                "cache_strategy": "standard",
                "estimated_total_time": sum(plan.estimated_time for plan in api_call_plans),
                "optimization_notes": ["ä½¿ç”¨åŸºç¡€é¡ºåºæ‰§è¡Œç­–ç•¥"]
            }

    def _identify_parallel_groups(self, api_call_plans: List[APICallPlan]) -> List[List[str]]:
        """è¯†åˆ«å¯å¹¶è¡Œæ‰§è¡Œçš„APIç»„"""

        parallel_groups = []
        processed = set()

        for plan in api_call_plans:
            if plan.call_method in processed:
                continue

            # æ‰¾åˆ°å¯ä»¥ä¸å½“å‰planå¹¶è¡Œæ‰§è¡Œçš„å…¶ä»–plan
            parallel_group = [plan.call_method]
            processed.add(plan.call_method)

            for other_plan in api_call_plans:
                if (other_plan.call_method not in processed and
                        not other_plan.depends_on and
                        not plan.depends_on and
                        other_plan.call_method != plan.call_method):
                    parallel_group.append(other_plan.call_method)
                    processed.add(other_plan.call_method)

            if len(parallel_group) > 1:
                parallel_groups.append(parallel_group)

        return parallel_groups

    def _generate_execution_sequence(self, api_call_plans: List[APICallPlan],
                                     parallel_groups: List[List[str]]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–çš„æ‰§è¡Œåºåˆ—"""

        sequence = []

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3, "optional": 4}
        sorted_plans = sorted(api_call_plans, key=lambda x: priority_order[x.priority.value])

        # æ„å»ºæ‰§è¡Œåºåˆ—
        processed = set()

        for plan in sorted_plans:
            if plan.call_method not in processed:
                sequence.append(plan.call_method)
                processed.add(plan.call_method)

        return sequence

    def _identify_batch_opportunities(self, api_call_plans: List[APICallPlan]) -> Dict[str, Any]:
        """è¯†åˆ«æ‰¹é‡è°ƒç”¨æœºä¼š"""

        batch_opportunities = {}

        # æŒ‰APIç«¯ç‚¹åˆ†ç»„
        endpoint_groups = {}
        for plan in api_call_plans:
            endpoint = plan.api_endpoint
            if endpoint not in endpoint_groups:
                endpoint_groups[endpoint] = []
            endpoint_groups[endpoint].append(plan)

        # è¯†åˆ«æ‰¹é‡æœºä¼š
        for endpoint, plans in endpoint_groups.items():
            if len(plans) > 1:
                batch_opportunities[endpoint] = {
                    "call_count": len(plans),
                    "estimated_savings": len(plans) * 0.1,  # å‡è®¾æ¯ä¸ªè°ƒç”¨èŠ‚çœ0.1ç§’
                    "batch_method": "parallel_batch" if len(plans) <= 10 else "sequential_batch"
                }

        return batch_opportunities

    def _optimize_cache_strategy(self, api_call_plans: List[APICallPlan]) -> str:
        """ä¼˜åŒ–ç¼“å­˜ç­–ç•¥"""

        cache_beneficial_count = sum(1 for plan in api_call_plans if plan.cache_strategy != "none")
        total_plans = len(api_call_plans)

        if cache_beneficial_count / total_plans > 0.7:
            return "aggressive_caching"
        elif cache_beneficial_count / total_plans > 0.3:
            return "standard_caching"
        else:
            return "minimal_caching"

    def _calculate_optimized_time(self, api_call_plans: List[APICallPlan],
                                  parallel_groups: List[List[str]]) -> float:
        """è®¡ç®—ä¼˜åŒ–åçš„æ€»æ‰§è¡Œæ—¶é—´"""

        total_time = 0
        processed = set()

        # è®¡ç®—å¹¶è¡Œç»„çš„æ—¶é—´
        for group in parallel_groups:
            group_times = []
            for call_method in group:
                plan = next((p for p in api_call_plans if p.call_method == call_method), None)
                if plan:
                    group_times.append(plan.estimated_time)
                    processed.add(call_method)

            if group_times:
                total_time += max(group_times)  # å¹¶è¡Œæ‰§è¡Œå–æœ€é•¿æ—¶é—´

        # è®¡ç®—å‰©ä½™é¡ºåºæ‰§è¡Œçš„æ—¶é—´
        for plan in api_call_plans:
            if plan.call_method not in processed:
                total_time += plan.estimated_time

        return total_time

    def _generate_optimization_notes(self, parallel_groups: List[List[str]],
                                     batch_optimizations: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""

        notes = []

        if parallel_groups:
            parallel_count = sum(len(group) for group in parallel_groups)
            notes.append(f"è¯†åˆ«åˆ°{len(parallel_groups)}ä¸ªå¹¶è¡Œç»„ï¼Œå…±{parallel_count}ä¸ªAPIè°ƒç”¨å¯å¹¶è¡Œæ‰§è¡Œ")

        if batch_optimizations:
            total_savings = sum(opt["estimated_savings"] for opt in batch_optimizations.values())
            notes.append(f"æ‰¹é‡è°ƒç”¨ä¼˜åŒ–å¯èŠ‚çœçº¦{total_savings:.1f}ç§’")

        notes.append("å»ºè®®å¯ç”¨æ™ºèƒ½ç¼“å­˜ä»¥æé«˜æ€§èƒ½")

        return notes

    # ============= æˆæœ¬åˆ†æå’Œæœ€ç»ˆæ„å»ºå±‚ =============

    def _analyze_cost_and_optimize(self, api_call_plans: List[APICallPlan],
                                   execution_strategy: Dict[str, Any]) -> Dict[str, Any]:
        """æˆæœ¬æ•ˆç›Šåˆ†æå’Œä¼˜åŒ–"""

        # è®¡ç®—åŸºç¡€æˆæœ¬
        total_cost = sum(plan.estimated_cost for plan in api_call_plans)

        # åº”ç”¨æ‰¹é‡ä¼˜åŒ–æŠ˜æ‰£
        batch_count = len(api_call_plans)
        if batch_count >= 50:
            cost_multiplier = self.cost_model["batch_cost_multiplier"]["50+"]
        elif batch_count >= 21:
            cost_multiplier = self.cost_model["batch_cost_multiplier"]["21-50"]
        elif batch_count >= 6:
            cost_multiplier = self.cost_model["batch_cost_multiplier"]["6-20"]
        else:
            cost_multiplier = self.cost_model["batch_cost_multiplier"]["1-5"]

        optimized_cost = total_cost * cost_multiplier

        # ç¼“å­˜èŠ‚çœ
        cache_savings = total_cost * self.cost_model["cache_cost_benefit"]

        # å¹¶è¡Œæ‰§è¡Œæ•ˆç‡æå‡
        parallel_efficiency = self.cost_model["parallel_efficiency"]

        return {
            "base_cost": total_cost,
            "optimized_cost": optimized_cost,
            "cost_savings": total_cost - optimized_cost + cache_savings,
            "efficiency_gain": parallel_efficiency,
            "optimization_recommendations": [
                "å¯ç”¨æ™ºèƒ½ç¼“å­˜",
                "ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œ",
                "è€ƒè™‘æ‰¹é‡è°ƒç”¨ä¼˜åŒ–"
            ]
        }

    def _build_acquisition_plan(self, query_summary: Dict[str, Any],
                                requirements: List[DataRequirement],
                                api_call_plans: List[APICallPlan],
                                execution_strategy: Dict[str, Any],
                                cost_analysis: Dict[str, Any]) -> DataAcquisitionPlan:
        """æ„å»ºæœ€ç»ˆçš„æ•°æ®è·å–è®¡åˆ’"""

        # è®¡ç®—è®¡åˆ’ç½®ä¿¡åº¦
        plan_confidence = self._calculate_plan_confidence(requirements, api_call_plans)

        # ç”ŸæˆæˆåŠŸæ ‡å‡†
        success_criteria = {
            "minimum_critical_data": "æ‰€æœ‰CRITICALä¼˜å…ˆçº§æ•°æ®å¿…é¡»æˆåŠŸè·å–",
            "data_quality_threshold": 0.8,
            "maximum_acceptable_failures": len([p for p in api_call_plans if p.priority.value in ["low", "optional"]]),
            "response_time_target": execution_strategy["estimated_total_time"] * 1.2
        }

        # ç”Ÿæˆé™çº§ç­–ç•¥
        fallback_strategies = {
            "api_failure_fallback": "ä½¿ç”¨ç¼“å­˜æ•°æ®æˆ–é™çº§åˆ°åŸºç¡€æ•°æ®é›†",
            "timeout_handling": "ä¼˜å…ˆè·å–CRITICALæ•°æ®ï¼Œè·³è¿‡OPTIONALæ•°æ®",
            "data_quality_fallback": "é™ä½è´¨é‡é˜ˆå€¼ä½†ä¿è¯æ ¸å¿ƒä¸šåŠ¡é€»è¾‘",
            "complete_failure_fallback": "è¿”å›åŸºç¡€ç³»ç»ŸçŠ¶æ€æ•°æ®"
        }

        return DataAcquisitionPlan(
            plan_id=f"plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            query_analysis_summary=query_summary,
            data_requirements=requirements,
            api_call_plans=api_call_plans,
            execution_sequence=execution_strategy["execution_sequence"],
            parallel_groups=execution_strategy["parallel_groups"],
            total_estimated_time=execution_strategy["estimated_total_time"],
            total_estimated_cost=cost_analysis["optimized_cost"],
            success_criteria=success_criteria,
            fallback_strategies=fallback_strategies,
            optimization_notes=execution_strategy["optimization_notes"],
            plan_confidence=plan_confidence
        )

    def _calculate_plan_confidence(self, requirements: List[DataRequirement],
                                   api_call_plans: List[APICallPlan]) -> float:
        """è®¡ç®—è®¡åˆ’ç½®ä¿¡åº¦"""

        # åŸºäºæ•°æ®éœ€æ±‚çš„é‡è¦æ€§å’ŒAPIè°ƒç”¨çš„å¯é æ€§è®¡ç®—ç½®ä¿¡åº¦
        total_weight = 0
        weighted_confidence = 0

        for req in requirements:
            weight = {"critical": 1.0, "high": 0.8, "medium": 0.6, "low": 0.4, "optional": 0.2}[req.priority.value]
            confidence = req.quality_threshold

            total_weight += weight
            weighted_confidence += weight * confidence

        return weighted_confidence / total_weight if total_weight > 0 else 0.5

    def _create_fallback_plan(self, query_analysis_result: Any, error: str) -> DataAcquisitionPlan:
        """åˆ›å»ºé™çº§æ•°æ®è·å–è®¡åˆ’"""

        logger.warning(f"åˆ›å»ºé™çº§è®¡åˆ’: {error}")

        # åˆ›å»ºåŸºç¡€çš„ç³»ç»Ÿæ•°æ®éœ€æ±‚
        fallback_requirement = DataRequirement(
            requirement_id="fallback_system_data",
            requirement_type="basic_system_status",
            data_source="system",
            data_scope=DataScope.POINT,
            freshness_requirement=DataFreshness.CURRENT,
            priority=DataPriority.CRITICAL,
            quality_threshold=0.7,
            fallback_options=[],
            processing_requirements={},
            business_justification="é™çº§æ–¹æ¡ˆçš„åŸºç¡€æ•°æ®"
        )

        # åˆ›å»ºåŸºç¡€APIè°ƒç”¨è®¡åˆ’
        fallback_api_plan = self._create_system_api_plan(fallback_requirement)

        return DataAcquisitionPlan(
            plan_id=f"fallback_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            query_analysis_summary={"error": error, "fallback": True},
            data_requirements=[fallback_requirement],
            api_call_plans=[fallback_api_plan],
            execution_sequence=[fallback_api_plan.call_method],
            parallel_groups=[],
            total_estimated_time=fallback_api_plan.estimated_time,
            total_estimated_cost=fallback_api_plan.estimated_cost,
            success_criteria={"minimum_data": "åŸºç¡€ç³»ç»ŸçŠ¶æ€"},
            fallback_strategies={"error_handling": "è¿”å›é”™è¯¯ä¿¡æ¯å’ŒåŸºç¡€æ•°æ®"},
            optimization_notes=["è¿™æ˜¯é”™è¯¯é™çº§è®¡åˆ’"],
            plan_confidence=0.3
        )

    # ============= å·¥å…·æ–¹æ³• =============

    def get_analysis_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        return self.analysis_stats.copy()

    def validate_acquisition_plan(self, plan: DataAcquisitionPlan) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®è·å–è®¡åˆ’"""

        validation_result = {
            "is_valid": True,
            "issues": [],
            "warnings": []
        }

        # æ£€æŸ¥å…³é”®æ•°æ®éœ€æ±‚
        critical_requirements = [req for req in plan.data_requirements if req.priority == DataPriority.CRITICAL]
        if not critical_requirements:
            validation_result["warnings"].append("æ²¡æœ‰CRITICALä¼˜å…ˆçº§çš„æ•°æ®éœ€æ±‚")

        # æ£€æŸ¥APIè°ƒç”¨è®¡åˆ’
        if not plan.api_call_plans:
            validation_result["is_valid"] = False
            validation_result["issues"].append("æ²¡æœ‰APIè°ƒç”¨è®¡åˆ’")

        # æ£€æŸ¥æ—¶é—´é¢„ä¼°åˆç†æ€§
        if plan.total_estimated_time > 300:  # 5åˆ†é’Ÿ
            validation_result["warnings"].append("é¢„ä¼°æ‰§è¡Œæ—¶é—´è¿‡é•¿")

        return validation_result


# ============= å·¥å‚å‡½æ•° =============

def create_data_requirements_analyzer(claude_client=None, gpt_client=None) -> DataRequirementsAnalyzer:
    """
    åˆ›å»ºæ•°æ®éœ€æ±‚åˆ†æå™¨å®ä¾‹

    Args:
        claude_client: Claudeå®¢æˆ·ç«¯å®ä¾‹
        gpt_client: GPTå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        DataRequirementsAnalyzer: æ•°æ®éœ€æ±‚åˆ†æå™¨å®ä¾‹
    """
    return DataRequirementsAnalyzer(claude_client, gpt_client)


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""

    # åˆ›å»ºæ•°æ®éœ€æ±‚åˆ†æå™¨
    analyzer = create_data_requirements_analyzer()

    print("=== æ•°æ®éœ€æ±‚åˆ†æå™¨æµ‹è¯• ===")

    # æ¨¡æ‹ŸæŸ¥è¯¢åˆ†æç»“æœ
    from dataclasses import dataclass
    from enum import Enum

    @dataclass
    class MockQueryResult:
        original_query: str = "è¿‡å»30å¤©å…¥é‡‘è¶‹åŠ¿åˆ†æ"
        complexity: Any = None
        query_type: Any = None
        business_scenario: Any = None
        confidence_score: float = 0.85
        time_requirements: Dict = None
        date_parse_result: Any = None
        business_parameters: Dict = None
        calculation_requirements: Dict = None

        def __post_init__(self):
            if self.time_requirements is None:
                self.time_requirements = {"complexity": "medium"}
            if self.business_parameters is None:
                self.business_parameters = {}
            if self.calculation_requirements is None:
                self.calculation_requirements = {}

    # åˆ›å»ºæ¨¡æ‹Ÿçš„æŸ¥è¯¢ç»“æœ
    mock_result = MockQueryResult()

    # åˆ†ææ•°æ®éœ€æ±‚
    acquisition_plan = await analyzer.analyze_data_requirements(mock_result)

    print(f"è®¡åˆ’ID: {acquisition_plan.plan_id}")
    print(f"æ•°æ®éœ€æ±‚æ•°é‡: {len(acquisition_plan.data_requirements)}")
    print(f"APIè°ƒç”¨æ•°é‡: {len(acquisition_plan.api_call_plans)}")
    print(f"é¢„ä¼°æ—¶é—´: {acquisition_plan.total_estimated_time:.1f}ç§’")
    print(f"è®¡åˆ’ç½®ä¿¡åº¦: {acquisition_plan.plan_confidence:.2f}")

    # éªŒè¯è®¡åˆ’
    validation = analyzer.validate_acquisition_plan(acquisition_plan)
    print(f"è®¡åˆ’éªŒè¯: {'é€šè¿‡' if validation['is_valid'] else 'å¤±è´¥'}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = analyzer.get_analysis_stats()
    print(f"æ€»åˆ†ææ¬¡æ•°: {stats['total_analyses']}")


if __name__ == "__main__":
    asyncio.run(main())