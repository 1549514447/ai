# core/data_orchestration/smart_data_fetcher.py
"""
ğŸš€ æ™ºèƒ½æ•°æ®è·å–æ‰§è¡Œå¼•æ“
å°†æ•°æ®éœ€æ±‚åˆ†æç»“æœè½¬åŒ–ä¸ºå®é™…çš„æ•°æ®è·å–è¡ŒåŠ¨

æ ¸å¿ƒç‰¹ç‚¹:
- æ‰§è¡Œå¤æ‚çš„æ•°æ®è·å–è®¡åˆ’
- æ™ºèƒ½APIè°ƒç”¨åè°ƒå’Œä¼˜åŒ–
- åŠ¨æ€æ‰§è¡Œç­–ç•¥è°ƒæ•´
- å®æ—¶æ•°æ®è´¨é‡ç›‘æ§
- å¤šçº§é™çº§å’Œå®¹é”™å¤„ç†
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import asyncio
import json
from dataclasses import dataclass
from enum import Enum
import time

# å¯¼å…¥æˆ‘ä»¬çš„ç»„ä»¶
from data.connectors.api_connector import APIConnector, create_enhanced_api_connector
from utils.data_transformers.time_series_builder import TimeSeriesBuilder, create_time_series_builder
from utils.helpers.validation_utils import ValidationUtils, create_validation_utils, ValidationLevel
from utils.helpers.date_utils import DateUtils, create_date_utils

logger = logging.getLogger(__name__)


class ExecutionStatus(Enum):
    """æ‰§è¡ŒçŠ¶æ€"""
    PENDING = "pending"  # ç­‰å¾…æ‰§è¡Œ
    RUNNING = "running"  # æ­£åœ¨æ‰§è¡Œ
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"  # æ‰§è¡Œå¤±è´¥
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ
    PARTIAL_SUCCESS = "partial_success"  # éƒ¨åˆ†æˆåŠŸ


class DataQualityLevel(Enum):
    """æ•°æ®è´¨é‡ç­‰çº§"""
    EXCELLENT = "excellent"  # ä¼˜ç§€ (>95%å®Œæ•´æ€§)
    GOOD = "good"  # è‰¯å¥½ (80-95%å®Œæ•´æ€§)
    ACCEPTABLE = "acceptable"  # å¯æ¥å— (60-80%å®Œæ•´æ€§)
    POOR = "poor"  # è¾ƒå·® (<60%å®Œæ•´æ€§)
    INSUFFICIENT = "insufficient"  # ä¸è¶³ (æ— æ³•ä½¿ç”¨)


@dataclass
class ExecutionResult:
    """æ‰§è¡Œç»“æœ"""
    result_id: str  # ç»“æœID
    execution_status: ExecutionStatus  # æ‰§è¡ŒçŠ¶æ€
    data_quality: DataQualityLevel  # æ•°æ®è´¨é‡

    # æ•°æ®å†…å®¹
    fetched_data: Dict[str, Any]  # è·å–çš„æ•°æ®
    processed_data: Dict[str, Any]  # å¤„ç†åçš„æ•°æ®
    time_series_data: Dict[str, Any]  # æ—¶é—´åºåˆ—æ•°æ®

    # æ‰§è¡Œç»Ÿè®¡
    execution_metadata: Dict[str, Any]  # æ‰§è¡Œå…ƒæ•°æ®
    api_call_results: List[Dict[str, Any]]  # APIè°ƒç”¨ç»“æœ
    timing_info: Dict[str, float]  # æ—¶é—´ä¿¡æ¯
    error_info: Dict[str, Any]  # é”™è¯¯ä¿¡æ¯

    # è´¨é‡æŒ‡æ ‡
    data_completeness: float  # æ•°æ®å®Œæ•´æ€§
    accuracy_score: float  # å‡†ç¡®æ€§è¯„åˆ†
    freshness_score: float  # æ–°é²œåº¦è¯„åˆ†

    # ä¸šåŠ¡æŒ‡æ ‡
    business_value_score: float  # ä¸šåŠ¡ä»·å€¼è¯„åˆ†
    confidence_level: float  # ç½®ä¿¡åº¦


@dataclass
class ExecutionProgress:
    """æ‰§è¡Œè¿›åº¦"""
    total_steps: int  # æ€»æ­¥éª¤æ•°
    completed_steps: int  # å·²å®Œæˆæ­¥éª¤
    current_step: str  # å½“å‰æ­¥éª¤
    progress_percentage: float  # è¿›åº¦ç™¾åˆ†æ¯”
    estimated_remaining_time: float  # é¢„ä¼°å‰©ä½™æ—¶é—´
    current_operation: str  # å½“å‰æ“ä½œæè¿°


class SmartDataFetcher:
    """
    ğŸš€ æ™ºèƒ½æ•°æ®è·å–æ‰§è¡Œå¼•æ“

    åŠŸèƒ½æ¶æ„:
    1. æ•°æ®è·å–è®¡åˆ’æ‰§è¡Œ
    2. æ™ºèƒ½APIè°ƒç”¨åè°ƒ
    3. åŠ¨æ€æ‰§è¡Œä¼˜åŒ–
    4. å®æ—¶è´¨é‡ç›‘æ§
    """

    def __init__(self, claude_client=None, gpt_client=None, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æ•°æ®è·å–å™¨

        Args:
            claude_client: Claudeå®¢æˆ·ç«¯ï¼Œç”¨äºæ•°æ®åˆ†æ
            gpt_client: GPTå®¢æˆ·ç«¯ï¼Œç”¨äºæ•°æ®éªŒè¯
            config: é…ç½®å‚æ•°
        """
        self.claude_client = claude_client
        self.gpt_client = gpt_client

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.api_connector = create_enhanced_api_connector(config, claude_client, gpt_client)
        self.time_series_builder = create_time_series_builder(claude_client, gpt_client)
        self.validator = create_validation_utils(claude_client, gpt_client)
        self.date_utils = create_date_utils(claude_client)

        # é…ç½®å‚æ•°
        self.config = config or self._load_default_config()

        # æ‰§è¡ŒçŠ¶æ€è·Ÿè¸ª
        self.current_executions = {}  # å½“å‰æ‰§è¡Œçš„ä»»åŠ¡
        self.execution_history = []  # æ‰§è¡Œå†å²

        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_executions': 0,
            'successful_executions': 0,
            'failed_executions': 0,
            'partial_success_executions': 0,
            'average_execution_time': 0.0,
            'average_data_quality': 0.0,
            'api_call_success_rate': 0.0,
            'cache_hit_rate': 0.0
        }

        logger.info("SmartDataFetcher initialized with intelligent orchestration capabilities")

    def _load_default_config(self) -> Dict[str, Any]:
        """åŠ è½½é»˜è®¤é…ç½®"""
        return {
            # æ‰§è¡Œæ§åˆ¶
            'max_concurrent_executions': 5,
            'execution_timeout': 300,  # 5åˆ†é’Ÿè¶…æ—¶
            'retry_max_attempts': 3,
            'retry_delay': 2.0,

            # æ•°æ®è´¨é‡æ§åˆ¶
            'min_data_quality_threshold': 0.6,
            'enable_data_validation': True,
            'enable_time_series_processing': True,

            # æ€§èƒ½ä¼˜åŒ–
            'enable_parallel_execution': True,
            'enable_intelligent_caching': True,
            'enable_batch_optimization': True,

            # é”™è¯¯å¤„ç†
            'enable_graceful_degradation': True,
            'fallback_to_basic_data': True,
            'continue_on_partial_failure': True
        }

    # ============= æ ¸å¿ƒæ•°æ®è·å–æ–¹æ³• =============

    async def execute_data_acquisition_plan(self, acquisition_plan: Any,
                                            progress_callback: Optional[callable] = None) -> ExecutionResult:
        """
        ğŸ¯ æ‰§è¡Œæ•°æ®è·å–è®¡åˆ’ - æ ¸å¿ƒå…¥å£æ–¹æ³•

        Args:
            acquisition_plan: æ•°æ®è·å–è®¡åˆ’ (æ¥è‡ªDataRequirementsAnalyzer)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ExecutionResult: å®Œæ•´çš„æ‰§è¡Œç»“æœ
        """
        try:
            execution_id = f"exec_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®è·å–è®¡åˆ’: {execution_id}")

            start_time = time.time()
            self.performance_stats['total_executions'] += 1

            # åˆå§‹åŒ–æ‰§è¡Œè·Ÿè¸ª
            execution_progress = ExecutionProgress(
                total_steps=len(acquisition_plan.api_call_plans) + 2,  # APIè°ƒç”¨ + é¢„å¤„ç† + éªŒè¯
                completed_steps=0,
                current_step="initialization",
                progress_percentage=0.0,
                estimated_remaining_time=acquisition_plan.total_estimated_time,
                current_operation="åˆå§‹åŒ–æ‰§è¡Œç¯å¢ƒ"
            )

            self.current_executions[execution_id] = execution_progress

            # ç¬¬1æ­¥: æ‰§è¡Œå‰éªŒè¯å’Œå‡†å¤‡
            await self._update_progress(execution_id, "validation", "æ‰§è¡Œå‰éªŒè¯", progress_callback)
            validation_result = await self._pre_execution_validation(acquisition_plan)

            if not validation_result["is_valid"]:
                return await self._handle_validation_failure(execution_id, validation_result)

            # ç¬¬2æ­¥: æ‰§è¡ŒAPIè°ƒç”¨è®¡åˆ’
            await self._update_progress(execution_id, "api_execution", "æ‰§è¡ŒAPIè°ƒç”¨", progress_callback)
            api_execution_result = await self._execute_api_calls(
                acquisition_plan, execution_id, progress_callback
            )

            # ç¬¬3æ­¥: æ•°æ®é¢„å¤„ç†å’Œæ—¶é—´åºåˆ—æ„å»º
            await self._update_progress(execution_id, "data_processing", "æ•°æ®é¢„å¤„ç†", progress_callback)
            processing_result = await self._process_fetched_data(
                api_execution_result, acquisition_plan, execution_id
            )

            # ç¬¬4æ­¥: æ•°æ®è´¨é‡è¯„ä¼°
            await self._update_progress(execution_id, "quality_assessment", "æ•°æ®è´¨é‡è¯„ä¼°", progress_callback)
            quality_assessment = await self._assess_data_quality(
                processing_result, acquisition_plan
            )

            # ç¬¬5æ­¥: æ„å»ºæœ€ç»ˆæ‰§è¡Œç»“æœ
            await self._update_progress(execution_id, "finalization", "æ„å»ºæ‰§è¡Œç»“æœ", progress_callback)
            execution_result = await self._build_execution_result(
                execution_id, acquisition_plan, api_execution_result,
                processing_result, quality_assessment, start_time
            )

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_performance_stats(execution_result)

            # æ¸…ç†æ‰§è¡ŒçŠ¶æ€
            del self.current_executions[execution_id]
            self.execution_history.append({
                'execution_id': execution_id,
                'completion_time': datetime.now().isoformat(),
                'status': execution_result.execution_status.value,
                'data_quality': execution_result.data_quality.value,
                'execution_time': time.time() - start_time
            })

            logger.info(f"âœ… æ•°æ®è·å–æ‰§è¡Œå®Œæˆ: {execution_id}, çŠ¶æ€: {execution_result.execution_status.value}")

            return execution_result

        except Exception as e:
            logger.error(f"âŒ æ•°æ®è·å–æ‰§è¡Œå¤±è´¥: {str(e)}")
            return await self._handle_execution_error(execution_id, str(e),
                                                      start_time if 'start_time' in locals() else time.time())

    # ============= APIè°ƒç”¨æ‰§è¡Œå±‚ =============

    async def _execute_api_calls(self, acquisition_plan: Any, execution_id: str,
                                 progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒAPIè°ƒç”¨è®¡åˆ’"""

        try:
            logger.info(f"ğŸ“¡ æ‰§è¡ŒAPIè°ƒç”¨è®¡åˆ’: {len(acquisition_plan.api_call_plans)}ä¸ªè°ƒç”¨")

            api_results = {}
            failed_calls = []
            successful_calls = []

            # ğŸ¯ æ ¹æ®æ‰§è¡Œç­–ç•¥é€‰æ‹©æ‰§è¡Œæ–¹å¼
            if self.config.get('enable_parallel_execution', True) and acquisition_plan.parallel_groups:
                # å¹¶è¡Œæ‰§è¡Œç­–ç•¥
                api_results = await self._execute_parallel_api_calls(
                    acquisition_plan, execution_id, progress_callback
                )
            else:
                # é¡ºåºæ‰§è¡Œç­–ç•¥
                api_results = await self._execute_sequential_api_calls(
                    acquisition_plan, execution_id, progress_callback
                )

            # åˆ†ææ‰§è¡Œç»“æœ
            for call_id, result in api_results.items():
                if result.get('success', False):
                    successful_calls.append(call_id)
                else:
                    error_message = result.get('error', 'Unknown error')
                    # åŒºåˆ†å…³é”®é”™è¯¯å’Œéå…³é”®é”™è¯¯
                    is_critical_error = True
                    
                    # éå…³é”®é”™è¯¯ç±»å‹åˆ¤æ–­
                    non_critical_errors = [
                        "Event loop is closed",
                        "cancelled",
                        "timeout",
                        "asyncio.CancelledError",
                        "concurrent.futures.CancelledError"
                    ]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯éå…³é”®é”™è¯¯
                    if any(err_type in error_message for err_type in non_critical_errors):
                        is_critical_error = False
                    
                    failed_calls.append({
                        'call_id': call_id,
                        'error': error_message,
                        'retry_attempted': result.get('retry_attempted', False),
                        'is_critical_error': is_critical_error
                    })

            # ğŸ” è¯„ä¼°æ‰§è¡Œç»“æœ
            execution_summary = {
                'total_calls': len(acquisition_plan.api_call_plans),
                'successful_calls': len(successful_calls),
                'failed_calls': len(failed_calls),
                'success_rate': len(successful_calls) / len(
                    acquisition_plan.api_call_plans) if acquisition_plan.api_call_plans else 0,
                'api_results': api_results,
                'failed_call_details': failed_calls
            }

            # è®¡ç®—å…³é”®é”™è¯¯çš„æ¯”ä¾‹
            critical_errors = [call for call in failed_calls if call.get('is_critical_error', True)]
            critical_error_rate = len(critical_errors) / len(acquisition_plan.api_call_plans) if acquisition_plan.api_call_plans else 0
            execution_summary['critical_error_rate'] = critical_error_rate

            # ç§»é™¤é™çº§å¤„ç†é€»è¾‘ï¼ŒAPIä¸ä¼šå­˜åœ¨æ‰çº¿æƒ…å†µ
            if critical_error_rate > 0:
                logger.info(f"APIè°ƒç”¨é”™è¯¯ç‡: {critical_error_rate:.1%}ï¼Œä½†APIä¸ä¼šæ‰çº¿ï¼Œç»§ç»­å¤„ç†")

            return execution_summary

        except Exception as e:
            logger.error(f"âŒ APIè°ƒç”¨æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {
                'total_calls': len(acquisition_plan.api_call_plans) if acquisition_plan.api_call_plans else 0,
                'successful_calls': 0,
                'failed_calls': len(acquisition_plan.api_call_plans) if acquisition_plan.api_call_plans else 0,
                'success_rate': 0.0,
                'api_results': {},
                'execution_error': str(e)
            }

    async def _execute_parallel_api_calls(self, acquisition_plan: Any, execution_id: str,
                                          progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡ŒAPIè°ƒç”¨"""

        api_results = {}

        try:
            # ğŸ¯ æŒ‰å¹¶è¡Œç»„æ‰§è¡Œ
            for group_index, parallel_group in enumerate(acquisition_plan.parallel_groups):
                logger.info(
                    f"ğŸ”„ æ‰§è¡Œå¹¶è¡Œç»„ {group_index + 1}/{len(acquisition_plan.parallel_groups)}: {len(parallel_group)}ä¸ªè°ƒç”¨")

                # ä¸ºå½“å‰ç»„åˆ›å»ºè°ƒç”¨ä»»åŠ¡
                group_tasks = []
                group_call_plans = []

                for call_method in parallel_group:
                    # æ‰¾åˆ°å¯¹åº”çš„APIè°ƒç”¨è®¡åˆ’
                    call_plan = next(
                        (plan for plan in acquisition_plan.api_call_plans if plan.call_method == call_method),
                        None
                    )

                    if call_plan:
                        task = self._execute_single_api_call(call_plan, execution_id)
                        group_tasks.append(task)
                        group_call_plans.append(call_plan)

                # å¹¶è¡Œæ‰§è¡Œå½“å‰ç»„çš„æ‰€æœ‰è°ƒç”¨
                group_results = await asyncio.gather(*group_tasks, return_exceptions=True)

                # å¤„ç†ç»„æ‰§è¡Œç»“æœ
                for i, result in enumerate(group_results):
                    call_plan = group_call_plans[i]
                    call_id = call_plan.call_method

                    if isinstance(result, Exception):
                        api_results[call_id] = {
                            'success': False,
                            'error': str(result),
                            'call_plan': call_plan.__dict__ if hasattr(call_plan, '__dict__') else str(call_plan)
                        }
                    else:
                        api_results[call_id] = result

                # æ›´æ–°è¿›åº¦
                completed_groups = group_index + 1
                progress = (completed_groups / len(acquisition_plan.parallel_groups)) * 0.6  # APIè°ƒç”¨å æ€»è¿›åº¦çš„60%
                await self._update_execution_progress(execution_id, progress, f"å®Œæˆå¹¶è¡Œç»„ {completed_groups}")

            # ğŸ¯ å¤„ç†ä¸åœ¨å¹¶è¡Œç»„ä¸­çš„å‰©ä½™è°ƒç”¨ï¼ˆé¡ºåºæ‰§è¡Œï¼‰
            remaining_calls = [
                plan for plan in acquisition_plan.api_call_plans
                if plan.call_method not in [call for group in acquisition_plan.parallel_groups for call in group]
            ]

            for call_plan in remaining_calls:
                result = await self._execute_single_api_call(call_plan, execution_id)
                api_results[call_plan.call_method] = result

            return api_results

        except Exception as e:
            logger.error(f"âŒ å¹¶è¡ŒAPIè°ƒç”¨æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

    async def _execute_sequential_api_calls(self, acquisition_plan: Any, execution_id: str,
                                            progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """é¡ºåºæ‰§è¡ŒAPIè°ƒç”¨"""

        api_results = {}

        try:
            total_calls = len(acquisition_plan.api_call_plans)

            for i, call_plan in enumerate(acquisition_plan.api_call_plans):
                logger.info(f"ğŸ“ æ‰§è¡ŒAPIè°ƒç”¨ {i + 1}/{total_calls}: {call_plan.call_method}")

                # æ‰§è¡Œå•ä¸ªAPIè°ƒç”¨
                result = await self._execute_single_api_call(call_plan, execution_id)
                api_results[call_plan.call_method] = result

                # æ›´æ–°è¿›åº¦
                progress = ((i + 1) / total_calls) * 0.6  # APIè°ƒç”¨å æ€»è¿›åº¦çš„60%
                await self._update_execution_progress(execution_id, progress, f"å®ŒæˆAPIè°ƒç”¨ {i + 1}/{total_calls}")

                # å¦‚æœæ˜¯å…³é”®è°ƒç”¨å¤±è´¥ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ç»§ç»­
                if (not result.get('success', False) and
                        hasattr(call_plan, 'priority') and
                        call_plan.priority.value == 'critical' and
                        not self.config.get('continue_on_partial_failure', True)):
                    logger.error(f"å…³é”®APIè°ƒç”¨å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ: {call_plan.call_method}")
                    break

            return api_results

        except Exception as e:
            logger.error(f"âŒ é¡ºåºAPIè°ƒç”¨æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

    async def _execute_single_api_call(self, call_plan: Any, execution_id: str) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªAPIè°ƒç”¨"""

        try:
            start_time = time.time()

            # ğŸ¯ æ ¹æ®APIæ–¹æ³•é€‰æ‹©è°ƒç”¨æ–¹å¼
            if call_plan.call_method == "get_system_data":
                result = await self.api_connector.get_system_data()

            elif call_plan.call_method == "get_daily_data":
                date = call_plan.parameters.get('date', '')
                result = await self.api_connector.get_daily_data(date)

            elif call_plan.call_method == "get_product_data":
                result = await self.api_connector.get_product_data()

            elif call_plan.call_method == "get_user_daily_data":
                date = call_plan.parameters.get('date', '')
                result = await self.api_connector.get_user_daily_data(date)

            elif call_plan.call_method == "get_user_data":
                page = call_plan.parameters.get('page', 1)
                result = await self.api_connector.get_user_data(page)

            elif call_plan.call_method == "get_product_end_data":
                date = call_plan.parameters.get('date', '')
                result = await self.api_connector.get_product_end_data(date)

            elif call_plan.call_method == "get_product_end_interval":
                start_date = call_plan.parameters.get('start_date', '')
                end_date = call_plan.parameters.get('end_date', '')
                result = await self.api_connector.get_product_end_interval(start_date, end_date)

            elif call_plan.call_method == "intelligent_data_fetch":
                # ä½¿ç”¨æ™ºèƒ½æ•°æ®è·å–
                query_analysis = call_plan.parameters.get('query_analysis', {})
                result = await self.api_connector.intelligent_data_fetch(query_analysis)

            else:
                logger.warning(f"æœªçŸ¥çš„APIè°ƒç”¨æ–¹æ³•: {call_plan.call_method}")
                result = {'success': False, 'error': f'Unknown API method: {call_plan.call_method}'}

            execution_time = time.time() - start_time

            # ğŸ” éªŒè¯APIè°ƒç”¨ç»“æœ
            if self.config.get('enable_data_validation', True):
                validation_result = await self._validate_api_result(result, call_plan)
                result['validation'] = validation_result

            # æ·»åŠ æ‰§è¡Œå…ƒæ•°æ®
            result['execution_metadata'] = {
                'call_method': call_plan.call_method,
                'execution_time': execution_time,
                'execution_id': execution_id,
                'parameters': call_plan.parameters,
                'timestamp': datetime.now().isoformat()
            }

            return result

        except asyncio.CancelledError as e:
            logger.warning(f"âš ï¸ APIè°ƒç”¨è¢«å–æ¶ˆ {call_plan.call_method}: {str(e)}")
            return {
                'success': False,
                'error': f'API call cancelled: {str(e)}',
                'error_type': 'cancelled',
                'call_method': call_plan.call_method,
                'execution_metadata': {
                    'call_method': call_plan.call_method,
                    'error_time': datetime.now().isoformat(),
                    'execution_id': execution_id
                }
            }
        except asyncio.TimeoutError as e:
            logger.warning(f"âš ï¸ APIè°ƒç”¨è¶…æ—¶ {call_plan.call_method}: {str(e)}")
            return {
                'success': False,
                'error': f'API call timeout: {str(e)}',
                'error_type': 'timeout',
                'call_method': call_plan.call_method,
                'execution_metadata': {
                    'call_method': call_plan.call_method,
                    'error_time': datetime.now().isoformat(),
                    'execution_id': execution_id
                }
            }
        except RuntimeError as e:
            if "Event loop is closed" in str(e):
                logger.warning(f"âš ï¸ äº‹ä»¶å¾ªç¯å·²å…³é—­ {call_plan.call_method}: {str(e)}")
                return {
                    'success': False,
                    'error': f'Event loop is closed: {str(e)}',
                    'error_type': 'event_loop_closed',
                    'call_method': call_plan.call_method,
                    'execution_metadata': {
                        'call_method': call_plan.call_method,
                        'error_time': datetime.now().isoformat(),
                        'execution_id': execution_id
                    }
                }
            else:
                logger.error(f"âŒ APIè°ƒç”¨è¿è¡Œæ—¶é”™è¯¯ {call_plan.call_method}: {str(e)}")
                return {
                    'success': False,
                    'error': f'Runtime error: {str(e)}',
                    'error_type': 'runtime_error',
                    'call_method': call_plan.call_method,
                    'execution_metadata': {
                        'call_method': call_plan.call_method,
                        'error_time': datetime.now().isoformat(),
                        'execution_id': execution_id
                    }
                }
        except Exception as e:
            logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥ {call_plan.call_method}: {str(e)}")

            # ğŸ”„ é‡è¯•æœºåˆ¶
            if hasattr(call_plan, 'retry_strategy') and call_plan.retry_strategy != 'none':
                return await self._retry_api_call(call_plan, execution_id, str(e))

            return {
                'success': False,
                'error': str(e),
                'error_type': 'general_error',
                'call_method': call_plan.call_method,
                'execution_metadata': {
                    'call_method': call_plan.call_method,
                    'error_time': datetime.now().isoformat(),
                    'execution_id': execution_id
                }
            }

    async def _retry_api_call(self, call_plan: Any, execution_id: str, original_error: str) -> Dict[str, Any]:
        """é‡è¯•APIè°ƒç”¨"""

        max_retries = self.config.get('retry_max_attempts', 3)
        retry_delay = self.config.get('retry_delay', 2.0)

        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ é‡è¯•APIè°ƒç”¨ {call_plan.call_method} (å°è¯• {attempt + 1}/{max_retries})")

                await asyncio.sleep(retry_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ

                result = await self._execute_single_api_call(call_plan, execution_id)

                if result.get('success', False):
                    result['retry_info'] = {
                        'retry_attempted': True,
                        'retry_attempt': attempt + 1,
                        'original_error': original_error
                    }
                    return result

            except Exception as e:
                logger.warning(f"é‡è¯•å¤±è´¥ {attempt + 1}: {str(e)}")
                continue

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return {
            'success': False,
            'error': f'All {max_retries} retry attempts failed. Original error: {original_error}',
            'error_type': 'retry_failed',
            'retry_attempted': True,
            'max_retries_exceeded': True,
            'call_method': call_plan.call_method
        }

    # ============= æ•°æ®å¤„ç†å±‚ =============

    async def _process_fetched_data(self, api_execution_result: Dict[str, Any],
                                    acquisition_plan: Any, execution_id: str) -> Dict[str, Any]:
        """å¤„ç†è·å–çš„æ•°æ®"""

        try:
            logger.info("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†")

            processing_result = {
                'processed_data': {},
                'time_series_data': {},
                'processing_metadata': {},
                'processing_errors': []
            }

            # ğŸ¯ æå–æˆåŠŸè·å–çš„æ•°æ®
            successful_data = {}
            for call_id, result in api_execution_result.get('api_results', {}).items():
                if result.get('success', False) and result.get('data'):
                    successful_data[call_id] = result['data']

            if not successful_data:
                logger.warning("æ²¡æœ‰æˆåŠŸè·å–çš„æ•°æ®è¿›è¡Œå¤„ç†")
                return processing_result

            # ğŸ¯ æ•°æ®æ ¼å¼åŒ–å’Œæ ‡å‡†åŒ–
            processed_data = await self._standardize_data_format(successful_data)
            processing_result['processed_data'] = processed_data

            # ğŸ¯ æ„å»ºæ—¶é—´åºåˆ—æ•°æ® (å¦‚æœå¯ç”¨)
            if self.config.get('enable_time_series_processing', True):
                time_series_data = await self._build_time_series_data(processed_data, acquisition_plan)
                processing_result['time_series_data'] = time_series_data

            # ğŸ¯ æ•°æ®å¢å¼ºå’Œè®¡ç®—
            enhanced_data = await self._enhance_processed_data(processed_data, acquisition_plan)
            processing_result['enhanced_data'] = enhanced_data

            # æ·»åŠ å¤„ç†å…ƒæ•°æ®
            processing_result['processing_metadata'] = {
                'processing_time': datetime.now().isoformat(),
                'data_sources_processed': len(successful_data),
                'time_series_built': bool(processing_result.get('time_series_data')),
                'enhancement_applied': bool(enhanced_data),
                'execution_id': execution_id
            }

            logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(successful_data)}ä¸ªæ•°æ®æº")
            return processing_result

        except Exception as e:
            logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return {
                'processed_data': {},
                'time_series_data': {},
                'processing_metadata': {'error': str(e)},
                'processing_errors': [str(e)]
            }

    async def _standardize_data_format(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–æ•°æ®æ ¼å¼"""

        standardized = {}

        for data_source, data_content in raw_data.items():
            try:
                if 'system' in data_source:
                    standardized['system_data'] = self._standardize_system_data(data_content)
                elif 'daily' in data_source:
                    if 'daily_data' not in standardized:
                        standardized['daily_data'] = []
                    standardized['daily_data'].append(self._standardize_daily_data(data_content))
                elif 'product' in data_source and 'end' not in data_source:
                    standardized['product_data'] = self._standardize_product_data(data_content)
                elif 'product_end' in data_source:
                    if 'expiry_data' not in standardized:
                        standardized['expiry_data'] = []
                    standardized['expiry_data'].append(self._standardize_expiry_data(data_content))
                elif 'user' in data_source:
                    if 'user_data' not in standardized:
                        standardized['user_data'] = []
                    standardized['user_data'].append(self._standardize_user_data(data_content))
                else:
                    standardized[f'other_{data_source}'] = data_content

            except Exception as e:
                logger.warning(f"æ•°æ®æ ‡å‡†åŒ–å¤±è´¥ {data_source}: {str(e)}")
                continue

        return standardized

    def _standardize_system_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–ç³»ç»Ÿæ•°æ®"""
        return {
            'total_balance': float(data.get('æ€»ä½™é¢', 0)),
            'total_inflow': float(data.get('æ€»å…¥é‡‘', 0)),
            'total_outflow': float(data.get('æ€»å‡ºé‡‘', 0)),
            'total_investment': float(data.get('æ€»æŠ•èµ„é‡‘é¢', 0)),
            'total_rewards': float(data.get('æ€»å¥–åŠ±å‘æ”¾', 0)),
            'user_stats': data.get('ç”¨æˆ·ç»Ÿè®¡', {}),
            'product_stats': data.get('äº§å“ç»Ÿè®¡', {}),
            'expiry_overview': data.get('åˆ°æœŸæ¦‚è§ˆ', {}),
            'timestamp': datetime.now().isoformat()
        }

    def _standardize_daily_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–æ¯æ—¥æ•°æ®"""
        return {
            'date': data.get('æ—¥æœŸ', ''),
            'new_registrations': int(data.get('æ³¨å†Œäººæ•°', 0)),
            'active_holdings': int(data.get('æŒä»“äººæ•°', 0)),
            'products_purchased': int(data.get('è´­ä¹°äº§å“æ•°é‡', 0)),
            'products_expired': int(data.get('åˆ°æœŸäº§å“æ•°é‡', 0)),
            'daily_inflow': float(data.get('å…¥é‡‘', 0)),
            'daily_outflow': float(data.get('å‡ºé‡‘', 0)),
            'net_flow': float(data.get('å…¥é‡‘', 0)) - float(data.get('å‡ºé‡‘', 0)),
            'timestamp': datetime.now().isoformat()
        }

    def _standardize_product_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–äº§å“æ•°æ®"""
        return {
            'total_products': data.get('äº§å“æ€»æ•°', 0),
            'product_list': data.get('äº§å“åˆ—è¡¨', []),
            'processed_at': datetime.now().isoformat()
        }

    def _standardize_expiry_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–åˆ°æœŸæ•°æ®"""
        return {
            'date_range': data.get('æ—¥æœŸ', ''),
            'expiry_quantity': int(data.get('åˆ°æœŸæ•°é‡', 0)),
            'expiry_amount': float(data.get('åˆ°æœŸé‡‘é¢', 0)),
            'product_breakdown': data.get('äº§å“åˆ—è¡¨', []),
            'processed_at': datetime.now().isoformat()
        }

    def _standardize_user_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–ç”¨æˆ·æ•°æ®"""
        if 'user_list' in data or 'ç”¨æˆ·åˆ—è¡¨' in data:
            # è¯¦ç»†ç”¨æˆ·æ•°æ®
            return {
                'total_records': data.get('æ€»è®°å½•æ•°', 0),
                'current_page': data.get('å½“å‰é¡µ', 1),
                'user_details': data.get('ç”¨æˆ·åˆ—è¡¨', []),
                'data_type': 'detailed_user_data'
            }
        else:
            # æ¯æ—¥ç”¨æˆ·æ•°æ®
            return {
                'daily_data': data.get('æ¯æ—¥æ•°æ®', []),
                'data_type': 'daily_user_data',
                'processed_at': datetime.now().isoformat()
            }

    async def _build_time_series_data(self, processed_data: Dict[str, Any],
                                      acquisition_plan: Any) -> Dict[str, Any]:
        """æ„å»ºæ—¶é—´åºåˆ—æ•°æ®"""

        try:
            time_series_result = {}

            # ğŸ¯ æ„å»ºæ¯æ—¥æŒ‡æ ‡æ—¶é—´åºåˆ—
            if 'daily_data' in processed_data:
                daily_data_list = processed_data['daily_data']

                # æ„å»ºä¸»è¦æŒ‡æ ‡çš„æ—¶é—´åºåˆ—
                key_metrics = ['daily_inflow', 'daily_outflow', 'new_registrations', 'active_holdings']

                for metric in key_metrics:
                    try:
                        series_result = await self.time_series_builder.build_daily_time_series(
                            daily_data_list, metric, 'date'
                        )

                        if series_result.get('success'):
                            time_series_result[f'{metric}_series'] = series_result

                    except Exception as e:
                        logger.warning(f"æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥ {metric}: {str(e)}")
                        continue

                # ğŸ¯ æ„å»ºå¤šæŒ‡æ ‡æ—¶é—´åºåˆ—å¯¹æ¯”
                try:
                    multi_series_result = await self.time_series_builder.build_multi_metric_series(
                        daily_data_list, key_metrics, 'date'
                    )

                    if multi_series_result.get('success'):
                        time_series_result['multi_metric_analysis'] = multi_series_result

                except Exception as e:
                    logger.warning(f"å¤šæŒ‡æ ‡æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {str(e)}")

            return time_series_result

        except Exception as e:
            logger.error(f"âŒ æ—¶é—´åºåˆ—æ•°æ®æ„å»ºå¤±è´¥: {str(e)}")
            return {}

    async def _enhance_processed_data(self, processed_data: Dict[str, Any],
                                      acquisition_plan: Any) -> Dict[str, Any]:
        """å¢å¼ºå¤„ç†åçš„æ•°æ®"""

        try:
            enhanced_data = {}

            # ğŸ¯ è®¡ç®—åŸºç¡€è´¢åŠ¡æŒ‡æ ‡
            if 'system_data' in processed_data:
                system_data = processed_data['system_data']
                enhanced_data['financial_ratios'] = self._calculate_financial_ratios(system_data)

            # ğŸ¯ è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
            if 'daily_data' in processed_data:
                daily_data = processed_data['daily_data']
                enhanced_data['trend_indicators'] = self._calculate_trend_indicators(daily_data)

            # ğŸ¯ è®¡ç®—ç”¨æˆ·æŒ‡æ ‡
            if 'user_data' in processed_data:
                user_data = processed_data['user_data']
                enhanced_data['user_metrics'] = self._calculate_user_metrics(user_data)

            # ğŸ¯ è®¡ç®—äº§å“è¡¨ç°æŒ‡æ ‡
            if 'product_data' in processed_data:
                product_data = processed_data['product_data']
                enhanced_data['product_performance'] = self._calculate_product_performance(product_data)

            return enhanced_data

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¢å¼ºå¤±è´¥: {str(e)}")
            return {}

    def _calculate_financial_ratios(self, system_data: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—è´¢åŠ¡æ¯”ç‡"""

        total_balance = system_data.get('total_balance', 0)
        total_inflow = system_data.get('total_inflow', 0)
        total_outflow = system_data.get('total_outflow', 0)
        total_investment = system_data.get('total_investment', 0)

        ratios = {}

        if total_inflow > 0:
            ratios['outflow_ratio'] = total_outflow / total_inflow
            ratios['net_inflow_ratio'] = (total_inflow - total_outflow) / total_inflow

        if total_balance > 0:
            ratios['investment_ratio'] = total_investment / total_balance

        ratios['liquidity_indicator'] = total_balance / total_outflow if total_outflow > 0 else float('inf')

        return ratios

    def _calculate_trend_indicators(self, daily_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡"""

        if len(daily_data) < 2:
            return {}

        # æŒ‰æ—¥æœŸæ’åº
        sorted_data = sorted(daily_data, key=lambda x: x.get('date', ''))

        # è®¡ç®—æœ€è¿‘å‡ å¤©çš„å¹³å‡å€¼
        recent_data = sorted_data[-7:] if len(sorted_data) >= 7 else sorted_data

        avg_inflow = sum(d.get('daily_inflow', 0) for d in recent_data) / len(recent_data)
        avg_outflow = sum(d.get('daily_outflow', 0) for d in recent_data) / len(recent_data)
        avg_registrations = sum(d.get('new_registrations', 0) for d in recent_data) / len(recent_data)

        return {
            'avg_daily_inflow': avg_inflow,
            'avg_daily_outflow': avg_outflow,
            'avg_daily_registrations': avg_registrations,
            'trend_period_days': len(recent_data)
        }

    def _calculate_user_metrics(self, user_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç”¨æˆ·æŒ‡æ ‡"""

        # è¿™é‡Œå¯ä»¥æ ¹æ®ç”¨æˆ·æ•°æ®è®¡ç®—å„ç§ç”¨æˆ·ç›¸å…³æŒ‡æ ‡
        return {
            'user_data_sources': len(user_data),
            'analysis_pending': True  # å®é™…å®ç°ä¸­ä¼šæœ‰å…·ä½“çš„ç”¨æˆ·åˆ†æé€»è¾‘
        }

    def _calculate_product_performance(self, product_data: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—äº§å“è¡¨ç°æŒ‡æ ‡"""

        product_list = product_data.get('product_list', [])

        if not product_list:
            return {}

        total_products = len(product_list)
        active_products = len([p for p in product_list if p.get('å½“å‰æŒæœ‰æ•°', 0) > 0])

        return {
            'total_products': total_products,
            'active_products': active_products,
            'product_utilization_rate': active_products / total_products if total_products > 0 else 0
        }

    # ============= æ•°æ®è´¨é‡è¯„ä¼°å±‚ =============

    async def _assess_data_quality(self, processing_result: Dict[str, Any],
                                   acquisition_plan: Any) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®è´¨é‡"""

        try:
            quality_assessment = {
                'overall_quality': DataQualityLevel.GOOD,
                'quality_score': 0.8,
                'completeness_score': 0.0,
                'accuracy_score': 0.0,
                'freshness_score': 0.0,
                'business_value_score': 0.0,
                'quality_issues': [],
                'recommendations': []
            }

            # ğŸ¯ è®¡ç®—æ•°æ®å®Œæ•´æ€§
            completeness_score = self._calculate_data_completeness(processing_result, acquisition_plan)
            quality_assessment['completeness_score'] = completeness_score

            # ğŸ¯ è®¡ç®—æ•°æ®å‡†ç¡®æ€§
            accuracy_score = await self._calculate_data_accuracy(processing_result)
            quality_assessment['accuracy_score'] = accuracy_score

            # ğŸ¯ è®¡ç®—æ•°æ®æ–°é²œåº¦
            freshness_score = self._calculate_data_freshness(processing_result)
            quality_assessment['freshness_score'] = freshness_score

            # ğŸ¯ è®¡ç®—ä¸šåŠ¡ä»·å€¼å¾—åˆ†
            business_value_score = self._calculate_business_value_score(processing_result, acquisition_plan)
            quality_assessment['business_value_score'] = business_value_score

            # ğŸ¯ è®¡ç®—ç»¼åˆè´¨é‡å¾—åˆ†
            overall_score = (
                    completeness_score * 0.3 +
                    accuracy_score * 0.3 +
                    freshness_score * 0.2 +
                    business_value_score * 0.2
            )
            quality_assessment['quality_score'] = overall_score

            # ğŸ¯ ç¡®å®šè´¨é‡ç­‰çº§
            if overall_score >= 0.95:
                quality_assessment['overall_quality'] = DataQualityLevel.EXCELLENT
            elif overall_score >= 0.8:
                quality_assessment['overall_quality'] = DataQualityLevel.GOOD
            elif overall_score >= 0.6:
                quality_assessment['overall_quality'] = DataQualityLevel.ACCEPTABLE
            elif overall_score >= 0.4:
                quality_assessment['overall_quality'] = DataQualityLevel.POOR
            else:
                quality_assessment['overall_quality'] = DataQualityLevel.INSUFFICIENT

            # ğŸ¯ ç”Ÿæˆè´¨é‡é—®é¢˜å’Œå»ºè®®
            quality_assessment['quality_issues'] = self._identify_quality_issues(processing_result, overall_score)
            quality_assessment['recommendations'] = self._generate_quality_recommendations(quality_assessment)

            return quality_assessment

        except Exception as e:
            logger.error(f"âŒ æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return {
                'overall_quality': DataQualityLevel.POOR,
                'quality_score': 0.3,
                'error': str(e)
            }

    def _calculate_data_completeness(self, processing_result: Dict[str, Any],
                                     acquisition_plan: Any) -> float:
        """è®¡ç®—æ•°æ®å®Œæ•´æ€§"""

        expected_data_sources = len(acquisition_plan.data_requirements) if acquisition_plan.data_requirements else 1
        actual_data_sources = len(processing_result.get('processed_data', {}))

        return min(actual_data_sources / expected_data_sources, 1.0) if expected_data_sources > 0 else 0.0

    async def _calculate_data_accuracy(self, processing_result: Dict[str, Any]) -> float:
        """è®¡ç®—æ•°æ®å‡†ç¡®æ€§"""

        # åŸºç¡€å‡†ç¡®æ€§æ£€æŸ¥
        accuracy_score = 1.0

        # æ£€æŸ¥ç³»ç»Ÿæ•°æ®çš„é€»è¾‘ä¸€è‡´æ€§
        system_data = processing_result.get('processed_data', {}).get('system_data', {})
        if system_data:
            total_balance = system_data.get('total_balance', 0)
            total_inflow = system_data.get('total_inflow', 0)
            total_outflow = system_data.get('total_outflow', 0)

            # åŸºç¡€åˆç†æ€§æ£€æŸ¥
            if total_balance < 0:
                accuracy_score -= 0.2
            if total_outflow > total_inflow * 1.5:  # å‡ºé‡‘è¶…è¿‡å…¥é‡‘çš„1.5å€å¯èƒ½æœ‰é—®é¢˜
                accuracy_score -= 0.1

        return max(accuracy_score, 0.0)

    def _calculate_data_freshness(self, processing_result: Dict[str, Any]) -> float:
        """è®¡ç®—æ•°æ®æ–°é²œåº¦"""

        current_time = datetime.now()
        freshness_scores = []

        # æ£€æŸ¥å„æ•°æ®æºçš„æ—¶é—´æˆ³
        for data_type, data_content in processing_result.get('processed_data', {}).items():
            if isinstance(data_content, dict) and 'timestamp' in data_content:
                try:
                    data_time = datetime.fromisoformat(data_content['timestamp'].replace('Z', '+00:00'))
                    time_diff = (current_time - data_time).total_seconds()

                    # æ ¹æ®æ—¶é—´å·®è®¡ç®—æ–°é²œåº¦å¾—åˆ†
                    if time_diff < 300:  # 5åˆ†é’Ÿå†…
                        freshness_scores.append(1.0)
                    elif time_diff < 1800:  # 30åˆ†é’Ÿå†…
                        freshness_scores.append(0.9)
                    elif time_diff < 3600:  # 1å°æ—¶å†…
                        freshness_scores.append(0.8)
                    elif time_diff < 86400:  # 24å°æ—¶å†…
                        freshness_scores.append(0.6)
                    else:
                        freshness_scores.append(0.3)

                except Exception:
                    freshness_scores.append(0.5)  # æ— æ³•è§£ææ—¶é—´æˆ³
            elif isinstance(data_content, list):
                # å¯¹äºåˆ—è¡¨æ•°æ®ï¼Œæ£€æŸ¥æ¯ä¸ªé¡¹ç›®
                for item in data_content:
                    if isinstance(item, dict) and 'timestamp' in item:
                        try:
                            data_time = datetime.fromisoformat(item['timestamp'].replace('Z', '+00:00'))
                            time_diff = (current_time - data_time).total_seconds()
                            if time_diff < 86400:  # 24å°æ—¶å†…
                                freshness_scores.append(0.8)
                            else:
                                freshness_scores.append(0.5)
                        except Exception:
                            freshness_scores.append(0.5)

        return sum(freshness_scores) / len(freshness_scores) if freshness_scores else 0.7

    def _calculate_business_value_score(self, processing_result: Dict[str, Any],
                                        acquisition_plan: Any) -> float:
        """è®¡ç®—ä¸šåŠ¡ä»·å€¼å¾—åˆ†"""

        # æ ¹æ®è·å–çš„æ•°æ®ç±»å‹å’ŒæŸ¥è¯¢éœ€æ±‚è¯„ä¼°ä¸šåŠ¡ä»·å€¼
        business_value = 0.5  # åŸºç¡€åˆ†

        processed_data = processing_result.get('processed_data', {})

        # ç³»ç»Ÿæ•°æ®çš„ä¸šåŠ¡ä»·å€¼è¾ƒé«˜
        if 'system_data' in processed_data:
            business_value += 0.2

        # æ—¶é—´åºåˆ—æ•°æ®çš„ä¸šåŠ¡ä»·å€¼
        if processing_result.get('time_series_data'):
            business_value += 0.2

        # å¢å¼ºæ•°æ®çš„ä¸šåŠ¡ä»·å€¼
        if processing_result.get('enhanced_data'):
            business_value += 0.1

        return min(business_value, 1.0)

    def _identify_quality_issues(self, processing_result: Dict[str, Any], overall_score: float) -> List[str]:
        """è¯†åˆ«è´¨é‡é—®é¢˜"""

        issues = []

        if overall_score < 0.6:
            issues.append("æ•´ä½“æ•°æ®è´¨é‡è¾ƒä½")

        if not processing_result.get('processed_data'):
            issues.append("ç¼ºå°‘å¤„ç†åçš„æ•°æ®")

        if not processing_result.get('time_series_data') and self.config.get('enable_time_series_processing'):
            issues.append("æ—¶é—´åºåˆ—æ•°æ®æ„å»ºå¤±è´¥")

        if processing_result.get('processing_errors'):
            issues.append(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿ{len(processing_result['processing_errors'])}ä¸ªé”™è¯¯")

        return issues

    def _generate_quality_recommendations(self, quality_assessment: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®"""

        recommendations = []

        if quality_assessment['completeness_score'] < 0.8:
            recommendations.append("å»ºè®®å¢åŠ æ•°æ®æºæˆ–æ”¹è¿›æ•°æ®è·å–ç­–ç•¥")

        if quality_assessment['accuracy_score'] < 0.9:
            recommendations.append("å»ºè®®åŠ å¼ºæ•°æ®éªŒè¯å’Œæ¸…æ´—æµç¨‹")

        if quality_assessment['freshness_score'] < 0.7:
            recommendations.append("å»ºè®®å¢åŠ æ•°æ®è·å–é¢‘ç‡æˆ–ä½¿ç”¨å®æ—¶æ•°æ®æº")

        if quality_assessment['business_value_score'] < 0.7:
            recommendations.append("å»ºè®®ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹ä»¥æå‡ä¸šåŠ¡ä»·å€¼")

        return recommendations

    # ============= è¾…åŠ©æ–¹æ³•å’Œå·¥å…·å‡½æ•° =============

    async def _pre_execution_validation(self, acquisition_plan: Any) -> Dict[str, Any]:
        """æ‰§è¡Œå‰éªŒè¯"""

        validation_result = {
            'is_valid': True,
            'issues': [],
            'warnings': []
        }

        # æ£€æŸ¥APIè°ƒç”¨è®¡åˆ’
        if not acquisition_plan.api_call_plans:
            validation_result['is_valid'] = False
            validation_result['issues'].append("æ²¡æœ‰APIè°ƒç”¨è®¡åˆ’")

        # æ£€æŸ¥æ—¶é—´é¢„ä¼°åˆç†æ€§
        if acquisition_plan.total_estimated_time > self.config.get('execution_timeout', 300):
            validation_result['warnings'].append("é¢„ä¼°æ‰§è¡Œæ—¶é—´å¯èƒ½è¶…è¿‡è¶…æ—¶é™åˆ¶")

        return validation_result

    async def _validate_api_result(self, result: Dict[str, Any], call_plan: Any) -> Dict[str, Any]:
        """éªŒè¯APIè°ƒç”¨ç»“æœ"""

        if not self.validator:
            return {'validation_performed': False, 'reason': 'no_validator'}

        try:
            # ä½¿ç”¨validation_utilséªŒè¯APIå“åº”
            validation_result = await self.validator.validate_api_response(result)

            return {
                'validation_performed': True,
                'is_valid': validation_result.is_valid,
                'quality_score': validation_result.overall_score,
                'issues_count': len(validation_result.issues),
                'validation_details': validation_result
            }

        except Exception as e:
            logger.warning(f"APIç»“æœéªŒè¯å¤±è´¥: {str(e)}")
            return {'validation_performed': False, 'error': str(e)}

    async def _execute_fallback_strategy(self, acquisition_plan: Any,
                                         failed_calls: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰§è¡Œé™çº§ç­–ç•¥ - å·²ç¦ç”¨ï¼ŒAPIä¸ä¼šæ‰çº¿"""

        # è¿”å›ç©ºç»“æœï¼Œä¸æ‰§è¡Œä»»ä½•é™çº§æ“ä½œ
        logger.info("APIé™çº§ç­–ç•¥å·²ç¦ç”¨ - APIä¸ä¼šæ‰çº¿")
        
        return {
            'fallback_executed': False,
            'fallback_data': {},
            'fallback_success': True,
            'message': 'APIé™çº§å·²ç¦ç”¨ï¼ŒAPIä¸ä¼šæ‰çº¿'
        }

    async def _update_progress(self, execution_id: str, step: str, description: str,
                               progress_callback: Optional[callable] = None):
        """æ›´æ–°æ‰§è¡Œè¿›åº¦"""

        if execution_id in self.current_executions:
            progress = self.current_executions[execution_id]
            progress.current_step = step
            progress.current_operation = description
            progress.completed_steps += 1
            progress.progress_percentage = (progress.completed_steps / progress.total_steps) * 100

            if progress_callback:
                try:
                    await progress_callback(progress)
                except Exception as e:
                    logger.warning(f"è¿›åº¦å›è°ƒå¤±è´¥: {str(e)}")

    async def _update_execution_progress(self, execution_id: str, progress_percentage: float,
                                         operation_description: str):
        """æ›´æ–°æ‰§è¡Œè¿›åº¦ç™¾åˆ†æ¯”"""

        if execution_id in self.current_executions:
            progress = self.current_executions[execution_id]
            progress.progress_percentage = min(progress_percentage * 100, 100)
            progress.current_operation = operation_description

    async def _build_execution_result(self, execution_id: str, acquisition_plan: Any,
                                      api_execution_result: Dict[str, Any],
                                      processing_result: Dict[str, Any],
                                      quality_assessment: Dict[str, Any],
                                      start_time: float) -> ExecutionResult:
        """æ„å»ºæ‰§è¡Œç»“æœ"""

        execution_time = time.time() - start_time

        # ç¡®å®šæ‰§è¡ŒçŠ¶æ€
        api_success_rate = api_execution_result.get('success_rate', 0)

        if api_success_rate >= 0.9:
            execution_status = ExecutionStatus.COMPLETED
        elif api_success_rate >= 0.5:
            execution_status = ExecutionStatus.PARTIAL_SUCCESS
        else:
            # å°†FAILEDæ”¹ä¸ºPARTIAL_SUCCESSï¼Œé¿å…orchestratoræŠ›å‡ºå¼‚å¸¸
            execution_status = ExecutionStatus.PARTIAL_SUCCESS
            logger.warning(f"APIæˆåŠŸç‡è¿‡ä½ ({api_success_rate:.1%})ï¼Œä½†ä¸ä¼šè§¦å‘é™çº§ï¼Œè¿”å›éƒ¨åˆ†æˆåŠŸçŠ¶æ€")

        return ExecutionResult(
            result_id=execution_id,
            execution_status=execution_status,
            data_quality=quality_assessment.get('overall_quality', DataQualityLevel.ACCEPTABLE),

            fetched_data=api_execution_result.get('api_results', {}),
            processed_data=processing_result.get('processed_data', {}),
            time_series_data=processing_result.get('time_series_data', {}),

            execution_metadata={
                'execution_id': execution_id,
                'acquisition_plan_id': acquisition_plan.plan_id,
                'execution_time': execution_time,
                'completion_timestamp': datetime.now().isoformat()
            },
            api_call_results=list(api_execution_result.get('api_results', {}).values()),
            timing_info={
                'total_execution_time': execution_time,
                'api_execution_time': execution_time * 0.6,  # ä¼°ç®—
                'processing_time': execution_time * 0.3,  # ä¼°ç®—
                'validation_time': execution_time * 0.1  # ä¼°ç®—
            },
            error_info=api_execution_result.get('failed_call_details', []),

            data_completeness=quality_assessment.get('completeness_score', 0.0),
            accuracy_score=quality_assessment.get('accuracy_score', 0.0),
            freshness_score=quality_assessment.get('freshness_score', 0.0),

            business_value_score=quality_assessment.get('business_value_score', 0.0),
            confidence_level=quality_assessment.get('quality_score', 0.0)
        )

    async def _handle_validation_failure(self, execution_id: str,
                                         validation_result: Dict[str, Any]) -> ExecutionResult:
        """å¤„ç†éªŒè¯å¤±è´¥"""

        # ä¿®æ”¹ä¸ºè¿”å›PARTIAL_SUCCESSè€Œä¸æ˜¯FAILEDï¼Œé¿å…orchestratoræŠ›å‡ºå¼‚å¸¸
        logger.warning(f"éªŒè¯å¤±è´¥ï¼Œä½†ä¸è§¦å‘é™çº§ï¼Œè¿”å›éƒ¨åˆ†æˆåŠŸçŠ¶æ€: {validation_result.get('issues', [])}")
        
        return ExecutionResult(
            result_id=execution_id,
            execution_status=ExecutionStatus.PARTIAL_SUCCESS,  # æ”¹ä¸ºPARTIAL_SUCCESS
            data_quality=DataQualityLevel.INSUFFICIENT,

            fetched_data={},
            processed_data={},
            time_series_data={},

            execution_metadata={'validation_failure': validation_result},
            api_call_results=[],
            timing_info={'total_execution_time': 0.0},
            error_info={'validation_errors': validation_result.get('issues', [])},

            data_completeness=0.0,
            accuracy_score=0.0,
            freshness_score=0.0,
            business_value_score=0.0,
            confidence_level=0.0
        )

    async def _handle_execution_error(self, execution_id: str, error: str,
                                      start_time: float) -> ExecutionResult:
        """å¤„ç†æ‰§è¡Œé”™è¯¯"""

        execution_time = time.time() - start_time

        return ExecutionResult(
            result_id=execution_id,
            execution_status=ExecutionStatus.FAILED,
            data_quality=DataQualityLevel.INSUFFICIENT,

            fetched_data={},
            processed_data={},
            time_series_data={},

            execution_metadata={'execution_error': error},
            api_call_results=[],
            timing_info={'total_execution_time': execution_time},
            error_info={'execution_error': error},

            data_completeness=0.0,
            accuracy_score=0.0,
            freshness_score=0.0,
            business_value_score=0.0,
            confidence_level=0.0
        )

    def _update_performance_stats(self, execution_result: ExecutionResult):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""

        # æ›´æ–°æ‰§è¡Œç»Ÿè®¡
        if execution_result.execution_status == ExecutionStatus.COMPLETED:
            self.performance_stats['successful_executions'] += 1
        elif execution_result.execution_status == ExecutionStatus.PARTIAL_SUCCESS:
            self.performance_stats['partial_success_executions'] += 1
        else:
            self.performance_stats['failed_executions'] += 1

        # æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´
        total_time = self.performance_stats['average_execution_time'] * (self.performance_stats['total_executions'] - 1)
        new_avg_time = (total_time + execution_result.timing_info['total_execution_time']) / self.performance_stats[
            'total_executions']
        self.performance_stats['average_execution_time'] = new_avg_time

        # æ›´æ–°å¹³å‡æ•°æ®è´¨é‡
        total_quality = self.performance_stats['average_data_quality'] * (
                    self.performance_stats['total_executions'] - 1)
        new_avg_quality = (total_quality + execution_result.confidence_level) / self.performance_stats[
            'total_executions']
        self.performance_stats['average_data_quality'] = new_avg_quality

    # ============= å¤–éƒ¨æ¥å£æ–¹æ³• =============

    async def get_execution_progress(self, execution_id: str) -> Optional[ExecutionProgress]:
        """è·å–æ‰§è¡Œè¿›åº¦"""
        return self.current_executions.get(execution_id)

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return self.performance_stats.copy()

    def get_execution_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æ‰§è¡Œå†å²"""
        return self.execution_history[-limit:] if self.execution_history else []

    async def cancel_execution(self, execution_id: str) -> bool:
        """å–æ¶ˆæ‰§è¡Œ"""
        if execution_id in self.current_executions:
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæœ‰æ›´å¤æ‚çš„å–æ¶ˆé€»è¾‘
            del self.current_executions[execution_id]
            logger.info(f"ğŸš« æ‰§è¡Œå·²å–æ¶ˆ: {execution_id}")
            return True
        return False


# ============= å·¥å‚å‡½æ•° =============

def create_smart_data_fetcher(claude_client=None, gpt_client=None,
                              config: Dict[str, Any] = None) -> SmartDataFetcher:
    """
    åˆ›å»ºæ™ºèƒ½æ•°æ®è·å–å™¨å®ä¾‹

    Args:
        claude_client: Claudeå®¢æˆ·ç«¯å®ä¾‹
        gpt_client: GPTå®¢æˆ·ç«¯å®ä¾‹
        config: é…ç½®å‚æ•°

    Returns:
        SmartDataFetcher: æ™ºèƒ½æ•°æ®è·å–å™¨å®ä¾‹
    """
    return SmartDataFetcher(claude_client, gpt_client, config)


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""

    # åˆ›å»ºæ™ºèƒ½æ•°æ®è·å–å™¨
    fetcher = create_smart_data_fetcher()

    print("=== æ™ºèƒ½æ•°æ®è·å–å™¨æµ‹è¯• ===")

    # æ¨¡æ‹Ÿæ•°æ®è·å–è®¡åˆ’
    from dataclasses import dataclass

    @dataclass
    class MockAcquisitionPlan:
        plan_id: str = "test_plan_001"
        api_call_plans: List = None
        parallel_groups: List = None
        total_estimated_time: float = 10.0
        data_requirements: List = None

        def __post_init__(self):
            if self.api_call_plans is None:
                self.api_call_plans = []
            if self.parallel_groups is None:
                self.parallel_groups = []
            if self.data_requirements is None:
                self.data_requirements = []

    # åˆ›å»ºæ¨¡æ‹Ÿè®¡åˆ’
    mock_plan = MockAcquisitionPlan()

    # å®šä¹‰è¿›åº¦å›è°ƒ
    async def progress_callback(progress: ExecutionProgress):
        print(f"è¿›åº¦: {progress.progress_percentage:.1f}% - {progress.current_operation}")

    # æ‰§è¡Œæ•°æ®è·å–
    result = await fetcher.execute_data_acquisition_plan(mock_plan, progress_callback)

    print(f"æ‰§è¡Œç»“æœID: {result.result_id}")
    print(f"æ‰§è¡ŒçŠ¶æ€: {result.execution_status.value}")
    print(f"æ•°æ®è´¨é‡: {result.data_quality.value}")
    print(f"æ‰§è¡Œæ—¶é—´: {result.timing_info['total_execution_time']:.2f}ç§’")
    print(f"ç½®ä¿¡åº¦: {result.confidence_level:.2f}")

    # è·å–æ€§èƒ½ç»Ÿè®¡
    stats = fetcher.get_performance_stats()
    print(f"\n=== æ€§èƒ½ç»Ÿè®¡ ===")
    print(f"æ€»æ‰§è¡Œæ¬¡æ•°: {stats['total_executions']}")
    print(f"æˆåŠŸæ‰§è¡Œ: {stats['successful_executions']}")
    print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {stats['average_execution_time']:.2f}ç§’")
    print(f"å¹³å‡æ•°æ®è´¨é‡: {stats['average_data_quality']:.2f}")


if __name__ == "__main__":
    asyncio.run(main())