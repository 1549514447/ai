# api/qa_routes.py - å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬
from flask import Blueprint, jsonify, request
from core.orchestrator.intelligent_qa_orchestrator import get_orchestrator
import asyncio
import logging
import traceback
from datetime import datetime
from typing import Dict, Any, List, Optional
import json
import time

logger = logging.getLogger(__name__)
qa_routes_bp = Blueprint('qa_routes', __name__, url_prefix='/api/qa')

# ğŸ¯ ä½¿ç”¨å•ä¸€ç¼–æ’å™¨å®ä¾‹
orchestrator = get_orchestrator()


# ============= å·¥å…·å‡½æ•° =============

def async_wrapper(f):
    """å¼‚æ­¥åŒ…è£…å™¨è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(f(*args, **kwargs))
        finally:
            loop.close()
    wrapper.__name__ = f.__name__
    return wrapper


def validate_query_request(data: Dict[str, Any]) -> Dict[str, Any]:
    """éªŒè¯æŸ¥è¯¢è¯·æ±‚æ•°æ®"""
    if not data:
        raise ValueError("è¯·æ±‚æ•°æ®ä¸ºç©º")
    
    query = data.get('query', '').strip()
    if not query:
        raise ValueError("æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    if len(query) > 1000:
        raise ValueError("æŸ¥è¯¢å†…å®¹è¿‡é•¿ï¼Œè¯·æ§åˆ¶åœ¨1000å­—ç¬¦ä»¥å†…")
    
    user_id = data.get('user_id', 1)
    if not isinstance(user_id, int) or user_id < 1:
        raise ValueError("user_idå¿…é¡»æ˜¯æ­£æ•´æ•°")
    
    return {
        'query': query,
        'user_id': user_id,
        'conversation_id': data.get('conversation_id'),
        'preferences': data.get('preferences', {}),
        'context': data.get('context', {}),
        'analysis_depth': data.get('analysis_depth', 'standard'),
        'include_visualizations': data.get('include_visualizations', True),
        'response_format': data.get('response_format', 'detailed')
    }


def create_success_response(data: Dict[str, Any], message: str = "æ“ä½œæˆåŠŸ") -> Dict[str, Any]:
    """åˆ›å»ºæˆåŠŸå“åº”"""
    return jsonify({
        'success': True,
        'message': message,
        'data': data,
        'timestamp': datetime.now().isoformat(),
        'service': 'intelligent_qa_service'
    })


def create_error_response(message: str, error_type: str = "processing_error", 
                         status_code: int = 500, details: Dict[str, Any] = None) -> tuple:
    """åˆ›å»ºé”™è¯¯å“åº”"""
    error_response = {
        'success': False,
        'error_type': error_type,
        'message': message,
        'timestamp': datetime.now().isoformat(),
        'service': 'intelligent_qa_service'
    }
    
    if details:
        error_response['details'] = details
    
    return jsonify(error_response), status_code


# ============= æ ¸å¿ƒé—®ç­”API =============

@qa_routes_bp.route('/ask', methods=['POST'])
def intelligent_question_processing():
    """ğŸ§  æ™ºèƒ½é—®ç­” - å®Œæ•´ç¼–æ’å™¨å¤„ç†"""

    @async_wrapper
    async def _process_query():
        try:
            # å‚æ•°éªŒè¯
            request_data = request.get_json()
            if not request_data:
                return create_error_response("è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯", "validation_error", 400)
            
            validated_data = validate_query_request(request_data)
            
            logger.info(f"ğŸ§  å¤„ç†æ™ºèƒ½æŸ¥è¯¢: {validated_data['query'][:50]}...")
            
            # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
            if not orchestrator.initialized:
                return create_error_response("æ™ºèƒ½é—®ç­”ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
            
            start_time = time.time()
            
            # ä½¿ç”¨ç¼–æ’å™¨å¤„ç†æŸ¥è¯¢
            result = await orchestrator.process_intelligent_query(
                user_query=validated_data['query'],
                user_id=validated_data['user_id'],
                conversation_id=validated_data['conversation_id'],
                preferences=validated_data['preferences']
            )
            
            processing_time = time.time() - start_time
            
            # å¢å¼ºå“åº”æ•°æ®
            response_data = {
                'query_result': {
                    'success': result.success,
                    'response_text': result.response_text,
                    'insights': result.insights,
                    'visualizations': result.visualizations if validated_data['include_visualizations'] else [],
                    'confidence_score': result.confidence_score,
                    'conversation_id': result.conversation_id
                },
                'analysis_metadata': {
                    'query_complexity': result.processing_metadata.get('query_complexity', 'unknown'),
                    'processing_time': processing_time,
                    'ai_models_used': result.processing_metadata.get('ai_models_used', []),
                    'data_sources_accessed': result.processing_metadata.get('data_sources', []),
                    'analysis_depth': validated_data['analysis_depth']
                },
                'quality_metrics': {
                    'response_completeness': len(result.response_text) > 100,
                    'insights_provided': len(result.insights) > 0,
                    'visualizations_included': len(result.visualizations) > 0,
                    'confidence_level': 'high' if result.confidence_score > 0.8 else 'medium' if result.confidence_score > 0.6 else 'low'
                }
            }
            
            logger.info(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆ: ç½®ä¿¡åº¦={result.confidence_score:.2f}, è€—æ—¶={processing_time:.2f}ç§’")
            
            return create_success_response(response_data, "æ™ºèƒ½æŸ¥è¯¢å¤„ç†å®Œæˆ")
        
        except ValueError as e:
            return create_error_response(str(e), "validation_error", 400)
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return create_error_response(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)

    return _process_query()


@qa_routes_bp.route('/analyze', methods=['POST'])
def intelligent_data_analysis():
    """ğŸ“Š æ™ºèƒ½æ•°æ®åˆ†æ - ä½¿ç”¨ç¼–æ’å™¨çš„åˆ†æèƒ½åŠ›"""

    @async_wrapper
    async def _analyze_data():
        try:
            request_data = request.get_json()
            if not request_data:
                return create_error_response("è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯", "validation_error", 400)
            
            # å‚æ•°éªŒè¯
            data_source = request_data.get('data_source', 'system')
            metric = request_data.get('metric', 'total_balance')
            time_range = int(request_data.get('time_range', 30))
            analysis_type = request_data.get('analysis_type', 'trend')  # trend, performance, anomaly, comprehensive
            
            # å‚æ•°åˆæ³•æ€§æ£€æŸ¥
            valid_data_sources = ['system', 'daily', 'product', 'user', 'expiry']
            valid_analysis_types = ['trend', 'performance', 'anomaly', 'comprehensive']
            
            if data_source not in valid_data_sources:
                return create_error_response(
                    f"ä¸æ”¯æŒçš„æ•°æ®æº: {data_source}ï¼Œæ”¯æŒ: {valid_data_sources}",
                    "validation_error", 400
                )
            
            if analysis_type not in valid_analysis_types:
                return create_error_response(
                    f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}ï¼Œæ”¯æŒ: {valid_analysis_types}",
                    "validation_error", 400
                )
            
            if not (1 <= time_range <= 365):
                return create_error_response(
                    "æ—¶é—´èŒƒå›´å¿…é¡»åœ¨1-365å¤©ä¹‹é—´",
                    "validation_error", 400
                )
            
            logger.info(f"ğŸ“Š æ‰§è¡Œæ•°æ®åˆ†æ: {analysis_type} - {data_source}.{metric}")
            
            # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
            if not orchestrator.initialized:
                return create_error_response("æ•°æ®åˆ†æç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
            
            start_time = time.time()
            analysis_results = []
            
            # æ ¹æ®åˆ†æç±»å‹æ‰§è¡Œä¸åŒçš„åˆ†æ
            if analysis_type == 'trend':
                # è¶‹åŠ¿åˆ†æ
                analysis_result = await orchestrator.data_analyzer.analyze_trend(
                    data_source=data_source,
                    metric=metric,
                    time_range=time_range
                )
                analysis_results.append(analysis_result)
                
            elif analysis_type == 'performance':
                # ä¸šåŠ¡è¡¨ç°åˆ†æ
                analysis_result = await orchestrator.data_analyzer.analyze_business_performance(
                    scope=data_source,
                    time_range=time_range
                )
                analysis_results.append(analysis_result)
                
            elif analysis_type == 'anomaly':
                # å¼‚å¸¸æ£€æµ‹
                metrics_to_check = [metric] if metric else ['total_balance', 'daily_inflow', 'daily_outflow']
                analysis_result = await orchestrator.data_analyzer.detect_anomalies(
                    data_source=data_source,
                    metrics=metrics_to_check,
                    sensitivity=2.0
                )
                analysis_results.append(analysis_result)
                
            elif analysis_type == 'comprehensive':
                # ç»¼åˆåˆ†æ
                trend_result = await orchestrator.data_analyzer.analyze_trend(
                    data_source=data_source, metric=metric, time_range=time_range
                )
                performance_result = await orchestrator.data_analyzer.analyze_business_performance(
                    scope=data_source, time_range=time_range
                )
                anomaly_result = await orchestrator.data_analyzer.detect_anomalies(
                    data_source=data_source, metrics=[metric]
                )
                analysis_results.extend([trend_result, performance_result, anomaly_result])
            
            processing_time = time.time() - start_time
            
            # æ•´ç†åˆ†æç»“æœ
            formatted_results = []
            for result in analysis_results:
                formatted_result = {
                    'analysis_id': result.analysis_id if hasattr(result, 'analysis_id') else f'analysis_{len(formatted_results)}',
                    'analysis_type': result.analysis_type.value if hasattr(result, 'analysis_type') else analysis_type,
                    'confidence_score': result.confidence_score if hasattr(result, 'confidence_score') else 0.0,
                    'key_findings': result.key_findings if hasattr(result, 'key_findings') else [],
                    'trends': result.trends if hasattr(result, 'trends') else [],
                    'anomalies': result.anomalies if hasattr(result, 'anomalies') else [],
                    'metrics': result.metrics if hasattr(result, 'metrics') else {},
                    'business_insights': result.business_insights if hasattr(result, 'business_insights') else [],
                    'recommendations': result.recommendations if hasattr(result, 'recommendations') else [],
                    'processing_time': result.processing_time if hasattr(result, 'processing_time') else 0
                }
                formatted_results.append(formatted_result)
            
            # è®¡ç®—ç»¼åˆæŒ‡æ ‡
            avg_confidence = sum(r['confidence_score'] for r in formatted_results) / len(formatted_results) if formatted_results else 0
            total_findings = sum(len(r['key_findings']) for r in formatted_results)
            total_anomalies = sum(len(r['anomalies']) for r in formatted_results)
            
            response_data = {
                'analysis_results': formatted_results,
                'analysis_summary': {
                    'total_analyses': len(formatted_results),
                    'avg_confidence': avg_confidence,
                    'total_key_findings': total_findings,
                    'total_anomalies_detected': total_anomalies,
                    'overall_status': 'healthy' if total_anomalies == 0 else 'needs_attention'
                },
                'request_parameters': {
                    'data_source': data_source,
                    'metric': metric,
                    'time_range': time_range,
                    'analysis_type': analysis_type
                },
                'processing_metadata': {
                    'processing_time': processing_time,
                    'data_quality': 'high',
                    'ai_enhanced': True
                }
            }
            
            logger.info(f"âœ… æ•°æ®åˆ†æå®Œæˆ: {len(formatted_results)}ä¸ªåˆ†æç»“æœ")
            
            return create_success_response(response_data, "æ•°æ®åˆ†æå®Œæˆ")
        
        except ValueError as e:
            return create_error_response(str(e), "validation_error", 400)
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return create_error_response(f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}", "internal_error", 500)

    return _analyze_data()


@qa_routes_bp.route('/insights', methods=['POST'])
def generate_business_insights():
    """ğŸ’¡ ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆ - ä½¿ç”¨ç¼–æ’å™¨çš„æ´å¯Ÿç”Ÿæˆå™¨"""

    @async_wrapper
    async def _generate_insights():
        try:
            request_data = request.get_json()
            if not request_data:
                return create_error_response("è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯", "validation_error", 400)
            
            # å‚æ•°éªŒè¯
            data_type = request_data.get('data_type', 'comprehensive')
            focus_areas = request_data.get('focus_areas', [])
            time_range = int(request_data.get('time_range', 30))
            insight_depth = request_data.get('insight_depth', 'standard')  # basic, standard, comprehensive
            include_recommendations = request_data.get('include_recommendations', True)
            
            # å‚æ•°åˆæ³•æ€§æ£€æŸ¥
            valid_data_types = ['system', 'daily', 'product', 'user', 'expiry', 'financial', 'comprehensive']
            valid_depths = ['basic', 'standard', 'comprehensive']
            
            if data_type not in valid_data_types:
                return create_error_response(
                    f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}ï¼Œæ”¯æŒ: {valid_data_types}",
                    "validation_error", 400
                )
            
            if insight_depth not in valid_depths:
                return create_error_response(
                    f"ä¸æ”¯æŒçš„æ´å¯Ÿæ·±åº¦: {insight_depth}ï¼Œæ”¯æŒ: {valid_depths}",
                    "validation_error", 400
                )
            
            if not (1 <= time_range <= 365):
                return create_error_response(
                    "æ—¶é—´èŒƒå›´å¿…é¡»åœ¨1-365å¤©ä¹‹é—´",
                    "validation_error", 400
                )
            
            logger.info(f"ğŸ’¡ ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ: {data_type} - æ·±åº¦: {insight_depth}")
            
            # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
            if not orchestrator.initialized:
                return create_error_response("æ´å¯Ÿç”Ÿæˆç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
            
            start_time = time.time()
            
            # è·å–ç›¸å…³åˆ†æç»“æœä½œä¸ºæ´å¯Ÿç”Ÿæˆçš„åŸºç¡€
            analysis_results = []
            
            if data_type in ['system', 'financial', 'comprehensive']:
                # è·å–ç³»ç»Ÿå’Œè´¢åŠ¡åˆ†æ
                trend_result = await orchestrator.data_analyzer.analyze_trend('system', 'total_balance', time_range)
                performance_result = await orchestrator.data_analyzer.analyze_business_performance('financial', time_range)
                analysis_results.extend([trend_result, performance_result])
            
            if data_type in ['daily', 'comprehensive']:
                # è·å–æ¯æ—¥æ•°æ®åˆ†æ
                daily_trend = await orchestrator.data_analyzer.analyze_trend('daily', 'net_inflow', min(time_range, 14))
                analysis_results.append(daily_trend)
            
            if data_type in ['product', 'comprehensive']:
                # è·å–äº§å“åˆ†æ
                product_performance = await orchestrator.data_analyzer.analyze_business_performance('product', time_range)
                analysis_results.append(product_performance)
            
            if data_type in ['user', 'comprehensive']:
                # è·å–ç”¨æˆ·åˆ†æ
                user_performance = await orchestrator.data_analyzer.analyze_business_performance('user', time_range)
                analysis_results.append(user_performance)
            
            if data_type in ['expiry', 'comprehensive']:
                # è·å–åˆ°æœŸé£é™©åˆ†æ
                expiry_anomalies = await orchestrator.data_analyzer.detect_anomalies('system', ['expiry_amount'])
                analysis_results.append(expiry_anomalies)
            
            # ä½¿ç”¨ç¼–æ’å™¨çš„æ´å¯Ÿç”Ÿæˆå™¨
            insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
                analysis_results=analysis_results,
                user_context=None,
                focus_areas=focus_areas if focus_areas else [data_type]
            )
            
            processing_time = time.time() - start_time
            
            # æŒ‰ä¼˜å…ˆçº§åˆ†ç±»æ´å¯Ÿ
            critical_insights = [i for i in insights if hasattr(i, 'priority') and i.priority.value == 'critical']
            high_insights = [i for i in insights if hasattr(i, 'priority') and i.priority.value == 'high']
            medium_insights = [i for i in insights if hasattr(i, 'priority') and i.priority.value == 'medium']
            low_insights = [i for i in insights if hasattr(i, 'priority') and i.priority.value == 'low']
            
            # æå–è¡ŒåŠ¨å»ºè®®
            all_recommendations = []
            for insight in insights:
                if hasattr(insight, 'recommended_actions'):
                    all_recommendations.extend(insight.recommended_actions)
            
            # æ ¼å¼åŒ–æ´å¯Ÿæ•°æ®
            formatted_insights = []
            for insight in insights:
                formatted_insight = {
                    'insight_id': insight.insight_id if hasattr(insight, 'insight_id') else f'insight_{len(formatted_insights)}',
                    'type': insight.insight_type.value if hasattr(insight, 'insight_type') else 'general',
                    'priority': insight.priority.value if hasattr(insight, 'priority') else 'medium',
                    'title': insight.title if hasattr(insight, 'title') else 'Business Insight',
                    'summary': insight.summary if hasattr(insight, 'summary') else '',
                    'detailed_analysis': insight.detailed_analysis if hasattr(insight, 'detailed_analysis') else '',
                    'confidence_score': insight.confidence_score if hasattr(insight, 'confidence_score') else 0.0,
                    'key_metrics': insight.key_metrics if hasattr(insight, 'key_metrics') else {},
                    'recommended_actions': insight.recommended_actions if hasattr(insight, 'recommended_actions') else [],
                    'expected_impact': insight.expected_impact if hasattr(insight, 'expected_impact') else '',
                    'data_sources': insight.data_sources if hasattr(insight, 'data_sources') else []
                }
                formatted_insights.append(formatted_insight)
            
            response_data = {
                'insights': formatted_insights,
                'insights_summary': {
                    'total_insights': len(formatted_insights),
                    'critical_count': len(critical_insights),
                    'high_priority_count': len(high_insights),
                    'medium_priority_count': len(medium_insights),
                    'low_priority_count': len(low_insights),
                    'avg_confidence': sum(i['confidence_score'] for i in formatted_insights) / len(formatted_insights) if formatted_insights else 0,
                    'actionable_insights': len([i for i in formatted_insights if i['recommended_actions']])
                },
                'recommendations': all_recommendations if include_recommendations else [],
                'metadata': metadata,
                'request_parameters': {
                    'data_type': data_type,
                    'focus_areas': focus_areas,
                    'time_range': time_range,
                    'insight_depth': insight_depth
                },
                'processing_metadata': {
                    'processing_time': processing_time,
                    'analysis_inputs': len(analysis_results),
                    'ai_collaboration': True,
                    'business_context_applied': True
                }
            }
            
            logger.info(f"âœ… ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå®Œæˆ: {len(formatted_insights)}æ¡æ´å¯Ÿ")
            
            return create_success_response(response_data, "ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå®Œæˆ")
        
        except ValueError as e:
            return create_error_response(str(e), "validation_error", 400)
        except Exception as e:
            logger.error(f"âŒ ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return create_error_response(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}", "internal_error", 500)

    return _generate_insights()


@qa_routes_bp.route('/query/parse', methods=['POST'])
def parse_complex_query():
    """ğŸ” å¤æ‚æŸ¥è¯¢è§£æ - æŸ¥è¯¢ç†è§£å’Œåˆ†æ"""

    @async_wrapper
    async def _parse_query():
        try:
            request_data = request.get_json()
            if not request_data:
                return create_error_response("è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯", "validation_error", 400)
            
            query = request_data.get('query', '').strip()
            if not query:
                return create_error_response("æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º", "validation_error", 400)
            
            context = request_data.get('context', {})
            parse_depth = request_data.get('parse_depth', 'standard')  # basic, standard, comprehensive
            
            logger.info(f"ğŸ” è§£æå¤æ‚æŸ¥è¯¢: {query[:50]}...")
            
            # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
            if not orchestrator.initialized:
                return create_error_response("æŸ¥è¯¢è§£æç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
            
            start_time = time.time()
            
            # ä½¿ç”¨ç¼–æ’å™¨çš„æŸ¥è¯¢è§£æå™¨
            parse_result = await orchestrator.query_parser.parse_complex_query(query, context)
            
            processing_time = time.time() - start_time
            
            # æ ¼å¼åŒ–è§£æç»“æœ
            response_data = {
                'query_analysis': {
                    'original_query': parse_result.original_query,
                    'complexity': parse_result.complexity.value,
                    'query_type': parse_result.query_type.value,
                    'business_scenario': parse_result.business_scenario.value,
                    'confidence_score': parse_result.confidence_score
                },
                'time_requirements': parse_result.time_requirements,
                'data_requirements': {
                    'required_apis': parse_result.required_apis,
                    'data_sources': parse_result.data_requirements
                },
                'business_parameters': parse_result.business_parameters,
                'execution_plan': [
                    {
                        'step_id': step.step_id,
                        'step_type': step.step_type,
                        'description': step.description,
                        'estimated_time': step.estimated_time,
                        'ai_model_preference': step.ai_model_preference
                    }
                    for step in parse_result.execution_plan
                ],
                'ai_collaboration_plan': parse_result.ai_collaboration_plan,
                'processing_strategy': parse_result.processing_strategy,
                'estimated_total_time': parse_result.estimated_total_time,
                'processing_metadata': {
                    'parsing_time': processing_time,
                    'parse_depth': parse_depth,
                    'date_parse_success': parse_result.date_parse_result is not None,
                    'execution_steps': len(parse_result.execution_plan)
                }
            }
            
            logger.info(f"âœ… æŸ¥è¯¢è§£æå®Œæˆ: å¤æ‚åº¦={parse_result.complexity.value}")
            
            return create_success_response(response_data, "æŸ¥è¯¢è§£æå®Œæˆ")
        
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢è§£æå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return create_error_response(f"æŸ¥è¯¢è§£æå¤±è´¥: {str(e)}", "internal_error", 500)

    return _parse_query()


@qa_routes_bp.route('/users/analyze', methods=['POST'])
def analyze_user_behavior():
    """ğŸ‘¥ ç”¨æˆ·è¡Œä¸ºåˆ†æ - ä¸“é—¨çš„ç”¨æˆ·æ•°æ®åˆ†æ"""
    
    @async_wrapper
    async def _analyze_users():
        try:
            request_data = request.get_json()
            if not request_data:
                return create_error_response("è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯", "validation_error", 400)
            
            analysis_type = request_data.get('analysis_type', 'behavior')  # behavior, growth, retention, vip
            time_range = int(request_data.get('time_range', 30))
            include_vip_analysis = request_data.get('include_vip_analysis', True)
            segment_users = request_data.get('segment_users', False)
            
            # å‚æ•°éªŒè¯
            valid_analysis_types = ['behavior', 'growth', 'retention', 'vip', 'comprehensive']
            if analysis_type not in valid_analysis_types:
                return create_error_response(
                    f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}ï¼Œæ”¯æŒ: {valid_analysis_types}",
                    "validation_error", 400
                )
            
            if not (1 <= time_range <= 365):
                return create_error_response(
                    "æ—¶é—´èŒƒå›´å¿…é¡»åœ¨1-365å¤©ä¹‹é—´",
                    "validation_error", 400
                )
            
            logger.info(f"ğŸ‘¥ ç”¨æˆ·è¡Œä¸ºåˆ†æ: {analysis_type} - {time_range}å¤©")
            
            # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
            if not orchestrator.initialized:
                return create_error_response("ç”¨æˆ·åˆ†æç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
            
            start_time = time.time()
            analysis_results = []
            
            # æ ¹æ®åˆ†æç±»å‹æ‰§è¡Œç›¸åº”åˆ†æ
            if analysis_type in ['behavior', 'comprehensive']:
                # ç”¨æˆ·è¡Œä¸ºåˆ†æ
                behavior_result = await orchestrator.data_analyzer.analyze_business_performance('user', time_range)
                analysis_results.append(behavior_result)
            
            if analysis_type in ['growth', 'comprehensive']:
                # ç”¨æˆ·å¢é•¿åˆ†æ
                growth_result = await orchestrator.data_analyzer.analyze_trend('user', 'new_users', time_range)
                analysis_results.append(growth_result)
            
            if analysis_type in ['vip', 'comprehensive'] and include_vip_analysis:
                # VIPç”¨æˆ·åˆ†æ
                vip_result = await orchestrator.data_analyzer.analyze_business_performance('user', time_range)
                analysis_results.append(vip_result)
            
            # è·å–ç”¨æˆ·è¯¦ç»†æ•°æ®è¿›è¡Œè¡¥å……åˆ†æ
            user_data_result = await orchestrator.api_connector.get_user_data(1)  # ç¬¬ä¸€é¡µç”¨æˆ·æ•°æ®
            user_daily_result = await orchestrator.api_connector.get_user_daily_data()  # ç”¨æˆ·æ¯æ—¥æ•°æ®
            
            # ç”¨æˆ·æ´å¯Ÿç”Ÿæˆ
            user_insights = []
            if analysis_results:
                insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
                    analysis_results=analysis_results,
                    user_context=None,
                    focus_areas=['user_behavior', 'user_growth']
                )
                user_insights = insights
            
            processing_time = time.time() - start_time
            
            # ç”¨æˆ·ç»Ÿè®¡æ‘˜è¦
            user_summary = {}
            if user_data_result.get('success') and user_daily_result.get('success'):
                user_list = user_data_result['data'].get('ç”¨æˆ·åˆ—è¡¨', [])
                daily_data = user_daily_result['data'].get('æ¯æ—¥æ•°æ®', [])
                
                if user_list:
                    total_investment = sum(float(u.get('æ€»æŠ•å…¥', 0)) for u in user_list)
                    total_rewards = sum(float(u.get('ç´¯è®¡è·å¾—å¥–åŠ±é‡‘é¢', 0)) for u in user_list)
                    avg_roi = sum(float(u.get('æŠ•æŠ¥æ¯”', 0)) for u in user_list) / len(user_list)
                    
                    user_summary = {
                        'total_users_analyzed': len(user_list),
                        'total_investment': total_investment,
                        'total_rewards': total_rewards,
                        'avg_roi': avg_roi,
                        'high_value_users': len([u for u in user_list if float(u.get('æŠ•æŠ¥æ¯”', 0)) > 0.1])
                    }
                
                if daily_data:
                    latest_day = daily_data[-1] if daily_data else {}
                    total_users = sum(latest_day.get(f'vip{i}çš„äººæ•°', 0) for i in range(11))
                    user_summary.update({
                        'total_registered_users': total_users,
                        'vip_distribution': {
                            f'vip{i}': latest_day.get(f'vip{i}çš„äººæ•°', 0) 
                            for i in range(11)
                        }
                    })
            
            # æ ¼å¼åŒ–åˆ†æç»“æœ
            formatted_results = []
            for result in analysis_results:
                formatted_result = {
                    'analysis_type': result.analysis_type.value if hasattr(result, 'analysis_type') else analysis_type,
                    'confidence_score': result.confidence_score if hasattr(result, 'confidence_score') else 0.0,
                    'key_findings': result.key_findings if hasattr(result, 'key_findings') else [],
                    'business_insights': result.business_insights if hasattr(result, 'business_insights') else [],
                    'recommendations': result.recommendations if hasattr(result, 'recommendations') else []
                }
                formatted_results.append(formatted_result)
            
            response_data = {
                'analysis_results': formatted_results,
                'user_insights': [
                    {
                        'insight_id': insight.insight_id if hasattr(insight, 'insight_id') else f'insight_{i}',
                        'title': insight.title if hasattr(insight, 'title') else '',
                        'summary': insight.summary if hasattr(insight, 'summary') else '',
                        'priority': insight.priority.value if hasattr(insight, 'priority') else 'medium',
                        'recommended_actions': insight.recommended_actions if hasattr(insight, 'recommended_actions') else []
                    }
                    for i, insight in enumerate(user_insights)
                ],
                'user_summary': user_summary,
                'analysis_parameters': {
                    'analysis_type': analysis_type,
                    'time_range': time_range,
                    'include_vip_analysis': include_vip_analysis,
                    'segment_users': segment_users
                },
                'processing_metadata': {
                    'processing_time': processing_time,
                    'analyses_performed': len(formatted_results),
                    'insights_generated': len(user_insights),
                    'data_quality': 'high'
                }
            }
            
            logger.info(f"âœ… ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ: {len(formatted_results)}ä¸ªåˆ†æç»“æœ")
            
            return create_success_response(response_data, "ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ")
        
        except ValueError as e:
            return create_error_response(str(e), "validation_error", 400)
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·è¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return create_error_response(f"ç”¨æˆ·åˆ†æå¤±è´¥: {str(e)}", "internal_error", 500)
    
    return _analyze_users()


@qa_routes_bp.route('/conversations/<int:user_id>', methods=['GET'])
def get_user_conversations(user_id):
    """ğŸ’¬ è·å–ç”¨æˆ·å¯¹è¯ - ä½¿ç”¨ç¼–æ’å™¨çš„å¯¹è¯ç®¡ç†å™¨"""
    try:
        # å‚æ•°éªŒè¯
        if user_id < 1:
            return create_error_response("ç”¨æˆ·IDå¿…é¡»æ˜¯æ­£æ•´æ•°", "validation_error", 400)
        
        limit = request.args.get('limit', 20, type=int)
        offset = request.args.get('offset', 0, type=int)
        
        if limit < 1 or limit > 100:
            return create_error_response("limitå¿…é¡»åœ¨1-100ä¹‹é—´", "validation_error", 400)
        
        if offset < 0:
            return create_error_response("offsetä¸èƒ½ä¸ºè´Ÿæ•°", "validation_error", 400)
        
        logger.info(f"ğŸ’¬ è·å–ç”¨æˆ·å¯¹è¯: user_id={user_id}")
        
        # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
        if not orchestrator.initialized:
            return create_error_response("å¯¹è¯ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
        
        conversations = orchestrator.conversation_manager.get_user_conversations(
            user_id=user_id,
            limit=limit,
            offset=offset
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        conversation_stats = {
            'total_conversations': len(conversations),
            'active_conversations': len([c for c in conversations if c.get('status') == 'active']),
            'avg_messages_per_conversation': sum(c.get('message_count', 0) for c in conversations) / len(conversations) if conversations else 0
        }
        
        response_data = {
            'conversations': conversations,
            'conversation_stats': conversation_stats,
            'pagination': {
                'user_id': user_id,
                'limit': limit,
                'offset': offset,
                'returned_count': len(conversations)
            }
        }
        
        return create_success_response(response_data, "ç”¨æˆ·å¯¹è¯è·å–æˆåŠŸ")

    except Exception as e:
        logger.error(f"âŒ è·å–ç”¨æˆ·å¯¹è¯å¤±è´¥: {str(e)}")
        return create_error_response(f"å¯¹è¯è·å–å¤±è´¥: {str(e)}", "internal_error", 500)


@qa_routes_bp.route('/system/health', methods=['GET'])
def system_health_check():
    """ğŸ” ç³»ç»Ÿå¥åº·æ£€æŸ¥ - ä½¿ç”¨ç¼–æ’å™¨çš„å¥åº·æ£€æŸ¥"""

    @async_wrapper
    async def _health_check():
        try:
            logger.info("ğŸ” æ‰§è¡ŒQAç³»ç»Ÿå¥åº·æ£€æŸ¥")
            
            # ç¼–æ’å™¨å¥åº·æ£€æŸ¥
            health_status = await orchestrator.health_check()
            
            # è·å–ç³»ç»Ÿç»Ÿè®¡
            orchestrator_stats = orchestrator.get_orchestrator_stats()
            
            # å„ç»„ä»¶çŠ¶æ€æ£€æŸ¥
            component_health = {
                'query_parser': 'healthy' if hasattr(orchestrator, 'query_parser') else 'unavailable',
                'data_analyzer': 'healthy' if hasattr(orchestrator, 'data_analyzer') else 'unavailable',
                'insight_generator': 'healthy' if hasattr(orchestrator, 'insight_generator') else 'unavailable',
                'conversation_manager': 'healthy' if hasattr(orchestrator, 'conversation_manager') else 'unavailable',
                'api_connector': 'healthy' if hasattr(orchestrator, 'api_connector') else 'unavailable'
            }
            
            # AIæ¨¡å‹çŠ¶æ€
            ai_models_status = {
                'claude_available': orchestrator.claude_client is not None,
                'gpt_available': orchestrator.gpt_client is not None,
                'dual_ai_collaboration': orchestrator.claude_client is not None and orchestrator.gpt_client is not None
            }
            
            # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
            healthy_components = sum(1 for status in component_health.values() if status == 'healthy')
            total_components = len(component_health)
            health_score = healthy_components / total_components
            
            overall_status = 'healthy' if health_score >= 0.8 else 'degraded' if health_score >= 0.5 else 'unhealthy'
            
            response_data = {
                'overall_status': overall_status,
                'health_score': health_score,
                'orchestrator_health': health_status,
                'component_health': component_health,
                'ai_models_status': ai_models_status,
                'system_stats': orchestrator_stats,
                'service_info': {
                    'service_name': 'intelligent_qa_service',
                    'version': '2.0.0',
                    'components_count': total_components,
                    'healthy_components': healthy_components,
                    'ai_models': ['claude-sonnet-4', 'gpt-4o'],
                    'capabilities': [
                        'intelligent_query_processing',
                        'complex_data_analysis',
                        'business_insight_generation',
                        'conversation_management',
                        'multi_source_data_integration',
                        'real_time_analysis',
                        'predictive_analytics',
                        'risk_assessment'
                    ]
                },
                'performance_indicators': {
                    'avg_query_processing_time': health_status.get('avg_processing_time', 0),
                    'success_rate': health_status.get('success_rate', 0),
                    'ai_collaboration_usage': health_status.get('ai_collaboration_rate', 0)
                }
            }
            
            status_code = 200 if overall_status == 'healthy' else 503
            
            return jsonify(response_data), status_code
        
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return create_error_response("å¥åº·æ£€æŸ¥å¤±è´¥", "health_check_error", 500)

    return _health_check()


@qa_routes_bp.route('/charts', methods=['POST'])
def generate_intelligent_charts():
    """ğŸ¨ æ™ºèƒ½å›¾è¡¨ç”Ÿæˆ - ä½¿ç”¨ç¼–æ’å™¨çš„å›¾è¡¨ç”Ÿæˆå™¨"""
    try:
        request_data = request.get_json()
        if not request_data:
            return create_error_response("è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯", "validation_error", 400)
        
        # å‚æ•°éªŒè¯
        data = request_data.get('data', {})
        chart_type = request_data.get('chart_type', 'auto')  # auto, line, bar, pie, scatter
        config = request_data.get('config', {})
        preferences = request_data.get('preferences', {})
        
        if not data:
            return create_error_response("å›¾è¡¨æ•°æ®ä¸èƒ½ä¸ºç©º", "validation_error", 400)
        
        logger.info(f"ğŸ¨ ç”Ÿæˆæ™ºèƒ½å›¾è¡¨: {chart_type}")
        
        # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
        if not orchestrator.initialized:
            return create_error_response("å›¾è¡¨ç”Ÿæˆç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
        
        # ä½¿ç”¨ç¼–æ’å™¨çš„å›¾è¡¨ç”Ÿæˆå™¨
        chart_result = orchestrator.chart_generator.generate_chart(
            data=data,
            config=config,
            preferences=preferences
        )
        
        # å¢å¼ºå›¾è¡¨ç»“æœ
        if chart_result.get('success'):
            chart_result['generation_metadata'] = {
                'chart_type_used': chart_result.get('chart_type', chart_type),
                'data_points': len(data.get('values', [])) if isinstance(data, dict) else len(data),
                'ai_optimized': True,
                'generation_time': datetime.now().isoformat()
            }
        
        return create_success_response(chart_result, "å›¾è¡¨ç”Ÿæˆå®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")
        return create_error_response(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}", "internal_error", 500)


@qa_routes_bp.route('/stats', methods=['GET'])
def get_qa_service_stats():
    """ğŸ“Š è·å–QAæœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    
    @async_wrapper
    async def _get_stats():
        try:
            # ç¼–æ’å™¨ç»Ÿè®¡
            orchestrator_stats = orchestrator.get_orchestrator_stats()
            
            # æŸ¥è¯¢è§£æå™¨ç»Ÿè®¡
            parser_stats = {}
            if hasattr(orchestrator, 'query_parser'):
                parser_stats = orchestrator.query_parser.get_processing_stats()
            
            # æ•°æ®åˆ†æå™¨ç»Ÿè®¡
            analyzer_stats = {}
            if hasattr(orchestrator, 'data_analyzer'):
                analyzer_stats = orchestrator.data_analyzer.get_analysis_stats()
            
            # æ´å¯Ÿç”Ÿæˆå™¨ç»Ÿè®¡
            insight_stats = {}
            if hasattr(orchestrator, 'insight_generator'):
                insight_stats = orchestrator.insight_generator.get_insight_stats()
            
            # è®¡ç®—ç»¼åˆæŒ‡æ ‡
            total_queries = parser_stats.get('total_queries', 0)
            total_analyses = analyzer_stats.get('total_analyses', 0)
            total_insights = insight_stats.get('total_insights_generated', 0)
            
            response_data = {
                'service_overview': {
                    'total_queries_processed': total_queries,
                    'total_analyses_performed': total_analyses,
                    'total_insights_generated': total_insights,
                    'avg_confidence_score': (
                        parser_stats.get('average_confidence', 0) +
                        analyzer_stats.get('avg_confidence_score', 0) +
                        insight_stats.get('avg_confidence_score', 0)
                    ) / 3,
                    'service_uptime': orchestrator_stats.get('uptime', 0)
                },
                'component_stats': {
                    'orchestrator': orchestrator_stats,
                    'query_parser': parser_stats,
                    'data_analyzer': analyzer_stats,
                    'insight_generator': insight_stats
                },
                'performance_metrics': {
                    'ai_collaboration_rate': parser_stats.get('ai_collaboration_usage', 0) / max(total_queries, 1) * 100,
                    'critical_insights_rate': insight_stats.get('critical_insights', 0) / max(total_insights, 1) * 100,
                    'actionable_insights_rate': insight_stats.get('actionable_insights', 0) / max(total_insights, 1) * 100
                }
            }
            
            return create_success_response(response_data, "QAæœåŠ¡ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ")
        
        except Exception as e:
            logger.error(f"âŒ è·å–QAç»Ÿè®¡å¤±è´¥: {str(e)}")
            return create_error_response(f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {str(e)}", "internal_error", 500)
    
    return _get_stats()


# ============= é”™è¯¯å¤„ç† =============

@qa_routes_bp.errorhandler(404)
def qa_not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return create_error_response("QAæœåŠ¡ç«¯ç‚¹ä¸å­˜åœ¨", "not_found", 404)


@qa_routes_bp.errorhandler(500)
def qa_internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    logger.error(f"QAæœåŠ¡å†…éƒ¨é”™è¯¯: {str(error)}")
    return create_error_response("QAæœåŠ¡å†…éƒ¨é”™è¯¯", "internal_error", 500)


@qa_routes_bp.errorhandler(400)
def qa_bad_request(error):
    """400é”™è¯¯å¤„ç†"""
    return create_error_response("è¯·æ±‚å‚æ•°é”™è¯¯", "bad_request", 400)