# api/data_routes.py - å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬
from flask import Blueprint, jsonify, request, Response
from typing import Dict, Any, List, Optional
import logging
import asyncio
from datetime import datetime
import json
import traceback
import time
import re

# å¯¼å…¥ç¼–æ’å™¨ - æ ¸å¿ƒæ”¹åŠ¨ï¼
from core.orchestrator.intelligent_qa_orchestrator import get_orchestrator
# å¯¼å…¥æŠ¥å‘Šå’Œå›¾è¡¨ç›¸å…³çš„æšä¸¾å’Œå·¥å…·ï¼ˆå¦‚æœéœ€è¦ç›´æ¥åœ¨æ•°æ®è·¯ç”±ä¸­ä½¿ç”¨ï¼‰
from utils.formatters.report_generator import ReportFormat
from utils.formatters.chart_generator import ChartType

logger = logging.getLogger(__name__)

# åˆ›å»ºæ•°æ®APIè“å›¾
data_bp = Blueprint('data_api', __name__, url_prefix='/api/data')

# ğŸ¯ ä½¿ç”¨å•ä¸€ç¼–æ’å™¨å®ä¾‹ - é¿å…é‡å¤åˆå§‹åŒ–
orchestrator = get_orchestrator()


# ============= å·¥å…·å‡½æ•° =============

def async_route(f):
    """å¼‚æ­¥è·¯ç”±è£…é¥°å™¨"""

    # @wraps(f) # å¯é€‰ï¼Œç”¨äºä¿ç•™åŸå‡½æ•°å…ƒä¿¡æ¯
    def wrapper(*args, **kwargs):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(f(*args, **kwargs))
        finally:
            loop.close()

    wrapper.__name__ = f.__name__
    return wrapper


def validate_request_params(required_params: List[str] = None, optional_params: Dict[str, Any] = None) -> Dict[
    str, Any]:
    """
    éªŒè¯è¯·æ±‚å‚æ•° (GETæŸ¥è¯¢å‚æ•°å’ŒPOST JSON body)
    å¹¶åº”ç”¨ç±»å‹è½¬æ¢å’Œé»˜è®¤å€¼ã€‚
    """
    params = {}
    # è·å–æŸ¥è¯¢å‚æ•°
    params.update(request.args.to_dict())

    # è·å–JSONæ•°æ®ï¼ˆå¦‚æœè¯·æ±‚æ˜¯JSONç±»å‹ï¼‰
    if request.content_type and 'application/json' in request.content_type:
        try:
            json_data = request.get_json()
            if json_data:
                params.update(json_data)
        except Exception as e:
            raise ValueError(f"æ— æ•ˆçš„JSONè¯·æ±‚ä½“: {e}")

    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if required_params:
        missing_params = [param for param in required_params if param not in params]
        if missing_params:
            raise ValueError(f"ç¼ºå°‘å¿…å¡«å‚æ•°: {', '.join(missing_params)}")

    # å¤„ç†å¯é€‰å‚æ•°å’Œç±»å‹è½¬æ¢
    if optional_params:
        for param_name, details in optional_params.items():
            default_value = details.get('default')
            param_type = details.get('type')
            allowed_values = details.get('allowed')

            if param_name in params:
                value = params[param_name]
                if param_type:
                    try:
                        if param_type == bool:  # ç‰¹æ®Šå¤„ç†å¸ƒå°”å€¼
                            value = str(value).lower() in ['true', '1', 'yes', 'on']
                        else:
                            value = param_type(value)
                    except (ValueError, TypeError) as e:
                        raise ValueError(
                            f"å‚æ•° '{param_name}' ç±»å‹é”™è¯¯: åº”ä¸º {param_type.__name__}, æ”¶åˆ° '{value}'. é”™è¯¯: {e}")
                if allowed_values and value not in allowed_values:
                    raise ValueError(
                        f"å‚æ•° '{param_name}' çš„å€¼ '{value}' æ— æ•ˆ. å…è®¸çš„å€¼ä¸º: {', '.join(map(str, allowed_values))}")
                params[param_name] = value
            elif default_value is not None:
                params[param_name] = default_value
            # å¦‚æœå‚æ•°ä¸å­˜åœ¨ä¸”æ²¡æœ‰é»˜è®¤å€¼ï¼Œåˆ™ä¸å°†å…¶æ·»åŠ åˆ°paramsä¸­
    return params


def validate_date_format(date_str: str, param_name: str = "date") -> Optional[str]:
    """éªŒè¯æ—¥æœŸæ ¼å¼YYYYMMDDï¼Œå¦‚æœä¸ºç©ºåˆ™è¿”å›None"""
    if not date_str:
        return None
    if not isinstance(date_str, str):  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
        raise ValueError(f"å‚æ•° '{param_name}' ç±»å‹é”™è¯¯: åº”ä¸ºå­—ç¬¦ä¸², æ”¶åˆ° '{type(date_str).__name__}'")
    if not re.match(r'^\d{8}$', date_str):
        raise ValueError(f"å‚æ•° '{param_name}' ('{date_str}') æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºYYYYMMDDæ ¼å¼")
    try:
        datetime.strptime(date_str, '%Y%m%d')
        return date_str
    except ValueError:
        raise ValueError(f"å‚æ•° '{param_name}' æ—¥æœŸæ— æ•ˆ: {date_str}")


def validate_date_range(start_date_str: Optional[str], end_date_str: Optional[str]) -> tuple[
    Optional[str], Optional[str]]:
    """éªŒè¯æ—¥æœŸèŒƒå›´ï¼Œå…è®¸éƒ¨åˆ†ä¸ºç©º"""
    start_date = validate_date_format(start_date_str, "start_date") if start_date_str else None
    end_date = validate_date_format(end_date_str, "end_date") if end_date_str else None

    if start_date and end_date:
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        if start_dt > end_dt:
            raise ValueError("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
        # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦åˆç†ï¼ˆä¸è¶…è¿‡1å¹´ï¼‰
        if (end_dt - start_dt).days > 365:  # å…è®¸æŸ¥è¯¢æœ€å¤šä¸€å¹´æ•°æ®
            raise ValueError("æ—¥æœŸèŒƒå›´ä¸èƒ½è¶…è¿‡1å¹´")
    elif start_date and not end_date:
        # å¦‚æœåªæœ‰å¼€å§‹æ—¥æœŸï¼Œé»˜è®¤ç»“æŸæ—¥æœŸä¸ºå¼€å§‹æ—¥æœŸ+åˆç†èŒƒå›´ï¼ˆä¾‹å¦‚30å¤©ï¼‰æˆ–å½“å¤©
        # æ ¹æ®ä¸šåŠ¡é€»è¾‘è°ƒæ•´ï¼Œè¿™é‡Œæš‚æ—¶è®¤ä¸ºéœ€è¦æˆå¯¹å‡ºç°æˆ–éƒ½ä¸å‡ºç°
        pass  # æˆ–è€…å¯ä»¥è®¾ç½®é»˜è®¤ç»“æŸæ—¥æœŸ
    elif end_date and not start_date:
        raise ValueError("æä¾›äº†ç»“æŸæ—¥æœŸä½†ç¼ºå°‘å¼€å§‹æ—¥æœŸ")

    return start_date, end_date


def create_success_response(data: Dict[str, Any], message: str = "æ“ä½œæˆåŠŸ", status_code: int = 200) -> tuple:
    """åˆ›å»ºæˆåŠŸå“åº”"""
    return jsonify({
        'success': True,
        'message': message,
        'data': data,
        'timestamp': datetime.now().isoformat(),
        'processed_by': 'intelligent_orchestrator'
    }), status_code


def create_error_response(message: str, error_type: str = "processing_error", status_code: int = 500,
                          details: Optional[Dict[str, Any]] = None) -> tuple:
    """åˆ›å»ºé”™è¯¯å“åº”"""
    response_body = {
        'success': False,
        'error_type': error_type,
        'message': message,
        'timestamp': datetime.now().isoformat(),
        'system_status': 'error'
    }
    if details:
        response_body['details'] = details
    return jsonify(response_body), status_code


# ============= æ ¸å¿ƒæ•°æ®API =============

@data_bp.route('/system', methods=['GET'])
@async_route
async def get_enhanced_system_data():
    """
    ğŸ“Š è·å–å¢å¼ºç³»ç»Ÿæ•°æ® - ä½¿ç”¨ç¼–æ’å™¨å®Œæ•´æ•°æ®é“¾
    æ­¤æ¥å£è¿”å›å½“å‰ç³»ç»Ÿçš„æ•´ä½“æ¦‚è§ˆæ•°æ®ï¼Œå·²ç»è¿‡å†…éƒ¨å¢å¼ºå’ŒéªŒè¯ã€‚
    """
    try:
        logger.info("ğŸ” API: è¯·æ±‚è·å–å¢å¼ºç³»ç»Ÿæ•°æ®...")

        if not orchestrator.initialized:
            await orchestrator.initialize()
            logger.info("Orchestrator re-initialized on demand.")

        # APIConnectorçš„get_system_data()å†…éƒ¨å·²åŒ…å«æ•°æ®å¢å¼ºå’ŒéªŒè¯
        result = await orchestrator.data_fetcher.api_connector.get_system_data()

        if not result.get("success"):
            return create_error_response(
                result.get("message", "ç³»ç»Ÿæ•°æ®è·å–å¤±è´¥"), "api_error", 500
            )

        # enhanced_dataå·²ç»æ˜¯APIConnectorå¤„ç†è¿‡çš„
        enhanced_data = result.get("data", {})
        validation_info = result.get("validation", {})  # APIConnectorè¿”å›çš„éªŒè¯ä¿¡æ¯

        # é¢å¤–çš„è´¢åŠ¡å¥åº·è¯„ä¼° (å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šåŸºäºè§„åˆ™çš„æˆ–ç®€å•çš„AIåˆ†æ)
        financial_health = {}
        try:
            total_inflow = float(enhanced_data.get('æ€»å…¥é‡‘', 0))
            total_outflow = float(enhanced_data.get('æ€»å‡ºé‡‘', 0))
            if total_inflow > 0:
                outflow_ratio = total_outflow / total_inflow
                financial_health = {
                    'outflow_ratio': outflow_ratio,
                    'health_status': 'healthy' if outflow_ratio < 0.7 else 'concerning' if outflow_ratio < 0.9 else 'risky',
                    'net_flow': total_inflow - total_outflow
                }
        except Exception as e:
            logger.warning(f"è´¢åŠ¡å¥åº·è¯„ä¼°å¤±è´¥: {str(e)}")
            financial_health = {'error': 'è¯„ä¼°å¤±è´¥'}

        response_data = {
            'system_overview': enhanced_data,
            'data_validation_summary': validation_info,
            'financial_health_snapshot': financial_health,
            'processing_metadata': {
                'data_source': '/api/sta/system',
                'enhancement_applied': True,  # å‡è®¾APIConnectorå†…éƒ¨å¤„ç†äº†
                'validation_performed': True,  # å‡è®¾APIConnectorå†…éƒ¨å¤„ç†äº†
            }
        }

        logger.info("âœ… API: ç³»ç»Ÿæ•°æ®å¢å¼ºå¤„ç†å®Œæˆ")
        return create_success_response(response_data, "ç³»ç»Ÿæ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except Exception as e:
        logger.error(f"âŒ API: è·å–ç³»ç»Ÿæ•°æ®å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"ç³»ç»Ÿæ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/daily', methods=['GET'])
@async_route
async def get_enhanced_daily_data():
    """
    ğŸ“… è·å–å¢å¼ºæ¯æ—¥æ•°æ® - æ™ºèƒ½æ—¥æœŸå¤„ç†
    å‚æ•°:
    - date (å¯é€‰): YYYYMMDDæ ¼å¼çš„æ—¥æœŸã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é»˜è®¤ä¸ºå½“å¤©ã€‚
    """
    try:
        logger.info("ğŸ“… API: è¯·æ±‚è·å–å¢å¼ºæ¯æ—¥æ•°æ®...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(optional_params={'date': {'type': str}})
        date_param = params.get('date')

        if date_param:  # ä»…å½“date_paramå­˜åœ¨æ—¶æ‰éªŒè¯
            date_param = validate_date_format(date_param, "date")

        # APIConnectorçš„get_daily_data()å†…éƒ¨å·²åŒ…å«æ•°æ®å¢å¼ºå’ŒéªŒè¯
        result = await orchestrator.data_fetcher.api_connector.get_daily_data(date_param)

        if not result.get("success"):
            return create_error_response(result.get("message", "æ¯æ—¥æ•°æ®è·å–å¤±è´¥"), "api_error", 500)

        enhanced_data = result.get("data", {})
        validation_info = result.get("validation", {})

        # ç®€å•çš„æ—¥åº¦è¡¨ç°è¯„ä¼°
        daily_performance = {}
        if enhanced_data:
            try:
                inflow = float(enhanced_data.get('å…¥é‡‘', 0))
                outflow = float(enhanced_data.get('å‡ºé‡‘', 0))
                registrations = int(enhanced_data.get('æ³¨å†Œäººæ•°', 0))
                purchases = int(enhanced_data.get('è´­ä¹°äº§å“æ•°é‡', 0))

                daily_performance = {
                    'net_flow': inflow - outflow,
                    'flow_ratio': (inflow / outflow) if outflow > 0 else float('inf') if inflow > 0 else 0,
                    'conversion_indicator': (purchases / registrations) if registrations > 0 else 0,  # ç®€åŒ–æŒ‡æ ‡
                    'activity_level_indicator': (registrations + purchases) / 2  # ç®€åŒ–æŒ‡æ ‡
                }
            except Exception as e:
                logger.warning(f"æ—¥åº¦è¡¨ç°è¯„ä¼°å¤±è´¥: {str(e)}")
                daily_performance = {'error': 'è¯„ä¼°å¤±è´¥'}

        response_data = {
            'daily_summary': enhanced_data,
            'data_validation_summary': validation_info,
            'daily_performance_snapshot': daily_performance,
            'query_date': date_param or datetime.now().strftime('%Y%m%d'),
            'processing_metadata': {
                'date_processing': 'automatic_latest' if not date_param else 'specified_date'
            }
        }
        logger.info("âœ… API: æ¯æ—¥æ•°æ®å¢å¼ºå¤„ç†å®Œæˆ")
        return create_success_response(response_data, "æ¯æ—¥æ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: è·å–æ¯æ—¥æ•°æ®å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"æ¯æ—¥æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/products', methods=['GET'])
@async_route
async def get_enhanced_products_data():
    """
    ğŸ›ï¸ è·å–å¢å¼ºäº§å“æ•°æ® - åŒ…å«åŸºæœ¬çš„äº§å“è¡¨ç°å¿«ç…§
    å‚æ•°:
    - include_expiry (å¯é€‰, bool, é»˜è®¤true): æ˜¯å¦åŒ…å«äº§å“åˆ°æœŸä¿¡æ¯ã€‚
    - include_analysis (å¯é€‰, bool, é»˜è®¤false): æ˜¯å¦è¿›è¡Œç®€å•çš„äº§å“è¡¨ç°åˆ†æã€‚
    """
    try:
        logger.info("ğŸ›ï¸ API: è¯·æ±‚è·å–å¢å¼ºäº§å“æ•°æ®...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(optional_params={
            'include_expiry': {'type': bool, 'default': True},
            'include_analysis': {'type': bool, 'default': False}
        })
        include_expiry = params['include_expiry']
        include_analysis = params['include_analysis']

        result = await orchestrator.data_fetcher.api_connector.get_product_data()

        if not result.get("success"):
            return create_error_response(result.get("message", "äº§å“æ•°æ®è·å–å¤±è´¥"), "api_error", 500)

        # APIConnector å†…éƒ¨çš„ _enhance_product_data ä¼¼ä¹æœªè¢«å…¶å…¬å…±æ–¹æ³• get_product_data è°ƒç”¨
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾ result["data"] æ˜¯åŸå§‹æˆ–è½»åº¦å¢å¼ºçš„æ•°æ®
        raw_product_data = result.get("data", {})

        # å¦‚æœéœ€è¦ï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾å¼è°ƒç”¨ä¸€ä¸ªç¼–æ’å™¨å±‚é¢çš„å¢å¼ºæ–¹æ³•ï¼Œæˆ–è€… SmartDataFetcher çš„æ–¹æ³•
        # ä¸ºç®€å•èµ·è§ï¼Œæ­¤å¤„ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œå¹¶è¿›è¡Œä¸€äº›åŸºæœ¬åˆ†æ
        product_list = raw_product_data.get("äº§å“åˆ—è¡¨", [])
        product_performance_snapshot = {}
        product_insights = {}

        if include_analysis and product_list:
            try:
                top_products = sorted(product_list, key=lambda x: int(x.get('æ€»è´­ä¹°æ¬¡æ•°', 0)), reverse=True)[:3]
                low_utilization_products = [
                    p for p in product_list
                    if int(p.get('å½“å‰æŒæœ‰æ•°', 0)) / max(int(p.get('æ€»è´­ä¹°æ¬¡æ•°', 1)), 1) < 0.2  # å‡è®¾åˆ©ç”¨ç‡ä½äº20%ä¸ºä½
                ]
                product_performance_snapshot = {
                    'total_products': raw_product_data.get("äº§å“æ€»æ•°", len(product_list)),
                    'top_3_performing_by_purchase': [p.get('äº§å“åç§°', 'æœªçŸ¥äº§å“') for p in top_products],
                    'low_utilization_product_count': len(low_utilization_products),
                    'products_with_upcoming_expiry_count': sum(
                        1 for p in product_list if p.get("æŒæœ‰æƒ…å†µ", {}).get("å³å°†åˆ°æœŸæ•°", {}).get("7å¤©å†…", 0) > 0)
                }
                # ç®€å•çš„æ´å¯Ÿ
                product_insights = {
                    "key_observation": f"å…±è¿½è¸ª {product_performance_snapshot['total_products']} æ¬¾äº§å“ã€‚",
                    "popular_products_info": f"æœ€å—æ¬¢è¿çš„äº§å“åŒ…æ‹¬ï¼š{', '.join(product_performance_snapshot['top_3_performing_by_purchase'])}ã€‚",
                    "utilization_alert": f"æœ‰ {product_performance_snapshot['low_utilization_product_count']} æ¬¾äº§å“åˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯èƒ½éœ€è¦å…³æ³¨ã€‚" if
                    product_performance_snapshot[
                        'low_utilization_product_count'] > 0 else "æ‰€æœ‰äº§å“åˆ©ç”¨ç‡å‡åœ¨å¯æ¥å—èŒƒå›´ã€‚",
                    "expiry_outlook": f"æœªæ¥7å¤©å†…æœ‰ {product_performance_snapshot['products_with_upcoming_expiry_count']} æ¬¾äº§å“æœ‰åˆ°æœŸæƒ…å†µã€‚" if include_expiry else "æœªåˆ†æäº§å“åˆ°æœŸæƒ…å†µã€‚"
                }

            except Exception as e:
                logger.warning(f"äº§å“è¡¨ç°å¿«ç…§åˆ†æå¤±è´¥: {str(e)}")
                product_performance_snapshot = {'error': 'å¿«ç…§åˆ†æå¤±è´¥'}
                product_insights = {'error': 'æ´å¯Ÿç”Ÿæˆå¤±è´¥'}

        response_data = {
            'product_catalog': raw_product_data,  # è¿”å›APIConnectorè·å–çš„æ•°æ®
            'product_performance_snapshot': product_performance_snapshot,
            'product_insights_summary': product_insights,  # æ›¿æ¢åŸproduct_insights
            'filters_applied': {
                'include_expiry_info': include_expiry,  # APIConnectorå†…éƒ¨å·²å¤„ç†
                'simple_analysis_included': include_analysis
            }
        }
        logger.info("âœ… API: äº§å“æ•°æ®å¤„ç†å®Œæˆ")
        return create_success_response(response_data, "äº§å“æ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: è·å–äº§å“æ•°æ®å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"äº§å“æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/users/daily', methods=['GET'])
@async_route
async def get_enhanced_user_daily_data():
    """
    ğŸ‘¥ è·å–ç”¨æˆ·æ¯æ—¥æ•°æ® - åŒ…å«åŸºæœ¬çš„VIPåˆ†å¸ƒå’Œå¢é•¿å¿«ç…§
    å‚æ•°:
    - date (å¯é€‰): YYYYMMDDæ ¼å¼çš„æ—¥æœŸã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é»˜è®¤ä¸ºå½“å¤©ã€‚
    """
    try:
        logger.info("ğŸ‘¥ API: è¯·æ±‚è·å–ç”¨æˆ·æ¯æ—¥æ•°æ®...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(optional_params={'date': {'type': str}})
        date_param = params.get('date')
        if date_param:
            date_param = validate_date_format(date_param, "date")

        result = await orchestrator.data_fetcher.api_connector.get_user_daily_data(date_param)

        if not result.get("success"):
            return create_error_response(result.get("message", "ç”¨æˆ·æ¯æ—¥æ•°æ®è·å–å¤±è´¥"), "api_error", 500)

        raw_user_daily_data = result.get("data", {})
        # APIConnectorçš„get_user_daily_dataä¼¼ä¹ä¸ç›´æ¥è°ƒç”¨å†…éƒ¨çš„_enhance_user_daily_data
        # æˆ‘ä»¬åœ¨æ­¤å¤„åšä¸€äº›ç®€å•åˆ†æ

        vip_analysis_snapshot = {}
        growth_snapshot = {}

        daily_data_list = raw_user_daily_data.get("æ¯æ—¥æ•°æ®", [])
        if daily_data_list:
            latest_day_data = daily_data_list[-1]  # é€šå¸¸APIè¿”å›çš„æ˜¯å•ä¸ªæ—¥æœŸæˆ–æ’åºåçš„åˆ—è¡¨

            try:  # VIPåˆ†æ
                total_users_on_date = sum(int(latest_day_data.get(f'vip{i}çš„äººæ•°', 0)) for i in range(11))
                vip_distribution = {
                    f'vip{i}': {
                        'count': int(latest_day_data.get(f'vip{i}çš„äººæ•°', 0)),
                        'percentage': (int(latest_day_data.get(f'vip{i}çš„äººæ•°',
                                                               0)) / total_users_on_date * 100) if total_users_on_date > 0 else 0
                    } for i in range(11)
                }
                vip_analysis_snapshot = {
                    'total_users_on_record_date': total_users_on_date,
                    'vip_distribution': vip_distribution,
                    'new_users_on_record_date': int(latest_day_data.get('æ–°å¢ç”¨æˆ·æ•°', 0)),
                    'analysis_date': latest_day_data.get('æ—¥æœŸ', date_param or datetime.now().strftime('%Y%m%d'))
                }
            except Exception as e:
                logger.warning(f"VIPåˆ†æå¿«ç…§å¤±è´¥: {str(e)}")
                vip_analysis_snapshot = {'error': 'VIPåˆ†æå¤±è´¥'}

            try:  # å¢é•¿å¿«ç…§ (å¦‚æœæ•°æ®æ˜¯åˆ—è¡¨ä¸”å¤šäºä¸€å¤©)
                if len(daily_data_list) > 1:
                    new_users_series = [int(d.get('æ–°å¢ç”¨æˆ·æ•°', 0)) for d in daily_data_list]
                    avg_daily_growth_period = sum(new_users_series) / len(new_users_series)
                    growth_snapshot = {
                        'avg_daily_new_users_in_period': avg_daily_growth_period,
                        'period_trend_indicator': 'increasing' if len(new_users_series) > 1 and new_users_series[-1] >
                                                                  new_users_series[0] else 'stable/decreasing',
                        'data_points_in_period': len(new_users_series)
                    }
                elif daily_data_list:  # å•æ—¥æ•°æ®
                    growth_snapshot = {'new_users_today': int(daily_data_list[0].get('æ–°å¢ç”¨æˆ·æ•°', 0))}

            except Exception as e:
                logger.warning(f"å¢é•¿åˆ†æå¿«ç…§å¤±è´¥: {str(e)}")
                growth_snapshot = {'error': 'å¢é•¿åˆ†æå¤±è´¥'}

        response_data = {
            'user_daily_records': raw_user_daily_data,
            'vip_analysis_snapshot': vip_analysis_snapshot,
            'growth_snapshot': growth_snapshot,
            'query_date_effective': date_param or vip_analysis_snapshot.get('analysis_date',
                                                                            datetime.now().strftime('%Y%m%d'))
        }
        logger.info("âœ… API: ç”¨æˆ·æ¯æ—¥æ•°æ®å¤„ç†å®Œæˆ")
        return create_success_response(response_data, "ç”¨æˆ·æ¯æ—¥æ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: è·å–ç”¨æˆ·æ¯æ—¥æ•°æ®å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"ç”¨æˆ·æ¯æ—¥æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/users/detailed', methods=['GET'])
@async_route
async def get_enhanced_user_detailed_data():
    """
    ğŸ“Š è·å–è¯¦ç»†ç”¨æˆ·æ•°æ® - åˆ†é¡µï¼ŒåŒ…å«åŸºæœ¬çš„é¡µé¢ç»Ÿè®¡
    å‚æ•°:
    - page (å¯é€‰, int, é»˜è®¤1): é¡µç ã€‚
    - include_stats (å¯é€‰, bool, é»˜è®¤true): æ˜¯å¦åŒ…å«æœ¬é¡µç”¨æˆ·æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    try:
        logger.info("ğŸ“Š API: è¯·æ±‚è·å–è¯¦ç»†ç”¨æˆ·æ•°æ®...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(optional_params={
            'page': {'type': int, 'default': 1},
            'include_stats': {'type': bool, 'default': True}
        })
        page = params['page']
        include_stats = params['include_stats']

        if page < 1:
            return create_error_response("é¡µç å¿…é¡»æ˜¯æ­£æ•´æ•°", "validation_error", 400)

        result = await orchestrator.data_fetcher.api_connector.get_user_data(page)

        if not result.get("success"):
            return create_error_response(result.get("message", "ç”¨æˆ·è¯¦ç»†æ•°æ®è·å–å¤±è´¥"), "api_error", 500)

        raw_user_data = result.get("data", {})
        user_page_statistics = {}

        if include_stats:
            user_list = raw_user_data.get("ç”¨æˆ·åˆ—è¡¨", [])
            if user_list:
                try:
                    total_investment_page = sum(float(u.get('æ€»æŠ•å…¥', 0)) for u in user_list)
                    total_rewards_page = sum(float(u.get('ç´¯è®¡è·å¾—å¥–åŠ±é‡‘é¢', 0)) for u in user_list)
                    roi_values_page = [float(u.get('æŠ•æŠ¥æ¯”', 0)) for u in user_list if u.get('æŠ•æŠ¥æ¯”') is not None]

                    user_page_statistics = {
                        'users_on_this_page': len(user_list),
                        'total_investment_on_page': total_investment_page,
                        'total_rewards_on_page': total_rewards_page,
                        'avg_roi_on_page': (sum(roi_values_page) / len(roi_values_page)) if roi_values_page else 0,
                        'avg_investment_per_user_on_page': (total_investment_page / len(user_list)) if user_list else 0
                    }
                except Exception as e:
                    logger.warning(f"ç”¨æˆ·é¡µé¢ç»Ÿè®¡è®¡ç®—å¤±è´¥: {str(e)}")
                    user_page_statistics = {'error': 'é¡µé¢ç»Ÿè®¡è®¡ç®—å¤±è´¥'}

        response_data = {
            'user_data_page': raw_user_data,
            'user_page_statistics': user_page_statistics,
            'pagination_info': {
                'current_page': page,
                'total_records': raw_user_data.get('æ€»è®°å½•æ•°', 0),
                'total_pages': raw_user_data.get('æ€»é¡µæ•°', 0)
            }
        }
        logger.info(f"âœ… API: ç”¨æˆ·è¯¦ç»†æ•°æ®å¤„ç†å®Œæˆ: ç¬¬{page}é¡µ")
        return create_success_response(response_data, f"ç”¨æˆ·è¯¦ç»†æ•°æ®(ç¬¬{page}é¡µ)è·å–æˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: è·å–ç”¨æˆ·è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"ç”¨æˆ·è¯¦ç»†æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/expiry', methods=['GET'])
@async_route
async def get_enhanced_expiry_data():
    """
    â° è·å–å¢å¼ºåˆ°æœŸæ•°æ® - åŒ…å«åŸºæœ¬çš„é£é™©å’Œå¤æŠ•å¿«ç…§
    å‚æ•°:
    - period (å¯é€‰, str, é»˜è®¤'week'): æŸ¥è¯¢å‘¨æœŸ ('today', 'tomorrow', 'week', 'single', 'range')
    - date (å¯é€‰, str): YYYYMMDDæ ¼å¼, period='single'æ—¶å¿…éœ€
    - start_date (å¯é€‰, str): YYYYMMDDæ ¼å¼, period='range'æ—¶å¿…éœ€
    - end_date (å¯é€‰, str): YYYYMMDDæ ¼å¼, period='range'æ—¶å¿…éœ€
    - include_analysis (å¯é€‰, bool, é»˜è®¤true): æ˜¯å¦åŒ…å«é£é™©å’Œå¤æŠ•å¿«ç…§
    """
    try:
        logger.info("â° API: è¯·æ±‚è·å–å¢å¼ºåˆ°æœŸæ•°æ®...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(optional_params={
            'period': {'type': str, 'default': 'week', 'allowed': ['today', 'tomorrow', 'week', 'single', 'range']},
            'date': {'type': str},
            'start_date': {'type': str},
            'end_date': {'type': str},
            'include_analysis': {'type': bool, 'default': True}
        })
        period = params['period']
        date_param = params.get('date')
        start_date_param = params.get('start_date')
        end_date_param = params.get('end_date')
        include_analysis = params['include_analysis']

        api_call_params = {}
        if period == 'single':
            if not date_param: return create_error_response("å•æ—¥æŸ¥è¯¢ç¼ºå°‘ 'date' å‚æ•°", "validation_error", 400)
            api_call_params['date'] = validate_date_format(date_param, "date")
            result = await orchestrator.data_fetcher.api_connector.get_product_end_data(api_call_params['date'])
        elif period == 'range':
            if not start_date_param or not end_date_param: return create_error_response(
                "èŒƒå›´æŸ¥è¯¢ç¼ºå°‘ 'start_date' æˆ– 'end_date' å‚æ•°", "validation_error", 400)
            api_call_params['start_date'], api_call_params['end_date'] = validate_date_range(start_date_param,
                                                                                             end_date_param)
            result = await orchestrator.data_fetcher.api_connector.get_product_end_interval(
                api_call_params['start_date'], api_call_params['end_date'])
        elif period == 'today':
            result = await orchestrator.data_fetcher.api_connector.get_expiring_products_today()
        elif period == 'tomorrow':
            result = await orchestrator.data_fetcher.api_connector.get_expiring_products_tomorrow()
        elif period == 'week':
            result = await orchestrator.data_fetcher.api_connector.get_expiring_products_week()
        else:  # Should not happen due to 'allowed' in validate_request_params
            return create_error_response(f"ä¸æ”¯æŒçš„æœŸé—´ç±»å‹: {period}", "validation_error", 400)

        if not result.get("success"):
            return create_error_response(result.get("message", "åˆ°æœŸæ•°æ®è·å–å¤±è´¥"), "api_error", 500)

        raw_expiry_data = result.get("data", {})
        expiry_analysis_snapshot = {}

        if include_analysis and raw_expiry_data:
            try:
                expiry_amount = float(raw_expiry_data.get("åˆ°æœŸé‡‘é¢", 0))
                expiry_quantity = int(raw_expiry_data.get("åˆ°æœŸæ•°é‡", 0))
                risk_level = 'high' if expiry_amount > 1000000 else 'medium' if expiry_amount > 200000 else 'low'  # ç®€åŒ–é£é™©è¯„ä¼°

                # å‡è®¾çš„å¤æŠ•ç‡å’Œæœºä¼š
                assumed_reinvestment_rate = 0.5  # 50%
                potential_reinvestment = expiry_amount * assumed_reinvestment_rate

                expiry_analysis_snapshot = {
                    'total_expiry_amount_in_period': expiry_amount,
                    'total_expiry_quantity_in_period': expiry_quantity,
                    'liquidity_risk_indicator': risk_level,
                    'potential_reinvestment_amount': potential_reinvestment,
                    'estimated_cash_outflow': expiry_amount * (1 - assumed_reinvestment_rate)
                }
            except Exception as e:
                logger.warning(f"åˆ°æœŸæ•°æ®å¿«ç…§åˆ†æå¤±è´¥: {str(e)}")
                expiry_analysis_snapshot = {'error': 'å¿«ç…§åˆ†æå¤±è´¥'}

        response_data = {
            'expiry_data_for_period': raw_expiry_data,
            'expiry_analysis_snapshot': expiry_analysis_snapshot,
            'query_parameters': {
                'period_type': period,
                **api_call_params
            }
        }
        logger.info("âœ… API: åˆ°æœŸæ•°æ®å¤„ç†å®Œæˆ")
        return create_success_response(response_data, "åˆ°æœŸæ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: è·å–åˆ°æœŸæ•°æ®å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"åˆ°æœŸæ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/analytics', methods=['POST'])  # Changed to POST to accept more complex parameters
@async_route
async def get_enhanced_analytics_data():
    """
    ğŸ“ˆ è·å–å¢å¼ºåˆ†ææ•°æ® - è§¦å‘ç‰¹å®šçš„åå°åˆ†ææµç¨‹
    è¯·æ±‚ä½“ (JSON):
    {
        "analysis_type": "comprehensive" | "trend" | "performance" | "risk",
        "time_range_days": 30, // åˆ†æçš„æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰
        "metrics": ["total_balance", "net_inflow"], // å¯é€‰ï¼Œå…·ä½“åˆ†æçš„æŒ‡æ ‡
        "scope": "financial" // å¯é€‰ï¼Œåˆ†æèŒƒå›´ï¼Œå¦‚ financial, user, product
        "include_insights": true // æ˜¯å¦ç”ŸæˆAIæ´å¯Ÿ
    }
    """
    try:
        logger.info("ğŸ“ˆ API: è¯·æ±‚è·å–å¢å¼ºåˆ†ææ•°æ®...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(
            required_params=['analysis_type'],
            optional_params={
                'time_range_days': {'type': int, 'default': 30},
                'metrics': {'type': list, 'default': []},
                'scope': {'type': str, 'default': 'financial'},  # 'financial' is a sensible default
                'include_insights': {'type': bool, 'default': True}
            }
        )
        analysis_type = params['analysis_type']
        time_range_days = params['time_range_days']
        metrics_to_analyze = params['metrics']
        analysis_scope = params['scope']
        include_insights = params['include_insights']

        if time_range_days < 1 or time_range_days > 365:
            return create_error_response("æ—¶é—´èŒƒå›´å¿…é¡»åœ¨1-365å¤©ä¹‹é—´", "validation_error", 400)

        valid_analysis_types = ['comprehensive', 'trend', 'performance', 'risk']
        if analysis_type not in valid_analysis_types:
            return create_error_response(f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}", "validation_error", 400)

        analysis_results = []
        # æ ¹æ® orchestrator çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åº”è¯¥è°ƒç”¨å…¶åˆ†æç»„ä»¶
        # FinancialDataAnalyzer æ˜¯ä¸»è¦çš„åˆ†æå¼•æ“

        if analysis_type == 'comprehensive' or analysis_type == 'trend':
            for metric_name in metrics_to_analyze if metrics_to_analyze else ['total_balance',
                                                                              'net_inflow']:  # Default metrics
                trend_res = await orchestrator.financial_data_analyzer.analyze_trend(
                    data_source=analysis_scope if analysis_scope in ['system', 'daily', 'product',
                                                                     'user'] else 'system',  # Adapt scope
                    metric=metric_name,
                    time_range=time_range_days
                )
                analysis_results.append(trend_res)

        if analysis_type == 'comprehensive' or analysis_type == 'performance':
            perf_res = await orchestrator.financial_data_analyzer.analyze_business_performance(
                scope=analysis_scope,
                time_range=time_range_days
            )
            analysis_results.append(perf_res)

        # if analysis_type == 'comprehensive' or analysis_type == 'risk':
        #     risk_metrics = metrics_to_analyze if metrics_to_analyze else ['total_balance', 'daily_outflow']
        #     risk_res = await orchestrator.financial_data_analyzer.detect_anomalies(
        #         data_source=analysis_scope if analysis_scope in ['system', 'daily'] else 'system',  # Adapt scope
        #         metrics=risk_metrics
        #     )
        #     analysis_results.append(risk_res)

        # å¦‚æœæ²¡æœ‰é€‰æ‹©å…·ä½“åˆ†æç±»å‹ä½†æœ‰æŒ‡æ ‡ï¼Œé»˜è®¤åšè¶‹åŠ¿åˆ†æ
        if not analysis_results and metrics_to_analyze:
            for metric_name in metrics_to_analyze:
                trend_res = await orchestrator.financial_data_analyzer.analyze_trend(
                    data_source=analysis_scope if analysis_scope in ['system', 'daily', 'product',
                                                                     'user'] else 'system',
                    metric=metric_name,
                    time_range=time_range_days
                )
                analysis_results.append(trend_res)

        insights_package = {}
        if include_insights and analysis_results:
            try:
                # è¿‡æ»¤æ‰Noneæˆ–é”™è¯¯çš„åˆ†æç»“æœ
                valid_analysis_results = [res for res in analysis_results if hasattr(res, 'analysis_type')]

                insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
                    analysis_results=valid_analysis_results,
                    user_context=None,  # å¯ä»¥ä»è¯·æ±‚ä¸­è·å–ç”¨æˆ·ä¸Šä¸‹æ–‡
                    focus_areas=[analysis_type] if analysis_type != 'comprehensive' else ['financial_health',
                                                                                          'risk_management',
                                                                                          'growth_analysis']
                )
                insights_package = {
                    'generated_insights': [insight.__dict__ if hasattr(insight, '__dict__') else insight for insight in
                                           insights],
                    'generation_metadata': metadata
                }
            except Exception as e:
                logger.warning(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
                insights_package = {'error': f'æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}'}

        # æ¸…ç†å’Œæ ¼å¼åŒ–åˆ†æç»“æœ
        formatted_analysis_results = []
        for res in analysis_results:
            if hasattr(res, 'analysis_type'):  # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„AnalysisResultå¯¹è±¡
                formatted_analysis_results.append({
                    'analysis_id': res.analysis_id,
                    'type': res.analysis_type.value,
                    'scope': res.analysis_scope.value,
                    'confidence': res.confidence_score,
                    'key_findings': res.key_findings,
                    'metrics': res.metrics,
                    'trends': res.trends,
                    'anomalies': res.anomalies,
                    'recommendations': res.recommendations,
                    'processing_time': res.processing_time
                })

        response_data = {
            'triggered_analysis_type': analysis_type,
            'analysis_time_range_days': time_range_days,
            'analysis_scope': analysis_scope,
            'metrics_analyzed_explicitly': metrics_to_analyze,
            'detailed_analysis_results': formatted_analysis_results,
            'ai_insights_package': insights_package
        }
        logger.info(f"âœ… API: {analysis_type}åˆ†ææ•°æ®å¤„ç†å®Œæˆ")
        return create_success_response(response_data, f"{analysis_type}åˆ†æå®Œæˆ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: è·å–åˆ†ææ•°æ®å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"åˆ†ææ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


# generate-report å’Œ generate-data-report è·¯ç”±ç›®å‰çœ‹èµ·æ¥è€¦åˆäº†æ•°æ®è·å–å’ŒæŠ¥å‘Šç”Ÿæˆã€‚
# åœ¨ä¸€ä¸ªæ›´çº¯ç²¹çš„ "data_routes" ä¸­ï¼Œå®ƒä»¬å¯èƒ½åªè´Ÿè´£æä¾›æ ¼å¼åŒ–çš„æ•°æ®ç»™å‰ç«¯æˆ–å¦ä¸€ä¸ªæœåŠ¡æ¥ç”ŸæˆæŠ¥å‘Šã€‚
# ä½†æ—¢ç„¶å®ƒä»¬åœ¨è¿™é‡Œï¼Œæˆ‘ä¼šç¡®ä¿å®ƒä»¬ä½¿ç”¨ orchestrator çš„ç»„ä»¶ã€‚

@data_bp.route('/generate-report', methods=['POST'])
@async_route
async def generate_analysis_report():
    """
    ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š - åŸºäºæä¾›çš„åˆ†ææ•°æ®æˆ–è§¦å‘æ–°çš„åˆ†ææ¥åˆ›å»ºä¸“ä¸šæŠ¥å‘Š
    è¯·æ±‚ä½“ (JSON):
    {
        "report_type": "financial" | "trend" | "comparison", // æŠ¥å‘Šç±»å‹
        "data": {}, // ç”¨äºç”ŸæˆæŠ¥å‘Šçš„æ•°æ®ï¼Œå¦‚æœä¸ºç©ºï¼Œåˆ™ç³»ç»Ÿä¼šå°è¯•è·å–é»˜è®¤æ•°æ®
        "title": "è‡ªå®šä¹‰æŠ¥å‘Šæ ‡é¢˜", // å¯é€‰
        "format": "html" | "pdf" | "markdown", // è¾“å‡ºæ ¼å¼ï¼Œé»˜è®¤html
        "time_range_days": 30 // å¦‚æœdataä¸ºç©ºï¼Œç”¨äºè·å–æ•°æ®çš„é»˜è®¤æ—¶é—´èŒƒå›´
    }
    """
    try:
        logger.info("ğŸ“Š API: è¯·æ±‚ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(
            required_params=['report_type'],
            optional_params={
                'data': {'type': dict, 'default': {}},
                'title': {'type': str, 'default': 'æ•°æ®åˆ†ææŠ¥å‘Š'},
                'format': {'type': str, 'default': 'html', 'allowed': ['html', 'pdf', 'markdown']},
                'time_range_days': {'type': int, 'default': 30},
                'period': {'type': str}  # For trend reports
            }
        )
        report_type = params['report_type']
        report_data_from_request = params['data']
        report_title = params['title']
        report_format_str = params['format']
        time_range_days = params['time_range_days']
        period_for_trend = params.get('period')

        valid_report_types = ['financial', 'trend', 'comparison']
        if report_type not in valid_report_types:
            return create_error_response(f"æ— æ•ˆçš„æŠ¥å‘Šç±»å‹: {report_type}", "validation_error", 400)

        # å¦‚æœè¯·æ±‚ä¸­æ²¡æœ‰æä¾›æ•°æ®ï¼Œåˆ™å°è¯•è·å–æ•°æ®
        if not report_data_from_request:
            logger.info(f"æœªæä¾›æŠ¥å‘Šæ•°æ®ï¼Œå°†ä¸º'{report_type}'ç±»å‹æŠ¥å‘Šè·å–é»˜è®¤æ•°æ®...")
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ•°æ®è·å–é€»è¾‘ï¼Œå®é™…å¯èƒ½æ›´å¤æ‚
            if report_type == 'financial':
                financial_performance = await orchestrator.financial_data_analyzer.analyze_business_performance(
                    'financial', time_range_days)
                # å°†AnalysisResultè½¬æ¢ä¸ºgenerate_financial_reportæœŸæœ›çš„å­—å…¸æ ¼å¼
                report_data_from_request = {
                    'subtitle': f"{time_range_days}å¤©è´¢åŠ¡è¡¨ç°",
                    'summary': {'content': '; '.join(financial_performance.key_findings),
                                'key_metrics': [{'name': k, 'value': v, 'format_type': 'auto'} for k, v in
                                                financial_performance.metrics.items()]},
                    'analysis_sections': [{'title': 'è¶‹åŠ¿åˆ†æ', 'content': '; '.join(
                        t.get('direction', 'stable') for t in financial_performance.trends),
                                           'charts': []}] if financial_performance.trends else []
                }
            elif report_type == 'trend':
                trend_analysis = await orchestrator.financial_data_analyzer.analyze_trend('system', 'total_balance',
                                                                                          time_range_days)
                # è½¬æ¢ä¸ºgenerate_trend_reportæœŸæœ›çš„æ ¼å¼
                report_data_from_request = {
                    'summary': {'content': '; '.join(trend_analysis.key_findings)},
                    'trends': [{'title': trend_analysis.metrics.get('metric_name', 'è¶‹åŠ¿'),
                                'description': trend_analysis.trends[0].get('direction', 'æœªçŸ¥'),
                                'data': trend_analysis.metrics.get('supporting_data',
                                                                   [])}] if trend_analysis.trends else []
                }
                if not period_for_trend: period_for_trend = f"æœ€è¿‘{time_range_days}å¤©"

            # å¯ä»¥ä¸º 'comparison' ç±»å‹æ·»åŠ ç±»ä¼¼çš„æ•°æ®è·å–é€»è¾‘
            else:
                logger.warning(f"æŠ¥å‘Šç±»å‹ '{report_type}' çš„è‡ªåŠ¨æ•°æ®è·å–æœªå®Œå…¨å®ç°ï¼Œå¯èƒ½å¯¼è‡´æŠ¥å‘Šæ•°æ®ä¸è¶³ã€‚")
                # è·å–é€šç”¨ç³»ç»Ÿæ•°æ®ä½œä¸ºåŸºç¡€
                system_data_res = await orchestrator.data_fetcher.api_connector.get_system_data()
                if system_data_res.get("success"):
                    report_data_from_request = system_data_res["data"]
                else:
                    return create_error_response("è‡ªåŠ¨è·å–æŠ¥å‘Šæ•°æ®å¤±è´¥", "data_fetch_error", 500)

        output_format_enum = ReportFormat[report_format_str.upper()]

        # ä½¿ç”¨ç¼–æ’å™¨ä¸­çš„æŠ¥å‘Šç”Ÿæˆå™¨
        report_object = None
        if report_type == 'financial':
            report_object = orchestrator.report_generator.generate_financial_report(
                data=report_data_from_request, title=report_title, output_format=output_format_enum
            )
        elif report_type == 'trend':
            report_object = orchestrator.report_generator.generate_trend_report(
                trend_data=report_data_from_request, title=report_title, period=period_for_trend,
                output_format=output_format_enum
            )
        elif report_type == 'comparison':
            report_object = orchestrator.report_generator.generate_comparison_report(
                comparison_data=report_data_from_request, title=report_title, output_format=output_format_enum
            )

        if not report_object:
            return create_error_response(f"æ— æ³•ä¸ºç±»å‹ '{report_type}' ç”ŸæˆæŠ¥å‘Šå¯¹è±¡", "report_generation_error", 500)

        report_content = ""
        if output_format_enum == ReportFormat.HTML:
            report_content = report_object.to_html()
            mimetype = 'text/html'
        elif output_format_enum == ReportFormat.MARKDOWN:
            report_content = report_object.to_markdown()
            mimetype = 'text/markdown'
        elif output_format_enum == ReportFormat.PDF:
            import tempfile, os
            temp_dir = tempfile.gettempdir()
            # ç¡®ä¿æ–‡ä»¶åæ˜¯å”¯ä¸€çš„ï¼Œå¹¶ä¸”æœ‰.pdfæ‰©å±•å
            timestamp = int(time.time())
            filename = f"report_{timestamp}.pdf"
            temp_file_path = os.path.join(temp_dir, filename)

            try:
                # report_object.to_pdf(temp_file_path) # generate_financial_reportç­‰æ–¹æ³•è‹¥æŒ‡å®šoutput_pathåˆ™ç›´æ¥ä¿å­˜
                # æˆ‘ä»¬éœ€è¦å…ˆè·å¾— Report å¯¹è±¡ï¼Œç„¶åè°ƒç”¨å…¶ to_pdf æ–¹æ³•
                if isinstance(report_object, str) and os.path.exists(report_object):  # å¦‚æœç”Ÿæˆå™¨æ–¹æ³•ç›´æ¥è¿”å›è·¯å¾„
                    temp_file_path = report_object
                else:  # å‡è®¾ report_object æ˜¯ Report ç±»çš„å®ä¾‹
                    report_object.to_pdf(temp_file_path)

                with open(temp_file_path, 'rb') as f:
                    report_content_bytes = f.read()

                # å¯é€‰ï¼šåˆ é™¤ä¸´æ—¶æ–‡ä»¶
                # os.remove(temp_file_path)

                # å¯¹äºPDFï¼Œé€šå¸¸æ˜¯ä½œä¸ºæ–‡ä»¶ä¸‹è½½ï¼Œæˆ–è¿”å›æ–‡ä»¶è·¯å¾„/é“¾æ¥
                # è¿™é‡Œä¸ºäº†APIä¸€è‡´æ€§ï¼Œå¯ä»¥è¿”å›ä¸€ä¸ªæ¶ˆæ¯æŒ‡ç¤ºPDFå·²ç”Ÿæˆï¼Œæˆ–base64ç¼–ç 
                # ä¸ºäº†ç®€å•ï¼Œè¿™é‡Œè¿”å›ä¸€ä¸ªæŒ‡ç¤ºæ¶ˆæ¯ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ–‡ä»¶æœåŠ¡
                return create_success_response({
                    'message': 'PDFæŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·é€šè¿‡æŒ‡å®šè·¯å¾„æˆ–åç»­æœºåˆ¶è·å–ã€‚',
                    'report_path_info': temp_file_path,  # ä»…ç”¨äºæ¼”ç¤ºï¼Œç”Ÿäº§ç¯å¢ƒä¸åº”ç›´æ¥æš´éœ²ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                    'report_type': report_type,
                    'format': report_format_str
                }, "PDFæŠ¥å‘Šå¤„ç†å®Œæˆ")

            except Exception as pdf_err:
                logger.error(f"PDFæŠ¥å‘Šç”Ÿæˆæˆ–è¯»å–å¤±è´¥: {pdf_err}\n{traceback.format_exc()}")
                return create_error_response(f"PDFæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {pdf_err}", "report_generation_error", 500)

        logger.info(f"âœ… API: {report_type}æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œæ ¼å¼: {report_format_str}")
        return Response(report_content, mimetype=mimetype) if report_format_str != 'pdf' else \
            create_error_response("PDFç”Ÿæˆé€»è¾‘éœ€è¦è°ƒæ•´ä»¥é€‚é…Response", "internal_error", 500)  # PDF case handled above

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}", "report_generation_error", 500)


@data_bp.route('/generate-chart', methods=['POST'])
@async_route
async def generate_data_chart():
    """
    ğŸ¨ ç”Ÿæˆæ•°æ®å›¾è¡¨ - åŸºäºæä¾›çš„æ•°æ®å’Œé…ç½®æ™ºèƒ½ç”Ÿæˆå›¾è¡¨
    è¯·æ±‚ä½“ (JSON):
    {
        "data": {}, // ç”¨äºå›¾è¡¨çš„æ•°æ®ï¼Œä¾‹å¦‚ {"labels": ["A", "B"], "values": [10, 20]}
        "chart_type": "line", // å›¾è¡¨ç±»å‹ (line, bar, pie, etc.)ï¼Œæˆ– "auto"
        "title": "å›¾è¡¨æ ‡é¢˜", // å¯é€‰
        "config": {}, // å›¾è¡¨åº“ç‰¹å®šçš„é…ç½®ï¼Œå¯é€‰
        "preferences": {"theme": "financial"} // å¯é€‰çš„ç”¨æˆ·åå¥½
    }
    """
    try:
        logger.info("ğŸ¨ API: è¯·æ±‚ç”Ÿæˆæ•°æ®å›¾è¡¨...")
        if not orchestrator.initialized:
            await orchestrator.initialize()

        params = validate_request_params(
            required_params=['data'],
            optional_params={
                'chart_type': {'type': str, 'default': 'auto'},
                'title': {'type': str, 'default': 'æ•°æ®å›¾è¡¨'},
                'config': {'type': dict, 'default': {}},
                'preferences': {'type': dict, 'default': {}}
            }
        )
        chart_data = params['data']
        chart_type_str = params['chart_type']
        chart_title = params['title']
        chart_config_extra = params['config']
        chart_preferences = params['preferences']

        # å°†å­—ç¬¦ä¸²çš„ chart_type è½¬æ¢ä¸º ChartType æšä¸¾ï¼Œå¦‚æœå®ƒæ˜¯æœ‰æ•ˆæˆå‘˜çš„è¯
        try:
            chart_type_enum = ChartType[chart_type_str.upper()] if chart_type_str != 'auto' else None
        except KeyError:
            return create_error_response(f"ä¸æ”¯æŒçš„å›¾è¡¨ç±»å‹: {chart_type_str}", "validation_error", 400)

        # ä½¿ç”¨ç¼–æ’å™¨ä¸­çš„å›¾è¡¨ç”Ÿæˆå™¨
        # ChartGenerator.py (utils/formatters)çš„ generate_chart æ–¹æ³•
        chart_result = orchestrator.chart_generator.generate_chart(
            data=chart_data,
            chart_type=chart_type_enum,  # ä¼ é€’æšä¸¾æˆ–None
            title=chart_title,
            config=chart_config_extra
        )

        if not chart_result or chart_result.get("error"):
            return create_error_response(chart_result.get("error", "å›¾è¡¨ç”Ÿæˆå¤±è´¥"), "chart_generation_error", 500)

        # image_data åŒ…å«äº† base64, svg, æˆ– binary, ä»¥åŠ format
        # æ ¹æ®å‰ç«¯éœ€æ±‚ï¼Œå¯èƒ½è¿”å›base64ç¼–ç çš„å›¾ç‰‡æˆ–å›¾è¡¨é…ç½®æœ¬èº«
        response_data = {
            'chart_type_generated': chart_result.get('type', chart_type_str),
            'title': chart_result.get('title', chart_title),
            'image_data': chart_result.get('image_data'),  # { "base64": "...", "format": "png" } or { "svg": "..." }
            'raw_chart_config': chart_result.get('chart_config_for_frontend')  # å¦‚æœå›¾è¡¨åº“è¿”å›å‰ç«¯å¯æ¸²æŸ“çš„é…ç½®
        }

        logger.info(f"âœ… API: å›¾è¡¨ç”Ÿæˆå®Œæˆ: {response_data['chart_type_generated']}")
        return create_success_response(response_data, "å›¾è¡¨ç”ŸæˆæˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ API: å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}", "chart_generation_error", 500)


# ============= ç³»ç»Ÿç›‘æ§å’ŒçŠ¶æ€API (å¯ä»¥ä¿ç•™æˆ–å¢å¼º) =============

@data_bp.route('/health', methods=['GET'])
@async_route
async def data_service_health():
    """ğŸ” æ•°æ®æœåŠ¡å¥åº·æ£€æŸ¥ (å·²é€šè¿‡ç¼–æ’å™¨èƒ½åŠ›å¢å¼º)"""
    try:
        if not orchestrator.initialized:
            await orchestrator.initialize()

        health_status = await orchestrator.health_check()  # è°ƒç”¨ç¼–æ’å™¨çš„å…¨é¢å¥åº·æ£€æŸ¥

        # ä»ç¼–æ’å™¨å¥åº·æ£€æŸ¥ç»“æœä¸­æå–æ•°æ®æœåŠ¡ç›¸å…³çš„éƒ¨åˆ†
        api_connector_health = "unavailable"
        if hasattr(orchestrator, 'data_fetcher') and hasattr(orchestrator.data_fetcher, 'api_connector'):
            api_connector_status = await orchestrator.data_fetcher.api_connector.health_check()
            api_connector_health = api_connector_status.get('status', 'unknown')

        data_services_status = {
            'overall_orchestrator_status': health_status.get('status', 'unknown'),
            'api_connector_status': api_connector_health,
            'data_analyzer_available': hasattr(orchestrator,
                                               'financial_data_analyzer') and orchestrator.financial_data_analyzer is not None,
            'time_series_builder_available': hasattr(orchestrator.data_fetcher,
                                                     'time_series_builder') and orchestrator.data_fetcher.time_series_builder is not None,
            'financial_calculator_available': hasattr(orchestrator.financial_data_analyzer,
                                                      'financial_calculator') and orchestrator.financial_data_analyzer.financial_calculator is not None,
            'report_generator_available': hasattr(orchestrator,
                                                  'report_generator') and orchestrator.report_generator is not None,
            'chart_generator_available': hasattr(orchestrator,
                                                 'chart_generator') and orchestrator.chart_generator is not None,
        }

        overall_data_health = 'healthy' if all(
            status == 'healthy' or status is True for status in data_services_status.values() if
            isinstance(status, (str, bool))) else 'degraded'

        return create_success_response({
            'data_service_health': overall_data_health,
            'details': data_services_status
        }, "æ•°æ®æœåŠ¡å¥åº·æ£€æŸ¥å®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ æ•°æ®æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"æ•°æ®æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}", "health_check_error", 500)


@data_bp.route('/stats', methods=['GET'])
@async_route
async def get_data_service_stats():
    """ğŸ“Š è·å–æ•°æ®æœåŠ¡ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯ (é€šè¿‡ç¼–æ’å™¨è·å–)"""
    try:
        if not orchestrator.initialized:
            await orchestrator.initialize()

        stats = orchestrator.get_orchestrator_stats()  # å…¨å±€ç»Ÿè®¡

        api_connector_stats = {}
        if hasattr(orchestrator, 'data_fetcher') and hasattr(orchestrator.data_fetcher, 'api_connector'):
            api_connector_stats = orchestrator.data_fetcher.api_connector.get_connector_stats()

        financial_analyzer_stats = {}
        if hasattr(orchestrator, 'financial_data_analyzer'):
            financial_analyzer_stats = orchestrator.financial_data_analyzer.get_analysis_stats()

        # æ›´å¤šç»„ä»¶çš„ç»Ÿè®¡ä¿¡æ¯å¯ä»¥æŒ‰éœ€æ·»åŠ 

        return create_success_response({
            'overall_system_stats': stats,
            'api_connector_module_stats': api_connector_stats,
            'financial_analyzer_module_stats': financial_analyzer_stats
            # ... å…¶ä»–ç»„ä»¶ç»Ÿè®¡
        }, "æ•°æ®æœåŠ¡ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ")

    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®æœåŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return create_error_response(f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {str(e)}", "internal_error", 500)


# ============= é”™è¯¯å¤„ç† (è“å›¾çº§åˆ«) =============

@data_bp.app_errorhandler(404)  # ä½¿ç”¨app_errorhandlerå¤„ç†è“å›¾å¤–çš„404
def handle_data_not_found_error(error):
    logger.warning(f"èµ„æºæœªæ‰¾åˆ° (404): {request.path}")
    return create_error_response("è¯·æ±‚çš„èµ„æºæœªæ‰¾åˆ°", "not_found_error", 404)


@data_bp.app_errorhandler(500)
def handle_data_internal_error(error):
    logger.error(f"æ•°æ®æœåŠ¡å†…éƒ¨æœåŠ¡å™¨é”™è¯¯ (500): {str(error)}\n{traceback.format_exc()}")
    return create_error_response(f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(error)}", "internal_server_error", 500,
                                 {"trace": traceback.format_exc() if orchestrator.config.get("DEBUG") else None})


@data_bp.app_errorhandler(400)
def handle_data_bad_request_error(error):
    # Flaskçš„é»˜è®¤400é”™è¯¯å¯¹è±¡æœ‰ä¸€ä¸ªdescriptionå±æ€§
    message = error.description if hasattr(error, 'description') else "è¯·æ±‚å‚æ•°æ— æ•ˆæˆ–æ ¼å¼é”™è¯¯"
    logger.warning(f"é”™è¯¯çš„è¯·æ±‚ (400): {message}")
    return create_error_response(message, "bad_request_error", 400)


@data_bp.app_errorhandler(ValueError)  # æ•è·ç”±validate_...å‡½æ•°æŠ›å‡ºçš„ValueError
def handle_validation_error(error):
    logger.warning(f"å‚æ•°éªŒè¯é”™è¯¯ (ValueError): {str(error)}")
    return create_error_response(str(error), "validation_error", 400)