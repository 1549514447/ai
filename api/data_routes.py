# api/data_routes.py - å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬
from flask import Blueprint, jsonify, request, Response
from typing import Dict, Any, List, Optional
import logging
import asyncio
from datetime import datetime
import json
import traceback
import time
import re

# å¯¼å…¥ç¼–æ’å™¨ - æ ¸å¿ƒæ”¹åŠ¨ï¼
from core.orchestrator.intelligent_qa_orchestrator import get_orchestrator

logger = logging.getLogger(__name__)

# åˆ›å»ºæ•°æ®APIè“å›¾
data_bp = Blueprint('data_api', __name__, url_prefix='/api/data')

# ğŸ¯ ä½¿ç”¨å•ä¸€ç¼–æ’å™¨å®ä¾‹ - é¿å…é‡å¤åˆå§‹åŒ–
orchestrator = get_orchestrator()


# ============= å·¥å…·å‡½æ•° =============

def async_route(f):
    """å¼‚æ­¥è·¯ç”±è£…é¥°å™¨"""

    def wrapper(*args, **kwargs):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(f(*args, **kwargs))
        finally:
            loop.close()

    wrapper.__name__ = f.__name__
    return wrapper


def validate_request_params(required_params: List[str] = None) -> Dict[str, Any]:
    """éªŒè¯è¯·æ±‚å‚æ•°"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        params = request.args.to_dict()

        # è·å–JSONæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if request.content_type and 'application/json' in request.content_type:
            json_data = request.get_json() or {}
            params.update(json_data)

        if required_params:
            missing_params = [param for param in required_params if param not in params]
            if missing_params:
                raise ValueError(f"ç¼ºå°‘å¿…å¡«å‚æ•°: {missing_params}")

        return params
    except Exception as e:
        raise ValueError(f"å‚æ•°éªŒè¯å¤±è´¥: {str(e)}")


def validate_date_format(date_str: str, param_name: str = "date") -> str:
    """éªŒè¯æ—¥æœŸæ ¼å¼ YYYYMMDD"""
    if not date_str:
        return None
    
    if not re.match(r'^\d{8}$', date_str):
        raise ValueError(f"{param_name}æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºYYYYMMDDæ ¼å¼")
    
    try:
        datetime.strptime(date_str, '%Y%m%d')
        return date_str
    except ValueError:
        raise ValueError(f"{param_name}æ—¥æœŸæ— æ•ˆ: {date_str}")


def validate_date_range(start_date: str, end_date: str) -> tuple:
    """éªŒè¯æ—¥æœŸèŒƒå›´"""
    start_date = validate_date_format(start_date, "start_date")
    end_date = validate_date_format(end_date, "end_date")
    
    start_dt = datetime.strptime(start_date, '%Y%m%d')
    end_dt = datetime.strptime(end_date, '%Y%m%d')
    
    if start_dt > end_dt:
        raise ValueError("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
    
    # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦åˆç†ï¼ˆä¸è¶…è¿‡1å¹´ï¼‰
    if (end_dt - start_dt).days > 365:
        raise ValueError("æ—¥æœŸèŒƒå›´ä¸èƒ½è¶…è¿‡1å¹´")
    
    return start_date, end_date


def create_success_response(data: Dict[str, Any], message: str = "æ“ä½œæˆåŠŸ") -> Dict[str, Any]:
    """åˆ›å»ºæˆåŠŸå“åº”"""
    return jsonify({
        'success': True,
        'message': message,
        'data': data,
        'timestamp': datetime.now().isoformat(),
        'processed_by': 'intelligent_orchestrator'
    })


def create_error_response(message: str, error_type: str = "processing_error", status_code: int = 500) -> tuple:
    """åˆ›å»ºé”™è¯¯å“åº”"""
    return jsonify({
        'success': False,
        'error_type': error_type,
        'message': message,
        'timestamp': datetime.now().isoformat(),
        'system_status': 'error'
    }), status_code


# ============= æ ¸å¿ƒæ•°æ®API =============

@data_bp.route('/system', methods=['GET'])
@async_route
async def get_enhanced_system_data():
    """
    ğŸ“Š è·å–å¢å¼ºç³»ç»Ÿæ•°æ® - ä½¿ç”¨ç¼–æ’å™¨å®Œæ•´æ•°æ®é“¾
    """
    try:
        logger.info("ğŸ” è·å–å¢å¼ºç³»ç»Ÿæ•°æ®...")

        # æ£€æŸ¥ç¼–æ’å™¨çŠ¶æ€
        if not orchestrator.initialized:
            return create_error_response("æ™ºèƒ½ç¼–æ’å™¨æœªåˆå§‹åŒ–", "system_unavailable", 503)

        # ä½¿ç”¨ç¼–æ’å™¨çš„APIè¿æ¥å™¨è·å–åŸå§‹æ•°æ®
        result = await orchestrator.api_connector.get_system_data()

        if not result.get("success"):
            return create_error_response(
                result.get("message", "ç³»ç»Ÿæ•°æ®è·å–å¤±è´¥"),
                "api_error",
                500
            )

        # ä½¿ç”¨ç¼–æ’å™¨çš„æ•°æ®å¢å¼ºå™¨å¢å¼ºæ•°æ®
        enhanced_data = orchestrator.data_enhancer.enhance_system_data(result["data"])

        # ä½¿ç”¨ç¼–æ’å™¨çš„æ•°æ®éªŒè¯å™¨éªŒè¯æ•°æ®
        validation_result = orchestrator.data_analyzer._calculate_statistical_metrics([
            ('today', float(result["data"].get('æ€»ä½™é¢', 0)))
        ])

        # è´¢åŠ¡å¥åº·è¯„ä¼°
        financial_health = {}
        try:
            total_inflow = float(result["data"].get('æ€»å…¥é‡‘', 0))
            total_outflow = float(result["data"].get('æ€»å‡ºé‡‘', 0))
            if total_inflow > 0:
                outflow_ratio = total_outflow / total_inflow
                financial_health = {
                    'outflow_ratio': outflow_ratio,
                    'health_status': 'healthy' if outflow_ratio < 0.7 else 'concerning' if outflow_ratio < 0.9 else 'risky',
                    'net_flow': total_inflow - total_outflow
                }
        except Exception as e:
            logger.warning(f"è´¢åŠ¡å¥åº·è¯„ä¼°å¤±è´¥: {str(e)}")

        response_data = {
            'enhanced_data': enhanced_data,
            'validation': validation_result,
            'financial_health': financial_health,
            'processing_metadata': {
                'enhancement_applied': True,
                'validation_performed': True,
                'data_source': 'system_api',
                'processing_components': [
                    'api_connector',
                    'data_enhancer',
                    'data_validator'
                ]
            }
        }

        logger.info("âœ… ç³»ç»Ÿæ•°æ®å¢å¼ºå¤„ç†å®Œæˆ")
        return create_success_response(response_data, "ç³»ç»Ÿæ•°æ®è·å–å¹¶å¢å¼ºæˆåŠŸ")

    except Exception as e:
        logger.error(f"âŒ è·å–ç³»ç»Ÿæ•°æ®å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return create_error_response(f"ç³»ç»Ÿæ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/daily', methods=['GET'])
@async_route
async def get_enhanced_daily_data():
    """
    ğŸ“… è·å–å¢å¼ºæ¯æ—¥æ•°æ® - æ™ºèƒ½æ—¥æœŸå¤„ç†
    """
    try:
        logger.info("ğŸ“… è·å–å¢å¼ºæ¯æ—¥æ•°æ®...")

        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)

        # éªŒè¯å‚æ•°
        params = validate_request_params()
        date = params.get('date')  # å¯é€‰æ—¥æœŸå‚æ•°
        
        # éªŒè¯æ—¥æœŸæ ¼å¼
        if date:
            date = validate_date_format(date, "date")

        # ä½¿ç”¨ç¼–æ’å™¨è·å–æ¯æ—¥æ•°æ®
        result = await orchestrator.api_connector.get_daily_data(date)

        if not result.get("success"):
            return create_error_response(
                result.get("message", "æ¯æ—¥æ•°æ®è·å–å¤±è´¥"),
                "api_error",
                500
            )

        # ä½¿ç”¨ç¼–æ’å™¨å¢å¼ºæ¯æ—¥æ•°æ®
        enhanced_data = orchestrator.data_enhancer.enhance_daily_data(result["data"], date)

        # æ™ºèƒ½è¶‹åŠ¿åˆ†æ
        trend_info = {}
        try:
            trend_analysis = await orchestrator.data_analyzer.analyze_trend(
                'daily', 'net_inflow', 7
            )
            trend_info = {
                'direction': trend_analysis.trends[0]['direction'] if trend_analysis.trends else 'stable',
                'confidence': trend_analysis.confidence_score,
                'growth_rate': trend_analysis.metrics.get('growth_rate', 0)
            }
        except Exception as e:
            logger.warning(f"è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
            trend_info = {'direction': 'unknown', 'confidence': 0.0}

        # æ—¥åº¦è¡¨ç°è¯„ä¼°
        daily_performance = {}
        if result["data"]:
            try:
                inflow = float(result["data"].get('å…¥é‡‘', 0))
                outflow = float(result["data"].get('å‡ºé‡‘', 0))
                registrations = int(result["data"].get('æ³¨å†Œäººæ•°', 0))
                purchases = int(result["data"].get('è´­ä¹°äº§å“æ•°é‡', 0))
                
                daily_performance = {
                    'net_flow': inflow - outflow,
                    'flow_ratio': inflow / outflow if outflow > 0 else float('inf'),
                    'conversion_rate': purchases / registrations if registrations > 0 else 0,
                    'activity_score': (registrations + purchases) / 2
                }
            except Exception as e:
                logger.warning(f"æ—¥åº¦è¡¨ç°è¯„ä¼°å¤±è´¥: {str(e)}")

        response_data = {
            'enhanced_data': enhanced_data,
            'trend_analysis': trend_info,
            'daily_performance': daily_performance,
            'query_date': date or datetime.now().strftime('%Y%m%d'),
            'processing_metadata': {
                'intelligent_enhancement': True,
                'trend_analysis_included': True,
                'date_processing': 'automatic' if not date else 'specified'
            }
        }

        logger.info("âœ… æ¯æ—¥æ•°æ®å¢å¼ºå¤„ç†å®Œæˆ")
        return create_success_response(response_data, "æ¯æ—¥æ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ è·å–æ¯æ—¥æ•°æ®å¤±è´¥: {str(e)}")
        return create_error_response(f"æ¯æ—¥æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/products', methods=['GET'])
@async_route
async def get_enhanced_products_data():
    """
    ğŸ›ï¸ è·å–å¢å¼ºäº§å“æ•°æ® - æ™ºèƒ½äº§å“åˆ†æ
    """
    try:
        logger.info("ğŸ›ï¸ è·å–å¢å¼ºäº§å“æ•°æ®...")

        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)

        # éªŒè¯å‚æ•°
        params = validate_request_params()
        status = params.get('status', 'active')
        include_expiry = params.get('include_expiry', 'true').lower() == 'true'
        include_analysis = params.get('include_analysis', 'false').lower() == 'true'

        # ä½¿ç”¨ç¼–æ’å™¨è·å–äº§å“æ•°æ®
        result = await orchestrator.api_connector.get_product_data()

        if not result.get("success"):
            return create_error_response(
                result.get("message", "äº§å“æ•°æ®è·å–å¤±è´¥"),
                "api_error",
                500
            )

        # ä½¿ç”¨ç¼–æ’å™¨å¢å¼ºäº§å“æ•°æ®
        enhanced_data = orchestrator.data_enhancer.enhance_product_data(
            result["data"],
            include_expiry=include_expiry
        )

        # äº§å“è¡¨ç°åˆ†æ
        product_performance = {}
        try:
            product_list = result["data"].get("äº§å“åˆ—è¡¨", [])
            if product_list:
                # è®¡ç®—äº§å“è¡¨ç°æŒ‡æ ‡
                top_products = sorted(product_list, key=lambda x: x.get('æ€»è´­ä¹°æ¬¡æ•°', 0), reverse=True)[:5]
                low_utilization = [p for p in product_list 
                                 if p.get('å½“å‰æŒæœ‰æ•°', 0) / max(p.get('æ€»è´­ä¹°æ¬¡æ•°', 1), 1) < 0.3]
                
                product_performance = {
                    'top_performing_products': [p.get('äº§å“åç§°', '') for p in top_products],
                    'low_utilization_count': len(low_utilization),
                    'total_products': len(product_list),
                    'avg_utilization_rate': sum(
                        p.get('å½“å‰æŒæœ‰æ•°', 0) / max(p.get('æ€»è´­ä¹°æ¬¡æ•°', 1), 1) 
                        for p in product_list
                    ) / len(product_list) if product_list else 0
                }
        except Exception as e:
            logger.warning(f"äº§å“è¡¨ç°åˆ†æå¤±è´¥: {str(e)}")

        # å¯é€‰çš„æ™ºèƒ½äº§å“åˆ†æ
        product_insights = {}
        if include_analysis:
            try:
                # ä½¿ç”¨ç¼–æ’å™¨ç”Ÿæˆäº§å“æ´å¯Ÿ
                insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
                    analysis_results=[],
                    user_context=None,
                    focus_areas=['product_analysis']
                )
                product_insights = {
                    'insights': insights,
                    'analysis_metadata': metadata
                }
            except Exception as e:
                logger.warning(f"äº§å“æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
                product_insights = {'error': 'äº§å“æ´å¯Ÿåˆ†ææš‚ä¸å¯ç”¨'}

        response_data = {
            'enhanced_data': enhanced_data,
            'product_performance': product_performance,
            'product_insights': product_insights,
            'filters': {
                'status': status,
                'include_expiry': include_expiry,
                'include_analysis': include_analysis
            },
            'processing_metadata': {
                'enhancement_level': 'comprehensive',
                'insights_included': include_analysis,
                'expiry_analysis': include_expiry
            }
        }

        logger.info("âœ… äº§å“æ•°æ®å¢å¼ºå¤„ç†å®Œæˆ")
        return create_success_response(response_data, "äº§å“æ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ è·å–äº§å“æ•°æ®å¤±è´¥: {str(e)}")
        return create_error_response(f"äº§å“æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/users/daily', methods=['GET'])
@async_route
async def get_enhanced_user_daily_data():
    """
    ğŸ‘¥ è·å–ç”¨æˆ·æ¯æ—¥æ•°æ® - ç”¨æˆ·è¡Œä¸ºåˆ†æ
    """
    try:
        logger.info("ğŸ‘¥ è·å–ç”¨æˆ·æ¯æ—¥æ•°æ®...")
        
        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
        
        # éªŒè¯å‚æ•°
        params = validate_request_params()
        date = params.get('date')  # å¯é€‰æ—¥æœŸå‚æ•°
        
        # éªŒè¯æ—¥æœŸæ ¼å¼
        if date:
            date = validate_date_format(date, "date")
        
        # ä½¿ç”¨ç¼–æ’å™¨è·å–ç”¨æˆ·æ¯æ—¥æ•°æ®
        result = await orchestrator.api_connector.get_user_daily_data(date)
        
        if not result.get("success"):
            return create_error_response(
                result.get("message", "ç”¨æˆ·æ¯æ—¥æ•°æ®è·å–å¤±è´¥"),
                "api_error", 500
            )
        
        # å¢å¼ºç”¨æˆ·æ•°æ®åˆ†æ
        enhanced_data = orchestrator.data_enhancer.enhance_user_daily_data(
            result["data"], date
        )
        
        # VIPåˆ†å¸ƒåˆ†æ
        vip_analysis = {}
        try:
            daily_data = result["data"].get("æ¯æ—¥æ•°æ®", [])
            if daily_data:
                latest_data = daily_data[-1]  # æœ€æ–°æ—¥æœŸçš„æ•°æ®
                
                # è®¡ç®—VIPåˆ†å¸ƒ
                total_users = sum(latest_data.get(f'vip{i}çš„äººæ•°', 0) for i in range(11))
                vip_distribution = {}
                for i in range(11):
                    vip_count = latest_data.get(f'vip{i}çš„äººæ•°', 0)
                    vip_distribution[f'vip{i}'] = {
                        'count': vip_count,
                        'percentage': (vip_count / total_users * 100) if total_users > 0 else 0
                    }
                
                vip_analysis = {
                    'total_users': total_users,
                    'vip_distribution': vip_distribution,
                    'high_value_users': sum(latest_data.get(f'vip{i}çš„äººæ•°', 0) for i in range(3, 11)),
                    'analysis_date': latest_data.get('æ—¥æœŸ', '')
                }
        except Exception as e:
            logger.warning(f"VIPåˆ†æå¤±è´¥: {str(e)}")
        
        # ç”¨æˆ·å¢é•¿è¶‹åŠ¿
        growth_analysis = {}
        try:
            daily_data = result["data"].get("æ¯æ—¥æ•°æ®", [])
            if len(daily_data) > 1:
                # è®¡ç®—æ–°å¢ç”¨æˆ·è¶‹åŠ¿
                new_users = [d.get('æ–°å¢ç”¨æˆ·æ•°', 0) for d in daily_data[-7:]]  # æœ€è¿‘7å¤©
                avg_daily_growth = sum(new_users) / len(new_users) if new_users else 0
                
                growth_analysis = {
                    'avg_daily_new_users': avg_daily_growth,
                    'recent_trend': 'increasing' if len(new_users) > 1 and new_users[-1] > new_users[0] else 'stable',
                    'data_points': len(new_users)
                }
        except Exception as e:
            logger.warning(f"å¢é•¿åˆ†æå¤±è´¥: {str(e)}")
        
        response_data = {
            'enhanced_data': enhanced_data,
            'vip_analysis': vip_analysis,
            'growth_analysis': growth_analysis,
            'query_date': date,
            'processing_metadata': {
                'user_behavior_analysis': True,
                'vip_distribution_included': True,
                'growth_trend_analyzed': True
            }
        }
        
        logger.info("âœ… ç”¨æˆ·æ¯æ—¥æ•°æ®å¤„ç†å®Œæˆ")
        return create_success_response(response_data, "ç”¨æˆ·æ¯æ—¥æ•°æ®è·å–æˆåŠŸ")
        
    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ è·å–ç”¨æˆ·æ¯æ—¥æ•°æ®å¤±è´¥: {str(e)}")
        return create_error_response(f"ç”¨æˆ·æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/users/detailed', methods=['GET'])
@async_route
async def get_enhanced_user_detailed_data():
    """
    ğŸ“Š è·å–è¯¦ç»†ç”¨æˆ·æ•°æ® - åˆ†é¡µç”¨æˆ·è¯¦æƒ…
    """
    try:
        logger.info("ğŸ“Š è·å–è¯¦ç»†ç”¨æˆ·æ•°æ®...")
        
        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)
        
        # éªŒè¯å‚æ•°
        params = validate_request_params()
        page = int(params.get('page', 1))
        include_analysis = params.get('include_analysis', 'false').lower() == 'true'
        
        # å‚æ•°éªŒè¯
        if page < 1:
            return create_error_response("é¡µç å¿…é¡»å¤§äº0", "validation_error", 400)
        
        # ä½¿ç”¨ç¼–æ’å™¨è·å–ç”¨æˆ·è¯¦ç»†æ•°æ®
        result = await orchestrator.api_connector.get_user_data(page)
        
        if not result.get("success"):
            return create_error_response(
                result.get("message", "ç”¨æˆ·è¯¦ç»†æ•°æ®è·å–å¤±è´¥"),
                "api_error", 500
            )
        
        # å¢å¼ºç”¨æˆ·æ•°æ®
        enhanced_data = orchestrator.data_enhancer.enhance_user_detailed_data(
            result["data"], include_analysis
        )
        
        # ç”¨æˆ·ç»Ÿè®¡åˆ†æ
        user_statistics = {}
        try:
            user_list = result["data"].get("ç”¨æˆ·åˆ—è¡¨", [])
            if user_list:
                # è®¡ç®—ç”¨æˆ·ç»Ÿè®¡
                total_investment = sum(float(u.get('æ€»æŠ•å…¥', 0)) for u in user_list)
                total_rewards = sum(float(u.get('ç´¯è®¡è·å¾—å¥–åŠ±é‡‘é¢', 0)) for u in user_list)
                roi_values = [float(u.get('æŠ•æŠ¥æ¯”', 0)) for u in user_list if float(u.get('æŠ•æŠ¥æ¯”', 0)) > 0]
                
                user_statistics = {
                    'page_user_count': len(user_list),
                    'total_investment_this_page': total_investment,
                    'total_rewards_this_page': total_rewards,
                    'avg_roi': sum(roi_values) / len(roi_values) if roi_values else 0,
                    'high_roi_users': len([r for r in roi_values if r > 0.1]),  # ROI > 10%
                    'avg_investment_per_user': total_investment / len(user_list) if user_list else 0
                }
        except Exception as e:
            logger.warning(f"ç”¨æˆ·ç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}")
        
        # å¯é€‰çš„ç”¨æˆ·ç¾¤ä½“åˆ†æ
        user_insights = {}
        if include_analysis:
            try:
                insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
                    analysis_results=[],
                    user_context=None,
                    focus_areas=['user_behavior']
                )
                user_insights = {
                    'insights': insights,
                    'metadata': metadata
                }
            except Exception as e:
                logger.warning(f"ç”¨æˆ·æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
        
        response_data = {
            'enhanced_data': enhanced_data,
            'user_statistics': user_statistics,
            'user_insights': user_insights,
            'pagination_info': {
                'current_page': page,
                'total_records': enhanced_data.get('æ€»è®°å½•æ•°', 0),
                'total_pages': enhanced_data.get('æ€»é¡µæ•°', 0),
                'records_per_page': 1000
            },
            'processing_metadata': {
                'detailed_analysis': include_analysis,
                'enhancement_applied': True,
                'statistics_calculated': True
            }
        }
        
        logger.info(f"âœ… ç”¨æˆ·è¯¦ç»†æ•°æ®å¤„ç†å®Œæˆ: ç¬¬{page}é¡µ")
        return create_success_response(response_data, f"ç”¨æˆ·è¯¦ç»†æ•°æ®è·å–æˆåŠŸ(ç¬¬{page}é¡µ)")
        
    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ è·å–ç”¨æˆ·è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}")
        return create_error_response(f"ç”¨æˆ·è¯¦ç»†æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/expiry', methods=['GET'])
@async_route
async def get_enhanced_expiry_data():
    """
    â° è·å–å¢å¼ºåˆ°æœŸæ•°æ® - æ™ºèƒ½åˆ°æœŸåˆ†æ
    """
    try:
        logger.info("â° è·å–å¢å¼ºåˆ°æœŸæ•°æ®...")

        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)

        # éªŒè¯å‚æ•°
        params = validate_request_params()
        start_date = params.get('start_date')
        end_date = params.get('end_date')
        date = params.get('date')  # å•æ—¥æŸ¥è¯¢
        period = params.get('period', 'week')  # today, tomorrow, week, month, single, range

        # å‚æ•°éªŒè¯
        if period == 'range':
            if not start_date or not end_date:
                return create_error_response(
                    "èŒƒå›´æŸ¥è¯¢éœ€è¦start_dateå’Œend_dateå‚æ•°",
                    "validation_error", 400
                )
            start_date, end_date = validate_date_range(start_date, end_date)
        elif period == 'single':
            if not date:
                return create_error_response(
                    "å•æ—¥æŸ¥è¯¢éœ€è¦dateå‚æ•°",
                    "validation_error", 400
                )
            date = validate_date_format(date, "date")

        # ä½¿ç”¨ç¼–æ’å™¨æ ¹æ®periodè·å–æ•°æ®
        if period == 'today':
            result = await orchestrator.api_connector.get_expiring_products_today()
        elif period == 'tomorrow':
            result = await orchestrator.api_connector.get_expiring_products_tomorrow()
        elif period == 'week':
            result = await orchestrator.api_connector.get_expiring_products_week()
        elif period == 'single':
            result = await orchestrator.api_connector.get_product_end_data(date)
        elif period == 'range':
            result = await orchestrator.api_connector.get_expiring_products_range(start_date, end_date)
        else:
            return create_error_response(f"ä¸æ”¯æŒçš„æœŸé—´ç±»å‹: {period}", "validation_error", 400)

        if not result.get("success"):
            return create_error_response(
                result.get("message", "åˆ°æœŸæ•°æ®è·å–å¤±è´¥"),
                "api_error", 500
            )

        # ä½¿ç”¨ç¼–æ’å™¨å¢å¼ºåˆ°æœŸæ•°æ®
        enhanced_data = orchestrator.data_enhancer.enhance_expiry_data(result["data"], period)

        # æ™ºèƒ½é£é™©è¯„ä¼°
        risk_analysis = {}
        try:
            expiry_amount = float(result["data"].get("åˆ°æœŸé‡‘é¢", 0))
            
            # é£é™©çº§åˆ«è¯„ä¼°
            if expiry_amount > 5000000:  # 500ä¸‡ä»¥ä¸Š
                risk_level = 'high'
            elif expiry_amount > 1000000:  # 100ä¸‡ä»¥ä¸Š
                risk_level = 'medium'
            else:
                risk_level = 'low'
            
            risk_analysis = {
                'risk_level': risk_level,
                'expiry_amount': expiry_amount,
                'liquidity_requirement': expiry_amount * 1.1,  # 110%å‡†å¤‡é‡‘
                'preparation_days_needed': 3 if risk_level == 'high' else 1
            }
            
            # å¼‚å¸¸æ£€æµ‹
            anomaly_result = await orchestrator.data_analyzer.detect_anomalies(
                'system', ['expiry_amount'], sensitivity=1.5
            )
            risk_analysis.update({
                'anomaly_detected': len(anomaly_result.anomalies) > 0,
                'anomaly_count': len(anomaly_result.anomalies),
                'confidence': anomaly_result.confidence_score
            })
        except Exception as e:
            logger.warning(f"é£é™©åˆ†æå¤±è´¥: {str(e)}")
            risk_analysis = {'risk_level': 'unknown', 'confidence': 0.0}

        # å¤æŠ•æœºä¼šåˆ†æ
        reinvestment_analysis = {}
        try:
            expiry_amount = float(result["data"].get("åˆ°æœŸé‡‘é¢", 0))
            if expiry_amount > 0:
                # ä¼°ç®—å¤æŠ•æ½œåŠ›
                estimated_reinvestment_rate = 0.6  # å‡è®¾60%å¤æŠ•ç‡
                potential_reinvestment = expiry_amount * estimated_reinvestment_rate
                
                reinvestment_analysis = {
                    'estimated_reinvestment_rate': estimated_reinvestment_rate,
                    'potential_reinvestment_amount': potential_reinvestment,
                    'cash_outflow_risk': expiry_amount * (1 - estimated_reinvestment_rate),
                    'optimization_opportunity': expiry_amount * 0.1  # 10%æå‡ç©ºé—´
                }
        except Exception as e:
            logger.warning(f"å¤æŠ•åˆ†æå¤±è´¥: {str(e)}")

        response_data = {
            'enhanced_data': enhanced_data,
            'risk_analysis': risk_analysis,
            'reinvestment_analysis': reinvestment_analysis,
            'query_params': {
                'period': period,
                'start_date': start_date,
                'end_date': end_date,
                'date': date
            },
            'processing_metadata': {
                'risk_assessment_included': True,
                'enhancement_applied': True,
                'intelligent_analysis': True,
                'reinvestment_analysis': True
            }
        }

        logger.info("âœ… åˆ°æœŸæ•°æ®å¢å¼ºå¤„ç†å®Œæˆ")
        return create_success_response(response_data, "åˆ°æœŸæ•°æ®è·å–å¹¶åˆ†ææˆåŠŸ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ è·å–åˆ°æœŸæ•°æ®å¤±è´¥: {str(e)}")
        return create_error_response(f"åˆ°æœŸæ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/analytics', methods=['GET'])
@async_route
async def get_enhanced_analytics_data():
    """
    ğŸ“ˆ è·å–å¢å¼ºåˆ†ææ•°æ® - æ·±åº¦æ™ºèƒ½åˆ†æ
    """
    try:
        logger.info("ğŸ“ˆ è·å–å¢å¼ºåˆ†ææ•°æ®...")

        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)

        # éªŒè¯å‚æ•°
        params = validate_request_params()
        analysis_type = params.get('type', 'comprehensive')  # comprehensive, trend, performance, risk
        time_range = int(params.get('time_range', 30))
        include_insights = params.get('include_insights', 'true').lower() == 'true'
        
        # å‚æ•°éªŒè¯
        if time_range < 1 or time_range > 365:
            return create_error_response("æ—¶é—´èŒƒå›´å¿…é¡»åœ¨1-365å¤©ä¹‹é—´", "validation_error", 400)

        # ä½¿ç”¨ç¼–æ’å™¨è·å–ä¸åŒç±»å‹çš„åˆ†ææ•°æ®
        analysis_results = []

        if analysis_type == 'comprehensive':
            # ç»¼åˆåˆ†æ - ä½¿ç”¨ç¼–æ’å™¨çš„å¤šç§åˆ†æèƒ½åŠ›
            try:
                # è¶‹åŠ¿åˆ†æ
                trend_result = await orchestrator.data_analyzer.analyze_trend('system', 'total_balance', time_range)
                analysis_results.append(trend_result)

                # ä¸šåŠ¡ç»©æ•ˆåˆ†æ
                performance_result = await orchestrator.data_analyzer.analyze_business_performance('financial', time_range)
                analysis_results.append(performance_result)

                # å¼‚å¸¸æ£€æµ‹
                anomaly_result = await orchestrator.data_analyzer.detect_anomalies('system',
                                                                                   ['total_balance', 'daily_inflow'])
                analysis_results.append(anomaly_result)

            except Exception as e:
                logger.warning(f"éƒ¨åˆ†åˆ†æå¤±è´¥: {str(e)}")

        elif analysis_type == 'trend':
            # è¶‹åŠ¿åˆ†æ
            trend_result = await orchestrator.data_analyzer.analyze_trend('system', 'total_balance', time_range)
            analysis_results.append(trend_result)
            
        elif analysis_type == 'performance':
            # ä¸šåŠ¡è¡¨ç°åˆ†æ
            performance_result = await orchestrator.data_analyzer.analyze_business_performance('financial', time_range)
            analysis_results.append(performance_result)
            
        elif analysis_type == 'risk':
            # é£é™©åˆ†æ
            anomaly_result = await orchestrator.data_analyzer.detect_anomalies('system', ['total_balance', 'daily_inflow'])
            analysis_results.append(anomaly_result)

        # ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿï¼ˆå¦‚æœè¯·æ±‚ï¼‰
        insights_data = {}
        if include_insights and analysis_results:
            try:
                insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
                    analysis_results=analysis_results,
                    user_context=None,
                    focus_areas=[analysis_type]
                )
                insights_data = {
                    'insights': insights,
                    'metadata': metadata
                }
            except Exception as e:
                logger.warning(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
                insights_data = {'error': 'æ´å¯Ÿç”Ÿæˆæš‚ä¸å¯ç”¨'}

        # ç»¼åˆå“åº”æ•°æ®
        response_data = {
            'analysis_results': [
                {
                    'analysis_id': getattr(result, 'analysis_id', f'analysis_{i}'),
                    'type': result.analysis_type.value if hasattr(result, 'analysis_type') else 'unknown',
                    'confidence': result.confidence_score if hasattr(result, 'confidence_score') else 0.0,
                    'key_findings': result.key_findings if hasattr(result, 'key_findings') else [],
                    'trends': result.trends if hasattr(result, 'trends') else [],
                    'anomalies': result.anomalies if hasattr(result, 'anomalies') else [],
                    'metrics': result.metrics if hasattr(result, 'metrics') else {},
                    'business_insights': result.business_insights if hasattr(result, 'business_insights') else [],
                    'recommendations': result.recommendations if hasattr(result, 'recommendations') else []
                }
                for i, result in enumerate(analysis_results)
            ],
            'insights': insights_data,
            'analysis_summary': {
                'analysis_type': analysis_type,
                'time_range_days': time_range,
                'total_analyses': len(analysis_results),
                'insights_generated': include_insights and bool(insights_data),
                'overall_confidence': sum(getattr(r, 'confidence_score', 0) for r in analysis_results) / len(analysis_results) if analysis_results else 0
            },
            'processing_metadata': {
                'analysis_count': len(analysis_results),
                'insights_generated': include_insights and bool(insights_data),
                'intelligence_level': 'advanced',
                'processing_components': [
                    'data_analyzer',
                    'insight_generator' if include_insights else None,
                    'data_enhancer'
                ]
            }
        }

        logger.info(f"âœ… {analysis_type}åˆ†ææ•°æ®å¤„ç†å®Œæˆ")
        return create_success_response(response_data, f"{analysis_type}åˆ†æå®Œæˆ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ è·å–åˆ†ææ•°æ®å¤±è´¥: {str(e)}")
        return create_error_response(f"åˆ†ææ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/batch', methods=['POST'])
@async_route
async def get_enhanced_batch_data():
    """
    ğŸ“¦ æ‰¹é‡è·å–å¢å¼ºæ•°æ® - æ™ºèƒ½æ‰¹å¤„ç†
    """
    try:
        logger.info("ğŸ“¦ å¼€å§‹æ‰¹é‡æ•°æ®å¤„ç†...")

        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)

        # éªŒè¯è¯·æ±‚æ•°æ®
        try:
            request_data = validate_request_params(['requests'])
        except ValueError as e:
            return create_error_response(str(e), "validation_error", 400)

        data_requests = request_data.get('requests', [])
        include_analysis = request_data.get('include_analysis', False)
        max_concurrent = min(int(request_data.get('max_concurrent', 5)), 10)  # é™åˆ¶å¹¶å‘æ•°

        if not data_requests:
            return create_error_response("è¯·æ±‚åˆ—è¡¨ä¸ºç©º", "validation_error", 400)
        
        if len(data_requests) > 20:
            return create_error_response("æ‰¹é‡è¯·æ±‚ä¸èƒ½è¶…è¿‡20ä¸ª", "validation_error", 400)

        # å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®è¯·æ±‚
        batch_results = []
        start_time = time.time()

        # åˆ†æ‰¹å¤„ç†ä»¥æ§åˆ¶å¹¶å‘
        for i in range(0, len(data_requests), max_concurrent):
            batch = data_requests[i:i + max_concurrent]
            batch_tasks = []
            
            for req in batch:
                task = asyncio.create_task(
                    self._process_single_batch_request(req, i + data_requests.index(req))
                )
                batch_tasks.append(task)
            
            batch_results.extend(await asyncio.gather(*batch_tasks, return_exceptions=True))

        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(batch_results):
            if isinstance(result, Exception):
                processed_results.append({
                    'request_id': f'req_{i}',
                    'success': False,
                    'error': str(result)
                })
            else:
                processed_results.append(result)

        # å¯é€‰çš„ç»¼åˆåˆ†æ
        comprehensive_analysis = {}
        if include_analysis and processed_results:
            try:
                successful_results = [r for r in processed_results if r['success']]
                if successful_results:
                    insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
                        analysis_results=[],
                        user_context=None,
                        focus_areas=['batch_analysis']
                    )
                    comprehensive_analysis = {
                        'insights': insights,
                        'metadata': metadata
                    }
            except Exception as e:
                logger.warning(f"æ‰¹é‡ç»¼åˆåˆ†æå¤±è´¥: {str(e)}")
                comprehensive_analysis = {'error': 'ç»¼åˆåˆ†ææš‚ä¸å¯ç”¨'}

        processing_time = time.time() - start_time
        successful_count = sum(1 for r in processed_results if r["success"])

        response_data = {
            'results': processed_results,
            'comprehensive_analysis': comprehensive_analysis,
            'batch_summary': {
                'total_requests': len(data_requests),
                'successful_requests': successful_count,
                'failed_requests': len(data_requests) - successful_count,
                'success_rate': f"{(successful_count / len(data_requests)) * 100:.1f}%",
                'processing_time': f"{processing_time:.2f}ç§’",
                'avg_time_per_request': f"{processing_time / len(data_requests):.2f}ç§’"
            },
            'processing_metadata': {
                'batch_processing': True,
                'enhancement_applied': True,
                'comprehensive_analysis_included': include_analysis,
                'max_concurrent_used': max_concurrent
            }
        }

        logger.info(f"âœ… æ‰¹é‡æ•°æ®å¤„ç†å®Œæˆ: {successful_count}/{len(data_requests)} æˆåŠŸ")
        return create_success_response(response_data, "æ‰¹é‡æ•°æ®å¤„ç†å®Œæˆ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return create_error_response(f"æ‰¹é‡æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", "internal_error", 500)


async def _process_single_batch_request(self, req: Dict[str, Any], index: int) -> Dict[str, Any]:
    """å¤„ç†å•ä¸ªæ‰¹é‡è¯·æ±‚"""
    req_type = req.get('type')
    req_params = req.get('params', {})
    req_id = req.get('id', f'req_{index}')

    try:
        if req_type == 'system':
            result = await orchestrator.api_connector.get_system_data()
            if result.get('success'):
                enhanced_data = orchestrator.data_enhancer.enhance_system_data(result['data'])
                result['enhanced_data'] = enhanced_data

        elif req_type == 'daily':
            date = req_params.get('date')
            if date:
                date = validate_date_format(date, "date")
            result = await orchestrator.api_connector.get_daily_data(date)
            if result.get('success'):
                enhanced_data = orchestrator.data_enhancer.enhance_daily_data(
                    result['data'], date
                )
                result['enhanced_data'] = enhanced_data

        elif req_type == 'products':
            result = await orchestrator.api_connector.get_product_data()
            if result.get('success'):
                enhanced_data = orchestrator.data_enhancer.enhance_product_data(
                    result['data'], include_expiry=req_params.get('include_expiry', True)
                )
                result['enhanced_data'] = enhanced_data

        elif req_type == 'users_daily':
            date = req_params.get('date')
            if date:
                date = validate_date_format(date, "date")
            result = await orchestrator.api_connector.get_user_daily_data(date)
            if result.get('success'):
                enhanced_data = orchestrator.data_enhancer.enhance_user_daily_data(
                    result['data'], date
                )
                result['enhanced_data'] = enhanced_data

        elif req_type == 'users_detailed':
            page = int(req_params.get('page', 1))
            if page < 1:
                raise ValueError("é¡µç å¿…é¡»å¤§äº0")
            result = await orchestrator.api_connector.get_user_data(page)
            if result.get('success'):
                enhanced_data = orchestrator.data_enhancer.enhance_user_detailed_data(
                    result['data'], req_params.get('include_analysis', False)
                )
                result['enhanced_data'] = enhanced_data

        elif req_type == 'expiry':
            period = req_params.get('period', 'week')
            if period == 'today':
                result = await orchestrator.api_connector.get_expiring_products_today()
            elif period == 'week':
                result = await orchestrator.api_connector.get_expiring_products_week()
            elif period == 'single':
                date = validate_date_format(req_params.get('date'), "date")
                result = await orchestrator.api_connector.get_product_end_data(date)
            elif period == 'range':
                start_date, end_date = validate_date_range(
                    req_params.get('start_date'),
                    req_params.get('end_date')
                )
                result = await orchestrator.api_connector.get_expiring_products_range(start_date, end_date)
            else:
                result = await orchestrator.api_connector.get_expiring_products_week()

            if result.get('success'):
                enhanced_data = orchestrator.data_enhancer.enhance_expiry_data(result['data'], period)
                result['enhanced_data'] = enhanced_data

        else:
            result = {'success': False, 'error': f'ä¸æ”¯æŒçš„è¯·æ±‚ç±»å‹: {req_type}'}

        return {
            "request_id": req_id,
            "request_type": req_type,
            "success": result.get("success", False),
            "data": result.get("enhanced_data") or result.get("data") if result.get("success") else None,
            "error": result.get("error") or result.get("message") if not result.get("success") else None,
            "processing_time": time.time()
        }

    except Exception as e:
        logger.error(f"æ‰¹å¤„ç†è¯·æ±‚ {req_id} å¤±è´¥: {str(e)}")
        return {
            "request_id": req_id,
            "request_type": req_type,
            "success": False,
            "error": str(e),
            "processing_time": time.time()
        }


@data_bp.route('/insights/generate', methods=['POST'])
@async_route
async def generate_data_insights():
    """
    ğŸ’¡ æ•°æ®æ´å¯Ÿç”Ÿæˆ - ä½¿ç”¨ç¼–æ’å™¨çš„æ´å¯Ÿèƒ½åŠ›
    """
    try:
        logger.info("ğŸ’¡ ç”Ÿæˆæ•°æ®æ´å¯Ÿ...")

        if not orchestrator.initialized:
            return create_error_response("ç³»ç»Ÿæœªå°±ç»ª", "system_unavailable", 503)

        # éªŒè¯è¯·æ±‚æ•°æ®
        try:
            request_data = validate_request_params(['data_type'])
        except ValueError as e:
            return create_error_response(str(e), "validation_error", 400)

        data_type = request_data.get('data_type')  # system, daily, product, user, expiry
        focus_areas = request_data.get('focus_areas', [])
        analysis_depth = request_data.get('analysis_depth', 'standard')  # basic, standard, comprehensive
        time_range = int(request_data.get('time_range', 30))

        # å‚æ•°éªŒè¯
        valid_data_types = ['system', 'daily', 'product', 'user', 'expiry', 'financial']
        if data_type not in valid_data_types:
            return create_error_response(
                f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}ï¼Œæ”¯æŒçš„ç±»å‹: {valid_data_types}",
                "validation_error", 400
            )

        # æ ¹æ®æ•°æ®ç±»å‹è·å–ç›¸åº”çš„åˆ†æç»“æœ
        analysis_results = []

        if data_type == 'system':
            trend_result = await orchestrator.data_analyzer.analyze_trend('system', 'total_balance', time_range)
            performance_result = await orchestrator.data_analyzer.analyze_business_performance('financial', time_range)
            analysis_results.extend([trend_result, performance_result])

        elif data_type == 'daily':
            trend_result = await orchestrator.data_analyzer.analyze_trend('daily', 'net_inflow', min(time_range, 14))
            anomaly_result = await orchestrator.data_analyzer.detect_anomalies('daily', ['inflow', 'outflow'])
            analysis_results.extend([trend_result, anomaly_result])

        elif data_type == 'product':
            performance_result = await orchestrator.data_analyzer.analyze_business_performance('product', time_range)
            analysis_results.append(performance_result)

        elif data_type == 'user':
            performance_result = await orchestrator.data_analyzer.analyze_business_performance('user', time_range)
            analysis_results.append(performance_result)

        elif data_type == 'expiry':
            anomaly_result = await orchestrator.data_analyzer.detect_anomalies('system', ['expiry_amount'])
            analysis_results.append(anomaly_result)

        elif data_type == 'financial':
            trend_result = await orchestrator.data_analyzer.analyze_trend('system', 'total_balance', time_range)
            performance_result = await orchestrator.data_analyzer.analyze_business_performance('financial', time_range)
            risk_result = await orchestrator.data_analyzer.detect_anomalies('system', ['total_balance', 'daily_inflow'])
            analysis_results.extend([trend_result, performance_result, risk_result])

        # ä½¿ç”¨ç¼–æ’å™¨ç”Ÿæˆæ´å¯Ÿ
        insights, metadata = await orchestrator.insight_generator.generate_comprehensive_insights(
            analysis_results=analysis_results,
            user_context=None,
            focus_areas=focus_areas
        )

        response_data = {
            'insights': insights,
            'metadata': metadata,
            'analysis_summary': {
                'data_type': data_type,
                'analysis_depth': analysis_depth,
                'focus_areas': focus_areas,
                'time_range_days': time_range,
                'analysis_count': len(analysis_results),
                'insights_count': len(insights)
            },
            'processing_metadata': {
                'dual_ai_collaboration': True,
                'intelligent_analysis': True,
                'insight_quality': 'high',
                'business_actionable': True
            }
        }

        logger.info(f"âœ… {data_type}æ•°æ®æ´å¯Ÿç”Ÿæˆå®Œæˆ: {len(insights)}æ¡æ´å¯Ÿ")
        return create_success_response(response_data, "æ•°æ®æ´å¯Ÿç”Ÿæˆå®Œæˆ")

    except ValueError as e:
        return create_error_response(str(e), "validation_error", 400)
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
        return create_error_response(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}", "internal_error", 500)


@data_bp.route('/generate-report', methods=['POST'])
@async_route
async def generate_analysis_report():
    """
    ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š - åŸºäºæ•°æ®åˆ†æç»“æœåˆ›å»ºä¸“ä¸šæŠ¥å‘Š
    
    æ”¯æŒå¤šç§æŠ¥å‘Šç±»å‹:
    - é‡‘èåˆ†ææŠ¥å‘Š
    - è¶‹åŠ¿åˆ†ææŠ¥å‘Š
    - å¯¹æ¯”åˆ†ææŠ¥å‘Š
    
    æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼:
    - HTML
    - PDF
    - Markdown
    """
    try:
        # éªŒè¯è¯·æ±‚å‚æ•°
        params = validate_request_params(['report_type', 'data'])
        
        report_type = params.get('report_type', 'financial')
        report_data = params.get('data', {})
        report_title = params.get('title', 'æ•°æ®åˆ†ææŠ¥å‘Š')
        report_format = params.get('format', 'html').lower()
        output_path = params.get('output_path', None)
        
        # éªŒè¯æŠ¥å‘Šç±»å‹
        valid_report_types = ['financial', 'trend', 'comparison']
        if report_type not in valid_report_types:
            return create_error_response(
                f"æ— æ•ˆçš„æŠ¥å‘Šç±»å‹: {report_type}ï¼Œæ”¯æŒçš„ç±»å‹: {', '.join(valid_report_types)}", 
                "invalid_parameter", 
                400
            )
        
        # éªŒè¯æŠ¥å‘Šæ ¼å¼
        valid_formats = ['html', 'pdf', 'markdown']
        if report_format not in valid_formats:
            return create_error_response(
                f"æ— æ•ˆçš„æŠ¥å‘Šæ ¼å¼: {report_format}ï¼Œæ”¯æŒçš„æ ¼å¼: {', '.join(valid_formats)}", 
                "invalid_parameter", 
                400
            )
        
        # è½¬æ¢æŠ¥å‘Šæ ¼å¼ä¸ºæšä¸¾ç±»å‹
        from utils.formatters.report_generator import ReportFormat
        format_mapping = {
            'html': ReportFormat.HTML,
            'pdf': ReportFormat.PDF,
            'markdown': ReportFormat.MARKDOWN
        }
        output_format = format_mapping[report_format]
        
        # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
        from utils.formatters.report_generator import create_report_generator
        report_generator = create_report_generator()
        
        # æ ¹æ®æŠ¥å‘Šç±»å‹ç”Ÿæˆç›¸åº”çš„æŠ¥å‘Š
        if report_type == 'financial':
            report_result = report_generator.generate_financial_report(
                data=report_data,
                title=report_title,
                output_format=output_format,
                output_path=output_path
            )
        elif report_type == 'trend':
            period = params.get('period', None)
            report_result = report_generator.generate_trend_report(
                trend_data=report_data,
                title=report_title,
                period=period,
                output_format=output_format,
                output_path=output_path
            )
        elif report_type == 'comparison':
            report_result = report_generator.generate_comparison_report(
                comparison_data=report_data,
                title=report_title,
                output_format=output_format,
                output_path=output_path
            )
        
        # å¤„ç†æŠ¥å‘Šç»“æœ
        if output_path and isinstance(report_result, str):
            # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„
            return create_success_response({
                'report_path': report_result,
                'report_type': report_type,
                'format': report_format
            }, "æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        else:
            # å¦åˆ™è¿”å›æŠ¥å‘Šå†…å®¹
            if report_format == 'html':
                report_content = report_result.to_html()
            elif report_format == 'markdown':
                report_content = report_result.to_markdown()
            else:
                # PDFæ ¼å¼éœ€è¦ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                import tempfile
                import os
                temp_dir = tempfile.gettempdir()
                temp_file = os.path.join(temp_dir, f"report_{int(time.time())}.pdf")
                report_result.save(temp_file, ReportFormat.PDF)
                report_content = temp_file
            
            return create_success_response({
                'report_content': report_content,
                'report_type': report_type,
                'format': report_format
            }, "æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            
    except Exception as e:
        logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return create_error_response(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}", "report_generation_error", 500)


@data_bp.route('/generate-data-report', methods=['POST'])
@async_route
async def generate_data_report():
    """
    ğŸ“ˆ ç”Ÿæˆæ•°æ®æŠ¥å‘Š - åŸºäºç‰¹å®šæ•°æ®ç±»å‹ç”Ÿæˆä¸“ä¸šæŠ¥å‘Š
    
    æ”¯æŒçš„æ•°æ®ç±»å‹:
    - user_data: ç”¨æˆ·æ•°æ®åˆ†ææŠ¥å‘Š
    - financial_data: è´¢åŠ¡æ•°æ®åˆ†ææŠ¥å‘Š
    - product_data: äº§å“æ•°æ®åˆ†ææŠ¥å‘Š
    - trend_data: è¶‹åŠ¿æ•°æ®åˆ†ææŠ¥å‘Š
    """
    try:
        # éªŒè¯è¯·æ±‚å‚æ•°
        params = validate_request_params(['data_type'])
        
        data_type = params.get('data_type')
        start_date = params.get('start_date')
        end_date = params.get('end_date')
        user_id = params.get('user_id')
        product_id = params.get('product_id')
        report_format = params.get('format', 'html').lower()
        
        # éªŒè¯æ—¥æœŸæ ¼å¼ï¼ˆå¦‚æœæä¾›ï¼‰
        if start_date and end_date:
            validate_date_range(start_date, end_date)
        
        # æ ¹æ®æ•°æ®ç±»å‹è·å–ç›¸åº”çš„æ•°æ®
        report_data = {}
        report_title = "æ•°æ®åˆ†ææŠ¥å‘Š"
        
        if data_type == 'user_data':
            # è·å–ç”¨æˆ·æ•°æ®
            if not user_id:
                return create_error_response("ç¼ºå°‘å¿…è¦å‚æ•°: user_id", "missing_parameter", 400)
            
            # è·å–è¯¦ç»†ç”¨æˆ·æ•°æ®
            user_data_params = {'user_id': user_id}
            if start_date and end_date:
                user_data_params.update({'start_date': start_date, 'end_date': end_date})
            
            user_data_result = await orchestrator.get_enhanced_user_data(user_data_params)
            
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            report_data = {
                'subtitle': f"ç”¨æˆ· {user_id} æ•°æ®åˆ†æ",
                'summary': {
                    'content': f"æœ¬æŠ¥å‘Šåˆ†æäº†ç”¨æˆ· {user_id} åœ¨ç³»ç»Ÿä¸­çš„è¡Œä¸ºå’Œè¡¨ç°ã€‚",
                    'key_metrics': []
                },
                'analysis_sections': []
            }
            
            # æ·»åŠ ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
            if 'user_info' in user_data_result:
                user_info = user_data_result['user_info']
                basic_info_section = {
                    'title': "ç”¨æˆ·åŸºæœ¬ä¿¡æ¯",
                    'content': "ç”¨æˆ·çš„åŸºæœ¬å±æ€§å’Œæ³¨å†Œä¿¡æ¯ã€‚",
                    'metrics': []
                }
                
                # æ·»åŠ ç”¨æˆ·æŒ‡æ ‡
                for key, value in user_info.items():
                    if key not in ['id', 'user_id']:
                        basic_info_section['metrics'].append({
                            'name': key,
                            'value': value,
                            'format_type': 'plain'
                        })
                
                report_data['analysis_sections'].append(basic_info_section)
            
            # æ·»åŠ ç”¨æˆ·æ´»åŠ¨æ•°æ®
            if 'activity_data' in user_data_result:
                activity_data = user_data_result['activity_data']
                activity_section = {
                    'title': "ç”¨æˆ·æ´»åŠ¨åˆ†æ",
                    'content': "ç”¨æˆ·çš„æ´»åŠ¨æ¨¡å¼å’Œå‚ä¸åº¦åˆ†æã€‚",
                    'charts': []
                }
                
                # åˆ›å»ºæ´»åŠ¨è¶‹åŠ¿å›¾è¡¨
                if 'daily_activity' in activity_data:
                    activity_section['charts'].append({
                        'chart_type': 'line',
                        'title': 'ç”¨æˆ·æ´»åŠ¨è¶‹åŠ¿',
                        'data': activity_data['daily_activity'],
                        'caption': 'ç”¨æˆ·æ¯æ—¥æ´»åŠ¨é‡å˜åŒ–è¶‹åŠ¿'
                    })
                
                report_data['analysis_sections'].append(activity_section)
            
            report_title = f"ç”¨æˆ· {user_id} æ•°æ®åˆ†ææŠ¥å‘Š"
            
        elif data_type == 'financial_data':
            # è·å–è´¢åŠ¡æ•°æ®
            financial_params = {}
            if start_date and end_date:
                financial_params.update({'start_date': start_date, 'end_date': end_date})
            
            financial_data_result = await orchestrator.get_enhanced_financial_data(financial_params)
            
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            period_text = ""
            if start_date and end_date:
                period_text = f"ï¼ˆ{start_date} è‡³ {end_date}ï¼‰"
            
            report_data = {
                'subtitle': f"è´¢åŠ¡æ•°æ®åˆ†æ{period_text}",
                'summary': {
                    'content': "æœ¬æŠ¥å‘Šæä¾›äº†å…³é”®è´¢åŠ¡æŒ‡æ ‡å’Œè¶‹åŠ¿çš„ç»¼åˆåˆ†æã€‚",
                    'key_metrics': []
                },
                'analysis_sections': []
            }
            
            # æ·»åŠ è´¢åŠ¡æ‘˜è¦æŒ‡æ ‡
            if 'summary' in financial_data_result:
                for key, value in financial_data_result['summary'].items():
                    report_data['summary']['key_metrics'].append({
                        'name': key,
                        'value': value,
                        'format_type': 'currency' if 'revenue' in key.lower() or 'profit' in key.lower() else 'percentage' if 'rate' in key.lower() or 'growth' in key.lower() else 'plain'
                    })
            
            # æ·»åŠ æ”¶å…¥åˆ†æéƒ¨åˆ†
            if 'revenue_data' in financial_data_result:
                revenue_data = financial_data_result['revenue_data']
                revenue_section = {
                    'title': "æ”¶å…¥åˆ†æ",
                    'content': "æ”¶å…¥æ¥æºå’Œè¶‹åŠ¿åˆ†æã€‚",
                    'charts': []
                }
                
                # åˆ›å»ºæ”¶å…¥è¶‹åŠ¿å›¾è¡¨
                if 'trend' in revenue_data:
                    revenue_section['charts'].append({
                        'chart_type': 'line',
                        'title': 'æ”¶å…¥è¶‹åŠ¿',
                        'data': revenue_data['trend'],
                        'caption': 'æ”¶å…¥å˜åŒ–è¶‹åŠ¿åˆ†æ'
                    })
                
                # åˆ›å»ºæ”¶å…¥åˆ†å¸ƒå›¾è¡¨
                if 'distribution' in revenue_data:
                    revenue_section['charts'].append({
                        'chart_type': 'pie',
                        'title': 'æ”¶å…¥åˆ†å¸ƒ',
                        'data': revenue_data['distribution'],
                        'caption': 'æ”¶å…¥æ¥æºåˆ†å¸ƒ'
                    })
                
                report_data['analysis_sections'].append(revenue_section)
            
            report_title = "è´¢åŠ¡æ•°æ®åˆ†ææŠ¥å‘Š"
            
        elif data_type == 'product_data':
            # è·å–äº§å“æ•°æ®
            product_params = {}
            if product_id:
                product_params['product_id'] = product_id
            if start_date and end_date:
                product_params.update({'start_date': start_date, 'end_date': end_date})
            
            product_data_result = await orchestrator.get_enhanced_product_data(product_params)
            
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            product_text = f"äº§å“ {product_id}" if product_id else "äº§å“"
            period_text = ""
            if start_date and end_date:
                period_text = f"ï¼ˆ{start_date} è‡³ {end_date}ï¼‰"
            
            report_data = {
                'subtitle': f"{product_text}æ•°æ®åˆ†æ{period_text}",
                'summary': {
                    'content': f"æœ¬æŠ¥å‘Šæä¾›äº†{product_text}æ€§èƒ½å’Œè¶‹åŠ¿çš„ç»¼åˆåˆ†æã€‚",
                    'key_metrics': []
                },
                'analysis_sections': []
            }
            
            # æ·»åŠ äº§å“æ‘˜è¦æŒ‡æ ‡
            if 'summary' in product_data_result:
                for key, value in product_data_result['summary'].items():
                    report_data['summary']['key_metrics'].append({
                        'name': key,
                        'value': value,
                        'format_type': 'currency' if 'revenue' in key.lower() or 'price' in key.lower() else 'plain'
                    })
            
            # æ·»åŠ äº§å“æ€§èƒ½åˆ†æéƒ¨åˆ†
            if 'performance_data' in product_data_result:
                performance_data = product_data_result['performance_data']
                performance_section = {
                    'title': "äº§å“æ€§èƒ½åˆ†æ",
                    'content': "äº§å“æ€§èƒ½æŒ‡æ ‡å’Œè¶‹åŠ¿åˆ†æã€‚",
                    'charts': []
                }
                
                # åˆ›å»ºæ€§èƒ½è¶‹åŠ¿å›¾è¡¨
                if 'trend' in performance_data:
                    performance_section['charts'].append({
                        'chart_type': 'line',
                        'title': 'æ€§èƒ½è¶‹åŠ¿',
                        'data': performance_data['trend'],
                        'caption': 'äº§å“æ€§èƒ½å˜åŒ–è¶‹åŠ¿'
                    })
                
                report_data['analysis_sections'].append(performance_section)
            
            report_title = f"{product_text}æ•°æ®åˆ†ææŠ¥å‘Š"
            
        elif data_type == 'trend_data':
            # è·å–è¶‹åŠ¿æ•°æ®
            trend_params = {}
            if start_date and end_date:
                trend_params.update({'start_date': start_date, 'end_date': end_date})
            
            trend_data_result = await orchestrator.get_enhanced_trend_data(trend_params)
            
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            period_text = ""
            if start_date and end_date:
                period_text = f"{start_date} è‡³ {end_date}"
            
            report_data = {
                'subtitle': "è¶‹åŠ¿åˆ†æ",
                'period': period_text,
                'summary': {
                    'content': "æœ¬æŠ¥å‘Šæä¾›äº†å…³é”®æŒ‡æ ‡çš„è¶‹åŠ¿åˆ†æå’Œé¢„æµ‹ã€‚",
                    'key_trends': []
                },
                'trends': []
            }
            
            # æ·»åŠ å…³é”®è¶‹åŠ¿æŒ‡æ ‡
            if 'key_trends' in trend_data_result:
                for trend in trend_data_result['key_trends']:
                    report_data['summary']['key_trends'].append({
                        'name': trend['name'],
                        'value': trend['value'],
                        'format_type': trend.get('format_type', 'percentage'),
                        'description': trend.get('description', '')
                    })
            
            # æ·»åŠ å„æŒ‡æ ‡è¶‹åŠ¿
            if 'trends' in trend_data_result:
                for trend in trend_data_result['trends']:
                    trend_item = {
                        'title': trend['title'],
                        'description': trend.get('description', ''),
                        'chart': trend.get('chart', {}),
                        'data': trend.get('data', []),
                        'analysis': trend.get('analysis', '')
                    }
                    report_data['trends'].append(trend_item)
            
            report_title = "è¶‹åŠ¿åˆ†ææŠ¥å‘Š"
            
        else:
            return create_error_response(
                f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}", 
                "unsupported_data_type", 
                400
            )
        
        # è½¬æ¢æŠ¥å‘Šæ ¼å¼ä¸ºæšä¸¾ç±»å‹
        from utils.formatters.report_generator import ReportFormat
        format_mapping = {
            'html': ReportFormat.HTML,
            'pdf': ReportFormat.PDF,
            'markdown': ReportFormat.MARKDOWN
        }
        output_format = format_mapping[report_format]
        
        # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
        from utils.formatters.report_generator import create_report_generator
        report_generator = create_report_generator()
        
        # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©é€‚å½“çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•
        if data_type == 'trend_data':
            report_result = report_generator.generate_trend_report(
                trend_data=report_data,
                title=report_title,
                period=period_text if period_text else None,
                output_format=output_format
            )
        else:
            report_result = report_generator.generate_financial_report(
                data=report_data,
                title=report_title,
                output_format=output_format
            )
        
        # å¤„ç†æŠ¥å‘Šç»“æœ
        if report_format == 'html':
            report_content = report_result.to_html()
        elif report_format == 'markdown':
            report_content = report_result.to_markdown()
        else:
            # PDFæ ¼å¼éœ€è¦ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            import tempfile
            import os
            temp_dir = tempfile.gettempdir()
            temp_file = os.path.join(temp_dir, f"report_{int(time.time())}.pdf")
            report_result.save(temp_file, ReportFormat.PDF)
            report_content = temp_file
        
        return create_success_response({
            'report_content': report_content,
            'report_type': data_type,
            'format': report_format
        }, "æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            
    except Exception as e:
        logger.error(f"æ•°æ®æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return create_error_response(f"æ•°æ®æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}", "report_generation_error", 500)


# ============= ç³»ç»Ÿç›‘æ§å’ŒçŠ¶æ€API =============

@data_bp.route('/health', methods=['GET'])
@async_route
async def data_service_health():
    """ğŸ” æ•°æ®æœåŠ¡å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥ç¼–æ’å™¨å¥åº·çŠ¶æ€
        health_status = await orchestrator.health_check()

        # æ£€æŸ¥å„æ•°æ®ç»„ä»¶çŠ¶æ€
        component_status = {
            'api_connector': 'healthy' if hasattr(orchestrator, 'api_connector') else 'unavailable',
            'data_analyzer': 'healthy' if hasattr(orchestrator, 'data_analyzer') else 'unavailable',
            'data_enhancer': 'healthy' if hasattr(orchestrator, 'data_enhancer') else 'unavailable',
            'insight_generator': 'healthy' if hasattr(orchestrator, 'insight_generator') else 'unavailable'
        }

        # ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡
        orchestrator_stats = orchestrator.get_orchestrator_stats()

        # APIè¿æ¥å™¨ç»Ÿè®¡
        api_stats = {}
        if hasattr(orchestrator, 'api_connector'):
            api_stats = orchestrator.api_connector.get_connector_stats()

        overall_status = 'healthy' if all(status == 'healthy' for status in component_status.values()) else 'degraded'

        response_data = {
            'overall_status': overall_status,
            'orchestrator_health': health_status,
            'component_status': component_status,
            'system_stats': orchestrator_stats,
            'api_connector_stats': api_stats,
            'service_info': {
                'service_name': 'intelligent_data_service',
                'version': '2.0.0',
                'supported_apis': [
                    '/api/sta/system',
                    '/api/sta/day',
                    '/api/sta/product',
                    '/api/sta/user_daily',
                    '/api/sta/user',
                    '/api/sta/product_end',
                    '/api/sta/product_end_interval'
                ],
                'features': [
                    'intelligent_data_enhancement',
                    'multi_source_analysis',
                    'automated_insights',
                    'batch_processing',
                    'real_time_analytics',
                    'risk_assessment',
                    'trend_analysis',
                    'user_behavior_analysis'
                ]
            },
            'api_endpoints': {
                'data_endpoints': 8,
                'analysis_endpoints': 2,
                'utility_endpoints': 2
            }
        }

        status_code = 200 if overall_status == 'healthy' else 503
        return jsonify(response_data), status_code

    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
        return create_error_response("å¥åº·æ£€æŸ¥å¤±è´¥", "health_check_error", 500)


@data_bp.route('/stats', methods=['GET'])
@async_route
async def get_data_service_stats():
    """ğŸ“Š è·å–æ•°æ®æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    try:
        orchestrator_stats = orchestrator.get_orchestrator_stats()
        
        # APIè¿æ¥å™¨ç»Ÿè®¡
        api_stats = {}
        if hasattr(orchestrator, 'api_connector'):
            api_stats = orchestrator.api_connector.get_connector_stats()

        # æ•°æ®åˆ†æå™¨ç»Ÿè®¡
        analyzer_stats = {}
        if hasattr(orchestrator, 'data_analyzer'):
            analyzer_stats = orchestrator.data_analyzer.get_analysis_stats()

        # æ´å¯Ÿç”Ÿæˆå™¨ç»Ÿè®¡
        insight_stats = {}
        if hasattr(orchestrator, 'insight_generator'):
            insight_stats = orchestrator.insight_generator.get_insight_stats()

        response_data = {
            'orchestrator_stats': orchestrator_stats,
            'api_connector_stats': api_stats,
            'data_analyzer_stats': analyzer_stats,
            'insight_generator_stats': insight_stats,
            'service_summary': {
                'total_components': 4,
                'active_components': sum(1 for stats in [api_stats, analyzer_stats, insight_stats] if stats),
                'health_score': 1.0 if all([api_stats, analyzer_stats, insight_stats]) else 0.75
            }
        }

        return create_success_response(response_data, "æ•°æ®æœåŠ¡ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ")

    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return create_error_response(f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {str(e)}", "internal_error", 500)


# ============= é”™è¯¯å¤„ç† =============

@data_bp.errorhandler(404)
def data_not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return create_error_response("æ•°æ®æœåŠ¡ç«¯ç‚¹ä¸å­˜åœ¨", "not_found", 404)


@data_bp.errorhandler(500)
def data_internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    logger.error(f"æ•°æ®æœåŠ¡å†…éƒ¨é”™è¯¯: {str(error)}")
    return create_error_response("æ•°æ®æœåŠ¡å†…éƒ¨é”™è¯¯", "internal_error", 500)


@data_bp.errorhandler(400)
def data_bad_request(error):
    """400é”™è¯¯å¤„ç†"""
    return create_error_response("è¯·æ±‚å‚æ•°é”™è¯¯", "bad_request", 400)