# utils/data_transformers/time_series_builder.py
"""
ğŸ“ˆ AIè¾…åŠ©çš„æ—¶é—´åºåˆ—æ•°æ®æ„å»ºå™¨
ä¸“ä¸ºé‡‘èAIåˆ†æç³»ç»Ÿè®¾è®¡ï¼Œå°†APIåŸå§‹æ•°æ®è½¬æ¢ä¸ºåˆ†æå‹å¥½çš„æ—¶é—´åºåˆ—æ ¼å¼

æ ¸å¿ƒç‰¹ç‚¹:
- æ™ºèƒ½æ•°æ®å¯¹é½å’Œè¡¥å…¨
- å¤šç»´åº¦æ—¶é—´åºåˆ—æ„å»º
- å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
- AIé©±åŠ¨çš„æ•°æ®è´¨é‡ä¼˜åŒ–
- ä¸ºé¢„æµ‹å’Œè¶‹åŠ¿åˆ†æä¼˜åŒ–çš„æ•°æ®ç»“æ„
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import asyncio
from dataclasses import dataclass
from enum import Enum
import json

# å¯¼å…¥æˆ‘ä»¬çš„å·¥å…·ç±»
from utils.helpers.date_utils import DateUtils, create_date_utils
from utils.calculators.financial_calculator import FinancialCalculator, create_financial_calculator

logger = logging.getLogger(__name__)


class TimeSeriesType(Enum):
    """æ—¶é—´åºåˆ—ç±»å‹"""
    DAILY = "daily"  # æ¯æ—¥æ•°æ®
    WEEKLY = "weekly"  # å‘¨åº¦æ•°æ®
    MONTHLY = "monthly"  # æœˆåº¦æ•°æ®
    CUMULATIVE = "cumulative"  # ç´¯è®¡æ•°æ®
    MOVING_AVERAGE = "moving_avg"  # ç§»åŠ¨å¹³å‡


class DataQuality(Enum):
    """æ•°æ®è´¨é‡ç­‰çº§"""
    EXCELLENT = "excellent"  # 95%ä»¥ä¸Šå®Œæ•´æ€§
    GOOD = "good"  # 80-95%å®Œæ•´æ€§
    FAIR = "fair"  # 60-80%å®Œæ•´æ€§
    POOR = "poor"  # 60%ä»¥ä¸‹å®Œæ•´æ€§


@dataclass
class TimeSeriesMetadata:
    """æ—¶é—´åºåˆ—å…ƒæ•°æ®"""
    series_type: TimeSeriesType
    date_range: Tuple[str, str]  # (start_date, end_date)
    total_points: int  # æ€»æ•°æ®ç‚¹æ•°
    missing_points: int  # ç¼ºå¤±æ•°æ®ç‚¹æ•°
    data_quality: DataQuality  # æ•°æ®è´¨é‡
    interpolated_points: int  # æ’å€¼æ•°æ®ç‚¹æ•°
    outliers_detected: int  # æ£€æµ‹åˆ°çš„å¼‚å¸¸å€¼æ•°é‡
    confidence_score: float  # æ•°æ®ç½®ä¿¡åº¦ (0-1)


@dataclass
class TimeSeriesPoint:
    """æ—¶é—´åºåˆ—æ•°æ®ç‚¹"""
    date: str  # æ—¥æœŸ (YYYY-MM-DD)
    value: float  # æ•°å€¼
    is_interpolated: bool = False  # æ˜¯å¦ä¸ºæ’å€¼
    is_outlier: bool = False  # æ˜¯å¦ä¸ºå¼‚å¸¸å€¼
    confidence: float = 1.0  # æ•°æ®ç½®ä¿¡åº¦
    metadata: Dict[str, Any] = None  # é¢å¤–å…ƒæ•°æ®


class TimeSeriesBuilder:
    """
    ğŸ“ˆ AIè¾…åŠ©çš„æ—¶é—´åºåˆ—æ•°æ®æ„å»ºå™¨

    åŠŸèƒ½ç‰¹ç‚¹:
    1. æ™ºèƒ½æ•°æ®å¯¹é½å’Œè¡¥å…¨
    2. å¤šç»´åº¦æ—¶é—´åºåˆ—æ„å»º
    3. AIé©±åŠ¨çš„å¼‚å¸¸å€¼æ£€æµ‹
    4. è‡ªåŠ¨æ•°æ®è´¨é‡è¯„ä¼°
    """

    def __init__(self, claude_client=None, gpt_client=None):
        """
        åˆå§‹åŒ–æ—¶é—´åºåˆ—æ„å»ºå™¨

        Args:
            claude_client: Claudeå®¢æˆ·ç«¯ï¼Œç”¨äºæ•°æ®æ¨¡å¼åˆ†æ
            gpt_client: GPTå®¢æˆ·ç«¯ï¼Œç”¨äºæ•°å€¼è®¡ç®—éªŒè¯
        """
        self.claude_client = claude_client
        self.gpt_client = gpt_client
        self.date_utils = create_date_utils(claude_client)
        self.financial_calculator = create_financial_calculator(gpt_client)

        # é…ç½®å‚æ•°
        self.config = {
            "outlier_threshold": 2.5,  # å¼‚å¸¸å€¼é˜ˆå€¼ (æ ‡å‡†å·®å€æ•°)
            "interpolation_method": "linear",  # æ’å€¼æ–¹æ³•
            "min_data_quality": 0.6,  # æœ€ä½æ•°æ®è´¨é‡è¦æ±‚
            "max_gap_days": 7,  # æœ€å¤§å…è®¸æ•°æ®é—´éš”(å¤©)
            "enable_ai_validation": True  # å¯ç”¨AIéªŒè¯
        }

        logger.info("TimeSeriesBuilder initialized with AI capabilities")

    # ============= æ ¸å¿ƒæ—¶é—´åºåˆ—æ„å»ºæ–¹æ³• =============

    async def build_daily_time_series(self, raw_data: List[Dict[str, Any]],
                                      metric_field: str,
                                      date_field: str = "æ—¥æœŸ") -> Dict[str, Any]:
        """
        æ„å»ºæ¯æ—¥æ—¶é—´åºåˆ—

        Args:
            raw_data: åŸå§‹æ•°æ®åˆ—è¡¨
            metric_field: æŒ‡æ ‡å­—æ®µå (å¦‚"å…¥é‡‘", "å‡ºé‡‘", "æ³¨å†Œäººæ•°")
            date_field: æ—¥æœŸå­—æ®µå

        Returns:
            Dict[str, Any]: æ—¶é—´åºåˆ—ç»“æœ
        """
        try:
            logger.info(f"ğŸ“ˆ æ„å»ºæ¯æ—¥æ—¶é—´åºåˆ—: {metric_field}")

            if not raw_data:
                return self._create_empty_series_result(metric_field)

            # ğŸ” æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
            cleaned_data = await self._preprocess_raw_data(raw_data, metric_field, date_field)

            if not cleaned_data:
                logger.warning("æ•°æ®é¢„å¤„ç†åä¸ºç©º")
                return self._create_empty_series_result(metric_field)

            # ğŸ”§ æ„å»ºåŸºç¡€æ—¶é—´åºåˆ—
            base_series = self._build_base_series(cleaned_data, metric_field, date_field)

            # ğŸ“… è¡¥å…¨ç¼ºå¤±æ—¥æœŸ
            complete_series = await self._fill_missing_dates(base_series)

            # ğŸš¨ å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
            processed_series = await self._detect_and_handle_outliers(complete_series, metric_field)

            # ğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°
            metadata = self._calculate_series_metadata(processed_series, TimeSeriesType.DAILY)

            # ğŸ§® ç»Ÿè®¡æŒ‡æ ‡è®¡ç®—
            statistics = await self._calculate_series_statistics(processed_series, metric_field)

            # ğŸ§  AIæ•°æ®éªŒè¯ (å¯é€‰)
            ai_validation = {}
            if self.config["enable_ai_validation"] and self.claude_client:
                ai_validation = await self._ai_validate_time_series(processed_series, metric_field)

            return {
                "success": True,
                "metric": metric_field,
                "series_type": TimeSeriesType.DAILY.value,
                "time_series": processed_series,
                "metadata": metadata,
                "statistics": statistics,
                "ai_validation": ai_validation,
                "total_points": len(processed_series),
                "date_range": self._get_date_range(processed_series),
                "build_timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"âŒ æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "metric": metric_field,
                "series_type": TimeSeriesType.DAILY.value
            }

    async def build_multi_metric_series(self, raw_data: List[Dict[str, Any]],
                                        metrics: List[str],
                                        date_field: str = "æ—¥æœŸ") -> Dict[str, Any]:
        """
        æ„å»ºå¤šæŒ‡æ ‡æ—¶é—´åºåˆ—

        Args:
            raw_data: åŸå§‹æ•°æ®åˆ—è¡¨
            metrics: æŒ‡æ ‡åˆ—è¡¨ (å¦‚["å…¥é‡‘", "å‡ºé‡‘", "æ³¨å†Œäººæ•°"])
            date_field: æ—¥æœŸå­—æ®µå

        Returns:
            Dict[str, Any]: å¤šæŒ‡æ ‡æ—¶é—´åºåˆ—ç»“æœ
        """
        try:
            logger.info(f"ğŸ“Š æ„å»ºå¤šæŒ‡æ ‡æ—¶é—´åºåˆ—: {metrics}")

            if not raw_data or not metrics:
                return {"success": False, "error": "æ•°æ®æˆ–æŒ‡æ ‡åˆ—è¡¨ä¸ºç©º"}

            # å¹¶è¡Œæ„å»ºå„ä¸ªæŒ‡æ ‡çš„æ—¶é—´åºåˆ—
            series_tasks = [
                self.build_daily_time_series(raw_data, metric, date_field)
                for metric in metrics
            ]

            series_results = await asyncio.gather(*series_tasks, return_exceptions=True)

            # æ•´ç†å¤šæŒ‡æ ‡ç»“æœ
            multi_series = {}
            successful_metrics = []
            failed_metrics = []

            for i, result in enumerate(series_results):
                metric = metrics[i]

                if isinstance(result, Exception):
                    logger.error(f"æŒ‡æ ‡ {metric} æ„å»ºå¤±è´¥: {str(result)}")
                    failed_metrics.append({"metric": metric, "error": str(result)})
                elif result.get("success"):
                    multi_series[metric] = result
                    successful_metrics.append(metric)
                else:
                    failed_metrics.append({"metric": metric, "error": result.get("error", "Unknown error")})

            if not successful_metrics:
                return {
                    "success": False,
                    "error": "æ‰€æœ‰æŒ‡æ ‡æ„å»ºéƒ½å¤±è´¥",
                    "failed_metrics": failed_metrics
                }

            # ğŸ”— æ„å»ºå¯¹é½çš„æ—¶é—´åºåˆ—çŸ©é˜µ
            aligned_series = await self._align_multi_series(multi_series)

            # ğŸ“ˆ è®¡ç®—æŒ‡æ ‡é—´ç›¸å…³æ€§
            correlations = await self._calculate_correlations(aligned_series, successful_metrics)

            # ğŸ“Š ç»¼åˆç»Ÿè®¡ä¿¡æ¯
            overall_stats = self._calculate_multi_series_stats(multi_series)

            return {
                "success": True,
                "metrics": successful_metrics,
                "failed_metrics": failed_metrics,
                "individual_series": multi_series,
                "aligned_series": aligned_series,
                "correlations": correlations,
                "overall_statistics": overall_stats,
                "build_timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"âŒ å¤šæŒ‡æ ‡æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "metrics": metrics
            }

    async def build_aggregated_series(self, daily_series: List[TimeSeriesPoint],
                                      aggregation_type: str = "weekly") -> Dict[str, Any]:
        """
        æ„å»ºèšåˆæ—¶é—´åºåˆ— (å‘¨åº¦/æœˆåº¦)

        Args:
            daily_series: æ¯æ—¥æ—¶é—´åºåˆ—æ•°æ®
            aggregation_type: èšåˆç±»å‹ ("weekly", "monthly")

        Returns:
            Dict[str, Any]: èšåˆæ—¶é—´åºåˆ—ç»“æœ
        """
        try:
            logger.info(f"ğŸ“… æ„å»º{aggregation_type}èšåˆæ—¶é—´åºåˆ—")

            if not daily_series:
                return {"success": False, "error": "è¾“å…¥åºåˆ—ä¸ºç©º"}

            # ğŸ—“ï¸ æ ¹æ®èšåˆç±»å‹åˆ†ç»„æ•°æ®
            if aggregation_type == "weekly":
                grouped_data = self._group_by_week(daily_series)
                series_type = TimeSeriesType.WEEKLY
            elif aggregation_type == "monthly":
                grouped_data = self._group_by_month(daily_series)
                series_type = TimeSeriesType.MONTHLY
            else:
                return {"success": False, "error": f"ä¸æ”¯æŒçš„èšåˆç±»å‹: {aggregation_type}"}

            # ğŸ§® è®¡ç®—èšåˆå€¼
            aggregated_points = []
            for period, points in grouped_data.items():
                if points:
                    # è®¡ç®—å¹³å‡å€¼ã€æ€»å’Œç­‰ç»Ÿè®¡é‡
                    values = [p.value for p in points if not p.is_interpolated]

                    if values:
                        agg_point = TimeSeriesPoint(
                            date=period,
                            value=sum(values),  # é»˜è®¤ä½¿ç”¨æ€»å’Œ
                            is_interpolated=False,
                            confidence=sum(p.confidence for p in points) / len(points),
                            metadata={
                                "period_type": aggregation_type,
                                "data_points_count": len(values),
                                "avg_value": sum(values) / len(values),
                                "max_value": max(values),
                                "min_value": min(values),
                                "interpolated_points": sum(1 for p in points if p.is_interpolated)
                            }
                        )
                        aggregated_points.append(agg_point)

            # ğŸ“Š è®¡ç®—èšåˆå…ƒæ•°æ®
            metadata = self._calculate_series_metadata(aggregated_points, series_type)

            return {
                "success": True,
                "aggregation_type": aggregation_type,
                "series_type": series_type.value,
                "aggregated_series": aggregated_points,
                "metadata": metadata,
                "original_points": len(daily_series),
                "aggregated_points": len(aggregated_points),
                "build_timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"âŒ èšåˆæ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "aggregation_type": aggregation_type
            }

    # ============= æ•°æ®é¢„å¤„ç†æ–¹æ³• =============

    async def _preprocess_raw_data(self, raw_data: List[Dict[str, Any]],
                                   metric_field: str, date_field: str) -> List[Dict[str, Any]]:
        """é¢„å¤„ç†åŸå§‹æ•°æ®"""

        cleaned_data = []

        for item in raw_data:
            try:
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                if date_field not in item or metric_field not in item:
                    continue

                # æ—¥æœŸæ ¼å¼è½¬æ¢å’ŒéªŒè¯
                date_value = item[date_field]
                if isinstance(date_value, str):
                    # å°è¯•è½¬æ¢APIæ ¼å¼ (YYYYMMDD) åˆ°æ ‡å‡†æ ¼å¼
                    if len(date_value) == 8 and date_value.isdigit():
                        date_value = self.date_utils.api_format_to_date(date_value)

                    # éªŒè¯æ—¥æœŸæ ¼å¼
                    if not self.date_utils.validate_date_format(date_value):
                        logger.warning(f"æ— æ•ˆæ—¥æœŸæ ¼å¼: {date_value}")
                        continue

                # æ•°å€¼æ ¼å¼è½¬æ¢å’ŒéªŒè¯
                metric_value = item[metric_field]
                try:
                    if isinstance(metric_value, str):
                        # ç§»é™¤å¯èƒ½çš„è´§å¸ç¬¦å·å’Œé€—å·
                        metric_value = metric_value.replace('Â¥', '').replace(',', '').strip()

                    numeric_value = float(metric_value)

                    # åŸºç¡€åˆç†æ€§æ£€æŸ¥
                    if numeric_value < 0 and metric_field in ["å…¥é‡‘", "å‡ºé‡‘", "æ³¨å†Œäººæ•°", "æ€»ä½™é¢"]:
                        logger.warning(f"è´Ÿæ•°å€¼å¯èƒ½å¼‚å¸¸: {metric_field}={numeric_value}")

                except (ValueError, TypeError):
                    logger.warning(f"æ— æ•ˆæ•°å€¼: {metric_field}={metric_value}")
                    continue

                # æ·»åŠ åˆ°æ¸…ç†åçš„æ•°æ®
                cleaned_item = item.copy()
                cleaned_item[date_field] = date_value
                cleaned_item[metric_field] = numeric_value
                cleaned_data.append(cleaned_item)

            except Exception as e:
                logger.warning(f"æ•°æ®é¡¹é¢„å¤„ç†å¤±è´¥: {str(e)}")
                continue

        # æŒ‰æ—¥æœŸæ’åº
        cleaned_data.sort(key=lambda x: x[date_field])

        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(raw_data)} -> {len(cleaned_data)} é¡¹")
        return cleaned_data

    def _build_base_series(self, cleaned_data: List[Dict[str, Any]],
                           metric_field: str, date_field: str) -> List[TimeSeriesPoint]:
        """æ„å»ºåŸºç¡€æ—¶é—´åºåˆ—"""

        series_points = []

        for item in cleaned_data:
            point = TimeSeriesPoint(
                date=item[date_field],
                value=float(item[metric_field]),
                is_interpolated=False,
                confidence=1.0,
                metadata={
                    "source": "raw_data",
                    "original_item": item
                }
            )
            series_points.append(point)

        return series_points

    async def _fill_missing_dates(self, series: List[TimeSeriesPoint]) -> List[TimeSeriesPoint]:
        """è¡¥å…¨ç¼ºå¤±æ—¥æœŸ"""

        if len(series) < 2:
            return series

        # è·å–æ—¥æœŸèŒƒå›´
        start_date = datetime.strptime(series[0].date, "%Y-%m-%d")
        end_date = datetime.strptime(series[-1].date, "%Y-%m-%d")

        # ç”Ÿæˆå®Œæ•´æ—¥æœŸåˆ—è¡¨
        complete_dates = []
        current_date = start_date
        while current_date <= end_date:
            complete_dates.append(current_date.strftime("%Y-%m-%d"))
            current_date += timedelta(days=1)

        # åˆ›å»ºæ—¥æœŸåˆ°æ•°æ®ç‚¹çš„æ˜ å°„
        existing_data = {point.date: point for point in series}

        # è¡¥å…¨åºåˆ—
        complete_series = []
        for date_str in complete_dates:
            if date_str in existing_data:
                complete_series.append(existing_data[date_str])
            else:
                # æ’å€¼å¤„ç†ç¼ºå¤±æ•°æ®
                interpolated_value = self._interpolate_missing_value(
                    date_str, complete_series, series
                )

                missing_point = TimeSeriesPoint(
                    date=date_str,
                    value=interpolated_value,
                    is_interpolated=True,
                    confidence=0.7,  # æ’å€¼æ•°æ®ç½®ä¿¡åº¦è¾ƒä½
                    metadata={
                        "source": "interpolation",
                        "method": self.config["interpolation_method"]
                    }
                )
                complete_series.append(missing_point)

        logger.info(f"æ—¥æœŸè¡¥å…¨: {len(series)} -> {len(complete_series)} ç‚¹")
        return complete_series

    def _interpolate_missing_value(self, missing_date: str,
                                   current_series: List[TimeSeriesPoint],
                                   original_series: List[TimeSeriesPoint]) -> float:
        """æ’å€¼è®¡ç®—ç¼ºå¤±å€¼"""

        if not current_series:
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªç‚¹ï¼Œä½¿ç”¨åŸå§‹åºåˆ—çš„ç¬¬ä¸€ä¸ªå€¼
            return original_series[0].value if original_series else 0.0

        if self.config["interpolation_method"] == "linear":
            # çº¿æ€§æ’å€¼
            last_point = current_series[-1]

            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå·²çŸ¥æ•°æ®ç‚¹
            missing_dt = datetime.strptime(missing_date, "%Y-%m-%d")
            next_point = None

            for point in original_series:
                point_dt = datetime.strptime(point.date, "%Y-%m-%d")
                if point_dt > missing_dt:
                    next_point = point
                    break

            if next_point:
                # çº¿æ€§æ’å€¼è®¡ç®—
                last_dt = datetime.strptime(last_point.date, "%Y-%m-%d")
                next_dt = datetime.strptime(next_point.date, "%Y-%m-%d")

                total_days = (next_dt - last_dt).days
                elapsed_days = (missing_dt - last_dt).days

                if total_days > 0:
                    ratio = elapsed_days / total_days
                    interpolated = last_point.value + (next_point.value - last_point.value) * ratio
                    return interpolated

            # å¦‚æœæ‰¾ä¸åˆ°ä¸‹ä¸€ä¸ªç‚¹ï¼Œä½¿ç”¨å‰ä¸€ä¸ªå€¼
            return last_point.value

        elif self.config["interpolation_method"] == "forward_fill":
            # å‰å‘å¡«å……
            return current_series[-1].value if current_series else 0.0

        else:
            # é»˜è®¤ä½¿ç”¨0
            return 0.0

    async def _detect_and_handle_outliers(self, series: List[TimeSeriesPoint],
                                          metric_field: str) -> List[TimeSeriesPoint]:
        """å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†"""

        if len(series) < 3:
            return series

        # æå–éæ’å€¼çš„æ•°å€¼ç”¨äºç»Ÿè®¡
        real_values = [p.value for p in series if not p.is_interpolated]

        if len(real_values) < 3:
            return series

        # è®¡ç®—ç»Ÿè®¡é‡
        mean_val = sum(real_values) / len(real_values)
        squared_diffs = [(x - mean_val) ** 2 for x in real_values]
        std_val = (sum(squared_diffs) / len(squared_diffs)) ** 0.5

        # å¼‚å¸¸å€¼é˜ˆå€¼
        threshold = self.config["outlier_threshold"]
        lower_bound = mean_val - threshold * std_val
        upper_bound = mean_val + threshold * std_val

        # æ ‡è®°å¼‚å¸¸å€¼
        processed_series = []
        outliers_count = 0

        for point in series:
            new_point = TimeSeriesPoint(
                date=point.date,
                value=point.value,
                is_interpolated=point.is_interpolated,
                is_outlier=False,
                confidence=point.confidence,
                metadata=point.metadata.copy() if point.metadata else {}
            )

            # æ£€æµ‹å¼‚å¸¸å€¼ (åªå¯¹éæ’å€¼æ•°æ®æ£€æµ‹)
            if not point.is_interpolated and (point.value < lower_bound or point.value > upper_bound):
                new_point.is_outlier = True
                new_point.confidence = min(new_point.confidence, 0.5)  # é™ä½ç½®ä¿¡åº¦
                outliers_count += 1

                if new_point.metadata:
                    new_point.metadata["outlier_info"] = {
                        "deviation": abs(point.value - mean_val) / std_val if std_val > 0 else 0,
                        "bounds": {"lower": lower_bound, "upper": upper_bound}
                    }

            processed_series.append(new_point)

        logger.info(f"å¼‚å¸¸å€¼æ£€æµ‹å®Œæˆ: {outliers_count}/{len(series)} ä¸ªå¼‚å¸¸å€¼")

        # ğŸ§  AIéªŒè¯å¼‚å¸¸å€¼ (å¯é€‰)
        if outliers_count > 0 and self.claude_client:
            await self._ai_validate_outliers(processed_series, metric_field)

        return processed_series

    async def _ai_validate_outliers(self, series: List[TimeSeriesPoint], metric_field: str):
        """AIéªŒè¯å¼‚å¸¸å€¼"""

        try:
            outliers = [p for p in series if p.is_outlier]

            if not outliers:
                return

            outlier_summary = [
                {
                    "date": p.date,
                    "value": p.value,
                    "deviation": p.metadata.get("outlier_info", {}).get("deviation", 0)
                }
                for p in outliers[:5]  # åªåˆ†æå‰5ä¸ªå¼‚å¸¸å€¼
            ]

            validation_prompt = f"""
åˆ†æä»¥ä¸‹é‡‘èæŒ‡æ ‡çš„å¼‚å¸¸å€¼æ˜¯å¦åˆç†ï¼š

æŒ‡æ ‡: {metric_field}
å¼‚å¸¸å€¼åˆ—è¡¨: {json.dumps(outlier_summary, ensure_ascii=False)}

è¯·åˆ¤æ–­ï¼š
1. è¿™äº›å¼‚å¸¸å€¼æ˜¯å¦å¯èƒ½æ˜¯çœŸå®çš„ä¸šåŠ¡äº‹ä»¶ï¼ˆå¦‚å¤§é¢äº¤æ˜“ã€è¥é”€æ´»åŠ¨ç­‰ï¼‰
2. æ˜¯å¦å­˜åœ¨æ•°æ®é”™è¯¯çš„å¯èƒ½æ€§
3. å»ºè®®çš„å¤„ç†æ–¹å¼

è¿”å›ç®€æ´çš„åˆ†æç»“æœã€‚
"""

            result = await self.claude_client.analyze_complex_query(validation_prompt, {
                "metric": metric_field,
                "outliers": outlier_summary
            })

            if result.get("success"):
                logger.info(f"AIå¼‚å¸¸å€¼éªŒè¯å®Œæˆ: {metric_field}")
                # å¯ä»¥æ ¹æ®AIå»ºè®®è°ƒæ•´å¼‚å¸¸å€¼çš„ç½®ä¿¡åº¦

        except Exception as e:
            logger.warning(f"AIå¼‚å¸¸å€¼éªŒè¯å¤±è´¥: {str(e)}")

    # ============= ç»Ÿè®¡è®¡ç®—æ–¹æ³• =============

    def _calculate_series_metadata(self, series: List[TimeSeriesPoint],
                                   series_type: TimeSeriesType) -> TimeSeriesMetadata:
        """è®¡ç®—æ—¶é—´åºåˆ—å…ƒæ•°æ®"""

        if not series:
            return TimeSeriesMetadata(
                series_type=series_type,
                date_range=("", ""),
                total_points=0,
                missing_points=0,
                data_quality=DataQuality.POOR,
                interpolated_points=0,
                outliers_detected=0,
                confidence_score=0.0
            )

        total_points = len(series)
        interpolated_points = sum(1 for p in series if p.is_interpolated)
        outliers_detected = sum(1 for p in series if p.is_outlier)
        missing_points = interpolated_points  # æ’å€¼ç‚¹å³ä¸ºåŸç¼ºå¤±ç‚¹

        # è®¡ç®—æ•°æ®è´¨é‡
        completeness_ratio = (total_points - missing_points) / total_points if total_points > 0 else 0

        if completeness_ratio >= 0.95:
            data_quality = DataQuality.EXCELLENT
        elif completeness_ratio >= 0.80:
            data_quality = DataQuality.GOOD
        elif completeness_ratio >= 0.60:
            data_quality = DataQuality.FAIR
        else:
            data_quality = DataQuality.POOR

        # è®¡ç®—ç½®ä¿¡åº¦
        avg_confidence = sum(p.confidence for p in series) / total_points if total_points > 0 else 0
        quality_penalty = outliers_detected * 0.05  # å¼‚å¸¸å€¼é™ä½ç½®ä¿¡åº¦
        confidence_score = max(0.0, avg_confidence - quality_penalty)

        return TimeSeriesMetadata(
            series_type=series_type,
            date_range=(series[0].date, series[-1].date),
            total_points=total_points,
            missing_points=missing_points,
            data_quality=data_quality,
            interpolated_points=interpolated_points,
            outliers_detected=outliers_detected,
            confidence_score=confidence_score
        )

    async def _calculate_series_statistics(self, series: List[TimeSeriesPoint],
                                           metric_field: str) -> Dict[str, Any]:
        """è®¡ç®—æ—¶é—´åºåˆ—ç»Ÿè®¡ä¿¡æ¯"""

        if not series:
            return {}

        values = [p.value for p in series]
        real_values = [p.value for p in series if not p.is_interpolated]

        # åŸºç¡€ç»Ÿè®¡
        stats = {
            "total_points": len(values),
            "real_data_points": len(real_values),
            "mean": sum(values) / len(values),
            "median": sorted(values)[len(values) // 2],
            "min": min(values),
            "max": max(values),
            "range": max(values) - min(values),
            "sum": sum(values)
        }

        # è®¡ç®—æ ‡å‡†å·®å’Œæ–¹å·®
        if len(values) > 1:
            mean_val = stats["mean"]
            variance = sum((x - mean_val) ** 2 for x in values) / len(values)
            stats["variance"] = variance
            stats["std_dev"] = variance ** 0.5
            stats["coefficient_of_variation"] = stats["std_dev"] / mean_val if mean_val != 0 else 0

        # ğŸ§® ä½¿ç”¨é‡‘èè®¡ç®—å™¨è®¡ç®—å¢é•¿ç‡
        if len(real_values) >= 2 and self.financial_calculator:
            growth_analysis = self.financial_calculator.calculate_growth_rate(real_values, "compound")
            stats["growth_analysis"] = {
                "growth_rate": growth_analysis.growth_rate,
                "trend_direction": growth_analysis.trend_direction,
                "volatility": growth_analysis.volatility,
                "confidence_level": growth_analysis.confidence_level
            }

        # è®¡ç®—ç§»åŠ¨å¹³å‡
        if len(values) >= 7:
            ma_7 = self._calculate_moving_average(values, 7)
            stats["moving_averages"] = {
                "ma_7": ma_7[-1] if ma_7 else None
            }

            if len(values) >= 30:
                ma_30 = self._calculate_moving_average(values, 30)
                stats["moving_averages"]["ma_30"] = ma_30[-1] if ma_30 else None

        # æ•°æ®è´¨é‡ç»Ÿè®¡
        stats["data_quality"] = {
            "interpolated_ratio": sum(1 for p in series if p.is_interpolated) / len(series),
            "outlier_ratio": sum(1 for p in series if p.is_outlier) / len(series),
            "avg_confidence": sum(p.confidence for p in series) / len(series)
        }

        return stats

    def _calculate_moving_average(self, values: List[float], window: int) -> List[float]:
        """è®¡ç®—ç§»åŠ¨å¹³å‡"""
        if len(values) < window:
            return []

        ma_values = []
        for i in range(window - 1, len(values)):
            window_values = values[i - window + 1:i + 1]
            ma_values.append(sum(window_values) / window)

        return ma_values

    # ============= å¤šæŒ‡æ ‡æ—¶é—´åºåˆ—å¤„ç† =============

    async def _align_multi_series(self, multi_series: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """å¯¹é½å¤šæŒ‡æ ‡æ—¶é—´åºåˆ—"""

        if not multi_series:
            return {}

        # æ‰¾åˆ°å…±åŒçš„æ—¥æœŸèŒƒå›´
        all_dates = set()
        for metric, series_data in multi_series.items():
            series_points = series_data.get("time_series", [])
            for point in series_points:
                all_dates.add(point.date)

        sorted_dates = sorted(list(all_dates))

        # æ„å»ºå¯¹é½çš„çŸ©é˜µ
        aligned_data = {
            "dates": sorted_dates,
            "metrics": {}
        }

        for metric, series_data in multi_series.items():
            series_points = series_data.get("time_series", [])
            point_dict = {p.date: p for p in series_points}

            metric_values = []
            for date in sorted_dates:
                if date in point_dict:
                    metric_values.append(point_dict[date].value)
                else:
                    metric_values.append(None)  # ç¼ºå¤±å€¼

            aligned_data["metrics"][metric] = metric_values

        return aligned_data

    async def _calculate_correlations(self, aligned_series: Dict[str, Any],
                                      metrics: List[str]) -> Dict[str, Any]:
        """è®¡ç®—æŒ‡æ ‡é—´ç›¸å…³æ€§"""

        correlations = {}

        if len(metrics) < 2:
            return correlations

        # è®¡ç®—ä¸¤ä¸¤ç›¸å…³æ€§
        for i, metric1 in enumerate(metrics):
            for j, metric2 in enumerate(metrics[i + 1:], i + 1):
                values1 = aligned_series["metrics"].get(metric1, [])
                values2 = aligned_series["metrics"].get(metric2, [])

                # è¿‡æ»¤Noneå€¼
                valid_pairs = [
                    (v1, v2) for v1, v2 in zip(values1, values2)
                    if v1 is not None and v2 is not None
                ]

                if len(valid_pairs) >= 3:  # è‡³å°‘éœ€è¦3ä¸ªæ•°æ®ç‚¹
                    correlation = self._calculate_pearson_correlation(valid_pairs)
                    correlations[f"{metric1}_vs_{metric2}"] = {
                        "correlation": correlation,
                        "data_points": len(valid_pairs),
                        "strength": self._interpret_correlation(correlation)
                    }

        return correlations

    def _calculate_pearson_correlation(self, pairs: List[Tuple[float, float]]) -> float:
        """è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°"""

        if len(pairs) < 2:
            return 0.0

        n = len(pairs)
        sum_x = sum(x for x, y in pairs)
        sum_y = sum(y for x, y in pairs)
        sum_xy = sum(x * y for x, y in pairs)
        sum_x2 = sum(x * x for x, y in pairs)
        sum_y2 = sum(y * y for x, y in pairs)

        numerator = n * sum_xy - sum_x * sum_y
        denominator = ((n * sum_x2 - sum_x * sum_x) * (n * sum_y2 - sum_y * sum_y)) ** 0.5

        if denominator == 0:
            return 0.0

        return numerator / denominator

    def _interpret_correlation(self, correlation: float) -> str:
        """è§£é‡Šç›¸å…³æ€§å¼ºåº¦"""
        abs_corr = abs(correlation)

        if abs_corr >= 0.8:
            return "å¾ˆå¼º"
        elif abs_corr >= 0.6:
            return "å¼º"
        elif abs_corr >= 0.4:
            return "ä¸­ç­‰"
        elif abs_corr >= 0.2:
            return "å¼±"
        else:
            return "å¾ˆå¼±"

    # ============= èšåˆæ—¶é—´åºåˆ—å¤„ç† =============

    def _group_by_week(self, series: List[TimeSeriesPoint]) -> Dict[str, List[TimeSeriesPoint]]:
        """æŒ‰å‘¨åˆ†ç»„"""
        grouped = {}

        for point in series:
            date_obj = datetime.strptime(point.date, "%Y-%m-%d")
            # è·å–å‘¨ä¸€çš„æ—¥æœŸä½œä¸ºå‘¨æ ‡è¯†
            monday = date_obj - timedelta(days=date_obj.weekday())
            week_key = monday.strftime("%Y-W%U")

            if week_key not in grouped:
                grouped[week_key] = []
            grouped[week_key].append(point)

        return grouped

    def _group_by_month(self, series: List[TimeSeriesPoint]) -> Dict[str, List[TimeSeriesPoint]]:
        """æŒ‰æœˆåˆ†ç»„"""
        grouped = {}

        for point in series:
            date_obj = datetime.strptime(point.date, "%Y-%m-%d")
            month_key = date_obj.strftime("%Y-%m")

            if month_key not in grouped:
                grouped[month_key] = []
            grouped[month_key].append(point)

        return grouped

    # ============= AIå¢å¼ºéªŒè¯ =============

    async def _ai_validate_time_series(self, series: List[TimeSeriesPoint],
                                       metric_field: str) -> Dict[str, Any]:
        """AIéªŒè¯æ—¶é—´åºåˆ—è´¨é‡"""

        if not self.claude_client:
            return {"validation_performed": False, "reason": "no_claude_client"}

        try:
            # å‡†å¤‡éªŒè¯æ•°æ®
            sample_points = series[:10] + series[-10:]  # å–å¼€å§‹å’Œç»“æŸçš„æ•°æ®ç‚¹
            validation_data = [
                {
                    "date": p.date,
                    "value": p.value,
                    "is_interpolated": p.is_interpolated,
                    "is_outlier": p.is_outlier,
                    "confidence": p.confidence
                }
                for p in sample_points
            ]

            validation_prompt = f"""
åˆ†æä»¥ä¸‹æ—¶é—´åºåˆ—æ•°æ®çš„è´¨é‡å’Œåˆç†æ€§ï¼š

æŒ‡æ ‡: {metric_field}
æ€»æ•°æ®ç‚¹: {len(series)}
é‡‡æ ·æ•°æ®: {json.dumps(validation_data, ensure_ascii=False)}

è¯·è¯„ä¼°ï¼š
1. æ•°æ®è¶‹åŠ¿æ˜¯å¦åˆç†
2. æ•°å€¼èŒƒå›´æ˜¯å¦æ­£å¸¸
3. æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„æ•°æ®è´¨é‡é—®é¢˜
4. æ•´ä½“å¯ä¿¡åº¦è¯„åˆ† (0-1)

è¿”å›ç®€æ´çš„éªŒè¯ç»“æœã€‚
"""

            result = await self.claude_client.analyze_complex_query(validation_prompt, {
                "metric": metric_field,
                "series_length": len(series),
                "sample_data": validation_data
            })

            if result.get("success"):
                return {
                    "validation_performed": True,
                    "ai_assessment": result.get("analysis", {}),
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {"validation_performed": False, "error": result.get("error", "Unknown error")}

        except Exception as e:
            logger.error(f"AIéªŒè¯å¤±è´¥: {str(e)}")
            return {"validation_performed": False, "error": str(e)}

    # ============= å·¥å…·æ–¹æ³• =============

    def _create_empty_series_result(self, metric_field: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºåºåˆ—ç»“æœ"""
        return {
            "success": False,
            "error": "æ— æœ‰æ•ˆæ•°æ®",
            "metric": metric_field,
            "series_type": TimeSeriesType.DAILY.value,
            "time_series": [],
            "metadata": None
        }

    def _get_date_range(self, series: List[TimeSeriesPoint]) -> Tuple[str, str]:
        """è·å–åºåˆ—æ—¥æœŸèŒƒå›´"""
        if not series:
            return "", ""
        return series[0].date, series[-1].date

    def _calculate_multi_series_stats(self, multi_series: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—å¤šåºåˆ—ç»Ÿè®¡ä¿¡æ¯"""

        total_metrics = len(multi_series)
        successful_series = sum(1 for s in multi_series.values() if s.get("success", False))

        avg_quality = 0
        total_points = 0

        for series_data in multi_series.values():
            if series_data.get("success"):
                metadata = series_data.get("metadata")
                if metadata:
                    avg_quality += metadata.confidence_score
                    total_points += metadata.total_points

        avg_quality = avg_quality / successful_series if successful_series > 0 else 0

        return {
            "total_metrics": total_metrics,
            "successful_metrics": successful_series,
            "success_rate": successful_series / total_metrics if total_metrics > 0 else 0,
            "average_quality_score": avg_quality,
            "total_data_points": total_points
        }


# ============= å·¥å‚å‡½æ•° =============

def create_time_series_builder(claude_client=None, gpt_client=None) -> TimeSeriesBuilder:
    """
    åˆ›å»ºæ—¶é—´åºåˆ—æ„å»ºå™¨å®ä¾‹

    Args:
        claude_client: Claudeå®¢æˆ·ç«¯å®ä¾‹
        gpt_client: GPTå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        TimeSeriesBuilder: æ—¶é—´åºåˆ—æ„å»ºå™¨å®ä¾‹
    """
    return TimeSeriesBuilder(claude_client, gpt_client)


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""

    # åˆ›å»ºæ—¶é—´åºåˆ—æ„å»ºå™¨
    builder = create_time_series_builder()

    print("=== æ—¶é—´åºåˆ—æ„å»ºå™¨æµ‹è¯• ===")

    # æ¨¡æ‹ŸåŸå§‹æ•°æ®
    sample_data = [
        {"æ—¥æœŸ": "20240601", "å…¥é‡‘": "180000.50", "æ³¨å†Œäººæ•°": 25},
        {"æ—¥æœŸ": "20240602", "å…¥é‡‘": "165000.30", "æ³¨å†Œäººæ•°": 30},
        {"æ—¥æœŸ": "20240603", "å…¥é‡‘": "195000.00", "æ³¨å†Œäººæ•°": 28},
        # ç¼ºå¤± 20240604 çš„æ•°æ®
        {"æ—¥æœŸ": "20240605", "å…¥é‡‘": "210000.80", "æ³¨å†Œäººæ•°": 35},
        {"æ—¥æœŸ": "20240606", "å…¥é‡‘": "185000.20", "æ³¨å†Œäººæ•°": 22},
    ]

    # 1. æ„å»ºå•æŒ‡æ ‡æ—¶é—´åºåˆ—
    inflow_series = await builder.build_daily_time_series(sample_data, "å…¥é‡‘")
    print(f"å…¥é‡‘æ—¶é—´åºåˆ—: {'æˆåŠŸ' if inflow_series.get('success') else 'å¤±è´¥'}")
    if inflow_series.get("success"):
        print(f"æ•°æ®ç‚¹æ•°: {inflow_series['total_points']}")
        print(f"è´¨é‡è¯„åˆ†: {inflow_series['metadata'].confidence_score:.2f}")

    # 2. æ„å»ºå¤šæŒ‡æ ‡æ—¶é—´åºåˆ—
    multi_series = await builder.build_multi_metric_series(sample_data, ["å…¥é‡‘", "æ³¨å†Œäººæ•°"])
    print(f"å¤šæŒ‡æ ‡åºåˆ—: {'æˆåŠŸ' if multi_series.get('success') else 'å¤±è´¥'}")
    if multi_series.get("success"):
        print(f"æˆåŠŸæŒ‡æ ‡: {multi_series['successful_metrics']}")
        print(f"ç›¸å…³æ€§: {len(multi_series.get('correlations', {}))} ç»„")

    # 3. æ„å»ºèšåˆåºåˆ— (å¦‚æœæœ‰è¶³å¤Ÿæ•°æ®)
    if inflow_series.get("success"):
        time_series = inflow_series["time_series"]
        if len(time_series) >= 7:
            weekly_series = await builder.build_aggregated_series(time_series, "weekly")
            print(f"å‘¨åº¦èšåˆ: {'æˆåŠŸ' if weekly_series.get('success') else 'å¤±è´¥'}")


if __name__ == "__main__":
    asyncio.run(main())