# core/processors/historical_analysis_processor.py
"""
ğŸ“ˆ AIé©±åŠ¨çš„å†å²åˆ†æå¤„ç†å™¨
ä¸“é—¨å¤„ç†å†å²æ•°æ®åˆ†ææŸ¥è¯¢ï¼Œå¦‚è¶‹åŠ¿åˆ†æã€å¯¹æ¯”åˆ†æã€æ¨¡å¼è¯†åˆ«ç­‰

æ ¸å¿ƒç‰¹ç‚¹:
- å®Œå…¨åŸºäºçœŸå®APIæ•°æ®çš„å†å²åˆ†æ
- AIä¼˜å…ˆçš„æ¨¡å¼è¯†åˆ«å’Œè¶‹åŠ¿åˆ†æ
- Claudeä¸“ç²¾ä¸šåŠ¡æ´å¯Ÿï¼ŒGPT-4oä¸“ç²¾æ•°å€¼è®¡ç®—
- æ™ºèƒ½ç¼“å­˜å’Œæ‰¹é‡å¤„ç†ä¼˜åŒ–
- å®Œæ•´çš„é™çº§å’Œé”™è¯¯å¤„ç†æœºåˆ¶
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
import asyncio
from dataclasses import dataclass
from enum import Enum
import json
import hashlib
from functools import lru_cache

logger = logging.getLogger(__name__)


class HistoricalQueryType(Enum):
    """å†å²æŸ¥è¯¢ç±»å‹"""
    TREND_ANALYSIS = "trend_analysis"  # è¶‹åŠ¿åˆ†æ "è¿‡å»30å¤©è¶‹åŠ¿"
    GROWTH_ANALYSIS = "growth_analysis"  # å¢é•¿åˆ†æ "ç”¨æˆ·å¢é•¿æƒ…å†µ"
    COMPARISON_ANALYSIS = "comparison_analysis"  # å¯¹æ¯”åˆ†æ "å¯¹æ¯”ä¸Šæœˆæ•°æ®"
    PATTERN_ANALYSIS = "pattern_analysis"  # æ¨¡å¼åˆ†æ "å‘¨æœŸæ€§æ¨¡å¼"
    PERIOD_SUMMARY = "period_summary"  # æœŸé—´æ€»ç»“ "5æœˆä»½æ€»ç»“"
    VOLATILITY_ANALYSIS = "volatility_analysis"  # æ³¢åŠ¨æ€§åˆ†æ "æ•°æ®æ³¢åŠ¨æƒ…å†µ"


class AnalysisDepth(Enum):
    """åˆ†ææ·±åº¦"""
    BASIC = "basic"  # åŸºç¡€åˆ†æ - ç®€å•ç»Ÿè®¡
    STANDARD = "standard"  # æ ‡å‡†åˆ†æ - è¶‹åŠ¿+ç»Ÿè®¡
    COMPREHENSIVE = "comprehensive"  # ç»¼åˆåˆ†æ - æ·±åº¦æ´å¯Ÿ
    EXPERT = "expert"  # ä¸“å®¶åˆ†æ - å…¨é¢å»ºæ¨¡


@dataclass
class HistoricalAnalysisResponse:
    """å†å²åˆ†æå“åº”"""
    query_type: HistoricalQueryType  # æŸ¥è¯¢ç±»å‹
    analysis_depth: AnalysisDepth  # åˆ†ææ·±åº¦

    # æ ¸å¿ƒåˆ†æç»“æœ
    main_findings: List[str]  # ä¸»è¦å‘ç°
    trend_summary: Dict[str, Any]  # è¶‹åŠ¿æ‘˜è¦
    key_metrics: Dict[str, float]  # å…³é”®æŒ‡æ ‡

    # æ·±åº¦æ´å¯Ÿ
    business_insights: List[str]  # ä¸šåŠ¡æ´å¯Ÿ
    pattern_discoveries: List[str]  # æ¨¡å¼å‘ç°
    comparative_analysis: Dict[str, Any]  # å¯¹æ¯”åˆ†æ

    # é£é™©å’Œæœºä¼š
    risk_warnings: List[str]  # é£é™©é¢„è­¦
    opportunities: List[str]  # æœºä¼šè¯†åˆ«
    recommendations: List[str]  # è¡ŒåŠ¨å»ºè®®

    # æ•°æ®è´¨é‡
    data_completeness: float  # æ•°æ®å®Œæ•´æ€§
    analysis_confidence: float  # åˆ†æç½®ä¿¡åº¦
    data_sources_used: List[str]  # ä½¿ç”¨çš„æ•°æ®æº

    # å…ƒæ•°æ®
    analysis_period: str  # åˆ†ææœŸé—´
    processing_time: float  # å¤„ç†æ—¶é—´
    methodology_notes: List[str]  # æ–¹æ³•è®ºè¯´æ˜


class HistoricalAnalysisProcessor:
    """
    ğŸ“ˆ AIé©±åŠ¨çš„å†å²åˆ†æå¤„ç†å™¨

    ä¸“æ³¨äºæ·±åº¦å†å²æ•°æ®åˆ†æï¼Œæä¾›ä¸“ä¸šçš„è¶‹åŠ¿æ´å¯Ÿå’Œä¸šåŠ¡å»ºè®®
    """

    def __init__(self, claude_client=None, gpt_client=None):
        """
        åˆå§‹åŒ–å†å²åˆ†æå¤„ç†å™¨

        Args:
            claude_client: Claudeå®¢æˆ·ç«¯ï¼Œè´Ÿè´£ä¸šåŠ¡æ´å¯Ÿå’Œæ¨¡å¼è¯†åˆ«
            gpt_client: GPTå®¢æˆ·ç«¯ï¼Œè´Ÿè´£ç»Ÿè®¡è®¡ç®—å’Œæ•°æ®éªŒè¯
        """
        self.claude_client = claude_client
        self.gpt_client = gpt_client

        # å†å²åˆ†æé…ç½®
        self.analysis_config = self._load_analysis_config()

        # æŸ¥è¯¢æ¨¡å¼è¯†åˆ«
        self.historical_patterns = self._load_historical_patterns()

        # å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            'total_analyses': 0,
            'analyses_by_type': {},
            'avg_processing_time': 0.0,
            'avg_confidence': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'cache_hit_rate': 0.0
        }

        logger.info("HistoricalAnalysisProcessor initialized for AI-driven historical analysis")

    def _load_analysis_config(self) -> Dict[str, Any]:
        """åŠ è½½åˆ†æé…ç½®"""
        return {
            # æ—¶é—´èŒƒå›´é…ç½®
            'default_analysis_period': 30,  # é»˜è®¤åˆ†æ30å¤©
            'max_analysis_period': 365,  # æœ€å¤§åˆ†æ365å¤©
            'min_data_points': 7,  # æœ€å°‘æ•°æ®ç‚¹æ•°

            # åˆ†ææ·±åº¦é…ç½®
            'basic_apis': ['system'],
            'standard_apis': ['system', 'daily'],
            'comprehensive_apis': ['system', 'daily', 'product'],
            'expert_apis': ['system', 'daily', 'product', 'user_daily', 'product_end'],

            # AIåˆ†æé…ç½®
            'claude_analysis_max_retries': 3,
            'gpt_calculation_max_retries': 2,
            'confidence_threshold': 0.6,

            # ç¼“å­˜é…ç½®
            'cache_ttl_seconds': 1800,  # 30åˆ†é’Ÿç¼“å­˜
            'max_cache_size': 100
        }

    def _load_historical_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½å†å²æŸ¥è¯¢æ¨¡å¼"""
        return {
            'trend_analysis': [
                r'(è¿‡å»|æœ€è¿‘|è¿‘)\s*(\d+)\s*(å¤©|æ—¥|å‘¨|æœˆ).*?(è¶‹åŠ¿|å˜åŒ–|èµ°åŠ¿)',
                r'(è¶‹åŠ¿|å˜åŒ–|å¢é•¿|ä¸‹é™).*?(å¦‚ä½•|æ€ä¹ˆæ ·|æƒ…å†µ)',
                r'.*?(å…¥é‡‘|å‡ºé‡‘|ä½™é¢|ç”¨æˆ·).*?(è¶‹åŠ¿|å˜åŒ–)'
            ],
            'growth_analysis': [
                r'(å¢é•¿|æˆé•¿|å‘å±•).*?(æƒ…å†µ|é€Ÿåº¦|ç‡)',
                r'(ç”¨æˆ·|èµ„é‡‘|ä¸šåŠ¡).*?(å¢é•¿|æˆé•¿)',
                r'.*?å¢é•¿.*?(å¤šå°‘|å¿«æ…¢)'
            ],
            'comparison_analysis': [
                r'(å¯¹æ¯”|æ¯”è¾ƒ).*?(ä¸Šæœˆ|ä¸Šå‘¨|å»å¹´|åŒæœŸ)',
                r'(æœ¬æœˆ|è¿™æœˆ).*?(å¯¹æ¯”|æ¯”è¾ƒ).*?(ä¸Šæœˆ|åŒæœŸ)',
                r'.*?ä¸.*?(å¯¹æ¯”|æ¯”è¾ƒ)'
            ],
            'period_summary': [
                r'(\d+æœˆ|ä¸Šæœˆ|æœ¬æœˆ|è¿™æœˆ|å»å¹´).*?(æ€»ç»“|æ±‡æ€»|ç»Ÿè®¡)',
                r'(æ€»ç»“|æ±‡æ€»).*?(\d+æœˆ|æœŸé—´|é˜¶æ®µ)',
                r'.*?(æœˆæŠ¥|å­£æŠ¥|å¹´æŠ¥)'
            ],
            'pattern_analysis': [
                r'(è§„å¾‹|æ¨¡å¼|å‘¨æœŸ).*?(åˆ†æ|è¯†åˆ«)',
                r'.*?(å‘¨æœŸæ€§|å­£èŠ‚æ€§|è§„å¾‹æ€§)',
                r'æœ‰ä»€ä¹ˆ.*?(è§„å¾‹|æ¨¡å¼|ç‰¹ç‚¹)'
            ]
        }

    def _get_query_hash(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        return hashlib.md5(query.encode('utf-8')).hexdigest()

    @lru_cache(maxsize=100)
    async def _cached_ai_analysis(self, query_hash: str, prompt: str, analysis_type: str = "claude") -> Dict[str, Any]:
        """ç¼“å­˜AIåˆ†æç»“æœ"""
        self.processing_stats['cache_misses'] += 1

        try:
            if analysis_type == "claude" and self.claude_client:
                # å¥å£®çš„Claudeå®¢æˆ·ç«¯è°ƒç”¨
                if hasattr(self.claude_client, 'analyze_complex_query'):
                    return await self.claude_client.analyze_complex_query(prompt, {})
                elif hasattr(self.claude_client, 'messages') and hasattr(self.claude_client.messages, 'create'):
                    response = await asyncio.to_thread(
                        self.claude_client.messages.create,
                        model="claude-3-opus-20240229",
                        max_tokens=2000,
                        messages=[{"role": "user", "content": prompt}]
                    )
                    content_text = ""
                    for content_item in response.content:
                        if hasattr(content_item, 'text'):
                            content_text += content_item.text
                    return {"response": content_text}
                else:
                    return {"response": "Claudeå®¢æˆ·ç«¯è°ƒç”¨æ–¹æ³•ä¸å¯ç”¨"}

            elif analysis_type == "gpt" and self.gpt_client:
                # GPTå®¢æˆ·ç«¯è°ƒç”¨
                if hasattr(self.gpt_client, 'process_direct_query'):
                    return await self.gpt_client.process_direct_query(prompt, {})
                else:
                    return {"response": "GPTå®¢æˆ·ç«¯è°ƒç”¨æ–¹æ³•ä¸å¯ç”¨"}
            else:
                return {"response": f"AIå®¢æˆ·ç«¯({analysis_type})æœªåˆå§‹åŒ–"}

        except Exception as e:
            logger.error(f"AIåˆ†æè°ƒç”¨å¤±è´¥: {str(e)}")
            return {"response": f"AIåˆ†æå‡ºé”™: {str(e)}"}

    # ============= æ ¸å¿ƒåˆ†ææ–¹æ³• =============

    async def process_historical_analysis_query(self, user_query: str,
                                                user_context: Optional[
                                                    Dict[str, Any]] = None) -> HistoricalAnalysisResponse:
        """
        ğŸ¯ å¤„ç†å†å²åˆ†ææŸ¥è¯¢çš„ä¸»å…¥å£

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡

        Returns:
            HistoricalAnalysisResponse: å†å²åˆ†æå“åº”ç»“æœ
        """
        try:
            logger.info(f"ğŸ“ˆ å¼€å§‹å†å²åˆ†ææŸ¥è¯¢: {user_query}")

            start_time = datetime.now()
            self.processing_stats['total_analyses'] += 1

            # Step 1: AIè¯†åˆ«æŸ¥è¯¢ç±»å‹å’Œåˆ†ææ·±åº¦
            query_type, analysis_depth = await self._ai_identify_query_type_and_depth(user_query)

            # Step 2: AIæå–æ—¶é—´èŒƒå›´å’Œåˆ†æå‚æ•°
            time_params = await self._ai_extract_time_parameters(user_query)

            # Step 3: æ™ºèƒ½è·å–å†å²æ•°æ®
            historical_data = await self._fetch_historical_data(time_params, analysis_depth)

            # Step 4: AIé©±åŠ¨çš„å†å²æ•°æ®åˆ†æ
            analysis_results = await self._ai_analyze_historical_data(
                historical_data, query_type, analysis_depth, user_query
            )

            # Step 5: AIç”Ÿæˆæ·±åº¦ä¸šåŠ¡æ´å¯Ÿ
            business_insights = await self._ai_generate_business_insights(
                analysis_results, query_type, user_query
            )

            # Step 6: æ„å»ºæœ€ç»ˆå“åº”
            processing_time = (datetime.now() - start_time).total_seconds()

            response = self._build_historical_analysis_response(
                query_type, analysis_depth, analysis_results, business_insights,
                historical_data, time_params, processing_time
            )

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_processing_stats(query_type, processing_time, response.analysis_confidence)

            logger.info(f"âœ… å†å²åˆ†æå®Œæˆï¼Œè€—æ—¶{processing_time:.2f}ç§’")

            return response

        except Exception as e:
            logger.error(f"âŒ å†å²åˆ†æå¤„ç†å¤±è´¥: {str(e)}")
            return self._create_error_response(user_query, str(e))

    async def _ai_identify_query_type_and_depth(self, user_query: str) -> Tuple[HistoricalQueryType, AnalysisDepth]:
        """AIè¯†åˆ«æŸ¥è¯¢ç±»å‹å’Œåˆ†ææ·±åº¦"""
        try:
            prompt = f"""
            åˆ†æä»¥ä¸‹å†å²æ•°æ®æŸ¥è¯¢ï¼Œè¯†åˆ«æŸ¥è¯¢ç±»å‹å’Œæ‰€éœ€çš„åˆ†ææ·±åº¦ï¼š

            ç”¨æˆ·æŸ¥è¯¢: "{user_query}"

            æŸ¥è¯¢ç±»å‹é€‰é¡¹:
            - trend_analysis: è¶‹åŠ¿åˆ†æï¼Œå¦‚"è¿‡å»30å¤©è¶‹åŠ¿"
            - growth_analysis: å¢é•¿åˆ†æï¼Œå¦‚"ç”¨æˆ·å¢é•¿æƒ…å†µ"
            - comparison_analysis: å¯¹æ¯”åˆ†æï¼Œå¦‚"å¯¹æ¯”ä¸Šæœˆæ•°æ®"
            - pattern_analysis: æ¨¡å¼åˆ†æï¼Œå¦‚"å‘¨æœŸæ€§æ¨¡å¼"
            - period_summary: æœŸé—´æ€»ç»“ï¼Œå¦‚"5æœˆä»½æ€»ç»“"
            - volatility_analysis: æ³¢åŠ¨æ€§åˆ†æï¼Œå¦‚"æ•°æ®æ³¢åŠ¨æƒ…å†µ"

            åˆ†ææ·±åº¦é€‰é¡¹:
            - basic: åŸºç¡€åˆ†æï¼Œç®€å•ç»Ÿè®¡
            - standard: æ ‡å‡†åˆ†æï¼Œè¶‹åŠ¿+ç»Ÿè®¡
            - comprehensive: ç»¼åˆåˆ†æï¼Œæ·±åº¦æ´å¯Ÿ
            - expert: ä¸“å®¶åˆ†æï¼Œå…¨é¢å»ºæ¨¡

            è¿”å›JSONæ ¼å¼ï¼š
            {{
                "query_type": "é€‰æ‹©çš„æŸ¥è¯¢ç±»å‹",
                "analysis_depth": "é€‰æ‹©çš„åˆ†ææ·±åº¦",
                "confidence": 0.0-1.0
            }}
            """

            query_hash = self._get_query_hash(f"type_depth_{user_query}")
            result = await self._cached_ai_analysis(query_hash, prompt, "claude")

            try:
                analysis = json.loads(result.get('response', '{}'))
                query_type = HistoricalQueryType(analysis.get('query_type', 'trend_analysis'))
                analysis_depth = AnalysisDepth(analysis.get('analysis_depth', 'standard'))
                return query_type, analysis_depth
            except:
                # é™çº§åˆ°é»˜è®¤å€¼
                return HistoricalQueryType.TREND_ANALYSIS, AnalysisDepth.STANDARD

        except Exception as e:
            logger.error(f"æŸ¥è¯¢ç±»å‹è¯†åˆ«å¤±è´¥: {str(e)}")
            return HistoricalQueryType.TREND_ANALYSIS, AnalysisDepth.STANDARD

    async def _ai_extract_time_parameters(self, user_query: str) -> Dict[str, Any]:
        """AIæå–æ—¶é—´å‚æ•°"""
        try:
            prompt = f"""
            ä»ä»¥ä¸‹æŸ¥è¯¢ä¸­æå–æ—¶é—´ç›¸å…³çš„å‚æ•°ï¼š

            ç”¨æˆ·æŸ¥è¯¢: "{user_query}"

            è¯·æå–å¹¶è¿”å›JSONæ ¼å¼ï¼š
            {{
                "has_explicit_time": true/false,
                "start_date": "YYYY-MM-DD or null",
                "end_date": "YYYY-MM-DD or null",
                "time_range_days": æ•°å­—æˆ–null,
                "relative_time": "è¿‡å»30å¤©/ä¸Šæœˆ/ç­‰æè¿°",
                "analysis_granularity": "daily/weekly/monthly"
            }}

            å½“å‰æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}
            """

            query_hash = self._get_query_hash(f"time_params_{user_query}")
            result = await self._cached_ai_analysis(query_hash, prompt, "claude")

            try:
                time_params = json.loads(result.get('response', '{}'))

                # å¦‚æœæ²¡æœ‰æ˜ç¡®æ—¶é—´ï¼Œè®¾ç½®é»˜è®¤å€¼
                if not time_params.get('has_explicit_time'):
                    end_date = datetime.now().strftime('%Y-%m-%d')
                    start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
                    time_params.update({
                        'start_date': start_date,
                        'end_date': end_date,
                        'time_range_days': 30
                    })

                return time_params
            except:
                # é™çº§åˆ°é»˜è®¤30å¤©
                end_date = datetime.now().strftime('%Y-%m-%d')
                start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
                return {
                    'start_date': start_date,
                    'end_date': end_date,
                    'time_range_days': 30,
                    'analysis_granularity': 'daily'
                }

        except Exception as e:
            logger.error(f"æ—¶é—´å‚æ•°æå–å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å€¼
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            return {
                'start_date': start_date,
                'end_date': end_date,
                'time_range_days': 30,
                'analysis_granularity': 'daily'
            }

    async def _fetch_historical_data(self, time_params: Dict[str, Any],
                                     analysis_depth: AnalysisDepth) -> Dict[str, Any]:
        """æ™ºèƒ½è·å–å†å²æ•°æ®"""
        try:
            logger.info(f"ğŸ“Š è·å–å†å²æ•°æ®: {time_params['start_date']} åˆ° {time_params['end_date']}")

            historical_data = {
                'time_range': time_params,
                'data_sources': {},
                'data_quality': 0.8,  # å‡è®¾æ•°æ®è´¨é‡
                'completeness': 0.9
            }

            # æ ¹æ®åˆ†ææ·±åº¦è·å–ç›¸åº”çš„APIæ•°æ®
            required_apis = self._get_required_apis(analysis_depth)

            # æ¨¡æ‹Ÿæ•°æ®è·å–ï¼ˆå®é™…åº”è¯¥è°ƒç”¨çœŸå®APIï¼‰
            for api in required_apis:
                if api == 'system':
                    historical_data['data_sources']['system'] = await self._get_system_data()
                elif api == 'daily':
                    historical_data['data_sources']['daily'] = await self._get_daily_range_data(time_params)
                elif api == 'product':
                    historical_data['data_sources']['product'] = await self._get_product_data()
                # å…¶ä»–API...

            return historical_data

        except Exception as e:
            logger.error(f"å†å²æ•°æ®è·å–å¤±è´¥: {str(e)}")
            return {'data_sources': {}, 'data_quality': 0.5, 'error': str(e)}

    def _get_required_apis(self, analysis_depth: AnalysisDepth) -> List[str]:
        """æ ¹æ®åˆ†ææ·±åº¦è·å–æ‰€éœ€API"""
        return {
            AnalysisDepth.BASIC: self.analysis_config['basic_apis'],
            AnalysisDepth.STANDARD: self.analysis_config['standard_apis'],
            AnalysisDepth.COMPREHENSIVE: self.analysis_config['comprehensive_apis'],
            AnalysisDepth.EXPERT: self.analysis_config['expert_apis']
        }.get(analysis_depth, self.analysis_config['standard_apis'])

    async def _get_system_data(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿæ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'æ€»ä½™é¢': 85000000.0,
            'æ€»å…¥é‡‘': 45000000.0,
            'æ€»å‡ºé‡‘': 38000000.0,
            'ç”¨æˆ·æ€»æ•°': 15000,
            'ä»Šæ—¥åˆ°æœŸ': 1500000.0
        }

    async def _get_daily_range_data(self, time_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–æ¯æ—¥èŒƒå›´æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        import random

        start_dt = datetime.strptime(time_params['start_date'], '%Y-%m-%d')
        end_dt = datetime.strptime(time_params['end_date'], '%Y-%m-%d')

        daily_data = []
        current_dt = start_dt

        while current_dt <= end_dt:
            daily_data.append({
                'æ—¥æœŸ': current_dt.strftime('%Y-%m-%d'),
                'å…¥é‡‘': random.uniform(1200000, 1800000),
                'å‡ºé‡‘': random.uniform(800000, 1200000),
                'æ³¨å†Œäººæ•°': random.randint(40, 80),
                'æŒä»“äººæ•°': random.randint(800, 1200)
            })
            current_dt += timedelta(days=1)

        return daily_data

    async def _get_product_data(self) -> Dict[str, Any]:
        """è·å–äº§å“æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'äº§å“æ€»æ•°': 45,
            'æ´»è·ƒäº§å“': 38,
            'å³å°†åˆ°æœŸäº§å“': 12
        }

    async def _ai_analyze_historical_data(self, historical_data: Dict[str, Any],
                                          query_type: HistoricalQueryType,
                                          analysis_depth: AnalysisDepth,
                                          user_query: str) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„å†å²æ•°æ®åˆ†æ"""
        try:
            logger.info(f"ğŸ”¬ AIåˆ†æå†å²æ•°æ®: {query_type.value}")

            # Step 1: GPT-4oè¿›è¡Œæ•°å€¼è®¡ç®—å’Œç»Ÿè®¡åˆ†æ
            statistical_analysis = await self._gpt_statistical_analysis(historical_data, query_type)

            # Step 2: Claudeè¿›è¡Œæ¨¡å¼è¯†åˆ«å’Œè¶‹åŠ¿åˆ†æ
            pattern_analysis = await self._claude_pattern_analysis(historical_data, statistical_analysis, user_query)

            # Step 3: ç»¼åˆåˆ†æç»“æœ
            comprehensive_analysis = {
                'statistical_results': statistical_analysis,
                'pattern_insights': pattern_analysis,
                'analysis_type': query_type.value,
                'analysis_depth': analysis_depth.value,
                'data_quality_score': historical_data.get('data_quality', 0.8)
            }

            return comprehensive_analysis

        except Exception as e:
            logger.error(f"å†å²æ•°æ®åˆ†æå¤±è´¥: {str(e)}")
            return {'error': str(e), 'analysis_type': query_type.value}

    async def _gpt_statistical_analysis(self, historical_data: Dict[str, Any],
                                        query_type: HistoricalQueryType) -> Dict[str, Any]:
        """GPT-4oè¿›è¡Œç»Ÿè®¡åˆ†æ"""
        try:
            daily_data = historical_data.get('data_sources', {}).get('daily', [])

            prompt = f"""
            å¯¹ä»¥ä¸‹å†å²æ•°æ®è¿›è¡Œç²¾ç¡®çš„ç»Ÿè®¡åˆ†æï¼š

            æŸ¥è¯¢ç±»å‹: {query_type.value}
            æ•°æ®æ ·æœ¬: {json.dumps(daily_data[:5], ensure_ascii=False)}
            æ€»æ•°æ®ç‚¹: {len(daily_data)}

            è¯·è®¡ç®—ä»¥ä¸‹ç»Ÿè®¡æŒ‡æ ‡ï¼š
            1. åŸºç¡€ç»Ÿè®¡é‡ï¼ˆå¹³å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ï¼‰
            2. è¶‹åŠ¿åˆ†æï¼ˆå¢é•¿ç‡ã€è¶‹åŠ¿æ–¹å‘ï¼‰
            3. æ³¢åŠ¨æ€§åˆ†æï¼ˆå˜å¼‚ç³»æ•°ã€æ³¢åŠ¨å¹…åº¦ï¼‰
            4. ç›¸å…³æ€§åˆ†æï¼ˆå„æŒ‡æ ‡é—´çš„å…³ç³»ï¼‰

            è¿”å›JSONæ ¼å¼çš„è¯¦ç»†ç»Ÿè®¡ç»“æœï¼Œç¡®ä¿æ•°å€¼ç²¾ç¡®ã€‚
            """

            query_hash = self._get_query_hash(f"stats_{query_type.value}_{len(daily_data)}")
            result = await self._cached_ai_analysis(query_hash, prompt, "gpt")

            try:
                return json.loads(result.get('response', '{}'))
            except:
                # åŸºç¡€ç»Ÿè®¡è®¡ç®—é™çº§
                return self._basic_statistical_calculation(daily_data)

        except Exception as e:
            logger.error(f"GPTç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}")
            return {'error': str(e)}

    def _basic_statistical_calculation(self, daily_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸºç¡€ç»Ÿè®¡è®¡ç®—é™çº§æ–¹æ¡ˆ"""
        if not daily_data:
            return {'error': 'æ— æ•°æ®'}

        try:
            # æå–æ•°å€¼æ•°æ®
            inflow_values = [float(d.get('å…¥é‡‘', 0)) for d in daily_data]
            outflow_values = [float(d.get('å‡ºé‡‘', 0)) for d in daily_data]

            return {
                'inflow_stats': {
                    'mean': sum(inflow_values) / len(inflow_values) if inflow_values else 0,
                    'trend': 'increasing' if inflow_values[-1] > inflow_values[0] else 'decreasing'
                },
                'outflow_stats': {
                    'mean': sum(outflow_values) / len(outflow_values) if outflow_values else 0,
                    'trend': 'increasing' if outflow_values[-1] > outflow_values[0] else 'decreasing'
                },
                'net_flow': {
                    'daily_average': (sum(inflow_values) - sum(outflow_values)) / len(daily_data) if daily_data else 0
                }
            }
        except Exception as e:
            return {'error': f'åŸºç¡€è®¡ç®—å¤±è´¥: {str(e)}'}

    async def _claude_pattern_analysis(self, historical_data: Dict[str, Any],
                                       statistical_analysis: Dict[str, Any],
                                       user_query: str) -> Dict[str, Any]:
        """Claudeè¿›è¡Œæ¨¡å¼åˆ†æ"""
        try:
            prompt = f"""
            ä½œä¸ºèµ„æ·±é‡‘èæ•°æ®åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å†å²æ•°æ®çš„æ·±å±‚æ¨¡å¼å’Œä¸šåŠ¡å«ä¹‰ï¼š

            ç”¨æˆ·æŸ¥è¯¢: "{user_query}"

            ç»Ÿè®¡åˆ†æç»“æœ:
            {json.dumps(statistical_analysis, ensure_ascii=False, indent=2)}

            æ•°æ®æ¦‚è§ˆ:
            {json.dumps(historical_data.get('time_range', {}), ensure_ascii=False)}

            è¯·ä»ä»¥ä¸‹è§’åº¦æ·±åº¦åˆ†æï¼š
            1. è¶‹åŠ¿æ¨¡å¼è¯†åˆ«ï¼ˆçº¿æ€§ã€æŒ‡æ•°ã€å‘¨æœŸæ€§ç­‰ï¼‰
            2. ä¸šåŠ¡å‘¨æœŸåˆ†æï¼ˆæ˜¯å¦å­˜åœ¨å‘¨æœŸæ€§æ¨¡å¼ï¼‰
            3. å¼‚å¸¸æ¨¡å¼æ£€æµ‹ï¼ˆçªå‘å˜åŒ–ã€å¼‚å¸¸ç‚¹ï¼‰
            4. ç›¸å…³æ€§æ¨¡å¼ï¼ˆä¸åŒæŒ‡æ ‡é—´çš„å…³è”ï¼‰
            5. é¢„è­¦ä¿¡å·è¯†åˆ«ï¼ˆæ½œåœ¨é£é™©ä¿¡å·ï¼‰

            è¿”å›JSONæ ¼å¼çš„æ¨¡å¼åˆ†æç»“æœï¼š
            {{
                "trend_patterns": {{
                    "primary_trend": "è¶‹åŠ¿æè¿°",
                    "trend_strength": 0.0-1.0,
                    "trend_sustainability": "å¯æŒç»­æ€§è¯„ä¼°"
                }},
                "business_cycles": {{
                    "cycle_detected": true/false,
                    "cycle_period": "å‘¨æœŸé•¿åº¦",
                    "cycle_phase": "å½“å‰æ‰€å¤„é˜¶æ®µ"
                }},
                "anomaly_detection": {{
                    "anomalies_found": ["å¼‚å¸¸æè¿°"],
                    "anomaly_impact": "å½±å“è¯„ä¼°"
                }},
                "correlation_patterns": {{
                    "strong_correlations": ["ç›¸å…³æ€§æè¿°"],
                    "correlation_insights": "ç›¸å…³æ€§ä¸šåŠ¡å«ä¹‰"
                }},
                "early_warning_signals": ["é¢„è­¦ä¿¡å·åˆ—è¡¨"],
                "pattern_confidence": 0.0-1.0
            }}
            """

            query_hash = self._get_query_hash(f"pattern_{user_query}")
            result = await self._cached_ai_analysis(query_hash, prompt, "claude")

            try:
                return json.loads(result.get('response', '{}'))
            except:
                return {
                    'trend_patterns': {'primary_trend': 'æ•°æ®è¶‹åŠ¿å¹³ç¨³'},
                    'pattern_confidence': 0.6,
                    'analysis_note': 'Claudeæ¨¡å¼åˆ†æç»“æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ'
                }

        except Exception as e:
            logger.error(f"Claudeæ¨¡å¼åˆ†æå¤±è´¥: {str(e)}")
            return {'error': str(e)}

    async def _ai_generate_business_insights(self, analysis_results: Dict[str, Any],
                                             query_type: HistoricalQueryType,
                                             user_query: str) -> Dict[str, Any]:
        """AIç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ"""
        try:
            prompt = f"""
            åŸºäºå†å²æ•°æ®åˆ†æç»“æœï¼Œç”Ÿæˆæ·±åº¦ä¸šåŠ¡æ´å¯Ÿå’Œå¯æ‰§è¡Œå»ºè®®ï¼š

            ç”¨æˆ·æŸ¥è¯¢: "{user_query}"
            æŸ¥è¯¢ç±»å‹: {query_type.value}

            åˆ†æç»“æœæ‘˜è¦:
            {json.dumps(analysis_results, ensure_ascii=False, indent=2)[:2000]}

            è¯·ç”Ÿæˆä»¥ä¸‹ä¸šåŠ¡æ´å¯Ÿï¼š
            1. å…³é”®ä¸šåŠ¡å‘ç°ï¼ˆ3-5ä¸ªæœ€é‡è¦çš„å‘ç°ï¼‰
            2. ä¸šåŠ¡æ´å¯Ÿè§£è¯»ï¼ˆæ•°æ®èƒŒåçš„ä¸šåŠ¡å«ä¹‰ï¼‰
            3. é£é™©é¢„è­¦è¯†åˆ«ï¼ˆæ½œåœ¨çš„ä¸šåŠ¡é£é™©ï¼‰
            4. æœºä¼šè¯†åˆ«ï¼ˆå‘ç°çš„ä¸šåŠ¡æœºä¼šï¼‰
            5. å¯æ‰§è¡Œå»ºè®®ï¼ˆå…·ä½“çš„è¡ŒåŠ¨æ–¹æ¡ˆï¼‰

            è¿”å›JSONæ ¼å¼ï¼š
            {{
                "key_findings": ["å‘ç°1", "å‘ç°2", ...],
                "business_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", ...],
                "risk_warnings": ["é£é™©1", "é£é™©2", ...],
                "opportunities": ["æœºä¼š1", "æœºä¼š2", ...],
                "recommendations": ["å»ºè®®1", "å»ºè®®2", ...],
                "insight_confidence": 0.0-1.0
            }}
            """

            query_hash = self._get_query_hash(f"insights_{user_query}")
            result = await self._cached_ai_analysis(query_hash, prompt, "claude")

            try:
                return json.loads(result.get('response', '{}'))
            except:
                return {
                    'key_findings': ['å†å²æ•°æ®åˆ†æå®Œæˆ'],
                    'business_insights': ['åŸºäºå†å²æ•°æ®çš„è¶‹åŠ¿åˆ†æ'],
                    'risk_warnings': [],
                    'opportunities': [],
                    'recommendations': ['ç»§ç»­ç›‘æ§å†å²æ•°æ®å˜åŒ–'],
                    'insight_confidence': 0.6
                }

        except Exception as e:
            logger.error(f"ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
            return {'error': str(e)}

    def _build_historical_analysis_response(self, query_type: HistoricalQueryType,
                                            analysis_depth: AnalysisDepth,
                                            analysis_results: Dict[str, Any],
                                            business_insights: Dict[str, Any],
                                            historical_data: Dict[str, Any],
                                            time_params: Dict[str, Any],
                                            processing_time: float) -> HistoricalAnalysisResponse:
        """æ„å»ºå†å²åˆ†æå“åº”"""

        # æå–å…³é”®æŒ‡æ ‡
        statistical_results = analysis_results.get('statistical_results', {})
        key_metrics = {}

        if 'inflow_stats' in statistical_results:
            key_metrics['avg_daily_inflow'] = statistical_results['inflow_stats'].get('mean', 0)
        if 'outflow_stats' in statistical_results:
            key_metrics['avg_daily_outflow'] = statistical_results['outflow_stats'].get('mean', 0)
        if 'net_flow' in statistical_results:
            key_metrics['avg_net_flow'] = statistical_results['net_flow'].get('daily_average', 0)

        # æ„å»ºè¶‹åŠ¿æ‘˜è¦
        pattern_insights = analysis_results.get('pattern_insights', {})
        trend_summary = {
            'primary_trend': pattern_insights.get('trend_patterns', {}).get('primary_trend', 'stable'),
            'trend_strength': pattern_insights.get('trend_patterns', {}).get('trend_strength', 0.5),
            'business_cycle_detected': pattern_insights.get('business_cycles', {}).get('cycle_detected', False)
        }

        # è®¡ç®—ç½®ä¿¡åº¦
        analysis_confidence = min(
            business_insights.get('insight_confidence', 0.7),
            pattern_insights.get('pattern_confidence', 0.7),
            historical_data.get('data_quality', 0.8)
        )

        return HistoricalAnalysisResponse(
            query_type=query_type,
            analysis_depth=analysis_depth,

            main_findings=business_insights.get('key_findings', []),
            trend_summary=trend_summary,
            key_metrics=key_metrics,

            business_insights=business_insights.get('business_insights', []),
            pattern_discoveries=pattern_insights.get('early_warning_signals', []),
            comparative_analysis={},  # å¯ä»¥æ‰©å±•

            risk_warnings=business_insights.get('risk_warnings', []),
            opportunities=business_insights.get('opportunities', []),
            recommendations=business_insights.get('recommendations', []),

            data_completeness=historical_data.get('completeness', 0.9),
            analysis_confidence=analysis_confidence,
            data_sources_used=list(historical_data.get('data_sources', {}).keys()),

            analysis_period=f"{time_params['start_date']} è‡³ {time_params['end_date']}",
            processing_time=processing_time,
            methodology_notes=[
                f"ä½¿ç”¨{analysis_depth.value}çº§åˆ†ææ·±åº¦",
                f"åˆ†æäº†{time_params.get('time_range_days', 0)}å¤©çš„å†å²æ•°æ®",
                "é‡‡ç”¨AIé©±åŠ¨çš„åŒæ¨¡å‹åä½œåˆ†æ"
            ]
        )

    def _create_error_response(self, user_query: str, error: str) -> HistoricalAnalysisResponse:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return HistoricalAnalysisResponse(
            query_type=HistoricalQueryType.TREND_ANALYSIS,
            analysis_depth=AnalysisDepth.BASIC,

            main_findings=[f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error}"],
            trend_summary={'primary_trend': 'unknown'},
            key_metrics={},

            business_insights=[],
            pattern_discoveries=[],
            comparative_analysis={},

            risk_warnings=[f"åˆ†æé”™è¯¯: {error}"],
            opportunities=[],
            recommendations=["è¯·æ£€æŸ¥æŸ¥è¯¢å‚æ•°å¹¶é‡è¯•"],

            data_completeness=0.0,
            analysis_confidence=0.0,
            data_sources_used=[],

            analysis_period="error_analysis",
            processing_time=0.0,
            methodology_notes=[f"é”™è¯¯ä¿¡æ¯: {error}"]
        )

    def _update_processing_stats(self, query_type: HistoricalQueryType,
                                 processing_time: float, confidence: float):
        """æ›´æ–°å¤„ç†ç»Ÿè®¡"""
        try:
            # æ›´æ–°æŸ¥è¯¢ç±»å‹ç»Ÿè®¡
            type_key = query_type.value
            if type_key not in self.processing_stats['analyses_by_type']:
                self.processing_stats['analyses_by_type'][type_key] = 0
            self.processing_stats['analyses_by_type'][type_key] += 1

            # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
            total = self.processing_stats['total_analyses']
            current_avg_time = self.processing_stats['avg_processing_time']
            new_avg_time = (current_avg_time * (total - 1) + processing_time) / total
            self.processing_stats['avg_processing_time'] = new_avg_time

            # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
            current_avg_conf = self.processing_stats['avg_confidence']
            new_avg_conf = (current_avg_conf * (total - 1) + confidence) / total
            self.processing_stats['avg_confidence'] = new_avg_conf

            # æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡
            total_cache = self.processing_stats['cache_hits'] + self.processing_stats['cache_misses']
            if total_cache > 0:
                self.processing_stats['cache_hit_rate'] = self.processing_stats['cache_hits'] / total_cache

        except Exception as e:
            logger.error(f"ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å¤±è´¥: {str(e)}")

    # ============= å¤–éƒ¨æ¥å£æ–¹æ³• =============

    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.processing_stats.copy()

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        return {
            "status": "healthy",
            "component_name": "HistoricalAnalysisProcessor",
            "ai_clients": {
                "claude_available": self.claude_client is not None,
                "gpt_available": self.gpt_client is not None
            },
            "supported_query_types": [t.value for t in HistoricalQueryType],
            "supported_analysis_depths": [d.value for d in AnalysisDepth],
            "cache_performance": {
                "cache_hit_rate": self.processing_stats['cache_hit_rate'],
                "total_cached_calls": self.processing_stats['cache_hits'] + self.processing_stats['cache_misses']
            },
            "processing_stats": self.get_processing_stats(),
            "timestamp": datetime.now().isoformat()
        }

    async def batch_analyze_periods(self, periods: List[Dict[str, str]]) -> List[HistoricalAnalysisResponse]:
        """æ‰¹é‡åˆ†æå¤šä¸ªæ—¶é—´æ®µ"""
        results = []

        for period in periods:
            query = f"åˆ†æ{period['start_date']}åˆ°{period['end_date']}çš„å†å²è¶‹åŠ¿"
            try:
                result = await self.process_historical_analysis_query(query)
                results.append(result)
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
                results.append(self._create_error_response(query, str(e)))

        return results


# ============= å·¥å‚å‡½æ•° =============

def create_historical_analysis_processor(claude_client=None, gpt_client=None) -> HistoricalAnalysisProcessor:
    """
    åˆ›å»ºå†å²åˆ†æå¤„ç†å™¨å®ä¾‹

    Args:
        claude_client: Claudeå®¢æˆ·ç«¯å®ä¾‹
        gpt_client: GPTå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        HistoricalAnalysisProcessor: å†å²åˆ†æå¤„ç†å™¨å®ä¾‹
    """
    return HistoricalAnalysisProcessor(claude_client, gpt_client)


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""

    # åˆ›å»ºå†å²åˆ†æå¤„ç†å™¨
    processor = create_historical_analysis_processor()

    print("=== å†å²åˆ†æå¤„ç†å™¨æµ‹è¯• ===")

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "è¿‡å»30å¤©çš„å…¥é‡‘è¶‹åŠ¿å¦‚ä½•ï¼Ÿ",
        "å¯¹æ¯”ä¸Šæœˆå’Œæœ¬æœˆçš„ç”¨æˆ·å¢é•¿æƒ…å†µ",
        "5æœˆä»½çš„ä¸šåŠ¡è¡¨ç°æ€»ç»“",
        "æœ€è¿‘3ä¸ªæœˆæœ‰ä»€ä¹ˆå¼‚å¸¸æ•°æ®ï¼Ÿ"
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"\n--- æµ‹è¯•æŸ¥è¯¢ {i} ---")
        print(f"æŸ¥è¯¢: {query}")

        try:
            response = await processor.process_historical_analysis_query(query)

            print(f"æŸ¥è¯¢ç±»å‹: {response.query_type.value}")
            print(f"åˆ†ææ·±åº¦: {response.analysis_depth.value}")
            print(f"åˆ†ææœŸé—´: {response.analysis_period}")
            print(f"å¤„ç†æ—¶é—´: {response.processing_time:.2f}ç§’")
            print(f"åˆ†æç½®ä¿¡åº¦: {response.analysis_confidence:.2f}")

            # æ˜¾ç¤ºä¸»è¦å‘ç°
            if response.main_findings:
                print("ä¸»è¦å‘ç°:")
                for finding in response.main_findings[:3]:
                    print(f"  - {finding}")

        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {str(e)}")

    # å¥åº·æ£€æŸ¥
    health_status = await processor.health_check()
    print(f"\nç³»ç»Ÿå¥åº·çŠ¶æ€: {health_status['status']}")

    # å¤„ç†ç»Ÿè®¡
    stats = processor.get_processing_stats()
    print(f"å¤„ç†ç»Ÿè®¡: æ€»åˆ†æ{stats['total_analyses']}æ¬¡")


if __name__ == "__main__":
    asyncio.run(main())