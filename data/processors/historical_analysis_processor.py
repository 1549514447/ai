# data/processors/historical_analysis_processor.py
"""
ğŸ“ˆ AIé©±åŠ¨çš„å†å²åˆ†æå¤„ç†å™¨ (ä¼˜åŒ–ç‰ˆ)
ä¸“é—¨å¤„ç†å†å²æ•°æ®åˆ†ææŸ¥è¯¢ï¼ŒåŸºäºå·²è·å–çš„æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æã€‚
"""

import logging
import statistics
import time
import traceback
from typing import Dict, Any, List, Optional, Tuple, Union  # Union éœ€è¦å¯¼å…¥
from datetime import datetime, timedelta
import asyncio
from dataclasses import dataclass, field  # å¯¼å…¥ field
from enum import Enum
import json
import hashlib
from functools import lru_cache  # ä¿ç•™ç”¨äºAIåˆ†æç»“æœç¼“å­˜

from core.analyzers.financial_data_analyzer import FinancialDataAnalyzer
# å¯¼å…¥æ‰€éœ€ç±»å‹
from core.analyzers.query_parser import (
    QueryAnalysisResult,
    QueryType as QueryParserQueryType,        # å¯¼å…¥ QueryType å¹¶åˆ«åä¸º QueryParserQueryType
    QueryComplexity as QueryParserComplexity  # å¯¼å…¥ QueryComplexity å¹¶åˆ«åä¸º QueryParserComplexity

)

from core.data_orchestration.smart_data_fetcher import (
    SmartDataFetcher, create_smart_data_fetcher,
    ExecutionResult as FetcherExecutionResult, # <--- ä¿®æ”¹è¿™é‡Œï¼šå¯¼å…¥ ExecutionResult å¹¶åˆ«åä¸º FetcherExecutionResult
    DataQualityLevel as FetcherDataQualityLevel,
    ExecutionStatus as FetcherExecutionStatus # ç¡®ä¿è¿™ä¸ªä¹Ÿå·²æ­£ç¡®å¯¼å…¥å’Œåˆ«å
)
from core.models.claude_client import ClaudeClient, CustomJSONEncoder  # AI å®¢æˆ·ç«¯
from core.models.openai_client import OpenAIClient  # AI å®¢æˆ·ç«¯
from data.connectors.api_connector import APIConnector
from utils.calculators.financial_calculator import FinancialCalculator
from utils.helpers.date_utils import DateUtils, DateParseResult

# è¿™äº›å·¥å…·ç±»å¦‚æœæœ¬å¤„ç†å™¨éœ€è¦ç›´æ¥ä½¿ç”¨ï¼Œåˆ™åº”ç”±Orchestratoræ³¨å…¥
# from utils.helpers.date_utils import DateUtils
# from utils.calculators.financial_calculator import FinancialCalculator
# from core.analyzers.financial_data_analyzer import FinancialDataAnalyzer # é€šå¸¸ä¸ç›´æ¥ä¾èµ–ï¼Œè€Œæ˜¯æ¥æ”¶å…¶åˆ†æç»“æœ

logger = logging.getLogger(__name__)


class HistoricalQueryType(Enum):
    TREND_ANALYSIS = "trend_analysis"
    GROWTH_ANALYSIS = "growth_analysis"
    COMPARISON_ANALYSIS = "comparison_analysis"
    PATTERN_ANALYSIS = "pattern_analysis"
    PERIOD_SUMMARY = "period_summary"
    VOLATILITY_ANALYSIS = "volatility_analysis"
    UNKNOWN_HISTORICAL = "unknown_historical"  # æ–°å¢ç”¨äºæ— æ³•åˆ†ç±»çš„æƒ…å†µ


class AnalysisDepth(Enum):
    BASIC = "basic"
    STANDARD = "standard"
    COMPREHENSIVE = "comprehensive"
    EXPERT = "expert"


@dataclass
class HistoricalAnalysisResponse:
    query_type: HistoricalQueryType
    analysis_depth: AnalysisDepth
    main_findings: List[str] = field(default_factory=list)
    trend_summary: Dict[str, Any] = field(default_factory=dict)
    key_metrics: Dict[str, Any] = field(default_factory=dict)  # å€¼å¯ä»¥æ˜¯æ•°å€¼æˆ–æ ¼å¼åŒ–å­—ç¬¦ä¸²
    business_insights: List[str] = field(default_factory=list)
    pattern_discoveries: List[str] = field(default_factory=list)
    comparative_analysis: Dict[str, Any] = field(default_factory=dict)
    risk_warnings: List[str] = field(default_factory=list)
    opportunities: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    data_completeness: float = 0.0
    analysis_confidence: float = 0.0
    data_sources_used: List[str] = field(default_factory=list)  # æ¥æºäº SmartDataFetcher ç»“æœ
    analysis_period: str = ""
    processing_time: float = 0.0  # æœ¬å¤„ç†å™¨è€—æ—¶
    methodology_notes: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)  # å­˜å‚¨AIè€—æ—¶ç­‰


class HistoricalAnalysisProcessor:
    """
    ğŸ“ˆ AIé©±åŠ¨çš„å†å²åˆ†æå¤„ç†å™¨ (ä¼˜åŒ–ç‰ˆ)
    ä¸“æ³¨äºåŸºäºå·²è·å–çš„çœŸå®å†å²æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æï¼Œæä¾›ä¸“ä¸šçš„è¶‹åŠ¿æ´å¯Ÿå’Œä¸šåŠ¡å»ºè®®ã€‚
    """

    def __init__(self, claude_client: Optional[ClaudeClient] = None,
                 gpt_client: Optional[OpenAIClient] = None):
        self.claude_client = claude_client
        self.gpt_client = gpt_client
        self.analysis_config = self._load_analysis_config()
        # self.historical_patterns = self._load_historical_patterns() # å¯é€‰ï¼Œç”¨äºè¾…åŠ©
        self.processing_stats = self._default_processing_stats()

        # ä¾èµ–æ³¨å…¥çš„ç»„ä»¶ï¼Œç”± Orchestrator è®¾ç½®
        self.api_connector: Optional[APIConnector] = None  # é€šå¸¸ä¸ç›´æ¥ç”¨ï¼Œæ•°æ®æ¥è‡ªFetcher
        self.financial_data_analyzer: Optional[FinancialDataAnalyzer] = None  # å¦‚æœéœ€è¦è°ƒç”¨æ›´åº•å±‚çš„åˆ†æ
        self.financial_calculator: Optional[FinancialCalculator] = None  # å¦‚æœéœ€è¦ç›´æ¥è¿›è¡Œç‰¹å®šè®¡ç®—
        self.date_utils: Optional[DateUtils] = None  # å¦‚æœéœ€è¦æ—¥æœŸå¤„ç†

        logger.info("HistoricalAnalysisProcessor (Optimized) initialized for AI-driven historical analysis")

    def _default_processing_stats(self) -> Dict[str, Any]:
        return {
            'total_analyses': 0, 'analyses_by_type': {},
            'avg_processing_time': 0.0, 'avg_confidence': 0.0,
            'ai_cache_hits': 0, 'ai_cache_misses': 0, 'ai_cache_hit_rate': 0.0
        }

    def _load_analysis_config(self) -> Dict[str, Any]:
        return {
            'default_analysis_period_days': 30,
            'max_analysis_period_days': 365,
            'min_data_points_for_trend': 7,
            'claude_max_retries': 2,  # ä¸Orchestratorçš„é…ç½®ä¿æŒä¸€è‡´æˆ–ç‹¬ç«‹
            'gpt_max_retries': 2,
            'cache_ttl_seconds': 1800,  # 30åˆ†é’Ÿ
            'max_cache_size': 50,  # è°ƒæ•´ç¼“å­˜å¤§å°
            # å¯ä»¥å®šä¹‰ä¸åŒåˆ†ææ·±åº¦æ‰€éœ€çš„æ•°æ®ç‰‡æ®µé”®å (ä¸SmartDataFetcherçº¦å®š)
            'data_keys_for_depth': {
                AnalysisDepth.BASIC: ['daily_summary_short_term'],
                AnalysisDepth.STANDARD: ['daily_summary_medium_term', 'key_product_trends_short_term'],
                AnalysisDepth.COMPREHENSIVE: ['daily_summary_long_term', 'all_product_trends_medium_term',
                                              'user_segment_summary_medium_term'],
                AnalysisDepth.EXPERT: ['all_historical_daily', 'all_historical_product', 'all_historical_user',
                                       'all_historical_expiry']
            }
        }

    def _get_query_hash(self, text_to_hash: str) -> str:
        return hashlib.md5(text_to_hash.encode('utf-8')).hexdigest()

    @lru_cache(maxsize=50)
    async def _cached_ai_analysis(self, query_or_prompt_hash: str, prompt: str,
                                  ai_client_type: str = "claude") -> Dict[str, Any]:
        ai_call_start_time = time.time()
        client_to_use: Union[ClaudeClient, OpenAIClient, None] = None
        model_name_for_log = "Unknown AI"
        response_data: Dict[str, Any] = {"success": False, "error": "AI client not configured or type unsupported."}

        if ai_client_type.lower() == "claude" and self.claude_client:
            client_to_use = self.claude_client
            model_name_for_log = "Claude"
        elif ai_client_type.lower() == "gpt" and self.gpt_client:
            client_to_use = self.gpt_client
            model_name_for_log = "GPT"

        if not client_to_use:
            logger.error(f"AIå®¢æˆ·ç«¯ '{ai_client_type}' ä¸å¯ç”¨ã€‚")
            return response_data

        # ç»Ÿè®¡ç¼“å­˜æœªå‘½ä¸­ (åªæœ‰åœ¨å®é™…è°ƒç”¨AIæ—¶æ‰ç®—æœªå‘½ä¸­)
        self.processing_stats['ai_cache_misses'] = self.processing_stats.get('ai_cache_misses', 0) + 1
        logger.debug(f"AI cache MISS for hash: {query_or_prompt_hash}. Calling {model_name_for_log}.")

        try:
            ai_raw_response = None
            if isinstance(client_to_use, ClaudeClient):
                if hasattr(client_to_use, 'messages') and callable(getattr(client_to_use.messages, 'create', None)):
                    ai_raw_response = await asyncio.to_thread(
                        client_to_use.messages.create,
                        model=getattr(client_to_use, 'model', "claude-3-sonnet-20240229"),
                        max_tokens=2048, messages=[{"role": "user", "content": prompt}]
                    )
                    content_text = ""
                    if hasattr(ai_raw_response, 'content') and ai_raw_response.content:
                        for item in ai_raw_response.content:
                            if hasattr(item, 'text'): content_text += item.text
                    response_data = {"success": True, "response": content_text}
                # Add other Claude call methods if needed, e.g., older SDKs
                else:
                    response_data = {"success": False, "error": "ClaudeClient method not found."}

            elif isinstance(client_to_use, OpenAIClient):
                if hasattr(client_to_use, 'chat') and hasattr(client_to_use.chat, 'completions') and callable(
                        getattr(client_to_use.chat.completions, 'create', None)):
                    ai_raw_response = await asyncio.to_thread(
                        client_to_use.chat.completions.create,
                        model=getattr(client_to_use, 'model', "gpt-4o"),
                        messages=[{"role": "user", "content": prompt}], max_tokens=2048
                    )
                    if ai_raw_response.choices and ai_raw_response.choices[0].message:
                        response_data = {"success": True, "response": ai_raw_response.choices[0].message.content}
                elif hasattr(client_to_use, 'process_direct_query'):  # Your custom method
                    response_data = await client_to_use.process_direct_query(user_query=prompt, data={})
                else:
                    response_data = {"success": False, "error": "OpenAIClient method not found."}

            response_data["model_used"] = model_name_for_log
            response_data["ai_call_duration"] = time.time() - ai_call_start_time
            return response_data

        except Exception as e:
            logger.error(f"AIåˆ†æè°ƒç”¨ ({model_name_for_log}) æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\n{traceback.format_exc()}")
            return {"success": False, "error": str(e), "response": f"AIåˆ†æå‡ºé”™: {str(e)}",
                    "ai_call_duration": time.time() - ai_call_start_time}

    async def process_historical_analysis_query(self, user_query: str,
                                                user_context: Optional[Dict[str, Any]] = None
                                                ) -> HistoricalAnalysisResponse:
        """
        ğŸ¯ å¤„ç†å†å²åˆ†ææŸ¥è¯¢çš„ä¸»å…¥å£ (ä¼˜åŒ–ç‰ˆ)
        æ•°æ®ä» user_context['data_acquisition_result'] è·å–ã€‚
        """
        method_start_time = datetime.now()
        logger.info(f"ğŸ“ˆ å¼€å§‹å†å²åˆ†ææŸ¥è¯¢: '{user_query[:100]}...'")
        self.processing_stats['total_analyses'] = self.processing_stats.get('total_analyses', 0) + 1

        internal_ai_processing_time = 0.0  # ç´¯åŠ æœ¬å¤„ç†å™¨å†…éƒ¨çš„AIè€—æ—¶

        # 1. ä» user_context ä¸­æå– QueryAnalysisResult å’Œ FetcherExecutionResult
        if not user_context or not isinstance(user_context, dict):
            logger.error("HistoricalAnalysisProcessor: user_context æœªæä¾›æˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚")
            return self._create_error_response(user_query, "å†…éƒ¨é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„å¤„ç†ä¸Šä¸‹æ–‡ã€‚")

        query_analysis: Optional[QueryAnalysisResult] = user_context.get('query_analysis')
        fetcher_result: Optional[FetcherExecutionResult] = user_context.get('data_acquisition_result')

        if not query_analysis or not isinstance(query_analysis, QueryAnalysisResult):
            logger.error("HistoricalAnalysisProcessor: 'query_analysis' æœªåœ¨ user_context ä¸­æä¾›æˆ–ç±»å‹ä¸æ­£ç¡®ã€‚")
            return self._create_error_response(user_query, "å†å²åˆ†æå¤±è´¥ï¼šç¼ºå°‘æŸ¥è¯¢è§£æç»“æœã€‚")

        if not fetcher_result or not isinstance(fetcher_result, FetcherExecutionResult):
            logger.error("HistoricalAnalysisProcessor: 'data_acquisition_result' æœªåœ¨ user_context ä¸­æä¾›æˆ–ç±»å‹ä¸æ­£ç¡®ã€‚")
            return self._create_error_response(user_query, "å†å²åˆ†æå¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„æ•°æ®è·å–ç»“æœã€‚")

        try:
            # Step 2: (AIè¯†åˆ«æŸ¥è¯¢ç±»å‹å’Œåˆ†ææ·±åº¦) - ç°åœ¨ä» query_analysis ä¸­è·å–ï¼Œæˆ–ç”±æœ¬å¤„ç†å™¨ç»†åŒ–
            # å‡è®¾ query_analysis.query_type å¯ä»¥æ˜ å°„åˆ° HistoricalQueryType
            # å¹¶ä¸” query_analysis.analysis_depth (å¦‚æœå­˜åœ¨) æˆ–åå¥½å¯ä»¥æ˜ å°„åˆ° AnalysisDepth
            # ä¸ºç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾Orchestratorå·²åšå¥½åˆæ­¥åˆ†ç±»ï¼Œè¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥ç»†åŒ–æˆ–ç›´æ¥ä½¿ç”¨

            # å‡è®¾è°ƒç”¨AIè¿›è¡Œç»†åŒ–åˆ†ç±»ï¼Œå¦‚æœ Orchestrator çš„åˆ†ç±»ä¸å¤Ÿå…·ä½“
            ai_call_start = time.time()
            determined_query_type, determined_analysis_depth = await self._determine_historical_query_details(
                user_query, query_analysis)
            internal_ai_processing_time += (time.time() - ai_call_start)
            logger.info(f"å†å²æŸ¥è¯¢ç»†åŒ–ç±»å‹: {determined_query_type.value}, åˆ†ææ·±åº¦: {determined_analysis_depth.value}")

            # Step 3: (AIæå–æ—¶é—´èŒƒå›´å’Œåˆ†æå‚æ•°) - ç°åœ¨ä» query_analysis.time_requirements è·å–
            # æˆ–è°ƒç”¨AI/DateUtilsè¿›è¡Œæ›´ç²¾ç¡®çš„æå–ï¼ˆå¦‚æœéœ€è¦ï¼‰
            ai_call_start = time.time()
            time_params = await self._extract_time_params_from_query_analysis(user_query, query_analysis)
            internal_ai_processing_time += (time.time() - ai_call_start)
            logger.info(f"æå–å¹¶ç¡®è®¤æ—¶é—´å‚æ•°: ä» {time_params.get('start_date')} åˆ° {time_params.get('end_date')}")

            # Step 4: ä»ä¼ å…¥çš„ fetcher_result å‡†å¤‡å†å²æ•°æ®
            prep_data_start = time.time()
            historical_data_payload = self._prepare_historical_data_from_context(
                time_params, determined_analysis_depth, fetcher_result
            )
            logger.info(
                f"å†å²æ•°æ®å‡†å¤‡å®Œæˆ ({time.time() - prep_data_start:.2f}s). æ•°æ®è´¨é‡è¯„ä¼°: {historical_data_payload.get('data_quality_assessment', {}).get('overall_quality', 'N/A')}")

            if historical_data_payload.get('error'):
                logger.error(f"å†å²æ•°æ®å‡†å¤‡å¤±è´¥: {historical_data_payload.get('error')}")
                return self._create_error_response(user_query, f"æ•°æ®å‡†å¤‡é”™è¯¯: {historical_data_payload.get('error')}")

            # Step 5: AIé©±åŠ¨çš„å†å²æ•°æ®åˆ†æ (æ ¸å¿ƒåˆ†æé€»è¾‘)
            analysis_main_start = time.time()
            analysis_results_dict = await self._ai_analyze_historical_data(
                historical_data_payload, determined_query_type, determined_analysis_depth, user_query
            )
            internal_ai_processing_time += analysis_results_dict.get('metadata', {}).get(
                'ai_processing_time_for_analysis_step', 0.0)  # ä»å­æ–¹æ³•è·å–AIè€—æ—¶
            logger.info(f"AIå†å²æ•°æ®åˆ†æå®Œæˆ ({time.time() - analysis_main_start:.2f}s).")

            # Step 6: AIç”Ÿæˆæ·±åº¦ä¸šåŠ¡æ´å¯Ÿ
            insights_gen_start = time.time()
            business_insights_dict = await self._ai_generate_business_insights(
                analysis_results_dict, determined_query_type, user_query
            )
            internal_ai_processing_time += business_insights_dict.get('metadata', {}).get(
                'ai_processing_time_for_insight_step', 0.0)
            logger.info(f"AIä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå®Œæˆ ({time.time() - insights_gen_start:.2f}s).")

            # Step 7: æ„å»ºæœ€ç»ˆå“åº”
            processing_time_wall = (datetime.now() - method_start_time).total_seconds()
            response = self._build_historical_analysis_response(
                determined_query_type, determined_analysis_depth,
                analysis_results_dict, business_insights_dict,
                historical_data_payload, time_params, processing_time_wall
            )
            response.metadata['ai_processing_time'] = internal_ai_processing_time  # è®°å½•æœ¬å¤„ç†å™¨å†…éƒ¨æ€»AIè€—æ—¶

            self._update_processing_stats(determined_query_type, processing_time_wall, response.analysis_confidence)
            logger.info(
                f"âœ… å†å²åˆ†ææˆåŠŸå®Œæˆï¼Œæ€»è€—æ—¶{processing_time_wall:.2f}s, å†…éƒ¨AIè€—æ—¶: {internal_ai_processing_time:.2f}s")
            return response

        except Exception as e:
            processing_time_wall_on_error = (datetime.now() - method_start_time).total_seconds()
            logger.error(f"âŒ å†å²åˆ†æå¤„ç†ä¸»æµç¨‹å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            err_resp = self._create_error_response(user_query, f"å†å²åˆ†æå†…éƒ¨é”™è¯¯: {str(e)}")
            err_resp.processing_time = processing_time_wall_on_error  # è®°å½•å®é™…è€—æ—¶
            err_resp.metadata['ai_processing_time'] = internal_ai_processing_time  # è®°å½•å‡ºé”™å‰çš„AIè€—æ—¶
            return err_resp

    async def _determine_historical_query_details(self, user_query: str, query_analysis: QueryAnalysisResult) -> Tuple[
        HistoricalQueryType, AnalysisDepth]:
        """
        æ ¹æ® QueryAnalysisResult æˆ–é€šè¿‡ AI è¿›ä¸€æ­¥ç»†åŒ–å†å²æŸ¥è¯¢çš„ç±»å‹å’Œåˆ†ææ·±åº¦ã€‚
        """
        # ä¼˜å…ˆä½¿ç”¨ QueryAnalysisResult ä¸­çš„ä¿¡æ¯è¿›è¡Œæ˜ å°„
        parser_qt: QueryParserQueryType = query_analysis.query_type
        parser_qc: QueryParserComplexity = query_analysis.complexity

        # æ˜ç¡® QueryParserQueryType åˆ° HistoricalQueryType çš„æ˜ å°„è§„åˆ™
        qt_map: Dict[QueryParserQueryType, HistoricalQueryType] = {
            QueryParserQueryType.TREND_ANALYSIS: HistoricalQueryType.TREND_ANALYSIS,
            QueryParserQueryType.GROWTH_ANALYSIS: HistoricalQueryType.GROWTH_ANALYSIS,  # å‡è®¾ QueryParser æœ‰æ­¤ç±»å‹
            QueryParserQueryType.COMPARISON: HistoricalQueryType.COMPARISON_ANALYSIS,
            QueryParserQueryType.PREDICTION: HistoricalQueryType.TREND_ANALYSIS,  # é¢„æµ‹é€šå¸¸åŸºäºè¶‹åŠ¿
            QueryParserQueryType.SCENARIO_SIMULATION: HistoricalQueryType.PATTERN_ANALYSIS,  # åœºæ™¯åˆ†æå¯èƒ½æ¶‰åŠæ¨¡å¼è¯†åˆ«
            QueryParserQueryType.RISK_ASSESSMENT: HistoricalQueryType.VOLATILITY_ANALYSIS,  # é£é™©è¯„ä¼°å¯èƒ½å…³æ³¨æ³¢åŠ¨æ€§
            QueryParserQueryType.DATA_RETRIEVAL: HistoricalQueryType.PERIOD_SUMMARY,  # è·å–å†å²æ•°æ®å¯èƒ½è§†ä¸ºæœŸé—´æ€»ç»“
            QueryParserQueryType.CALCULATION: HistoricalQueryType.PERIOD_SUMMARY,  # å†å²è®¡ç®—ä¹Ÿå¯èƒ½æ˜¯ä¸€ç§æ€»ç»“
            QueryParserQueryType.DEFINITION_EXPLANATION: HistoricalQueryType.UNKNOWN_HISTORICAL,  # å†å²åˆ†æä¸å¤„ç†å®šä¹‰
            QueryParserQueryType.OPTIMIZATION: HistoricalQueryType.UNKNOWN_HISTORICAL,  # å†å²åˆ†æä¸ç›´æ¥åšä¼˜åŒ–å»ºè®®
            QueryParserQueryType.GENERAL_KNOWLEDGE: HistoricalQueryType.UNKNOWN_HISTORICAL,  # é€šç”¨çŸ¥è¯†é€šå¸¸ä¸æ˜¯å†å²åˆ†æèŒƒç•´
            QueryParserQueryType.SYSTEM_COMMAND: HistoricalQueryType.UNKNOWN_HISTORICAL,
            QueryParserQueryType.UNKNOWN: HistoricalQueryType.UNKNOWN_HISTORICAL,
        }
        # å¦‚æœ QueryParserQueryType ä¸­æœ‰æ‚¨åœ¨ Orchestrator ä¸­å‡è®¾çš„ GENERAL_KNOWLEDGEï¼Œä¹Ÿéœ€è¦åœ¨è¿™é‡Œæ˜ å°„
        if hasattr(QueryParserQueryType, 'GENERAL_KNOWLEDGE'):
            qt_map[QueryParserQueryType.GENERAL_KNOWLEDGE] = HistoricalQueryType.UNKNOWN_HISTORICAL

        determined_query_type = qt_map.get(parser_qt, HistoricalQueryType.TREND_ANALYSIS)  # é»˜è®¤è¶‹åŠ¿åˆ†æ

        # æ˜ç¡® QueryParserComplexity åˆ° AnalysisDepth çš„æ˜ å°„è§„åˆ™
        depth_map: Dict[QueryParserComplexity, AnalysisDepth] = {
            QueryParserComplexity.SIMPLE: AnalysisDepth.BASIC,
            QueryParserComplexity.MEDIUM: AnalysisDepth.STANDARD,
            QueryParserComplexity.COMPLEX: AnalysisDepth.COMPREHENSIVE,
            QueryParserComplexity.EXPERT: AnalysisDepth.EXPERT,
        }
        determined_analysis_depth = depth_map.get(parser_qc, AnalysisDepth.STANDARD)  # é»˜è®¤æ ‡å‡†æ·±åº¦

        logger.debug(
            f"åˆæ­¥ç¡®å®šç±»å‹: {determined_query_type.value}, æ·±åº¦: {determined_analysis_depth.value} (åŸºäºQueryParserç»“æœ)")

        # (åç»­çš„AIç»†åŒ–é€»è¾‘ä¿æŒä¸å˜ï¼Œä½†åº”ä½¿ç”¨ä¸Šé¢ç¡®å®šçš„ determined_query_type å’Œ determined_analysis_depth ä½œä¸ºAIçš„è¾“å…¥æˆ–å‚è€ƒ)
        # ...
        if self.claude_client:  # AI ç»†åŒ–é€»è¾‘
            # ... (æ„å»ºä¸­æ–‡æç¤ºè¯ï¼Œè®©AIä»HistoricalQueryTypeå’ŒAnalysisDepthçš„é€‰é¡¹ä¸­é€‰æ‹©)
            # æç¤ºè¯åº”åŒ…å« HistoricalQueryType å’Œ AnalysisDepth çš„æ‰€æœ‰æšä¸¾å€¼ä½œä¸ºé€‰é¡¹
            historical_type_options = [ht.value for ht in HistoricalQueryType if
                                       ht != HistoricalQueryType.UNKNOWN_HISTORICAL]
            analysis_depth_options = [ad.value for ad in AnalysisDepth]

            prompt = f"""
            è¯·åŸºäºç”¨æˆ·åŸå§‹æŸ¥è¯¢å’Œç³»ç»Ÿåˆæ­¥çš„æŸ¥è¯¢åˆ†æç»“æœï¼Œè¿›ä¸€æ­¥ç²¾ç¡®åˆ¤æ–­å†å²æ•°æ®åˆ†æçš„å…·ä½“â€œæŸ¥è¯¢å­ç±»å‹â€å’Œâ€œåˆ†ææ·±åº¦â€ã€‚

            ç”¨æˆ·åŸå§‹æŸ¥è¯¢: "{user_query}"
            ç³»ç»Ÿåˆæ­¥åˆ†æ:
            - ä¸»è¦æ„å›¾ç±»å‹: {query_analysis.query_type.value}
            - å¤æ‚åº¦è¯„ä¼°: {query_analysis.complexity.value}
            - åˆæ­¥å»ºè®®çš„å†å²æŸ¥è¯¢å­ç±»å‹: {determined_query_type.value}
            - åˆæ­¥å»ºè®®çš„åˆ†ææ·±åº¦: {determined_analysis_depth.value}

            â€œå†å²æŸ¥è¯¢å­ç±»å‹â€é€‰é¡¹ (è¯·ä¸¥æ ¼ä»æ­¤åˆ—è¡¨é€‰æ‹©æœ€ç›¸å…³çš„ä¸€ä¸ªï¼Œä»…è¿”å›å…¶è‹±æ–‡ID):
            {json.dumps(historical_type_options, ensure_ascii=False)}

            â€œåˆ†ææ·±åº¦â€é€‰é¡¹ (è¯·ä¸¥æ ¼ä»æ­¤åˆ—è¡¨é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªï¼Œä»…è¿”å›å…¶è‹±æ–‡ID):
            {json.dumps(analysis_depth_options, ensure_ascii=False)}

            è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å« "query_type" (å†å²æŸ¥è¯¢å­ç±»å‹) å’Œ "analysis_depth" ä¸¤ä¸ªé”®ã€‚
            ä¾‹å¦‚: {{"query_type": "growth_analysis", "analysis_depth": "comprehensive"}}
            å¦‚æœåˆæ­¥å»ºè®®å·²ç»å¾ˆå‡†ç¡®ï¼Œå¯ä»¥ç›´æ¥é‡‡çº³ã€‚å¦‚æœéœ€è¦è°ƒæ•´ï¼Œè¯·ç»™å‡ºè°ƒæ•´åçš„é€‰æ‹©ã€‚
            """
            query_hash = self._get_query_hash(
                f"hist_subtype_depth_refined_{user_query}_{query_analysis.query_type.value}")
            ai_result = await self._cached_ai_analysis(query_hash, prompt, ai_client_type="claude")

            if ai_result.get('success'):
                try:
                    analysis = json.loads(ai_result.get('response', '{}'))
                    qt_str = analysis.get('query_type')
                    ad_str = analysis.get('analysis_depth')
                    if qt_str: determined_query_type = HistoricalQueryType(qt_str)  # ä½¿ç”¨AIçš„åˆ¤æ–­è¦†ç›–
                    if ad_str: determined_analysis_depth = AnalysisDepth(ad_str)  # ä½¿ç”¨AIçš„åˆ¤æ–­è¦†ç›–
                    logger.info(
                        f"AIç»†åŒ–åçš„ç±»å‹: {determined_query_type.value}, æ·±åº¦: {determined_analysis_depth.value}")
                except (json.JSONDecodeError, ValueError) as e:
                    logger.warning(
                        f"AIç»†åŒ–å†å²æŸ¥è¯¢ç±»å‹å¤±è´¥(è§£æ/ValueError): {e}. AIåŸå§‹è¿”å›: {ai_result.get('response', '')[:200]}. ä¿æŒè§„åˆ™åˆ¤æ–­ç»“æœã€‚")
            else:
                logger.warning(f"AIç»†åŒ–è°ƒç”¨å¤±è´¥: {ai_result.get('error')}. ä¿æŒè§„åˆ™åˆ¤æ–­ç»“æœã€‚")

        return determined_query_type, determined_analysis_depth

        # data/processors/historical_analysis_processor.py

    async def _extract_time_params_from_query_analysis(self, user_query: str,
                                                       query_analysis: QueryAnalysisResult) -> Dict[str, Any]:
        logger.debug(f"Extracting time parameters from QueryAnalysisResult for query: '{user_query[:50]}...'")

        time_req = query_analysis.time_requirements if query_analysis else {}  # query_analysis å¯èƒ½ä¸º None

        final_time_params: Dict[str, Any] = {
            'has_explicit_time': False,
            'start_date': None, 'end_date': None, 'time_range_days': None,
            'relative_time': None, 'analysis_granularity': 'daily'
        }

        # ä¼˜å…ˆä½¿ç”¨ QueryParser è§£æå‡ºçš„æ—¶é—´ä¿¡æ¯
        if time_req and isinstance(time_req, dict):
            if time_req.get('start_date') and time_req.get('end_date'):
                final_time_params['start_date'] = time_req['start_date']
                final_time_params['end_date'] = time_req['end_date']
                final_time_params['has_explicit_time'] = True
                if time_req.get('time_range_days'):
                    try:
                        final_time_params['time_range_days'] = int(time_req['time_range_days'])
                    except:
                        pass
                else:
                    try:
                        sd = datetime.strptime(time_req['start_date'], '%Y-%m-%d')
                        ed = datetime.strptime(time_req['end_date'], '%Y-%m-%d')
                        final_time_params['time_range_days'] = (ed - sd).days + 1
                    except Exception as e_calc_days:
                        logger.warning(f"Could not calculate time_range_days from QueryParser dates: {e_calc_days}")

            if time_req.get('relative_period'):
                final_time_params['relative_time'] = time_req['relative_period']
                final_time_params['has_explicit_time'] = True
            if time_req.get('granularity'):
                final_time_params['analysis_granularity'] = time_req['granularity']

        # å¦‚æœ QueryParser æœªèƒ½æä¾›å®Œæ•´çš„æ—¶é—´èŒƒå›´ï¼Œåˆ™è°ƒç”¨ DateUtils è¿›è¡ŒäºŒæ¬¡è§£æ
        if not final_time_params.get('start_date') or not final_time_params.get('end_date'):
            logger.info(
                f"QueryParser did not yield full time range. Attempting DateUtils.parse_dates_from_query for: '{user_query[:50]}...'")
            if not self.date_utils:
                logger.error(
                    "DateUtils not initialized in HistoricalAnalysisProcessor. Cannot perform detailed date parsing.")
            else:
                try:
                    # ++++++++++++++ ä¿®æ”¹æ­¤å¤„è°ƒç”¨ ++++++++++++++
                    date_util_parse_result: DateParseResult = await self.date_utils.parse_dates_from_query(
                        query=user_query)
                    # +++++++++++++++++++++++++++++++++++++++++++

                    if date_util_parse_result and date_util_parse_result.ranges:
                        # DateParseResult.ranges æ˜¯ List[DateRange]
                        # æˆ‘ä»¬å–ç¬¬ä¸€ä¸ªæœ€ç›¸å…³çš„èŒƒå›´ï¼ˆæˆ–è€…éœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥é€‰æ‹©ï¼‰
                        # å‡è®¾ DateUtils çš„ AI è§£æä¼šæŠŠæœ€ä¸»è¦çš„èŒƒå›´æ”¾åœ¨å‰é¢æˆ–åªè¿”å›ä¸€ä¸ªä¸»è¦èŒƒå›´
                        main_range = date_util_parse_result.ranges[0]
                        if main_range.start_date and main_range.end_date:
                            final_time_params['start_date'] = main_range.start_date
                            final_time_params['end_date'] = main_range.end_date
                            final_time_params['has_explicit_time'] = True  # æ ‡è®°ä¸ºå·²è§£æå‡º

                            # ä» DateRange å¯¹è±¡è®¡ç®— time_range_days
                            try:
                                sd_du = datetime.strptime(main_range.start_date, '%Y-%m-%d')
                                ed_du = datetime.strptime(main_range.end_date, '%Y-%m-%d')
                                final_time_params['time_range_days'] = (ed_du - sd_du).days + 1
                            except Exception as e_calc_days_du:
                                logger.warning(
                                    f"Could not calculate time_range_days from DateUtils range: {e_calc_days_du}")
                                # å¦‚æœ QueryParser ä¹‹å‰æœ‰å¤©æ•°ï¼Œå°è¯•ä½¿ç”¨
                                if time_req and time_req.get('time_range_days'):
                                    final_time_params['time_range_days'] = int(time_req['time_range_days'])

                            # å°è¯•è·å–åŸå§‹è¡¨è¿°å’Œç²’åº¦ (å¦‚æœ DateUtils èƒ½æä¾›)
                            # DateParseResult.ranges[DateRange] æ²¡æœ‰ original_expression å’Œ granularity å­—æ®µ
                            # ä½† DateParseResult.relative_terms å¯èƒ½åŒ…å«ä¸€äº›ä¿¡æ¯
                            if date_util_parse_result.relative_terms:
                                final_time_params['relative_time'] = ", ".join(
                                    date_util_parse_result.relative_terms)
                            # ç²’åº¦å¯èƒ½éœ€è¦ DateUtils æ›´æ˜ç¡®åœ°è¿”å›ï¼Œæˆ–åœ¨è¿™é‡ŒåŸºäºèŒƒå›´é•¿åº¦æ¨æ–­
                            # final_time_params['analysis_granularity'] = ...

                            logger.info(
                                f"DateUtils successfully parsed time range: {final_time_params['start_date']} to {final_time_params['end_date']}")
                        else:
                            logger.warning(
                                f"DateUtils.parse_dates_from_query returned a range with missing start/end dates: {main_range}")
                    elif date_util_parse_result and date_util_parse_result.dates:  # å¦‚æœæ²¡æœ‰èŒƒå›´ä½†æœ‰å…·ä½“æ—¥æœŸ
                        # å¦‚æœåªæœ‰ä¸€ä¸ªæ—¥æœŸï¼Œåˆ™å¼€å§‹å’Œç»“æŸæ˜¯åŒä¸€å¤©
                        if len(date_util_parse_result.dates) == 1:
                            final_time_params['start_date'] = date_util_parse_result.dates[0]
                            final_time_params['end_date'] = date_util_parse_result.dates[0]
                            final_time_params['time_range_days'] = 1
                            final_time_params['has_explicit_time'] = True
                        # å¦‚æœæœ‰å¤šä¸ªæ—¥æœŸï¼Œå¯ä»¥å–æœ€æ—©å’Œæœ€æ™šçš„æ„æˆèŒƒå›´ (éœ€è¦æ’åº)
                        elif len(date_util_parse_result.dates) > 1:
                            try:
                                sorted_dates = sorted(
                                    [datetime.strptime(d, '%Y-%m-%d') for d in date_util_parse_result.dates])
                                final_time_params['start_date'] = sorted_dates[0].strftime('%Y-%m-%d')
                                final_time_params['end_date'] = sorted_dates[-1].strftime('%Y-%m-%d')
                                final_time_params['time_range_days'] = (sorted_dates[-1] - sorted_dates[0]).days + 1
                                final_time_params['has_explicit_time'] = True
                            except Exception as e_sort_dates:
                                logger.warning(f"Error processing specific dates from DateUtils: {e_sort_dates}")
                    else:
                        logger.warning(
                            f"DateUtils.parse_dates_from_query did not return usable date ranges or specific dates. Result: {date_util_parse_result}")
                except Exception as e_du_call:
                    logger.error(f"Error calling DateUtils.parse_dates_from_query: {e_du_call}")

        # æœ€ç»ˆçš„é»˜è®¤å€¼è®¾ç½®ï¼ˆå¦‚æœæ‰€æœ‰è§£æéƒ½å¤±è´¥ï¼Œæˆ–è€…æ²¡æœ‰æ˜¾å¼æ—¶é—´ï¼‰
        if not final_time_params.get('start_date') or not final_time_params.get('end_date'):
            default_days = self.analysis_config.get('default_analysis_period_days', 30)
            end_dt = datetime.now()  # ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºç»“æŸ
            start_dt = end_dt - timedelta(days=default_days - 1)
            final_time_params.update({
                'start_date': start_dt.strftime('%Y-%m-%d'),
                'end_date': end_dt.strftime('%Y-%m-%d'),
                'time_range_days': default_days,
                'has_explicit_time': False,  # æ ‡è®°ä¸ºé»˜è®¤
                'relative_time': final_time_params.get('relative_time') or f"æœ€è¿‘{default_days}å¤© (ç³»ç»Ÿé»˜è®¤)"
            })
            logger.info(
                f"Using default time range: {final_time_params['start_date']} to {final_time_params['end_date']}")

        # å†æ¬¡ç¡®ä¿ time_range_days ä¸ start/end_date ä¸€è‡´
        if final_time_params.get('start_date') and final_time_params.get('end_date'):
            if not final_time_params.get('time_range_days') or not final_time_params.get(
                    'has_explicit_time'):  # å¦‚æœå¤©æ•°æœªè®¾ç½®æˆ–ä¹‹å‰æ˜¯é»˜è®¤
                try:
                    sd = datetime.strptime(final_time_params['start_date'], '%Y-%m-%d')
                    ed = datetime.strptime(final_time_params['end_date'], '%Y-%m-%d')
                    calculated_days = (ed - sd).days + 1
                    if calculated_days > 0:  # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„å¤©æ•°
                        final_time_params['time_range_days'] = calculated_days
                except Exception as e_recalc_days:
                    logger.warning(f"Final attempt to calculate time_range_days failed: {e_recalc_days}")

        logger.debug(f"Final time parameters for historical analysis: {final_time_params}")
        return final_time_params

    def _prepare_historical_data_from_context(self,
                                              time_params: Dict[str, Any],
                                              analysis_depth: AnalysisDepth,
                                              fetcher_result: FetcherExecutionResult
                                              ) -> Dict[str, Any]:
        """
        ä»å·²è·å–çš„ FetcherExecutionResult ä¸­æå–å’Œç»„ç»‡å†å²åˆ†ææ‰€éœ€çš„æ•°æ®ã€‚
        ä¸å†ç›´æ¥è°ƒç”¨APIã€‚
        """
        logger.info(
            f"Preparing historical data from FetcherExecutionResult. Time range: {time_params.get('start_date')} to {time_params.get('end_date')}")

        historical_data_payload: Dict[str, Any] = {
            'time_range': time_params,
            'data_sources': {},  # å°†ä» fetcher_result.processed_data ä¸­å¡«å……
            'data_quality_assessment': {  # å­˜å‚¨æ•°æ®è´¨é‡çš„è¯¦ç»†è¯„ä¼°
                'overall_quality': getattr(fetcher_result, 'data_quality', FetcherDataQualityLevel.POOR).value,
                'completeness': getattr(fetcher_result, 'data_completeness', 0.0),
                'accuracy': getattr(fetcher_result, 'accuracy_score', 0.0),
                'freshness': getattr(fetcher_result, 'freshness_score', 0.0),
                'fetcher_confidence': getattr(fetcher_result, 'confidence_level', 0.0),
            },
            'error': None  # åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
        }

        if not fetcher_result or not fetcher_result.processed_data:
            error_msg = "FetcherExecutionResult is missing or does not contain processed_data."
            logger.error(error_msg)
            historical_data_payload['error'] = error_msg
            historical_data_payload['data_quality_assessment'][
                'overall_quality'] = FetcherDataQualityLevel.INSUFFICIENT.value
            return historical_data_payload

        # æ ¹æ®åˆ†ææ·±åº¦ï¼Œä» fetcher_result.processed_data ä¸­æå–æ•°æ®
        # 'data_keys_for_depth' ä¸­çš„é”®ååº”ä¸ SmartDataFetcher åœ¨ processed_data ä¸­ä½¿ç”¨çš„é”®åä¸€è‡´
        required_data_keys = self.analysis_config.get('data_keys_for_depth', {}).get(analysis_depth, [])

        logger.debug(
            f"Analysis depth '{analysis_depth.value}' requires data keys: {required_data_keys} from processed_data.")

        found_any_data = False
        for data_key in required_data_keys:
            if data_key in fetcher_result.processed_data:
                historical_data_payload['data_sources'][data_key] = fetcher_result.processed_data[data_key]
                logger.debug(f"Successfully extracted data segment: '{data_key}' for historical analysis.")
                found_any_data = True
            else:
                logger.warning(
                    f"Required data segment '{data_key}' for depth '{analysis_depth.value}' not found in SmartDataFetcher's processed_data. Available keys: {list(fetcher_result.processed_data.keys())}")
                # å¯ä»¥è€ƒè™‘é™ä½æ•°æ®å®Œæ•´æ€§æˆ–è´¨é‡è¯„åˆ†
                historical_data_payload['data_quality_assessment']['completeness'] = \
                    min(historical_data_payload['data_quality_assessment']['completeness'], 0.5)

        if not found_any_data:
            warn_msg = "No relevant historical data segments found in pre-fetched data for the current analysis depth."
            logger.warning(warn_msg)
            # å³ä½¿æ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé”®ï¼Œä¹Ÿå°è¯•ä¼ é€’æ•´ä¸ª processed_dataï¼Œè®©ä¸‹æ¸¸åˆ†ææ­¥éª¤è‡ªå·±åˆ¤æ–­
            # ä½†è¿™é€šå¸¸è¡¨æ˜æ•°æ®å¥‘çº¦æˆ–æµç¨‹æœ‰é—®é¢˜
            # historical_data_payload['data_sources']['fallback_full_processed_data'] = fetcher_result.processed_data
            # æˆ–è€…æ›´ä¸¥æ ¼ï¼š
            historical_data_payload['error'] = warn_msg
            historical_data_payload['data_quality_assessment'][
                'overall_quality'] = FetcherDataQualityLevel.INSUFFICIENT.value

        # è¿˜å¯ä»¥é™„åŠ åŸå§‹è·å–çš„æ•°æ®ï¼Œå¦‚æœæŸäº›æ·±å±‚åˆ†æéœ€è¦å®ƒ
        # historical_data_payload['raw_fetched_api_data_snapshot'] = fetcher_result.fetched_data # å¯èƒ½éå¸¸å¤§ï¼Œè°¨æ…ä½¿ç”¨

        return historical_data_payload

    # ... (å…¶ä»–æ–¹æ³•çš„å®ç°ï¼Œå¦‚ _ai_analyze_historical_data, _gpt_statistical_analysis, ç­‰ï¼Œ
    #      è¿™äº›æ–¹æ³•ç°åœ¨ä¼šæ¥æ”¶é€šè¿‡ _prepare_historical_data_from_context å‡†å¤‡å¥½çš„æ•°æ®)

    async def _ai_analyze_historical_data(self, historical_data_payload: Dict[str, Any],
                                          query_type: HistoricalQueryType,
                                          analysis_depth: AnalysisDepth,
                                          user_query: str) -> Dict[str, Any]:
        logger.info(f"ğŸ”¬ AIåˆ†æå†å²æ•°æ®: æŸ¥è¯¢ç±»å‹='{query_type.value}', åˆ†ææ·±åº¦='{analysis_depth.value}'")
        ai_processing_time_this_step = 0.0

        # ä» historical_data_payload ä¸­è·å– data_sources
        data_for_analysis = historical_data_payload.get('data_sources', {})
        if not data_for_analysis:
            logger.warning("No data sources found in historical_data_payload for AI analysis.")
            return {'error': 'No data provided for AI analysis', 'statistical_results': {}, 'pattern_insights': {},
                    'metadata': {'ai_processing_time': 0.0}}

        # Step 1: GPT-4oè¿›è¡Œæ•°å€¼è®¡ç®—å’Œç»Ÿè®¡åˆ†æ (å¦‚æœé…ç½®äº†GPTä¸”éœ€è¦)
        statistical_analysis = {}
        if self.gpt_client and self.analysis_config.get('ai_analysis', {}).get('use_gpt_for_calculations', True):
            gpt_start_time = time.time()
            # _gpt_statistical_analysis ç°åœ¨æ¥æ”¶çš„æ˜¯å·²å‡†å¤‡å¥½çš„æ•°æ®å­—å…¸
            statistical_analysis = await self._gpt_statistical_analysis(data_for_analysis, query_type,
                                                                        historical_data_payload.get('time_range', {}))
            ai_processing_time_this_step += (time.time() - gpt_start_time)
            logger.debug(f"GPTç»Ÿè®¡åˆ†æå®Œæˆã€‚ç»“æœé”®: {list(statistical_analysis.keys())}")
        else:
            logger.info("GPTå®¢æˆ·ç«¯ä¸å¯ç”¨æˆ–é…ç½®ç¦ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€ç»Ÿè®¡è®¡ç®—ã€‚")
            # æå–ç”¨äºåŸºç¡€è®¡ç®—çš„æ—¥æ•°æ®éƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼š
            daily_data_for_basic_stats = data_for_analysis.get('daily_aggregates', data_for_analysis.get('daily', []))
            statistical_analysis = self._basic_statistical_calculation(daily_data_for_basic_stats)  # åŒæ­¥æ–¹æ³•
            logger.debug(f"åŸºç¡€ç»Ÿè®¡è®¡ç®—å®Œæˆã€‚ç»“æœé”®: {list(statistical_analysis.keys())}")

        # Step 2: Claudeè¿›è¡Œæ¨¡å¼è¯†åˆ«å’Œè¶‹åŠ¿åˆ†æ
        pattern_analysis = {}
        if self.claude_client and self.analysis_config.get('ai_analysis', {}).get('use_claude_for_insights', True):
            claude_start_time = time.time()
            # _claude_pattern_analysis æ¥æ”¶å·²å‡†å¤‡æ•°æ®å’Œç»Ÿè®¡ç»“æœ
            pattern_analysis = await self._claude_pattern_analysis(data_for_analysis, statistical_analysis, user_query,
                                                                   historical_data_payload.get('time_range', {}))
            ai_processing_time_this_step += (time.time() - claude_start_time)
            logger.debug(
                f"Claudeæ¨¡å¼åˆ†æå®Œæˆã€‚ä¸»è¦è¶‹åŠ¿: {pattern_analysis.get('trend_patterns', {}).get('primary_trend', 'N/A')}")
        else:
            logger.info("Claudeå®¢æˆ·ç«¯ä¸å¯ç”¨æˆ–é…ç½®ç¦ç”¨ï¼Œæ¨¡å¼åˆ†æå’Œæ·±åº¦æ´å¯Ÿå°†å—é™ã€‚")
            pattern_analysis = {"trend_patterns": {"primary_trend": "AIæ¨¡å¼åˆ†ææœªæ‰§è¡Œ"}, "pattern_confidence": 0.5,
                                "note": "Claude client unavailable"}

        return {
            'statistical_results': statistical_analysis,
            'pattern_insights': pattern_analysis,
            'analysis_type_performed': query_type.value,
            'analysis_depth_applied': analysis_depth.value,
            'data_quality_snapshot': historical_data_payload.get('data_quality_assessment', {}),
            'metadata': {'ai_processing_time_for_analysis_step': ai_processing_time_this_step}  # è®°å½•æ­¤æ­¥éª¤AIè€—æ—¶
        }

    async def _gpt_statistical_analysis(self,
                                        data_for_analysis: Dict[str, Any],
                                        # ä» _prepare_historical_data_from_context æ¥çš„æ•°æ®
                                        query_type: HistoricalQueryType,
                                        time_range: Dict[str, Any]) -> Dict[str, Any]:
        logger.debug(f"GPTè¿›è¡Œç»Ÿè®¡åˆ†æï¼ŒæŸ¥è¯¢ç±»å‹: {query_type.value}")
        if not self.gpt_client:
            logger.warning("GPTClientæœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡ŒGPTç»Ÿè®¡åˆ†æã€‚å›é€€åˆ°åŸºç¡€è®¡ç®—ã€‚")
            # æå–ç”¨äºåŸºç¡€è®¡ç®—çš„æ—¥æ•°æ®éƒ¨åˆ†
            daily_data_for_stats = data_for_analysis.get('daily_aggregates', data_for_analysis.get('daily', []))
            return self._basic_statistical_calculation(daily_data_for_stats)

        # å‡†å¤‡ç»™GPTçš„æç¤ºè¯ (ä¸­æ–‡)
        # éœ€è¦ä» data_for_analysis ä¸­é€‰æ‹©åˆé€‚çš„æ•°æ®ç‰‡æ®µå‘é€ç»™GPT
        # ä¾‹å¦‚ï¼Œå¦‚æœ 'daily_aggregates' åŒ…å«åˆ—è¡¨æ•°æ®ï¼š
        daily_aggregates_sample = []
        if 'daily_aggregates' in data_for_analysis and isinstance(data_for_analysis['daily_aggregates'], list):
            daily_aggregates_sample = data_for_analysis['daily_aggregates'][:5]  # å‘é€å‰5æ¡ä½œä¸ºæ ·æœ¬

        prompt = f"""
        æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èæ•°æ®ç»Ÿè®¡å¸ˆã€‚è¯·å¯¹ä»¥ä¸‹æä¾›çš„å†å²é‡‘èæ•°æ®æ ·æœ¬è¿›è¡Œç²¾ç¡®çš„ç»Ÿè®¡åˆ†æã€‚
        ç”¨æˆ·çš„æŸ¥è¯¢ç±»å‹æ˜¯å…³äºï¼šâ€œ{query_type.value}â€

        æ•°æ®æ ·æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ—¥èšåˆæ•°æ®çš„å‰å‡ æ¡è®°å½•ï¼‰:
        ```json
        {json.dumps(daily_aggregates_sample, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}
        ```
        æ€»æ•°æ®ç‚¹æ•°é‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰: {len(data_for_analysis.get('daily_aggregates', []))}
        åˆ†æçš„æ—¶é—´èŒƒå›´: ä» {time_range.get('start_date', 'æœªçŸ¥')} åˆ° {time_range.get('end_date', 'æœªçŸ¥')}

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œåˆ†æï¼Œå¹¶ä»¥JSONå¯¹è±¡æ ¼å¼è¿”å›ç»“æœï¼š
        1.  **ä¸»è¦æŒ‡æ ‡ç»Ÿè®¡**: é’ˆå¯¹æ•°æ®ä¸­çš„æ ¸å¿ƒæ•°å€¼åˆ—ï¼ˆä¾‹å¦‚ 'å…¥é‡‘', 'å‡ºé‡‘', 'æ³¨å†Œäººæ•°', 'æŒä»“äººæ•°'ï¼Œæ ¹æ®æ ·æœ¬åˆ¤æ–­å­˜åœ¨å“ªäº›ï¼‰ï¼Œè®¡ç®—å¹¶è¿”å›å®ƒä»¬çš„ï¼š
            * `mean` (å¹³å‡å€¼), `median` (ä¸­ä½æ•°), `std_dev` (æ ‡å‡†å·®), `min_value` (æœ€å°å€¼), `max_value` (æœ€å¤§å€¼), `total_sum` (æ€»å’Œ)ã€‚
        2.  **å¢é•¿ç‡åˆ†æ**: å¦‚æœæ•°æ®æ˜¯æ—¶é—´åºåˆ—ï¼Œè®¡ç®—ä¸»è¦æŒ‡æ ‡çš„æ•´ä½“å¢é•¿ç‡ï¼ˆä¾‹å¦‚ï¼Œæœ«æœŸå¯¹æ¯”åˆæœŸï¼‰ã€‚
            * `overall_growth_rate`: (ä¾‹å¦‚ï¼š0.15 è¡¨ç¤ºå¢é•¿15%)ã€‚
        3.  **è¶‹åŠ¿ç®€è¿°**: å¯¹ä¸»è¦æŒ‡æ ‡çš„æ•´ä½“è¶‹åŠ¿ç»™å‡ºä¸€ä¸ªç®€çŸ­æè¿° (ä¾‹å¦‚ï¼š"æ³¢åŠ¨ä¸Šå‡", "æŒç»­ä¸‹é™", "ä¿æŒç¨³å®š")ã€‚
            * `trend_description`ã€‚
        4.  **æ³¢åŠ¨æ€§è¯„ä¼°**: è®¡ç®—ä¸€ä¸ªæ³¢åŠ¨æ€§æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼Œå˜å¼‚ç³»æ•° CV = std_dev / meanï¼‰ã€‚
            * `volatility_coefficient`ã€‚

        è¿”å›çš„JSONå¯¹è±¡ç»“æ„åº”ç±»ä¼¼ï¼ˆæ ¹æ®å®é™…åŒ…å«çš„æŒ‡æ ‡è°ƒæ•´ï¼‰ï¼š
        ```json
        {{
          "inflow_analysis": {{ "mean": ..., "overall_growth_rate": ..., "trend_description": "...", "volatility_coefficient": ... }},
          "user_registration_analysis": {{ "mean": ..., "overall_growth_rate": ..., "trend_description": "..." }},
          "summary_confidence": 0.85
        }}
        ```
        è¯·ç¡®ä¿æ‰€æœ‰æ•°å€¼è®¡ç®—ç²¾ç¡®ã€‚å¦‚æœæŸäº›æŒ‡æ ‡æ— æ³•è®¡ç®—ï¼Œè¯·åœ¨è¯¥æŒ‡æ ‡çš„åˆ†æä¸­æ³¨æ˜æˆ–è¿”å›nullã€‚
        """
        query_hash = self._get_query_hash(
            f"gpt_stats_{query_type.value}_{json.dumps(daily_aggregates_sample, sort_keys=True, cls=CustomJSONEncoder)}")
        ai_result = await self._cached_ai_analysis(query_hash, prompt, ai_client_type="gpt")

        if ai_result.get('success'):
            try:
                response_str = ai_result.get('response', '{}')
                parsed_stats = json.loads(response_str)
                if isinstance(parsed_stats, dict):
                    parsed_stats['ai_model_used'] = 'gpt'
                    parsed_stats['processing_time_reported_by_ai'] = ai_result.get('ai_call_duration', 0.0)
                    return parsed_stats
                else:
                    logger.error(f"GPTç»Ÿè®¡åˆ†æè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSONå­—å…¸: {response_str[:200]}")
            except json.JSONDecodeError:
                logger.error(f"æ— æ³•è§£æGPTç»Ÿè®¡åˆ†æçš„JSONå“åº”: {ai_result.get('response', '')[:200]}")
        else:
            logger.error(f"GPTç»Ÿè®¡åˆ†æAPIè°ƒç”¨å¤±è´¥: {ai_result.get('error')}")

        logger.warning("GPTç»Ÿè®¡åˆ†æå¤±è´¥æˆ–è¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œå›é€€åˆ°åŸºç¡€è®¡ç®—ã€‚")
        return self._basic_statistical_calculation(data_for_analysis.get('daily_aggregates', []))

    async def _claude_pattern_analysis(self, data_for_analysis: Dict[str, Any],
                                       statistical_analysis: Dict[str, Any],
                                       user_query: str,
                                       time_range: Dict[str, Any]) -> Dict[str, Any]:
        logger.debug(f"Claudeè¿›è¡Œæ¨¡å¼åˆ†æï¼Œç”¨æˆ·æŸ¥è¯¢: {user_query[:50]}")
        if not self.claude_client:
            logger.warning("ClaudeClientæœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡ŒClaudeæ¨¡å¼åˆ†æã€‚")
            return {"trend_patterns": {"primary_trend": "AIæ¨¡å¼åˆ†ææœªæ‰§è¡Œ (æ— å®¢æˆ·ç«¯)"},
                    "overall_pattern_confidence": 0.3}

        stats_summary_for_claude = {}  # (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´çš„ç»Ÿè®¡æ‘˜è¦é€»è¾‘)
        if 'calculated_metrics' in statistical_analysis:
            stats_summary_for_claude = statistical_analysis['calculated_metrics']
        elif isinstance(statistical_analysis, dict):
            for key, value_dict in statistical_analysis.items():
                if isinstance(value_dict, dict) and key.endswith("_analysis"):  # æ¥è‡ªGPTçš„å¤æ‚key
                    stats_summary_for_claude[key.replace("_analysis", "")] = {
                        "mean": value_dict.get('mean'), "trend": value_dict.get('trend_description'),
                        "growth_rate": value_dict.get('overall_growth_rate'),
                        "volatility": value_dict.get('volatility_coefficient')
                    }
        if not stats_summary_for_claude and statistical_analysis and 'error' not in statistical_analysis:
            stats_summary_for_claude = {"generic_stats_available": True, "details_omitted_for_prompt": True,
                                        "raw_stats_keys": list(statistical_analysis.keys())}

        prompt = f"""
        æ‚¨æ˜¯ä¸€ä½é¡¶çº§çš„é‡‘èæ•°æ®æ¨¡å¼åˆ†æä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„å†å²æ•°æ®ç»Ÿè®¡æ‘˜è¦å’Œæ—¶é—´èŒƒå›´ï¼Œè¿›è¡Œæ·±å…¥çš„æ¨¡å¼åˆ†æã€‚
        ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢æ˜¯ï¼šâ€œ{user_query}â€

        å†å²æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼šä» {time_range.get('start_date', 'æœªçŸ¥')} åˆ° {time_range.get('end_date', 'æœªçŸ¥')} ({time_range.get('time_range_days', 'æœªçŸ¥')}å¤©)ã€‚
        æ•°æ®è´¨é‡è¯„ä¼°ï¼š{data_for_analysis.get('data_quality_assessment', {}).get('overall_quality', 'ä¸€èˆ¬')}

        æ ¸å¿ƒç»Ÿè®¡æŒ‡æ ‡æ‘˜è¦ï¼š
        ```json
        {json.dumps(stats_summary_for_claude, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}
        ```
        è¯·æ‚¨é‡ç‚¹åˆ†æå¹¶è¯†åˆ«ä»¥ä¸‹æ–¹é¢ï¼Œå¹¶ä»¥JSONå¯¹è±¡æ ¼å¼è¿”å›ç»“æœï¼š
        1.  `"trend_patterns"`: (å¯¹è±¡) æè¿°ä¸»è¦çš„è¶‹åŠ¿æ¨¡å¼ã€‚
            * `"primary_trend"`: (å­—ç¬¦ä¸²) å¯¹æ•´ä½“è¶‹åŠ¿çš„æè¿°ã€‚
            * `"trend_strength"`: (æµ®ç‚¹æ•°, 0.0-1.0) è¶‹åŠ¿çš„å¼ºåº¦ã€‚
            * `"trend_sustainability"`: (å­—ç¬¦ä¸²) è¶‹åŠ¿å¯æŒç»­æ€§è¯„ä¼°ã€‚
        2.  `"business_cycles"`: (å¯¹è±¡) æè¿°å¯èƒ½çš„ä¸šåŠ¡å‘¨æœŸæ€§ã€‚
            * `"cycle_detected"`: (å¸ƒå°”å€¼) æ˜¯å¦æ£€æµ‹åˆ°å‘¨æœŸã€‚
            * `"cycle_period"`: (å­—ç¬¦ä¸², å¯é€‰) å‘¨æœŸé•¿åº¦ã€‚
            * `"cycle_phase"`: (å­—ç¬¦ä¸², å¯é€‰) å½“å‰å‘¨æœŸé˜¶æ®µã€‚
        3.  `"anomaly_detection_summary"`: (å¯¹è±¡) å¼‚å¸¸æƒ…å†µæ€»ç»“ã€‚
            * `"anomalies_found_description"`: (å­—ç¬¦ä¸²) ä¸»è¦å¼‚å¸¸æè¿°ã€‚
            * `"anomaly_impact_assessment"`: (å­—ç¬¦ä¸²) å¼‚å¸¸çš„æ½œåœ¨ä¸šåŠ¡å½±å“ã€‚
        4.  `"correlation_patterns_summary"`: (å¯¹è±¡) æŒ‡æ ‡é—´æ½œåœ¨å…³è”ã€‚
            * `"strong_correlations_identified"`: (å­—ç¬¦ä¸²åˆ—è¡¨) è§‚å¯Ÿåˆ°çš„å¼ºç›¸å…³æ€§ã€‚
            * `"correlation_insights_text"`: (å­—ç¬¦ä¸²) ç›¸å…³æ€§çš„ä¸šåŠ¡è§£è¯»ã€‚
        5.  `"early_warning_signals_identified"`: (å­—ç¬¦ä¸²åˆ—è¡¨) æ½œåœ¨é£é™©æˆ–é¢„è­¦ä¿¡å·ã€‚
        6.  `"overall_pattern_confidence"`: (æµ®ç‚¹æ•°, 0.0-1.0) å¯¹æœ¬æ¬¡æ¨¡å¼åˆ†æç»“æœçš„æ•´ä½“ç½®ä¿¡åº¦ã€‚

        å¦‚æœæ•°æ®ä¸è¶³ä»¥è¿›è¡ŒæŸé¡¹åˆ†æï¼Œè¯·åœ¨è¯¥é¡¹ä¸­æ˜ç¡®æŒ‡å‡ºâ€œæ•°æ®ä¸è¶³æ— æ³•åˆ†æâ€ã€‚
        """
        query_hash = self._get_query_hash(
            f"claude_pattern_{user_query}_{json.dumps(stats_summary_for_claude, sort_keys=True, cls=CustomJSONEncoder)}")
        ai_result = await self._cached_ai_analysis(query_hash, prompt, ai_client_type="claude")

        if ai_result.get('success'):
            try:
                response_str = ai_result.get('response', '{}')
                parsed_patterns = json.loads(response_str)
                if isinstance(parsed_patterns, dict) and "trend_patterns" in parsed_patterns:
                    parsed_patterns['ai_model_used'] = 'claude'
                    parsed_patterns['processing_time_reported_by_ai'] = ai_result.get('ai_call_duration', 0.0)
                    return parsed_patterns
                else:
                    logger.error(f"Claudeæ¨¡å¼åˆ†æè¿”å›JSONç»“æ„ä¸æ­£ç¡®: {response_str[:300]}")
            except json.JSONDecodeError:
                logger.error(f"æ— æ³•è§£æClaudeæ¨¡å¼åˆ†æJSON: {ai_result.get('response', '')[:300]}")
        else:
            logger.error(f"Claudeæ¨¡å¼åˆ†æAPIè°ƒç”¨å¤±è´¥: {ai_result.get('error')}")

        return {  # é™çº§è¿”å›
            'trend_patterns': {'primary_trend': 'è¶‹åŠ¿åˆ†æAIè°ƒç”¨å¤±è´¥ï¼Œè¯·å‚è€ƒåŸºç¡€ç»Ÿè®¡', 'trend_strength': 0.3,
                               'trend_sustainability': 'æœªçŸ¥'},
            'business_cycles': {'cycle_detected': False},
            'anomaly_detection_summary': {'anomalies_found_description': 'AIå¼‚å¸¸æ£€æµ‹æœªæ‰§è¡Œ'},
            'correlation_patterns_summary': {'strong_correlations_identified': [],
                                             'correlation_insights_text': 'AIç›¸å…³æ€§åˆ†ææœªæ‰§è¡Œ'},
            'early_warning_signals_identified': ["AIæ¨¡å¼åˆ†æå¤±è´¥ï¼Œè¯·å…³æ³¨åŸºç¡€æ•°æ®"],
            'overall_pattern_confidence': 0.3, 'note': 'AIæ¨¡å¼åˆ†æå¤±è´¥ï¼Œç»“æœåŸºäºæœ‰é™æ¨æ–­ã€‚'
        }

    async def _ai_generate_business_insights(self, analysis_results_dict: Dict[str, Any],
                                             # æ¥è‡ª _ai_analyze_historical_data
                                             query_type: HistoricalQueryType,
                                             user_query: str) -> Dict[str, Any]:
        # (ä¸ä¹‹å‰ç‰ˆæœ¬ç±»ä¼¼ï¼Œç¡®ä¿ä¸­æ–‡æç¤ºè¯ï¼Œå¹¶ä½¿ç”¨self.claude_client)
        logger.debug(f"Claudeç”Ÿæˆä¸šåŠ¡æ´å¯Ÿï¼ŒæŸ¥è¯¢ç±»å‹: {query_type.value}")
        if not self.claude_client:
            logger.warning("ClaudeClientæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”ŸæˆAIä¸šåŠ¡æ´å¯Ÿã€‚")
            return {
                'key_findings': ['å†å²æ•°æ®åŸºç¡€åˆ†æå·²å®Œæˆã€‚'], 'business_insights': ["AIæ´å¯ŸæœåŠ¡å½“å‰ä¸å¯ç”¨ã€‚"],
                'risk_warnings': [], 'opportunities': [],
                'recommendations': ['è¯·ç»“åˆå…·ä½“ä¸šåŠ¡åœºæ™¯è§£è¯»æ•°æ®ã€‚'], 'insight_confidence': 0.4,
                'metadata': {'ai_processing_time': 0.0, 'note': "AIæ´å¯Ÿæœªæ‰§è¡Œ(å®¢æˆ·ç«¯ç¼ºå¤±)"}
            }

        stats_summary = analysis_results_dict.get('statistical_results', {})
        pattern_summary = analysis_results_dict.get('pattern_insights', {})

        condensed_summary_for_claude = {
            "ç»Ÿè®¡å‘ç°æ¦‚è¦": {k: v for k, v_dict in stats_summary.items() if isinstance(v_dict, dict) for k, v in
                             v_dict.items() if
                             not isinstance(v, dict) and k in ['mean', 'overall_growth_rate', 'trend_description',
                                                               'volatility_coefficient']},
            "æ¨¡å¼åˆ†ææ¦‚è¦": {
                "ä¸»è¦è¶‹åŠ¿": pattern_summary.get('trend_patterns', {}).get('primary_trend'),
                "ä¸šåŠ¡å‘¨æœŸ": "æ£€æµ‹åˆ°" if pattern_summary.get('business_cycles', {}).get(
                    'cycle_detected') else "æœªæ£€æµ‹åˆ°",
                "å¼‚å¸¸æ‘˜è¦": pattern_summary.get('anomaly_detection_summary', {}).get('anomalies_found_description')
            },
            "æ•°æ®è´¨é‡å¿«ç…§": analysis_results_dict.get('data_quality_snapshot', {})
        }

        prompt = f"""
        ä½œä¸ºä¸€ä½ç»éªŒä¸°å¯Œçš„é‡‘èç­–ç•¥åˆ†æå¸ˆå’Œä¸šåŠ¡é¡¾é—®ï¼Œè¯·åŸºäºä»¥ä¸‹æä¾›çš„å†å²æ•°æ®åˆ†ææ‘˜è¦ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆå…·æœ‰æ·±åº¦å’Œå¯æ“ä½œæ€§çš„ä¸­æ–‡ä¸šåŠ¡æ´å¯Ÿã€‚
        ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢æ˜¯ï¼šâ€œ{user_query}â€
        ç³»ç»Ÿè¯†åˆ«çš„æŸ¥è¯¢ç±»å‹ä¸ºï¼šâ€œ{query_type.value}â€

        å†å²æ•°æ®åˆ†ææ ¸å¿ƒæ‘˜è¦ï¼š
        ```json
        {json.dumps(condensed_summary_for_claude, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}
        ```
        è¯·æ‚¨é‡ç‚¹ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ€è€ƒå’Œé˜è¿°ï¼Œå¹¶ä»¥JSONå¯¹è±¡æ ¼å¼è¿”å›ç»“æœï¼š
        1.  `"key_findings"`: (å­—ç¬¦ä¸²åˆ—è¡¨) æ€»ç»“å‡º3-5ä¸ªæœ€æ ¸å¿ƒã€æœ€é‡è¦çš„ä¸šåŠ¡å‘ç°ã€‚
        2.  `"business_insights"`: (å­—ç¬¦ä¸²åˆ—è¡¨) å¯¹è¿™äº›å‘ç°è¿›è¡Œæ·±å…¥çš„ä¸šåŠ¡è§£è¯»ï¼Œé˜é‡Šå®ƒä»¬èƒŒåçš„å•†ä¸šå«ä¹‰ã€å¯èƒ½çš„åŸå› ä»¥åŠå¯¹å½“å‰ä¸šåŠ¡çŠ¶æ€çš„æŒ‡ç¤ºã€‚æ¯ä¸ªè§£è¯»æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å­—ç¬¦ä¸²ã€‚
        3.  `"risk_warnings"`: (å­—ç¬¦ä¸²åˆ—è¡¨) åŸºäºåˆ†æï¼Œæ˜ç¡®æŒ‡å‡º1-3ä¸ªæœ€å€¼å¾—å…³æ³¨çš„æ½œåœ¨ä¸šåŠ¡é£é™©æˆ–é¢„è­¦ä¿¡å·ã€‚å¦‚æœæ— æ˜æ˜¾é£é™©ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
        4.  `"opportunities"`: (å­—ç¬¦ä¸²åˆ—è¡¨) è¯†åˆ«å¹¶åˆ—å‡º1-3ä¸ªæ½œåœ¨çš„ä¸šåŠ¡å¢é•¿æœºä¼šæˆ–æ”¹è¿›ç‚¹ã€‚å¦‚æœæ— æ˜æ˜¾æœºä¼šï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
        5.  `"recommendations"`: (å­—ç¬¦ä¸²åˆ—è¡¨) é’ˆå¯¹ä¸Šè¿°é£é™©å’Œæœºä¼šï¼Œæä¾›2-3æ¡å…·ä½“çš„ã€å¯æ“ä½œçš„è¡ŒåŠ¨å»ºè®®ã€‚
        6.  `"insight_confidence"`: (æµ®ç‚¹æ•°, 0.0-1.0) æ‚¨å¯¹æœ¬æ¬¡æ´å¯Ÿåˆ†æçš„æ•´ä½“ä¸“ä¸šåˆ¤æ–­ç½®ä¿¡åº¦ã€‚
        """
        query_hash = self._get_query_hash(
            f"claude_biz_insights_{user_query}_{json.dumps(condensed_summary_for_claude, sort_keys=True, cls=CustomJSONEncoder)}")
        ai_result = await self._cached_ai_analysis(query_hash, prompt, ai_client_type="claude")
        ai_call_duration = ai_result.get('ai_call_duration', 0.0)  # è·å–AIè°ƒç”¨è€—æ—¶

        if ai_result.get('success'):
            try:
                response_str = ai_result.get('response', '{}')
                parsed_insights = json.loads(response_str)
                if isinstance(parsed_insights, dict) and \
                        all(k in parsed_insights for k in
                            ["key_findings", "business_insights", "risk_warnings", "opportunities", "recommendations",
                             "insight_confidence"]):
                    parsed_insights['metadata'] = {'ai_model_used': 'claude',
                                                   'ai_processing_time_for_insight_step': ai_call_duration}
                    return parsed_insights
                else:
                    logger.error(f"Claudeä¸šåŠ¡æ´å¯Ÿè¿”å›JSONç»“æ„ä¸å®Œæ•´: {response_str[:300]}")
            except json.JSONDecodeError:
                logger.error(f"æ— æ³•è§£æClaudeä¸šåŠ¡æ´å¯ŸJSON: {ai_result.get('response', '')[:300]}")
        else:
            logger.error(f"Claudeä¸šåŠ¡æ´å¯ŸAPIè°ƒç”¨å¤±è´¥: {ai_result.get('error')}")

        return {
            'key_findings': ['AIæ´å¯Ÿç”Ÿæˆå¤±è´¥ï¼Œè¯·å‚è€ƒç»Ÿè®¡å’Œæ¨¡å¼åˆ†æç»“æœã€‚'], 'business_insights': [],
            'risk_warnings': [], 'opportunities': [], 'recommendations': ['å»ºè®®äººå·¥å®¡æ ¸æ•°æ®ã€‚'],
            'insight_confidence': 0.3, 'metadata': {'ai_processing_time': ai_call_duration, 'note': "AIæ´å¯Ÿç”Ÿæˆå¤±è´¥"}
        }

    # _build_historical_analysis_response, _create_error_response, _update_processing_stats,
    # get_processing_stats, health_check, batch_analyze_periods ä¿æŒä¸å˜æˆ–åšå¾®å°è°ƒæ•´

        # åœ¨ HistoricalAnalysisProcessor ç±»ä¸­æ·»åŠ æ­¤æ–¹æ³•ï¼š
    def _basic_statistical_calculation(self, daily_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        (ç§æœ‰) æ‰§è¡ŒåŸºç¡€çš„ç»Ÿè®¡è®¡ç®—ï¼Œä½œä¸ºAIç»Ÿè®¡åˆ†æçš„é™çº§æ–¹æ¡ˆã€‚
        daily_data_list: ä¾‹å¦‚ï¼ŒåŒ…å« {'æ—¥æœŸ': 'YYYY-MM-DD', 'å…¥é‡‘': 123.45, 'å‡ºé‡‘': 50.0 ...} çš„å­—å…¸åˆ—è¡¨ã€‚
        """
        logger.info(f"Executing basic statistical calculation for {len(daily_data_list)} data points.")
        if not daily_data_list or not isinstance(daily_data_list, list):
            logger.warning("Basic statistical calculation: No data or invalid data format provided.")
            return {'error': 'æ— æœ‰æ•ˆæ•°æ®è¿›è¡ŒåŸºç¡€ç»Ÿè®¡è®¡ç®—', 'summary_confidence': 0.1}

        stats_summary: Dict[str, Any] = {
            'calculated_metrics': {},
            'summary_confidence': 0.5,  # åŸºç¡€è®¡ç®—çš„ç½®ä¿¡åº¦é€šå¸¸ä½äºAIå¢å¼ºçš„
            'note': "ç»“æœåŸºäºåŸºç¡€ç»Ÿè®¡è§„åˆ™ï¼Œæœªç»è¿‡AIæ·±åº¦åˆ†æã€‚"
        }

        # è¯†åˆ«æ•°æ®ä¸­å¯èƒ½å­˜åœ¨çš„æ•°å€¼åˆ— (è¿™é‡Œå‡è®¾ä¸€äº›å¸¸è§çš„åˆ—å)
        # åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦æ›´åŠ¨æ€åœ°è¯†åˆ«æ•°å€¼åˆ—æˆ–ä»å…ƒæ•°æ®ä¸­è·å–
        possible_numeric_keys = ["å…¥é‡‘", "å‡ºé‡‘", "æ³¨å†Œäººæ•°", "æŒä»“äººæ•°", "è´­ä¹°äº§å“æ•°é‡", "åˆ°æœŸäº§å“æ•°é‡", "å‡€æµå…¥"]

        # å°è¯•ä¸ºæ¯ä¸ªå¯èƒ½çš„æ•°å€¼åˆ—è®¡ç®—ç»Ÿè®¡æ•°æ®
        for key in possible_numeric_keys:
            values = []
            dates = []  # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥æ”¶é›†æ—¥æœŸ
            for item in daily_data_list:
                if isinstance(item, dict) and item.get(key) is not None:
                    try:
                        values.append(float(item[key]))
                        if 'æ—¥æœŸ' in item: dates.append(item['æ—¥æœŸ'])
                    except (ValueError, TypeError):
                        logger.debug(f"Skipping non-numeric value for key '{key}': {item[key]}")

            if len(values) >= 2:  # è‡³å°‘éœ€è¦ä¸¤ä¸ªæ•°æ®ç‚¹æ‰èƒ½è¿›è¡Œä¸€äº›æœ‰æ„ä¹‰çš„ç»Ÿè®¡
                mean_val = sum(values) / len(values)
                median_val = sorted(values)[len(values) // 2]
                std_dev_val = statistics.stdev(values) if len(values) > 1 else 0.0
                min_val = min(values)
                max_val = max(values)
                sum_val = sum(values)
                growth_rate_val = (values[-1] - values[0]) / abs(values[0]) if values[0] != 0 else (
                    1.0 if values[-1] > 0 else 0.0)
                trend_desc = 'ä¸Šå‡' if values[-1] > values[0] else ('ä¸‹é™' if values[-1] < values[0] else 'å¹³ç¨³')
                volatility_coeff = std_dev_val / abs(mean_val) if mean_val != 0 else float('inf')

                stats_summary['calculated_metrics'][f"{key}_stats"] = {
                    'mean': round(mean_val, 2),
                    'median': round(median_val, 2),
                    'std_dev': round(std_dev_val, 2),
                    'min_value': round(min_val, 2),
                    'max_value': round(max_val, 2),
                    'total_sum': round(sum_val, 2),
                    'count': len(values),
                    'overall_growth_rate': round(growth_rate_val, 4),
                    'trend_description': trend_desc,
                    'volatility_coefficient': round(volatility_coeff, 4) if volatility_coeff != float(
                        'inf') else "N/A (mean is zero)"
                }
            elif len(values) == 1:
                stats_summary['calculated_metrics'][f"{key}_stats"] = {'value': values[0], 'count': 1}

        # ç¤ºä¾‹ï¼šè®¡ç®—æ€»çš„å‡€æµå…¥ï¼ˆå¦‚æœå…¥é‡‘å’Œå‡ºé‡‘éƒ½å­˜åœ¨ï¼‰
        if 'inflow_stats' in stats_summary['calculated_metrics'] and 'outflow_stats' in stats_summary[
            'calculated_metrics']:
            total_inflow = stats_summary['calculated_metrics']['inflow_stats'].get('sum', 0.0)
            total_outflow = stats_summary['calculated_metrics']['outflow_stats'].get('sum', 0.0)
            stats_summary['calculated_metrics']['total_net_flow'] = round(total_inflow - total_outflow, 2)
            if daily_data_list:
                stats_summary['calculated_metrics']['average_daily_net_flow'] = round(
                    (total_inflow - total_outflow) / len(daily_data_list), 2)

        if not stats_summary['calculated_metrics']:
            logger.warning("Basic statistical calculation: No numeric metrics found or calculable.")
            stats_summary['error'] = "æœªèƒ½ä»æä¾›çš„æ•°æ®ä¸­æå–å¯è®¡ç®—çš„æ•°å€¼æŒ‡æ ‡ã€‚"
            stats_summary['summary_confidence'] = 0.2

        return stats_summary
    def _build_historical_analysis_response(self, query_type: HistoricalQueryType,
                                            analysis_depth: AnalysisDepth,
                                            analysis_results_dict: Dict[str, Any],
                                            business_insights_dict: Dict[str, Any],
                                            historical_data_payload: Dict[str, Any],
                                            time_params: Dict[str, Any],
                                            processing_time_wall: float) -> HistoricalAnalysisResponse:
        logger.debug("Building final HistoricalAnalysisResponse...")

        key_metrics_ext: Dict[str, Any] = {}  # å€¼å¯ä»¥æ˜¯æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        stats_res_for_metrics = analysis_results_dict.get('statistical_results', {})
        if isinstance(stats_res_for_metrics, dict):
            for analysis_key, metrics_val_dict in stats_res_for_metrics.items():
                if isinstance(metrics_val_dict, dict) and not any(
                        k in analysis_key for k in ['metadata', 'ai_model_used', 'note', 'confidence']):  # æ’é™¤å…ƒæ•°æ®
                    for metric_name, metric_val in metrics_val_dict.items():
                        # ç®€å•æå–ï¼Œå®é™…å¯èƒ½éœ€è¦æ ¹æ®æŒ‡æ ‡ç±»å‹æ ¼å¼åŒ–
                        key_metrics_ext[f"{analysis_key}_{metric_name}"] = metric_val

        pattern_res_for_metrics = analysis_results_dict.get('pattern_insights', {})
        if isinstance(pattern_res_for_metrics, dict) and isinstance(pattern_res_for_metrics.get('trend_patterns'),
                                                                    dict):
            key_metrics_ext['primary_trend_description'] = pattern_res_for_metrics['trend_patterns'].get(
                'primary_trend', 'N/A')
            key_metrics_ext['trend_strength_score'] = pattern_res_for_metrics['trend_patterns'].get('trend_strength',
                                                                                                    0.0)

        analysis_confidence = float(business_insights_dict.get('insight_confidence', 0.65))
        data_quality_info = historical_data_payload.get('data_quality_assessment', {})

        response = HistoricalAnalysisResponse(
            query_type=query_type,
            analysis_depth=analysis_depth,
            main_findings=business_insights_dict.get('key_findings', []),
            trend_summary=pattern_res_for_metrics.get('trend_patterns', {}),  # ä½¿ç”¨æ¨¡å¼åˆ†æä¸­çš„è¶‹åŠ¿æ€»ç»“
            key_metrics=key_metrics_ext,  # ç°åœ¨æ˜¯ Dict[str, Any]
            business_insights=business_insights_dict.get('business_insights', []),
            pattern_discoveries=pattern_res_for_metrics.get('early_warning_signals_identified', []),
            comparative_analysis=pattern_res_for_metrics.get('correlation_patterns_summary', {}),  # å‡è®¾æ¨¡å¼åˆ†æåŒ…å«ç›¸å…³æ€§
            risk_warnings=business_insights_dict.get('risk_warnings', []),
            opportunities=business_insights_dict.get('opportunities', []),
            recommendations=business_insights_dict.get('recommendations', []),
            data_completeness=float(data_quality_info.get('completeness', 0.0)),
            analysis_confidence=analysis_confidence,
            data_sources_used=list(historical_data_payload.get('data_sources', {}).keys()),  # å®é™…ä½¿ç”¨çš„æ•°æ®ç‰‡æ®µé”®å
            analysis_period=f"{time_params.get('start_date', 'N/A')} è‡³ {time_params.get('end_date', 'N/A')}",
            processing_time=processing_time_wall,
            methodology_notes=[
                f"åˆ†ææ·±åº¦çº§åˆ«: {analysis_depth.value}",
                f"æ•°æ®æ—¶é—´èŒƒå›´: {time_params.get('time_range_days', 'N/A')} å¤©",
                "åŸºäºAIåŒæ¨¡å‹åä½œåˆ†æï¼Œç»“åˆç»Ÿè®¡å­¦æ–¹æ³•ã€‚",
                f"æ•°æ®è´¨é‡è¯„ä¼°: {data_quality_info.get('overall_quality', 'N/A')} (è¯„åˆ†: {data_quality_info.get('quality_score', 0.0):.2f})"
            ],
            metadata=business_insights_dict.get('metadata', {})  # ä»æ´å¯Ÿç”Ÿæˆæ­¥éª¤è·å–AIè€—æ—¶ç­‰
        )
        # å°† statistical_results å’Œ pattern_insights ä¸­çš„è¯¦ç»†å†…å®¹ä¹Ÿæ”¾å…¥ metadataï¼Œä¾›è°ƒè¯•æˆ–å‰ç«¯æ·±åº¦å±•ç¤º
        response.metadata['full_statistical_analysis'] = stats_res_for_metrics
        response.metadata['full_pattern_analysis'] = pattern_res_for_metrics
        return response

    def _create_error_response(self, user_query: str, error: str) -> HistoricalAnalysisResponse:
        # (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´)
        return HistoricalAnalysisResponse(
            query_type=HistoricalQueryType.UNKNOWN_HISTORICAL, analysis_depth=AnalysisDepth.BASIC,
            main_findings=[f"å†å²æ•°æ®åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error}"],
            trend_summary={'primary_trend': 'é”™è¯¯', 'error_details': error}, key_metrics={"error_message": error},
            analysis_confidence=0.0, processing_time=0.01,
            methodology_notes=[f"é”™è¯¯è¯¦æƒ…: {error}"]
        )

    def _update_processing_stats(self, query_type: HistoricalQueryType,
                                 processing_time: float, confidence: float):
        # (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´)
        try:
            type_key = query_type.value
            self.processing_stats.setdefault('analyses_by_type', {})
            self.processing_stats['analyses_by_type'][type_key] = self.processing_stats['analyses_by_type'].get(
                type_key, 0) + 1

            total = self.processing_stats['total_analyses']
            if total == 0: total = 1  # é¿å…é¦–æ¬¡è°ƒç”¨æ—¶ total ä¸º 0

            current_avg_time = self.processing_stats.get('avg_processing_time', 0.0)
            self.processing_stats['avg_processing_time'] = (current_avg_time * (total - 1) + processing_time) / total

            current_avg_conf = self.processing_stats.get('avg_confidence', 0.0)
            self.processing_stats['avg_confidence'] = (current_avg_conf * (total - 1) + confidence) / total

            total_cache = self.processing_stats.get('ai_cache_hits', 0) + self.processing_stats.get('ai_cache_misses',
                                                                                                    0)
            if total_cache > 0:
                self.processing_stats['ai_cache_hit_rate'] = self.processing_stats['ai_cache_hits'] / total_cache
        except Exception as e:
            logger.error(f"HistoricalAnalysisProcessor ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å¤±è´¥: {str(e)}")

    def get_processing_stats(self) -> Dict[str, Any]:
        stats_copy = json.loads(json.dumps(self.processing_stats, cls=CustomJSONEncoder))
        return stats_copy

    async def health_check(self) -> Dict[str, Any]:
        # (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´)
        claude_ok = self.claude_client is not None
        gpt_ok = self.gpt_client is not None
        # æ›´ç²¾ç»†çš„å¥åº·æ£€æŸ¥å¯ä»¥å®é™…è°ƒç”¨AIå®¢æˆ·ç«¯çš„æµ‹è¯•æ¥å£
        status = "healthy"
        issues = []
        if not claude_ok: issues.append("ClaudeClient not configured.")
        if not gpt_ok: issues.append("GPTClient not configured.")
        if issues: status = "degraded" if (claude_ok or gpt_ok) else "unhealthy"
        return {
            "status": status, "component_name": self.__class__.__name__,
            "ai_clients_status": {"claude_available": claude_ok, "gpt_available": gpt_ok},
            "issues": issues, "timestamp": datetime.now().isoformat()
        }

    async def batch_analyze_periods(self, periods_info: List[Dict[str, Any]],
                                    user_context: Optional[Dict[str, Any]] = None) -> List[HistoricalAnalysisResponse]:
        # (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´)
        if not periods_info: return []
        # ç¡®ä¿ user_context ä¼ é€’ç»™æ¯ä¸ª process_historical_analysis_query è°ƒç”¨
        tasks = [self.process_historical_analysis_query(p_info['user_query'], user_context=user_context) for p_info in
                 periods_info]
        results_or_exceptions = await asyncio.gather(*tasks, return_exceptions=True)
        final_results = []
        for i, res_or_exc in enumerate(results_or_exceptions):
            if isinstance(res_or_exc, Exception):
                logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥ for query '{periods_info[i]['user_query']}': {res_or_exc}")
                final_results.append(self._create_error_response(periods_info[i]['user_query'], str(res_or_exc)))
            else:
                final_results.append(res_or_exc)
        return final_results


# --- å·¥å‚å‡½æ•° (ä¿æŒä¸å˜) ---
def create_historical_analysis_processor(claude_client=None, gpt_client=None) -> HistoricalAnalysisProcessor:
    return HistoricalAnalysisProcessor(claude_client, gpt_client)

# --- ç§»é™¤äº† async def main() ---