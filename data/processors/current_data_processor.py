# data/processors/current_data_processor.py
"""
ğŸ“Š å½“å‰æ•°æ®å¤„ç†å™¨ (ä¼˜åŒ–ç‰ˆ)
ä¸“é—¨å¤„ç†å·²è·å–çš„å®æ—¶çŠ¶æ€æ•°æ®ï¼Œè¿›è¡ŒAIè¾…åŠ©åˆ†æã€æ ¼å¼åŒ–å¹¶æ„å»ºå“åº”ã€‚
"""

import logging
import traceback
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import asyncio
from dataclasses import dataclass, field  # ç¡®ä¿å¯¼å…¥ field
from enum import Enum
import json
from functools import lru_cache  # ä¿ç•™ç”¨äºAIåˆ†æç»“æœç¼“å­˜
import hashlib

# AI å®¢æˆ·ç«¯å¯¼å…¥ (å‡è®¾ä»é¡¶å±‚ core.models å¯¼å…¥)
from core.models.claude_client import ClaudeClient, CustomJSONEncoder  # å‡è®¾ClaudeClientåœ¨core.models
from core.models.openai_client import OpenAIClient  # å‡è®¾OpenAIClientåœ¨core.models

# QueryAnalysisResult å¯¼å…¥ï¼Œå› ä¸ºå®ƒç°åœ¨æ˜¯è¾“å…¥å‚æ•°
from core.analyzers.query_parser import QueryAnalysisResult  # ä½¿ç”¨åˆ«å
from core.analyzers.query_parser import QueryType as QueryParserQueryType

logger = logging.getLogger(__name__)


class CurrentDataQueryType(Enum):
    """å½“å‰æ•°æ®æŸ¥è¯¢çš„ç»†åŒ–ç±»å‹ (ç”±æœ¬å¤„ç†å™¨å†…éƒ¨AIè¯†åˆ«æˆ–ä»QueryAnalysisResultæ˜ å°„)"""
    SYSTEM_OVERVIEW = "system_overview"
    BALANCE_CHECK = "balance_check"
    USER_STATUS = "user_status"
    TODAY_EXPIRY = "today_expiry"
    CASH_FLOW = "cash_flow"
    PRODUCT_STATUS = "product_status"
    QUICK_METRICS = "quick_metrics"
    UNKNOWN_CURRENT_QUERY = "unknown_current_query"  # æ–°å¢ï¼Œç”¨äºæ— æ³•ç»†åŒ–çš„æƒ…å†µ


class ResponseFormat(Enum):
    """å“åº”æ ¼å¼ç±»å‹"""
    SIMPLE_TEXT = "simple_text"
    DETAILED_SUMMARY = "detailed_summary"
    METRICS_FOCUSED = "metrics_focused"
    BUSINESS_ORIENTED = "business_oriented"


@dataclass
class CurrentDataResponse:
    """å½“å‰æ•°æ®å¤„ç†å™¨çš„å“åº”ç»“æœ"""
    query_type: CurrentDataQueryType
    response_format: ResponseFormat
    main_answer: str
    key_metrics: Dict[str, Any] = field(default_factory=dict)
    formatted_data: Dict[str, str] = field(default_factory=dict)  # <--- è¿™é‡ŒæœŸæœ› Dict[str, str]
    related_metrics: Dict[str, Any] = field(default_factory=dict)
    business_context: str = ""
    quick_insights: List[str] = field(default_factory=list)
    data_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    response_confidence: float = 0.0
    data_sources_references: List[str] = field(default_factory=list)  # æŒ‡å‘ç¼–æ’å™¨æä¾›çš„æ•°æ®æº
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)  # å¯å­˜å‚¨AIè€—æ—¶ç­‰


class CurrentDataProcessor:
    """
    ğŸ“Š å½“å‰æ•°æ®å¤„ç†å™¨ (ä¼˜åŒ–ç‰ˆ)
    ä¸“æ³¨äºåŸºäºå·²è·å–çš„å½“å‰ç³»ç»Ÿæ•°æ®ï¼Œè¿›è¡ŒAIè¾…åŠ©çš„åˆ†æã€æ ¼å¼åŒ–å’Œå“åº”æ„å»ºã€‚
    """

    def __init__(self, claude_client: Optional[ClaudeClient] = None,
                 gpt_client: Optional[OpenAIClient] = None):
        """
        åˆå§‹åŒ–å½“å‰æ•°æ®å¤„ç†å™¨ã€‚
        Args:
            claude_client: Claudeå®¢æˆ·ç«¯ï¼Œç”¨äºä¸šåŠ¡ç†è§£å’Œæ´å¯Ÿã€‚
            gpt_client: GPTå®¢æˆ·ç«¯ï¼Œç”¨äºæ•°æ®æ ¼å¼åŒ–å’Œè®¡ç®—è¾…åŠ©ã€‚
        """
        self.claude_client = claude_client
        self.gpt_client = gpt_client  # GPTç›®å‰åœ¨æ­¤å¤„ç†å™¨ä¸­ç”¨é€”è¾ƒå°‘ï¼Œä½†ä¿ç•™ä»¥ä¾¿æœªæ¥æ‰©å±•

        self.processing_config = self._load_processing_config()
        # self.query_patterns = self._load_current_data_patterns() # ç®€å•å…³é”®è¯åŒ¹é…å¯èƒ½ä¸å†éœ€è¦ï¼Œä¾èµ–ä¼ å…¥çš„QueryAnalysisResult

        self.processing_stats = {
            'total_queries_processed': 0,
            'queries_by_sub_type': {},
            'avg_response_time': 0.0,
            'avg_confidence': 0.0,
            'ai_cache_hits': 0,  # ä¸“é—¨ç»Ÿè®¡æœ¬å¤„ç†å™¨AIç¼“å­˜
            'ai_cache_misses': 0
        }
        logger.info("CurrentDataProcessor initialized (optimized for external data input and AI assistance)")

    def _get_query_hash(self, text_to_hash: str) -> str:
        """ä¸ºæ–‡æœ¬ç”ŸæˆMD5å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜é”®ã€‚"""
        return hashlib.md5(text_to_hash.encode('utf-8')).hexdigest()

    @lru_cache(maxsize=50)  # ç¼“å­˜æœ€è¿‘50ä¸ªAIåˆ†æç»“æœ
    async def _cached_ai_analysis(self, query_or_prompt_hash: str, prompt: str,
                                  ai_client_type: str = "claude") -> Dict[str, Any]:
        """
        å¸¦ç¼“å­˜çš„AIåˆ†æè°ƒç”¨ã€‚
        Args:
            query_or_prompt_hash: åŸºäºæŸ¥è¯¢æˆ–å®Œæ•´æç¤ºç”Ÿæˆçš„å“ˆå¸Œï¼Œç”¨ä½œç¼“å­˜é”®ã€‚
            prompt: å‘é€ç»™AIçš„å®Œæ•´æç¤ºè¯ã€‚
            ai_client_type: "claude" æˆ– "gpt"ã€‚
        Returns:
            AIçš„å“åº”å­—å…¸ï¼Œæˆ–åŒ…å«é”™è¯¯çš„å­—å…¸ã€‚
        """
        # æ³¨æ„ï¼šlru_cache å¯¹äºå¼‚æ­¥æ–¹æ³•éœ€è¦ç‰¹æ®Šå¤„ç†æˆ–ä½¿ç”¨å¼‚æ­¥ç¼“å­˜åº“ã€‚
        # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œçš„lru_cacheä¸»è¦ä½œç”¨äºåŒæ­¥éƒ¨åˆ†ï¼ˆå‚æ•°å“ˆå¸Œï¼‰ï¼Œå®é™…AIè°ƒç”¨æ˜¯å¼‚æ­¥çš„ã€‚
        # æ›´ä¼˜çš„å¼‚æ­¥ç¼“å­˜æ–¹æ¡ˆå¯ä»¥ä½¿ç”¨ä¾‹å¦‚ 'async-cache' åº“ã€‚
        # å½“å‰çš„ lru_cache è£…é¥°å™¨å¯èƒ½ä¸ä¼šæŒ‰é¢„æœŸå¯¹å¼‚æ­¥å‡½æ•°çš„ä¸åŒè°ƒç”¨ç¼“å­˜ï¼Œ
        # å› ä¸ºå®ƒç¼“å­˜çš„æ˜¯åç¨‹å¯¹è±¡æœ¬èº«ã€‚æˆ‘ä»¬å°†ä¾èµ– prompt_hash æ¥å®ç°æ‰‹åŠ¨æ£€æŸ¥ã€‚

        # æ‰‹åŠ¨æ¨¡æ‹Ÿç¼“å­˜æ£€æŸ¥ (å› ä¸ºlru_cacheå¯¹asyncæ–¹æ³•çš„é™åˆ¶)
        # if query_or_prompt_hash in manual_cache and (time.time() - manual_cache[query_or_prompt_hash]['timestamp'] < TTL):
        #     return manual_cache[query_or_prompt_hash]['data']

        self.processing_stats['ai_cache_misses'] += 1
        logger.debug(f"AI cache MISS for hash: {query_or_prompt_hash}. Executing AI call with {ai_client_type}.")

        client_to_use = self.claude_client if ai_client_type == "claude" else self.gpt_client

        if not client_to_use:
            logger.error(f"{ai_client_type.capitalize()}Client æœªåˆå§‹åŒ–ã€‚")
            return {"success": False, "error": f"{ai_client_type.capitalize()} æœåŠ¡ä¸å¯ç”¨",
                    "response": f"AIåˆ†ææœåŠ¡ï¼ˆ{ai_client_type}ï¼‰ä¸å¯ç”¨"}

        try:
            # ç»Ÿä¸€è°ƒç”¨AIå®¢æˆ·ç«¯çš„æ–¹æ³•ç­¾å (å‡è®¾éƒ½æœ‰ä¸€ä¸ªé€šç”¨çš„æ–‡æœ¬ç”Ÿæˆ/åˆ†ææ–¹æ³•)
            # ä»¥ä¸‹æ˜¯åŸºäº ClaudeClient çš„ messages API ç¤ºä¾‹
            if isinstance(client_to_use, ClaudeClient) and hasattr(client_to_use, 'messages') and callable(
                    getattr(client_to_use.messages, 'create', None)):
                logger.debug(f"ä½¿ç”¨ ClaudeClient (messages API) è¿›è¡Œåˆ†æã€‚Prompt hash: {query_or_prompt_hash}")
                # Claude SDK çš„ messages.create æ˜¯åŒæ­¥çš„ï¼Œéœ€è¦ç”¨ asyncio.to_thread è¿è¡Œ
                response_raw = await asyncio.to_thread(
                    client_to_use.messages.create,
                    model=getattr(client_to_use, 'model', "claude-3-sonnet-20240229"),  # ä»å®¢æˆ·ç«¯å®ä¾‹è·å–æ¨¡å‹æˆ–ç”¨é»˜è®¤
                    max_tokens=1024,  # æ ¹æ®ä»»åŠ¡è°ƒæ•´
                    messages=[{"role": "user", "content": prompt}]
                )
                content_text = ""
                if hasattr(response_raw, 'content') and response_raw.content:
                    for content_item in response_raw.content:
                        if hasattr(content_item, 'text'): content_text += content_item.text
                return {"success": True, "response": content_text, "model_used": ai_client_type}

            # å¯ä»¥æ·»åŠ å¯¹OpenAIClientæˆ–å…¶ä»–è°ƒç”¨æ–¹å¼çš„é€‚é…é€»è¾‘
            elif isinstance(client_to_use, OpenAIClient) and hasattr(client_to_use, 'chat') and hasattr(
                    client_to_use.chat, 'completions') and callable(
                    getattr(client_to_use.chat.completions, 'create', None)):
                logger.debug(f"ä½¿ç”¨ OpenAIClient (chat.completions API) è¿›è¡Œåˆ†æã€‚Prompt hash: {query_or_prompt_hash}")
                response_raw = await asyncio.to_thread(
                    client_to_use.chat.completions.create,
                    model=getattr(client_to_use, 'model', "gpt-4o"),
                    max_tokens=1024,
                    messages=[{"role": "user", "content": prompt}]
                )
                if response_raw.choices and response_raw.choices[0].message:
                    return {"success": True, "response": response_raw.choices[0].message.content,
                            "model_used": ai_client_type}

            logger.error(f"æ— æ³•æ‰¾åˆ°åˆé€‚çš„AIè°ƒç”¨æ–¹æ³• Ğ´Ğ»Ñ {ai_client_type}Clientã€‚")
            return {"success": False, "error": "AIå®¢æˆ·ç«¯æ–¹æ³•ä¸å…¼å®¹", "response": "AIåˆ†ææœåŠ¡è°ƒç”¨å¤±è´¥"}

        except Exception as e:
            logger.error(f"AIåˆ†æè°ƒç”¨æ—¶å‘ç”Ÿé”™è¯¯ ({ai_client_type}): {str(e)}\n{traceback.format_exc()}")
            return {"success": False, "error": str(e), "response": f"AIåˆ†æå‡ºé”™: {str(e)}"}

    async def _determine_current_data_sub_type(self, user_query: str,
                                               query_analysis: QueryAnalysisResult) -> CurrentDataQueryType:
        """
        æ ¹æ® SmartQueryParser çš„ç»“æœæˆ–é€šè¿‡AIè¿›ä¸€æ­¥ç»†åŒ–å½“å‰æ•°æ®æŸ¥è¯¢çš„å­ç±»å‹ã€‚
        """
        # ä¼˜å…ˆä½¿ç”¨ SmartQueryParser çš„ç»“æœè¿›è¡Œæ˜ å°„
        # QueryParserQueryType å®šä¹‰åœ¨ query_parser.py
        parser_query_type = query_analysis.query_type

        if parser_query_type == QueryParserQueryType.DATA_RETRIEVAL:
            # éœ€è¦æ›´ç»†è‡´çš„é€»è¾‘æ¥ä» query_analysis.data_requirements æˆ– entities ä¸­åˆ¤æ–­
            # ä¾‹å¦‚ï¼Œå¦‚æœå®ä½“æ˜¯â€œä½™é¢â€ï¼Œåˆ™ä¸º BALANCE_CHECK
            # ä¸ºç®€åŒ–ï¼Œè¿™é‡Œç›´æ¥è¿›è¡ŒAIç»†åŒ–æˆ–åŸºäºå…³é”®è¯ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
            pass  # ç»§ç»­ä¸‹é¢çš„AIç»†åŒ–
        elif parser_query_type == QueryParserQueryType.SYSTEM_COMMAND and "æ¦‚è§ˆ" in user_query:  # ç‰¹æ®Šå¤„ç†
            return CurrentDataQueryType.SYSTEM_OVERVIEW
        # å¦‚æœ SmartQueryParser å·²ç»ç»™å‡ºäº†éå¸¸æ˜ç¡®çš„ã€å¯ç›´æ¥æ˜ å°„çš„ç±»å‹ï¼Œå¯ä»¥åœ¨æ­¤å¤„ç†

        # å¦‚æœ SmartQueryParser çš„ç»“æœä¸å¤Ÿç»†åŒ–ï¼Œæˆ–éœ€è¦äºŒæ¬¡ç¡®è®¤ï¼Œå†è°ƒç”¨AI
        logger.debug(
            f"SmartQueryParser type '{parser_query_type.value}' not specific enough or needs confirmation for CurrentDataProcessor. Using AI to refine sub-type.")

        if not self.claude_client:
            logger.warning("ClaudeClientæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç»†åŒ–å½“å‰æ•°æ®æŸ¥è¯¢å­ç±»å‹ã€‚åŸºäºå…³é”®è¯è¿›è¡Œç®€å•åˆ¤æ–­ã€‚")
            # ç®€å•å…³é”®è¯åŒ¹é…ä½œä¸ºåå¤‡
            query_lower = user_query.lower()
            if any(
                kw in query_lower for kw in ["ä½™é¢", "æ€»èµ„é‡‘", "æœ‰å¤šå°‘é’±"]): return CurrentDataQueryType.BALANCE_CHECK
            if any(
                kw in query_lower for kw in ["ç”¨æˆ·æ•°", "ä¼šå‘˜æ•°", "æ´»è·ƒç”¨æˆ·"]): return CurrentDataQueryType.USER_STATUS
            if any(kw in query_lower for kw in ["ä»Šå¤©åˆ°æœŸ", "ä»Šæ—¥åˆ°æœŸ"]): return CurrentDataQueryType.TODAY_EXPIRY
            if any(kw in query_lower for kw in ["å…¥é‡‘", "å‡ºé‡‘", "èµ„é‡‘æµåŠ¨"]): return CurrentDataQueryType.CASH_FLOW
            if any(kw in query_lower for kw in ["äº§å“æƒ…å†µ", "ç†è´¢çŠ¶æ€"]): return CurrentDataQueryType.PRODUCT_STATUS
            if any(kw in query_lower for kw in ["å…³é”®æŒ‡æ ‡", "æ ¸å¿ƒæ•°æ®"]): return CurrentDataQueryType.QUICK_METRICS
            return CurrentDataQueryType.SYSTEM_OVERVIEW  # é»˜è®¤

        # ä¸­æ–‡æç¤ºè¯
        prompt = f"""
        è¯·ä»”ç»†åˆ†æä»¥ä¸‹ç”¨æˆ·é’ˆå¯¹â€œå½“å‰ç³»ç»ŸçŠ¶æ€â€çš„æŸ¥è¯¢ï¼Œå¹¶å°†å…¶ç²¾ç¡®åˆ†ç±»åˆ°é¢„å®šä¹‰çš„å­ç±»å‹ä¸­ã€‚
        ç”¨æˆ·æŸ¥è¯¢: "{user_query}"

        é¢„å®šä¹‰æŸ¥è¯¢å­ç±»å‹åŠå…¶å…¸å‹å…³é”®è¯æˆ–æ„å›¾:
        - "system_overview": è¯¢é—®æ•´ä½“ç³»ç»ŸçŠ¶å†µã€æ¦‚è§ˆã€ä»Šæ—¥æ€»ç»“ã€‚
        - "balance_check": æ˜ç¡®è¯¢é—®è´¦æˆ·ä½™é¢ã€æ€»èµ„é‡‘ã€å¯ç”¨èµ„é‡‘ç­‰ã€‚
        - "user_status": è¯¢é—®å½“å‰ç”¨æˆ·æ•°é‡ã€æ´»è·ƒç”¨æˆ·ã€æ–°å¢ç”¨æˆ·ç­‰ç”¨æˆ·ç»Ÿè®¡ã€‚
        - "today_expiry": æ˜ç¡®è¯¢é—®ä»Šæ—¥åˆ°æœŸçš„äº§å“æ•°é‡æˆ–é‡‘é¢ã€‚
        - "cash_flow": å…³æ³¨ä»Šæ—¥æˆ–å½“å‰çš„å…¥é‡‘ã€å‡ºé‡‘ã€å‡€æµå…¥ç­‰èµ„é‡‘æµåŠ¨æƒ…å†µã€‚
        - "product_status": è¯¢é—®å½“å‰äº§å“æ€»ä½“æƒ…å†µã€åœ¨å”®äº§å“æ•°é‡ç­‰ã€‚
        - "quick_metrics": è¦æ±‚å¿«é€Ÿè·å–ä¸€äº›å…³é”®æ€§èƒ½æŒ‡æ ‡çš„å³æ—¶æ•°æ®ã€‚
        - "unknown_current_query": å¦‚æœæŸ¥è¯¢ä¸å½“å‰æ•°æ®ç›¸å…³ä½†æ— æ³•æ˜ç¡®å½’å…¥ä»¥ä¸Šç±»å‹ã€‚

        è¯·åªè¿”å›æœ€åŒ¹é…çš„å­ç±»å‹IDï¼ˆä¾‹å¦‚ï¼š"balance_check"ï¼‰ã€‚
        å¦‚æœç”¨æˆ·æŸ¥è¯¢æ„å›¾æ¨¡ç³Šæˆ–ä¸å±äºä»¥ä¸Šä»»ä½•ä¸€ç§ï¼Œè¯·è¿”å› "unknown_current_query"ã€‚
        """
        prompt_hash = self._get_query_hash(f"identify_subtype_{user_query}")
        ai_result = await self._cached_ai_analysis(prompt_hash, prompt, ai_client_type="claude")

        if ai_result.get('success'):
            type_id_str = ai_result.get('response', '').strip().lower().replace('"', '').replace("'", "")
            try:
                return CurrentDataQueryType(type_id_str)
            except ValueError:
                logger.warning(f"AIè¿”å›äº†æ— æ³•è¯†åˆ«çš„å½“å‰æ•°æ®æŸ¥è¯¢å­ç±»å‹: '{type_id_str}'ã€‚å°†ä½¿ç”¨ UNKNOWN_CURRENT_QUERYã€‚")
                return CurrentDataQueryType.UNKNOWN_CURRENT_QUERY
        else:
            logger.error(f"AIç»†åŒ–æŸ¥è¯¢å­ç±»å‹å¤±è´¥: {ai_result.get('error')}")
            return CurrentDataQueryType.SYSTEM_OVERVIEW  # å‡ºé”™æ—¶é»˜è®¤

    async def _process_by_query_type(self,
                                     query_sub_type: CurrentDataQueryType,
                                     user_query: str,
                                     current_data: Dict[str, Any],  # è¿™æ˜¯ä» Orchestrator ä¼ å…¥çš„å·²è·å–æ•°æ®
                                     user_context: Optional[Dict[str, Any]] = None
                                     ) -> Dict[str, Any]:  # è¿”å›åŒ…å«æŒ‡æ ‡å’ŒAIå“åº”çš„å­—å…¸
        """
        æ ¹æ®ç»†åŒ–çš„æŸ¥è¯¢å­ç±»å‹ï¼Œä½¿ç”¨AIä»å·²æœ‰çš„`current_data`ä¸­æå–ä¿¡æ¯ã€è®¡ç®—å¹¶æ ¼å¼åŒ–ã€‚
        """
        logger.info(f"æ ¹æ®å­ç±»å‹ '{query_sub_type.value}' å¤„ç†å½“å‰æ•°æ®ã€‚")
        if not self.claude_client and not self.gpt_client:  # è‡³å°‘éœ€è¦ä¸€ä¸ªAI
            logger.error("æ²¡æœ‰å¯ç”¨çš„AIå®¢æˆ·ç«¯æ¥å¤„ç†å½“å‰æ•°æ®ã€‚")
            return {"key_metrics": {"é”™è¯¯": "AIæœåŠ¡ä¸å¯ç”¨"}, "response_format": "simple_text", "confidence": 0.1}

        # é€‰æ‹©ä¸€ä¸ªAIå®¢æˆ·ç«¯ï¼Œä¼˜å…ˆClaudeè¿›è¡Œç†è§£å’Œç»„ç»‡ï¼ŒGPTè¿›è¡Œç²¾ç¡®æå–ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        # å¯¹äºå½“å‰æ•°æ®å¤„ç†ï¼ŒClaudeçš„ç»¼åˆèƒ½åŠ›å¯èƒ½æ›´é€‚åˆç›´æ¥ç”Ÿæˆæ‰€éœ€ä¿¡æ¯
        primary_ai_client = self.claude_client if self.claude_client else self.gpt_client

        # æ„å»ºé’ˆå¯¹ç‰¹å®šå­ç±»å‹çš„ä¸­æ–‡æç¤ºè¯
        # current_data æ˜¯ Orchestrator é€šè¿‡ SmartDataFetcher è·å–çš„ï¼Œä¾‹å¦‚ /api/sta/system çš„å†…å®¹
        # æˆ‘ä»¬éœ€è¦ç¡®ä¿ current_data çš„ç»“æ„æ˜¯å·²çŸ¥çš„ï¼Œæˆ–è€…æç¤ºè¯è¶³å¤Ÿé€šç”¨

        # æå– current_data ä¸­çš„å…³é”®éƒ¨åˆ†ç”¨äºæç¤ºè¯ï¼Œé¿å…è¿‡é•¿
        data_summary_for_prompt = {
            "æ€»ä½™é¢": current_data.get("total_balance", current_data.get("æ€»ä½™é¢")),  # å°è¯•ä¸åŒå¯èƒ½çš„é”®å
            "ä»Šæ—¥å…¥é‡‘": current_data.get("daily_inflow", current_data.get("å…¥é‡‘")),
            "ä»Šæ—¥å‡ºé‡‘": current_data.get("daily_outflow", current_data.get("å‡ºé‡‘")),
            "ä»Šæ—¥æ–°å¢ç”¨æˆ·": current_data.get("new_registrations", current_data.get("æ³¨å†Œäººæ•°")),
            "ä»Šæ—¥æ´»è·ƒç”¨æˆ·": current_data.get("active_users_today", current_data.get("æŒä»“äººæ•°")),  # å‡è®¾ 'æŒä»“äººæ•°' ä»£è¡¨æ´»è·ƒ
            "ä»Šæ—¥åˆ°æœŸé‡‘é¢": current_data.get("today_expiry_details", {}).get("åˆ°æœŸé‡‘é¢",
                                                                             current_data.get("ä»Šæ—¥åˆ°æœŸé‡‘é¢"))
        }
        # ç§»é™¤å€¼ä¸ºNoneæˆ–0çš„é¡¹ä»¥ç®€åŒ–æç¤º
        data_summary_for_prompt = {k: v for k, v in data_summary_for_prompt.items() if v is not None and v != 0}

        prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„â€œå½“å‰ç³»ç»Ÿæ•°æ®å¿«ç…§â€å’Œç”¨æˆ·çš„å…·ä½“æŸ¥è¯¢ï¼Œæå–æˆ–è®¡ç®—å‡ºç”¨æˆ·æ‰€éœ€çš„ä¿¡æ¯ã€‚

        ç”¨æˆ·çš„å…·ä½“æŸ¥è¯¢æ˜¯ï¼šâ€œ{user_query}â€
        ç³»ç»Ÿå·²å°†æ­¤æŸ¥è¯¢ç»†åŒ–åˆ†ç±»ä¸ºï¼šâ€œ{query_sub_type.value}â€

        å½“å‰ç³»ç»Ÿæ•°æ®å¿«ç…§ï¼ˆéƒ¨åˆ†å…³é”®æŒ‡æ ‡ï¼‰:
        ```json
        {json.dumps(data_summary_for_prompt, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}
        ```

        æ ¹æ®ç»†åŒ–çš„æŸ¥è¯¢ç±»å‹ '{query_sub_type.value}'ï¼Œè¯·ä¸“æ³¨äºå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        ä¾‹å¦‚ï¼š
        - å¦‚æœæ˜¯ "balance_check"ï¼Œè¯·ä¸»è¦å…³æ³¨ä½™é¢ç›¸å…³æ•°æ®ã€‚
        - å¦‚æœæ˜¯ "user_status"ï¼Œè¯·ä¸»è¦å…³æ³¨ç”¨æˆ·ç»Ÿè®¡æ•°æ®ã€‚
        - å¦‚æœæ˜¯ "today_expiry"ï¼Œè¯·æ˜ç¡®ä»Šæ—¥åˆ°æœŸé‡‘é¢å’Œæ•°é‡ï¼ˆå¦‚æœæ•°æ®ä¸­æœ‰ï¼‰ã€‚
        - å¦‚æœæ˜¯ "system_overview" æˆ– "quick_metrics"ï¼Œå¯ä»¥æä¾›ä¸€ä¸ªç®€è¦çš„å…³é”®æŒ‡æ ‡æ±‡æ€»ã€‚

        æ‚¨çš„ä»»åŠ¡æ˜¯è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - "key_metrics": ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³çš„1-3ä¸ªæ ¸å¿ƒæŒ‡æ ‡åŠå…¶å€¼ã€‚è¯·ç¡®ä¿æ•°å€¼å‡†ç¡®ã€‚
                         ä¾‹å¦‚: {{"æ€»ä½™é¢": 12345.67, "ä»Šæ—¥å‡€æµå…¥": 1000.00}}
        - "related_metrics": (å¯é€‰) ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„1-2ä¸ªç›¸å…³æŒ‡æ ‡ã€‚
        - "main_answer_text": (å¯é€‰) ä¸€å¥ç®€çŸ­çš„ã€ç›´æ¥å›ç­”ç”¨æˆ·æ ¸å¿ƒé—®é¢˜çš„ä¸­æ–‡æ–‡æœ¬ã€‚å¦‚æœæŒ‡æ ‡æœ¬èº«å°±æ˜¯ç­”æ¡ˆï¼Œåˆ™æ­¤é¡¹å¯çœç•¥ã€‚
        - "response_format_suggestion": å»ºè®®çš„å“åº”æ ¼å¼ï¼Œä» ["simple_text", "detailed_summary", "metrics_focused", "business_oriented"] ä¸­é€‰æ‹©ä¸€ä¸ªã€‚
        - "confidence": æ‚¨å¯¹æœ¬æ¬¡æå–å’Œè®¡ç®—ç»“æœçš„ç½®ä¿¡åº¦ (0.0 åˆ° 1.0 ä¹‹é—´)ã€‚

        è¯·ç¡®ä¿æ‰€æœ‰æ•°å€¼éƒ½ä»æä¾›çš„â€œå½“å‰ç³»ç»Ÿæ•°æ®å¿«ç…§â€ä¸­è·å–æˆ–åŸºäºå…¶è®¡ç®—ã€‚å¦‚æœæ•°æ®ä¸è¶³ä»¥å›ç­”ï¼Œè¯·åœ¨ key_metrics ä¸­æŒ‡æ˜ã€‚
        ä¾‹å¦‚ï¼Œå¦‚æœé—®ä»Šæ—¥åˆ°æœŸäº§å“æ•°é‡ï¼Œä½†æ•°æ®å¿«ç…§ä¸­åªæœ‰åˆ°æœŸé‡‘é¢ï¼Œåˆ™å¯ä»¥è¿”å› {{"ä»Šæ—¥åˆ°æœŸé‡‘é¢": XXX, "ä»Šæ—¥åˆ°æœŸäº§å“æ•°é‡": "æ•°æ®æœªæä¾›"}}ã€‚
        """
        prompt_hash = self._get_query_hash(
            f"process_subtype_{query_sub_type.value}_{user_query}_{json.dumps(data_summary_for_prompt, sort_keys=True)}")
        ai_result = await self._cached_ai_analysis(prompt_hash, prompt, ai_client_type="claude")  # ä¼˜å…ˆ Claude

        processed_result_dict = {}
        if ai_result.get('success'):
            try:
                # AIåº”è¯¥è¿”å›JSONå­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬åœ¨æ­¤è§£æ
                response_content = ai_result.get('response', '{}')
                processed_result_dict = json.loads(response_content)
                if not isinstance(processed_result_dict.get('key_metrics'), dict):  # ç¡®ä¿æ ¼å¼æ­£ç¡®
                    processed_result_dict['key_metrics'] = {"è§£æé”™è¯¯": "AIæœªæŒ‰é¢„æœŸè¿”å›key_metrics"}
                logger.debug(
                    f"AI successfully processed current data for subtype {query_sub_type.value}. Metrics: {list(processed_result_dict.get('key_metrics', {}).keys())}")
            except json.JSONDecodeError:
                logger.error(
                    f"æ— æ³•è§£æAIä¸ºå­ç±»å‹ {query_sub_type.value} è¿”å›çš„JSON: {ai_result.get('response', '')[:200]}")
                processed_result_dict['key_metrics'] = {"é”™è¯¯": "AIå“åº”æ ¼å¼ä¸æ­£ç¡®"}
                processed_result_dict['confidence'] = 0.3
        else:
            logger.error(f"AIå¤„ç†å½“å‰æ•°æ®å­ç±»å‹ {query_sub_type.value} å¤±è´¥: {ai_result.get('error')}")
            processed_result_dict['key_metrics'] = {"é”™è¯¯": f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {ai_result.get('error')}"}
            processed_result_dict['confidence'] = 0.2

        # ç¡®ä¿é»˜è®¤å€¼
        processed_result_dict.setdefault('key_metrics', {})
        processed_result_dict.setdefault('related_metrics', {})
        processed_result_dict.setdefault('response_format', ResponseFormat.SIMPLE_TEXT.value)  # ä½¿ç”¨æšä¸¾å€¼
        processed_result_dict.setdefault('confidence', 0.5)  # å¦‚æœAIæœªæä¾›ï¼Œç»™ä¸ªé»˜è®¤

        return processed_result_dict

    async def _generate_quick_insights(self, query_sub_type: CurrentDataQueryType,
                                       key_metrics: Dict[str, Any],
                                       current_data: Dict[str, Any]) -> List[str]:  # è¿”å›å­—ç¬¦ä¸²æ´å¯Ÿåˆ—è¡¨
        """
        åŸºäºå…³é”®æŒ‡æ ‡å’Œå½“å‰æ•°æ®ï¼Œä½¿ç”¨AIç”Ÿæˆ1-2æ¡ç®€æ´çš„ä¸­æ–‡ä¸šåŠ¡æ´å¯Ÿã€‚
        """
        if not self.claude_client or not self.config.get('enable_insights_for_current_data', True):  # å‡è®¾é…ç½®é¡¹æ§åˆ¶
            logger.debug("ClaudeClientä¸å¯ç”¨æˆ–å½“å‰æ•°æ®æ´å¯Ÿå·²ç¦ç”¨ï¼Œè·³è¿‡å¿«é€Ÿæ´å¯Ÿç”Ÿæˆã€‚")
            return []

        if not key_metrics:  # å¦‚æœæ²¡æœ‰å…³é”®æŒ‡æ ‡ï¼Œåˆ™ä¸ç”Ÿæˆæ´å¯Ÿ
            return []

        # æ„å»ºæç¤ºè¯
        prompt = f"""
        ä»¥ä¸‹æ˜¯é’ˆå¯¹ç”¨æˆ·å…³äºâ€œ{query_sub_type.value}â€ç±»å‹æŸ¥è¯¢æ‰€æå–çš„å…³é”®æŒ‡æ ‡å’Œç›¸å…³çš„å½“å‰ç³»ç»Ÿæ•°æ®å¿«ç…§ã€‚

        å…³é”®æŒ‡æ ‡:
        ```json
        {json.dumps(key_metrics, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}
        ```

        éƒ¨åˆ†å½“å‰ç³»ç»Ÿæ•°æ®å¿«ç…§:
        ```json
        {json.dumps({k: current_data.get(k) for k in list(current_data.keys())[:5]}, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)} 
        ```

        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”¨ä¸­æ–‡ç”Ÿæˆ1åˆ°2æ¡æœ€ç›¸å…³çš„ã€ç®€æ´çš„ä¸šåŠ¡æ´å¯Ÿæˆ–è§‚å¯Ÿç‚¹ã€‚
        æ¯æ¡æ´å¯Ÿåº”ç›´æ¥ã€ç²¾ç‚¼ï¼Œä¸è¶…è¿‡60ä¸ªæ±‰å­—ã€‚
        å¦‚æœæ•°æ®æ­£å¸¸æˆ–æ— ç‰¹åˆ«ä¹‹å¤„ï¼Œå¯ä»¥æŒ‡å‡ºâ€œæŒ‡æ ‡åœ¨é¢„æœŸèŒƒå›´å†…â€æˆ–â€œæš‚æ— æ˜æ˜¾å¼‚å¸¸çš„å¿«é€Ÿæ´å¯Ÿâ€ã€‚
        è¯·ä»¥JSONå­—ç¬¦ä¸²æ•°ç»„çš„å½¢å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š["æ´å¯Ÿç‚¹ä¸€ï¼šXXXXã€‚", "æ´å¯Ÿç‚¹äºŒï¼šYYYYã€‚"]
        """
        prompt_hash = self._get_query_hash(
            f"quick_insights_{query_sub_type.value}_{json.dumps(key_metrics, sort_keys=True)}")
        ai_result = await self._cached_ai_analysis(prompt_hash, prompt, ai_client_type="claude")

        insights: List[str] = []
        if ai_result.get('success'):
            try:
                response_content = ai_result.get('response', '[]')
                parsed_insights = json.loads(response_content)
                if isinstance(parsed_insights, list) and all(isinstance(item, str) for item in parsed_insights):
                    insights = parsed_insights[:2]  # æœ€å¤šå–ä¸¤æ¡
                else:
                    logger.warning(f"AIå¿«é€Ÿæ´å¯Ÿè¿”å›çš„ä¸æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨: {response_content[:200]}")
                    if isinstance(response_content, str) and response_content.strip():  # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªå­—ç¬¦ä¸²æ´å¯Ÿ
                        insights = [response_content.strip()]

            except json.JSONDecodeError:
                logger.error(f"æ— æ³•è§£æAIå¿«é€Ÿæ´å¯Ÿçš„JSON: {ai_result.get('response', '')[:200]}")
                if isinstance(ai_result.get('response'), str) and ai_result.get('response').strip():
                    insights = [ai_result.get('response').strip()]  # å°è¯•ç›´æ¥ä½¿ç”¨æ–‡æœ¬
        else:
            logger.error(f"AIç”Ÿæˆå¿«é€Ÿæ´å¯Ÿå¤±è´¥: {ai_result.get('error')}")

        logger.debug(f"Generated {len(insights)} quick insights.")
        return insights

    async def _format_current_data_response(self, query_sub_type: CurrentDataQueryType,
                                            user_query: str,
                                            processed_result_dict: Dict[str, Any],  # æ¥è‡ª _process_by_query_type çš„ç»“æœ
                                            quick_insights: List[str]) -> Dict[
        str, Any]:  # åŒ…å« main_answer, formatted_data, business_context
        """
        ä½¿ç”¨AIï¼ˆä¼˜å…ˆGPTè¿›è¡Œæ ¼å¼åŒ–ï¼‰å°†å¤„ç†ç»“æœå’Œæ´å¯Ÿæ•´åˆæˆç”¨æˆ·å‹å¥½çš„å“åº”ã€‚
        """
        logger.debug(f"Formatting response for query subtype: {query_sub_type.value}")
        # ä¼˜å…ˆä½¿ç”¨GPTè¿›è¡Œç»“æ„åŒ–è¾“å‡ºå’Œæ ¼å¼åŒ–ï¼Œå¦‚æœGPTä¸å¯ç”¨ï¼Œåˆ™å°è¯•Claudeæˆ–åŸºç¡€æ¨¡æ¿
        formatting_ai_client = self.gpt_client if self.gpt_client else self.claude_client

        if not formatting_ai_client:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„AIå®¢æˆ·ç«¯è¿›è¡Œå“åº”æ ¼å¼åŒ–ï¼Œä½¿ç”¨åŸºç¡€æ ¼å¼åŒ–ã€‚")
            # æ„å»ºä¸€ä¸ªéå¸¸ç®€å•çš„ main_answer
            simple_main_answer = f"å…³äºæ‚¨çš„æŸ¥è¯¢â€œ{user_query[:30]}...â€ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³æ•°æ®ï¼š"
            metrics_str_parts = [f"{k}: {v}" for k, v in processed_result_dict.get('key_metrics', {}).items()]
            if metrics_str_parts:
                simple_main_answer += "\nå…³é”®æŒ‡æ ‡:\n" + "\n".join(metrics_str_parts)
            if quick_insights:
                simple_main_answer += "\nåˆæ­¥è§‚å¯Ÿ:\n" + "\n".join(quick_insights)

            return {
                "main_answer": simple_main_answer,
                "formatted_data": processed_result_dict.get('key_metrics', {}),  # ç›´æ¥ç”¨ key_metrics ä½œä¸ºæ ¼å¼åŒ–æ•°æ®
                "business_context": "è¿™æ˜¯æ ¹æ®å½“å‰ç³»ç»Ÿæ•°æ®è¿›è¡Œçš„å¿«é€ŸæŸ¥è¯¢ã€‚"
            }

        # ä» processed_result_dict ä¸­æå–AIå»ºè®®çš„å“åº”æ ¼å¼
        response_format_enum_val = processed_result_dict.get('response_format', ResponseFormat.SIMPLE_TEXT.value)
        try:
            response_format_for_prompt = ResponseFormat(response_format_enum_val).name  # è·å–æšä¸¾çš„åç§°ï¼Œå¦‚ 'SIMPLE_TEXT'
        except ValueError:
            response_format_for_prompt = ResponseFormat.SIMPLE_TEXT.name  # é»˜è®¤

        # ä¸­æ–‡æç¤ºè¯
        prompt = f"""
        è¯·å°†ä»¥ä¸‹åˆ†æç»“æœå’Œæ´å¯Ÿæ•´åˆæˆä¸€æ®µé€šé¡ºæµç•…ã€ç”¨æˆ·å‹å¥½çš„ä¸­æ–‡å›å¤ã€‚

        ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢ï¼šâ€œ{user_query}â€
        ç³»ç»Ÿè¯†åˆ«çš„æŸ¥è¯¢å­ç±»å‹ï¼šâ€œ{query_sub_type.value}â€
        å»ºè®®çš„å“åº”æ ¼å¼åå¥½ï¼šâ€œ{response_format_for_prompt}â€

        æ ¸å¿ƒæ•°æ®æŒ‡æ ‡ï¼š
        ```json
        {json.dumps(processed_result_dict.get('key_metrics', {}), ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}
        ```

        ç›¸å…³çš„å…¶ä»–æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰ï¼š
        ```json
        {json.dumps(processed_result_dict.get('related_metrics', {}), ensure_ascii=False, indent=2, cls=CustomJSONEncoder)}
        ```

        ç³»ç»Ÿç”Ÿæˆçš„åˆæ­¥æ´å¯Ÿ/è§‚å¯Ÿç‚¹ï¼š
        {chr(10).join([f"- {qi}" for qi in quick_insights]) if quick_insights else "æ— ç‰¹åˆ«çš„åˆæ­¥è§‚å¯Ÿã€‚"}

        è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå­—æ®µï¼š
        1.  `"main_answer"`: (å­—ç¬¦ä¸²) å¯¹ç”¨æˆ·æŸ¥è¯¢çš„ç›´æ¥ã€æ ¸å¿ƒçš„å›ç­”ã€‚åº”ç®€æ´æ˜äº†ã€‚å¦‚æœæŒ‡æ ‡æœ¬èº«å°±æ˜¯ç­”æ¡ˆï¼Œå¯ä»¥ç›´æ¥é™ˆè¿°ã€‚
        2.  `"formatted_data"`: (å­—å…¸) å°†æœ€é‡è¦çš„1-3ä¸ªå…³é”®æŒ‡æ ‡åŠå…¶å€¼ï¼Œä»¥é”®å€¼å¯¹å½¢å¼å‘ˆç°ï¼Œå€¼åº”ä¸ºå·²æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼Œè´§å¸å¸¦å•ä½ï¼Œç™¾åˆ†æ¯”å¸¦%ï¼‰ã€‚
                           ä¾‹å¦‚ï¼š{{"ä»Šæ—¥æ€»ä½™é¢": "Â¥12,345,678.90", "è¾ƒæ˜¨æ—¥å˜åŠ¨ç‡": "+1.25%"}}
        3.  `"business_context"`: (å­—ç¬¦ä¸², å¯é€‰) å¯¹è¿™äº›æ•°æ®æˆ–å›ç­”æä¾›1-2å¥ç®€çŸ­çš„ä¸šåŠ¡èƒŒæ™¯è¯´æ˜æˆ–è§£è¯»ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ã€‚

        è¯·ä¸¥æ ¼éµå¾ªå»ºè®®çš„å“åº”æ ¼å¼åå¥½ '{response_format_for_prompt}' æ¥ç»„ç»‡æ‚¨çš„å›ç­”ï¼š
        - 'SIMPLE_TEXT': `main_answer` ç®€çŸ­ç›´æ¥ï¼Œ`formatted_data` åªåŒ…å«æœ€æ ¸å¿ƒçš„1ä¸ªæŒ‡æ ‡ã€‚`business_context` å¯çœç•¥ã€‚
        - 'DETAILED_SUMMARY': `main_answer` å¯ä»¥ç¨é•¿ï¼ŒåŒ…å«æ›´å¤šç»†èŠ‚ã€‚`formatted_data` å¯åŒ…å«2-3ä¸ªæ ¸å¿ƒæŒ‡æ ‡ã€‚`business_context` åº”æä¾›ã€‚
        - 'METRICS_FOCUSED': `main_answer` ç®€æ´ï¼Œé‡ç‚¹çªå‡º `formatted_data` ä¸­çš„å¤šä¸ªæŒ‡æ ‡ã€‚`business_context` ç®€è¦ã€‚
        - 'BUSINESS_ORIENTED': `main_answer` å’Œ `business_context` å¼ºè°ƒä¸šåŠ¡å«ä¹‰å’Œæ´å¯Ÿçš„è§£è¯»ã€‚

        å¦‚æœæ•°æ®ä¸è¶³æˆ–å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œè¯·åœ¨ `main_answer` æˆ– `business_context` ä¸­å©‰è½¬è¡¨è¾¾ã€‚
        ç¡®ä¿æ‰€æœ‰æ•°å€¼éƒ½å·²æ°å½“æ ¼å¼åŒ–ï¼ˆä¾‹å¦‚ï¼Œè´§å¸ä½¿ç”¨å…ƒï¼Œå¤§æ•°å­—ä½¿ç”¨åƒåˆ†ä½åˆ†éš”ç¬¦ï¼Œç™¾åˆ†æ¯”å¸¦%ï¼‰ã€‚
        """
        prompt_hash = self._get_query_hash(
            f"format_resp_{query_sub_type.value}_{json.dumps(processed_result_dict.get('key_metrics', {}), sort_keys=True)}_{response_format_for_prompt}")
        ai_result = await self._cached_ai_analysis(prompt_hash, prompt,
                                                   ai_client_type="gpt" if self.gpt_client else "claude")  # ä¼˜å…ˆGPTè¿›è¡Œæ ¼å¼åŒ–

        formatted_response_dict: Dict[str, Any] = {}
        if ai_result.get('success'):
            try:
                response_content = ai_result.get('response', '{}')
                formatted_response_dict = json.loads(response_content)
                if not isinstance(formatted_response_dict.get('main_answer'), str) or \
                        not isinstance(formatted_response_dict.get('formatted_data'), dict):
                    raise ValueError("AIè¿”å›çš„æ ¼å¼åŒ–å“åº”ç»“æ„ä¸ç¬¦åˆé¢„æœŸã€‚")
                logger.debug(
                    f"AI successfully formatted response. Main answer: {formatted_response_dict.get('main_answer', '')[:50]}...")
            except json.JSONDecodeError:
                logger.error(f"æ— æ³•è§£æAIæ ¼å¼åŒ–å“åº”çš„JSON: {ai_result.get('response', '')[:200]}")
                formatted_response_dict['main_answer'] = "æŠ±æ­‰ï¼Œç³»ç»Ÿåœ¨ç»„ç»‡å›å¤æ—¶é‡åˆ°é—®é¢˜ã€‚" + \
                                                         (" è¯¦ç»†æ•°æ®è¯·å‚è€ƒæŒ‡æ ‡éƒ¨åˆ†ã€‚" if processed_result_dict.get(
                                                             'key_metrics') else "")
            except ValueError as ve:
                logger.error(f"AIæ ¼å¼åŒ–å“åº”ç»“æ„é”™è¯¯: {ve}")
                formatted_response_dict['main_answer'] = "AIæœªèƒ½æŒ‰é¢„æœŸç»“æ„ç»„ç»‡å›å¤ã€‚"
        else:
            logger.error(f"AIæ ¼å¼åŒ–å“åº”å¤±è´¥: {ai_result.get('error')}")
            formatted_response_dict['main_answer'] = "AIæœåŠ¡åœ¨æ ¼å¼åŒ–æœ€ç»ˆç­”å¤æ—¶å‡ºç°æ•…éšœã€‚"

        # ç¡®ä¿é»˜è®¤å€¼
        formatted_response_dict.setdefault('main_answer', "å·²è·å–ç›¸å…³æ•°æ®ï¼Œè¯·æŸ¥çœ‹æŒ‡æ ‡è¯¦æƒ…ã€‚")
        formatted_response_dict.setdefault('formatted_data',
                                           processed_result_dict.get('key_metrics', {}))  # é»˜è®¤ç”¨åŸå§‹key_metrics
        formatted_response_dict.setdefault('business_context', "è¿™æ˜¯åŸºäºå½“å‰æœ€æ–°æ•°æ®çš„å¿«é€ŸæŸ¥è¯¢ç»“æœã€‚")

        return formatted_response_dict

    async def process_current_data_query(
            self,
            user_query: str,
            query_analysis: QueryAnalysisResult,  # ä»ç¼–æ’å™¨æ¥æ”¶
            current_data: Dict[str, Any],  # ä»ç¼–æ’å™¨æ¥æ”¶ (å·²è·å–çš„ /api/sta/system æ•°æ®)
            user_context: Optional[Dict[str, Any]] = None
    ) -> CurrentDataResponse:
        """
        å¤„ç†å…³äºå½“å‰ç³»ç»ŸçŠ¶æ€çš„æŸ¥è¯¢ã€‚
        æ•°æ®å·²ç”±ç¼–æ’å™¨è·å–å¹¶ä¼ å…¥ã€‚
        """
        method_start_time = datetime.now()
        logger.info(f"CurrentDataProcessor: Processing query '{user_query[:50]}...' with pre-fetched current_data.")

        # æ›´æ–°å†…éƒ¨ç»Ÿè®¡ (total_queries_processed)
        self.processing_stats['total_queries_processed'] = self.processing_stats.get('total_queries_processed', 0) + 1

        try:
            if not current_data:  # æ£€æŸ¥ä¼ å…¥çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                logger.warning("CurrentDataProcessor: Received empty or None current_data. Cannot proceed effectively.")
                return self._create_error_response(user_query, "æ— æ³•è·å–å¿…è¦çš„å½“å‰ç³»ç»Ÿæ•°æ®æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚")

            # 1. è¿›ä¸€æ­¥ç»†åŒ–æŸ¥è¯¢å­ç±»å‹ (åŸºäº user_query å’Œ query_analysis)
            #    _determine_current_data_sub_type å†…éƒ¨ä½¿ç”¨AIï¼Œå¹¶åŸºäº user_query
            #    å®ƒå¯ä»¥é€‰æ‹©æ€§åœ°ä½¿ç”¨ query_analysis æ¥è¾…åŠ©åˆ¤æ–­
            current_query_sub_type: CurrentDataQueryType = await self._determine_current_data_sub_type(user_query,
                                                                                                       query_analysis)
            logger.info(f"Determined current data query sub-type: {current_query_sub_type.value}")

            # 2. æ ¹æ®ç»†åŒ–çš„å­ç±»å‹ï¼Œä»ä¼ å…¥çš„ current_data ä¸­æå–/è®¡ç®—æŒ‡æ ‡
            #    _process_by_query_type ç°åœ¨ä½¿ç”¨ä¼ å…¥çš„ current_data
            processed_result_dict: Dict[str, Any] = await self._process_by_query_type(
                query_sub_type=current_query_sub_type,
                user_query=user_query,
                current_data=current_data,  # ä½¿ç”¨ä¼ å…¥çš„æ•°æ®
                user_context=user_context
            )
            logger.info(
                f"Data processed by sub-type. Key metrics found: {list(processed_result_dict.get('key_metrics', {}).keys())}")

            # 3. ç”Ÿæˆå¿«é€Ÿæ´å¯Ÿ (åŸºäºå·²æå–çš„æŒ‡æ ‡å’Œä¼ å…¥çš„ current_data)
            quick_insights: List[str] = await self._generate_quick_insights(
                query_sub_type=current_query_sub_type,
                key_metrics=processed_result_dict.get('key_metrics', {}),
                current_data=current_data  # ä½¿ç”¨ä¼ å…¥çš„æ•°æ®
            )
            logger.info(f"Generated {len(quick_insights)} quick insights.")

            # 4. æ ¼å¼åŒ–æœ€ç»ˆå“åº” (åŸºäºå¤„ç†ç»“æœå’Œæ´å¯Ÿ)
            formatted_response_dict: Dict[str, Any] = await self._format_current_data_response(
                query_sub_type=current_query_sub_type,
                user_query=user_query,
                processed_result_dict=processed_result_dict,
                quick_insights=quick_insights
            )
            logger.info("Response formatted.")

            # 5. æ„å»ºå¹¶è¿”å› CurrentDataResponse å¯¹è±¡
            processing_time_seconds = (datetime.now() - method_start_time).total_seconds()

            # ä»AIå¤„ç†ç»“æœä¸­è·å–å»ºè®®çš„å“åº”æ ¼å¼ï¼Œæˆ–ä½¿ç”¨é»˜è®¤
            response_format_str = processed_result_dict.get('response_format', ResponseFormat.SIMPLE_TEXT.value)
            try:
                final_response_format = ResponseFormat(response_format_str)
            except ValueError:
                logger.warning(
                    f"Invalid response_format string '{response_format_str}' from AI. Defaulting to SIMPLE_TEXT.")
                final_response_format = ResponseFormat.SIMPLE_TEXT

            response = CurrentDataResponse(
                query_type=current_query_sub_type,
                response_format=final_response_format,
                main_answer=formatted_response_dict.get('main_answer', "æœªèƒ½ç”Ÿæˆä¸»è¦å›ç­”ã€‚"),
                key_metrics=processed_result_dict.get('key_metrics', {}),
                formatted_data=formatted_response_dict.get('formatted_data', {}),  # ç¡®ä¿è¿™æ˜¯ Dict[str, str]
                business_context=formatted_response_dict.get('business_context', ""),
                quick_insights=quick_insights,
                related_metrics=processed_result_dict.get('related_metrics', {}),
                data_timestamp=current_data.get('timestamp', datetime.now().isoformat()),  # ä½¿ç”¨ä¼ å…¥æ•°æ®çš„timestamp
                response_confidence=float(processed_result_dict.get('confidence', 0.75)),  # AIç»™å‡ºçš„ç½®ä¿¡åº¦
                data_sources_references=current_data.get('api_source_names', ["pre_fetched_system_data"]),  # æŒ‡æ˜æ•°æ®æ¥æº
                processing_time=processing_time_seconds,
                metadata={  # å¯ä»¥åŒ…å«æ­¤å¤„ç†å™¨å†…éƒ¨çš„AIè€—æ—¶ç­‰
                    'ai_processing_time': processed_result_dict.get('metadata', {}).get('ai_processing_time', 0.0)
                    # å‡è®¾_process_by_query_typeçš„metaåŒ…å«
                }
            )

            self._update_processing_stats(current_query_sub_type, processing_time_seconds,
                                          response.response_confidence)  # å‡è®¾æ­¤æ–¹æ³•å·²å®ç°
            logger.info(f"âœ… CurrentDataProcessor processed query successfully in {processing_time_seconds:.3f}s.")
            return response

        except Exception as e:
            logger.error(
                f"âŒ Error in CurrentDataProcessor.process_current_data_query for query '{user_query[:50]}...': {str(e)}\n{traceback.format_exc()}")
            # è°ƒç”¨å†…éƒ¨çš„é”™è¯¯å“åº”æ„å»ºæ–¹æ³•
            return self._create_error_response(user_query, f"å¤„ç†å½“å‰æ•°æ®æŸ¥è¯¢æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}")
    # ç§»é™¤äº† process_multiple_queries å’Œ _process_query_with_dataï¼Œ
    # å› ä¸ºæ‰¹é‡å¤„ç†çš„å¤æ‚æ€§æœ€å¥½æ”¾åœ¨ Orchestrator å±‚é¢æˆ–ä¸“é—¨çš„ BatchProcessor ä¸­ã€‚
    # CurrentDataProcessor åº”ä¸“æ³¨äºå•ä¸ªæŸ¥è¯¢çš„å¤„ç†ã€‚

    def _update_processing_stats(self, query_type: CurrentDataQueryType,
                                 processing_time: float, confidence: float) -> None:
        """æ›´æ–°æœ¬å¤„ç†å™¨çš„å†…éƒ¨ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        try:
            type_value = query_type.value
            self.processing_stats['queries_by_sub_type'][type_value] = \
                self.processing_stats['queries_by_sub_type'].get(type_value, 0) + 1

            total_processed = self.processing_stats['total_queries_processed']  # åœ¨ process_current_data_query ä¸­å¢åŠ 
            if total_processed == 0: return  # é¿å…é™¤é›¶

            current_avg_time = self.processing_stats['avg_response_time']
            self.processing_stats['avg_response_time'] = \
                (current_avg_time * (total_processed - 1) + processing_time) / total_processed

            current_avg_confidence = self.processing_stats['avg_confidence']
            self.processing_stats['avg_confidence'] = \
                (current_avg_confidence * (total_processed - 1) + confidence) / total_processed

            total_ai_cache_lookups = self.processing_stats['ai_cache_hits'] + self.processing_stats['ai_cache_misses']
            if total_ai_cache_lookups > 0:
                self.processing_stats['ai_cache_hit_rate'] = self.processing_stats[
                                                                 'ai_cache_hits'] / total_ai_cache_lookups

        except Exception as e:
            logger.error(f"æ›´æ–°CurrentDataProcessorç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

    def _create_error_response(self, user_query: str, error_message: str) -> CurrentDataResponse:
        """åˆ›å»ºä¸€ä¸ªæ ‡å‡†çš„é”™è¯¯å“åº”å¯¹è±¡ã€‚"""
        logger.error(f"Creating error response for query '{user_query[:50]}...': {error_message}")
        return CurrentDataResponse(
            query_type=CurrentDataQueryType.UNKNOWN_CURRENT_QUERY,  # æˆ–ä¸€ä¸ªç‰¹å®šçš„ERRORç±»å‹
            response_format=ResponseFormat.SIMPLE_TEXT,
            main_answer=f"å¤„ç†æ‚¨çš„å³æ—¶æŸ¥è¯¢â€œ{user_query[:30]}...â€æ—¶é‡åˆ°é”™è¯¯: {error_message}",
            key_metrics={"é”™è¯¯è¯¦æƒ…": error_message},
            formatted_data={"é”™è¯¯": error_message},
            business_context="ç³»ç»Ÿæ— æ³•å®Œæˆæ‚¨çš„å½“å‰æ•°æ®è¯·æ±‚ã€‚",
            quick_insights=["å»ºè®®æ£€æŸ¥æŸ¥è¯¢æˆ–ç¨åé‡è¯•ã€‚"],
            related_metrics={},
            data_timestamp=datetime.now().isoformat(),
            response_confidence=0.0,  # é”™è¯¯å‘ç”Ÿï¼Œç½®ä¿¡åº¦ä¸º0
            data_sources_references=["N/A"],
            processing_time=0.01,  # è±¡å¾æ€§çš„å¤„ç†æ—¶é—´
            metadata={"error_flag": True, "error_message": error_message}
        )

    def _load_processing_config(self) -> Dict[str, Any]:
        """åŠ è½½å½“å‰æ•°æ®å¤„ç†å™¨çš„ç‰¹å®šé…ç½®ã€‚"""
        return {
            "max_response_time_ms": 3000,  # æœ¬å¤„ç†å™¨è‡ªèº«çš„ç›®æ ‡å“åº”æ—¶é—´
            "data_freshness_threshold_sec": 120,  # å¯¹ä¼ å…¥æ•°æ®çš„â€œæ–°é²œåº¦â€è¦æ±‚
            "enable_insights_for_current_data": True,  # æ˜¯å¦ä¸ºæ­¤å¤„ç†å™¨ç”Ÿæˆå¿«é€Ÿæ´å¯Ÿ
            "max_quick_insights_count": 2,
            "ai_cache_ttl_sec": 600  # AIåˆ†æç»“æœçš„ç¼“å­˜æ—¶é—´ (10åˆ†é’Ÿ)
        }

    # _load_current_data_patterns å¯èƒ½ä¸å†éœ€è¦ï¼Œå› ä¸ºç±»å‹è¯†åˆ«æ›´å¤šä¾èµ–AIæˆ–ä¼ å…¥çš„QueryAnalysisResult
    # def _load_current_data_patterns(self) -> Dict[str, List[str]]: ...

    # å¤–éƒ¨æ¥å£ï¼Œç”¨äºè·å–æœ¬å¤„ç†å™¨çš„ç»Ÿè®¡ä¿¡æ¯
    def get_processor_stats(self) -> Dict[str, Any]:
        return self.processing_stats.copy()

    async def health_check(self) -> Dict[str, Any]:
        """æ‰§è¡Œæœ¬å¤„ç†å™¨çš„å¥åº·æ£€æŸ¥ã€‚"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        claude_ok = self.claude_client is not None  # and await self.claude_client.is_healthy() # å‡è®¾AIå®¢æˆ·ç«¯æœ‰å¥åº·æ£€æŸ¥
        gpt_ok = self.gpt_client is not None  # and await self.gpt_client.is_healthy()
        status = "healthy"
        issues = []
        if not claude_ok: issues.append("ClaudeClient not available/healthy.")
        if not gpt_ok: issues.append("GPTClient not available/healthy.")

        if issues:
            status = "degraded" if (claude_ok or gpt_ok) else "unhealthy"

        return {
            "status": status,
            "component_name": self.__class__.__name__,
            "dependencies_status": {
                "claude_client": "ok" if claude_ok else "error",
                "gpt_client": "ok" if gpt_ok else "error",
            },
            "internal_stats": self.get_processor_stats(),
            "issues": issues,
            "timestamp": datetime.now().isoformat()
        }